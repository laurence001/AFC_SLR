Code,Title,Abstract,Field,Type,Year,Journalism,Use,Citations,Text,APA
ML_DB_004,FactRank: Developing automated claim detection for Dutchlanguage factchecker,"Fact-checking has always been a central task of journalism, but given the ever-growing amount and speed of news offline and online, as well as the growing amounts of misinformation and disinformation, it is becoming increasingly important to support human fact-checkers with (semi-)automated methods to make their work more efficient. Within fact-checking, the detection of check-worthy claims is a crucial initial step, since it limits the number of claims that require or deserve to be checked for their truthfulness. In this paper, we present FactRank, a novel claim detection tool for journalists specifically created for the Dutch language. To the best of our knowledge, this is the first and still the only such tool for Dutch. FactRank thus complements existing online claim detection tools for English and (a small number of) other languages. FactRank performs similarly to claim detection in ClaimBuster, the state-of-the-art fact-checking tool for English. Our comparisons with a human baseline also indicate that given how much even expert human fact-checkers disagree, there may be a natural upper bound” on the accuracy of check-worthiness detection by machine-learning methods. The specific quality of FactRank derives from the interdisciplinary and iterative process in which it was created, which includes not only a high-performance deep-learning neural network architecture, but also a principled approach to defining and operationalising the concept of check-worthiness via a detailed codebook. This codebook was created jointly by expert fact-checkers from the two countries that have Dutch as an official language (Belgium/Flanders and the Netherlands). We expect FactRank to be very useful exactly because of the way we defined check-worthiness, and because of how we have made this explicit and traceable.",Computer Science,Article,2021,Y,Y,3,"FactRank concentrates on the task of claim detection: sifting through large volumes of texts to find statements that are check-worthy: not only factual (and thus amenable, in principle, to a fact-check), but relevant to a broad public (and thus worth the effort of a fact-check). The FactRank website also performs the task of gathering potential claims, by continuously monitoring relevant sources. Monitoring and claim detection can be thought of as part of a pipeline of further tasks, in particular determining the veracity of statements (part of which may be matching statements to already fact-checked content) and building up a knowledge base. As highlighted above, claim detection is regarded by our journalistic experts as the most helpful candidate for automation in their work. FactRank’s classification algorithm was developed in a novel itera- tive process that rests on expert fact-checker input, a codebook to sup- port reliable human labelling, and an active-learning approach to combining machine learning and human expertise. Initial tests of an upvote/downvote functionality that is novel in the domain of fact- checking show promising results. Experiments on a dataset of 7037 human-labelled sentences and one involving an additional 1270 human- upvoted sentences show a classification accuracy of up to 74.6%, which is similar to state-of-the-art results for English as well as close to a human-annotators baseline of 75.5% that illustrates the inherent am- bivalences of the task and possible upper bounds for machine claim detection. 2. Related work Automated fact-checking projects generally focus on distinct parts of the fact-checking process, using a variety of approaches, such as NLP and machine learning. Several studies survey progress in the field for a wider audience, comprising journalists and fact-checkers [3,9,27]. Taken together, these present a number of promising, mostly small-scale projects, with few exceptions based on English-language materials. Although considerable progress has been achieved in the past decade, reliable end-to-end systems work, if at all, only for a very limited number of input categories. Automated verification is available for claims that have been fact-checked before, or for relatively simple statements, e.g. the current height of the national deficit, or the name of the president of Brazil. An area that has seen more success than others, according to Graves [[9], p. 3], is the first stage of the fact-checking process, claim detection. This entails source monitoring and identifying statements that are both factual and ‘check-worthy’, i.e. relevant input for fact-checkers (or systems) tasked with verification. An obvious addition to identifying check-worthy statements is ranking these according to their relevance. Babakar and Moy [[3], p. 14] break up claim spotting into four distinct tasks: 1 Monitoring claims that have been fact-checked before in new text. 2 Identifying new factual claims that have not been fact-checked before in new text. 3 Making editorial judgments about the priority of different claims. 4 Dealing with different phrasing for the same or similar claims. The present project, FactRank, deals with items 2 and 3: identifying and ranking new claims. Babakar and Moy [[3], pp. 27–31] list a number of projects in this category, most of which are not relevant for the present study since they cover a different range of sources (e.g., Vlachos’ Simple Numerical Fact Checker, meant to spot and check claims such as ‘Lesotho has a population of nearly 2 million’; [3], p. 28). The four projects that are closest to FactRank regarding scope and approach are ClaimBuster from the University of Texas ([14] and [15]; https://idir.uta.edu/claimbuster/), ContentCheck (a collaboration of academics and Le Monde, http://contentcheck.inria.fr/), Full Fact’s claim spotting module ([19]; https://fullfact.org/automated), and Chequeado’s Chequeabot ([9], p. 5). ClaimBuster aims to support all stages of the process (end-to-end fact-checking”), in which claim detection is one stage; ContentCheck focuses on the actual checking and looking up of facts, e.g. in Linked Open Data, rather than on claim detection. Full Fact deploys a claim detection system that leverages transfer learning and universal sentence representations, and it outperforms ClaimBuster and ClaimRank that use word-level representations. Konstantinovskyi et al. [19] also discuss an ontological approach to the labelling of claims (i.e. which categories of claims should be distinguished) and methods for obtaining these labels (especially crowdsourcing, see also ClaimBuster). These systems work on English-language texts. ClaimRank ([17] and https://claimrank.qcri.org/) uses a richer set of features than ClaimBuster and is, like Chequeado, one of the currently still limited number of claim detection systems that work on languages other than English (Chequeado: Spanish, ClaimRank: Arabic). An over- view also of earlier computational work in claim detection is given by Leblay, Manolescu, and Tannier [23]. The basic approach of applying supervised learning has remained the same, while a closer inspection of the classes used in early studies also shows the roots of the claim-detection task in sentiment mining: For example, the earliest cited article [31] aimed at separating facts from opinions and then focussed, like much of the work in sentiment mining, on a further analysis of the opinion sentences. It appears that the increasing interaction with pro- fessional fact-checkers over the years since then has brought the rele- vance of differentiating within the facts” class to the fore. Much work has been done on detecting specific signals in texts. Factmata (https://factmata.com), for example, detects signals of, for instance, hyperpartisanship, clickbait, deception, stance, claims vali- dation (whether a claim is supported or refuted by the evidence found”), subjectivity and arguments.2 We believe that these signals could be components of check-worthiness, but they are very specific and lack the overarching notion of relevance to a broad audience that we have identfied as well as circumscribed by features and questions, as central to check-worthiness.. The recent projects tend to go beyond machine learning and involve journalists throughout. However, based on the published papers, it is difficult or impossible to determine how concepts are defined, what procedures and materials have been established, and who contributes what at which stage. Based on our experience of collaboration, we are convinced that a principled approach is needed and that publicly available documentation is useful. In addition to studies and tools, datasets have been published. Through the CLEF CheckThat! Competition that has taken place annu- ally since 2018,3 claim detection and veracity detection algorithms have been tested on datasets in English and in Arabic. Datasets have comprised between 50 documents and 1500 tweets, and domains include web pages, social media, debates/speeches/press conferences. The authors of ClaimBuster have, in 2020, released a dataset4 of 23,533 sentences from all U.S. general election presidential debates (1960–2016) along with human-annotated check-worthiness labels. The dataset, example sentences for the label concepts, and the procedure are described by Arslan et al. [2]. The main novelty of FactRank, compared to these systems, is (a) its being the first system for claim detection for the Dutch language, (b) a principled and openly documented approach to defining the concept of check-worthiness, including a codebook, and (c) an iterative architec- ture that leverages the skills of both human annotators and machine learning. As part of our work, we have created (d) a Dutch-language 2 https://factmata.com/signals.html. 3 https://sites.google.com/view/clef2020-checkthat/datasets-tools, see also [6]. 4 https://figshare.com/articles/ClaimBuster_A_Benchmark_Dataset_of_Ch eck-worthy_Factual_Claims/11635293/1. B. Berendt et al. Online Social Networks and Media 22 (2021) 100113 3dataset of more than 8000 sentences with human annotations of check- worthiness. FactRank is a product of an interdisciplinary collaboration between professional fact-checkers, computer scientists and political scientists, and it has resulted in a live website that is being used by professional fact-checkers and journalists. An important part of the approach is the creation of a codebook that guides human coders in labelling examples as check-worthy or other- wise. The concept of check-worthiness (and in particular the notion of relevance that it involves) is notoriously difficult to define both in terms of its meaning and in terms of example datasets; it depends on time, place and context (e.g., [1]). We provide an English-language version of our codebook as part of the documentation of our approach, with the aim to help and encourage others build codebooks tailored to their materials’ times, places, and contexts. 3. Method FactRank is first of all meant to be a tool for fact-checkers. It should provide them with an instrument that can save them time, by auto- matically collecting claims that could be relevant to fact-check. Towards this end, FactRank aims to detect check-worthy claims in texts. We use the term ‘check-worthy’ to denote claims that are factual (meaning that it is possible to check whether they are accurate) and relevant (not every factual claim is relevant for fact-checkers to investigate). Of course, not only fact-checkers, but everyone interested in a critical reading of (on- line and offline) claims can profit from using FactRank. Within this large domain, FactRank focusses on political content and in particular on utterances of politicians. This influenced our choice of data sources as well as of labellers, and the design of our codebook. The process was bootstrapped and is being accompanied with professional fact checks and expert coders. The remainder of this section describes these choices in detail. The organisation of the section reflects the mixed-methods approach of the current paper. Since this yields inherently interleaved writing, we provide a reading recommendation for our two main audiences: (1) The reader used to computer-science documentation will recognise, after a flow-chart description of the processing pipeline (Section 3.1), a section on data (Section 3.4) and on the machine-learning models and training and test set-up (Section 3.5). For this reader, Sections 3.2 and 3.3 pro- vide background information on how the data were labelled, i.e. how the human ground-truth labels were defined and obtained, and what role the machine-learning models played in the selection of instances to be labelled. (2) The reader used to descriptions of human-subjects studies will recognise, after the description of the overall procedure (Section 3.1), the standard components of method: participants (Section 3.3), procedure (detailed actions of different participants: Section 3.3), and materials (data: Section 3.4). Since the concepts underlying the coding were themselves operationalised as part of the overall procedure (namely by the creation and use of the codebook), this conceptual description is given in Section 3.2. For this reader, Section 3.5, which describes the machine-learning model, provides background informa- tion on how the materials that the human participants saw were generated and on how the tool generates labels. 3.1. Procedure (1): the FactRank approach At the heart of the FactRank approach is an iterative procedure that combines human expertise and machine-learning capabilities to achieve a continuous improvement of the FactRank model’s automatic detection of check-worthy claims in incoming streams of text. Fig. 1 shows the basic procedure, split into nine phases. The archi- tecture rests on an interleaved sequence of training and labelling by humans and a machine-learning classifier model, with the size of the labelled datasets increasing across phases. Four of these phases involve the introduction of new, unlabelled data. In phase 1, human fact-checking experts labelled sentences concerning their check-worthiness”, and they created a codebook that described the reasoning behind their decisions. The purpose of the codebook was to serve as instructions to knowledgeable (not necessarily expert) human fact-checkers. Specifically, a group of student coders went through a training phase 2, in which they were given the codebook and asked to label the 367 sentences whose ground-truth label had already been established. They received feedback upon mislabelling (the ground-truth label and an explanation of why the experts had assigned that label). At the end of this phase, results were discussed with the experts, and the codebook refined. Equipped with the refined codebook and their knowledge gained in training, in phase 3 the student coders labelled a set of 2000 new sen- tences. These labelled sentences were used to train a first version of the machine-learning model in phase 4, in which the model was trained with the student labels as feedback. In phase 5, this model was used to generate a new dataset by selecting a further 5000 sentences that promised to be particularly relevant for learning the concept of check- worthiness, and these 5000 sentences were labelled by ‘the best’ of the student coders in phase 6. In phase 7, all human-labelled sentences accumulated so far were used to train the second version of the machine- learning model. This model is currently (phase 8) being applied ‘live’ on the FactRank website to label new sentences on an ongoing basis, ob- tained from daily crawls from the Flemish, Belgian and Dutch Parlia- ments, Twitter, FactCheck Flanders and VRT (Flemish public television) subtitles. Users of the website can give quality feedback by voting sen- tences up or down, according to their own perception of check- worthiness (phase 9). The usefulness of phase 9 for improving the model was investigated in a pilot study with student coders (different from the ones in phases 2–6). All coders were trained with the codebook and had access to it in all phases of labelling. Further iterations can be added to continue to improve the model. In the following sections, we provide more details on our notion of check-worthiness and how we operationalised it via a codebook (Sec- tion 3.2), the human actors in this pipeline (Section 3.3), the data (Section 3.4), the machine learning (Section 3.5), and the resulting quality of the automatic detection of check-worthiness, including how quality improved through the iterations (Section 4). 3.2. Concepts and their operationalisation: check-worthiness and the codebook The first step was to compose a codebook5 with guidelines on how to decide whether sentences contain check-worthy claims. The guiding principle for the coders was to look at sentences from the perspective of a fact-checker: does this sentence contain a claim that could give rise to a fact-check? Coders were therefore instructed as follows. Take the perspective of a fact-checker: Could this sentence be the start of a fact-checking? Thus, for a sentence to be check-worthy, it should be: 1 Factual. That means that a sentence should contain a claim that revolves around a fact that can be checked, in other words, that it can be deter- mined whether or not the claim is true. 2 Relevant. Not every factual claim is relevant for a fact-checker. Fact- checkers are only concerned with facts that matter to a broad audience. In other words, they are only concerned with claims that, if they turn out to be wrong, are reprehensibly false claims. The coding units were entire sentences. They were coded without further context. In other words, the previous and next sentences were not provided to the coders. The coders had to assign every sentence to one of the following 5 Available at https://people.cs.kuleuven.be/~bettina.berendt/FactRank/Co deboek_FactRank.pdf (in Dutch) and https://people.cs.kuleuven.be/~bettina. berendt/FactRank/Codebook_FactRank_EN.pdf (in English). B. Berendt et al. Online Social Networks and Media 22 (2021) 100113 4coding categories: 1 NF: Not factual 2 FR: Factual and relevant 3 FNR: Factual and non-relevant 4 Error: Not applicable. This code was used for incomprehensible sentences. These category names derive from ClaimBuster [14,15], and they operationalise journalistic concepts and practices (e.g. [21]). However, a previous study [22] had shown that these categories are far from trivial and require a more rigorous approach, involving explicit coding in- structions created by experts. This motivated us to create a detailed codebook that helped to explicate the meaning of the categories. The codebook contains a set of guidelines that should make the coding procedure reliable. The expert coders (see Section 3.3) applied the codebook to a set of sentences and discussed the results. These dis- cussions resulted in additions, adjustments, and refinements of the codebook. The discussions also gave rise to the Reference Dataset (see Section 3.4). The Reference Dataset was used to train six student coders. After a session in which the codebook was explained, the students coded six batches of 50 sentences (phase 2 in Fig. 1). Each batch was discussed with the researchers, and sometimes this resulted in adjustments or re- finements of the codebook. At first sight, the categories appear relatively clear and easy to spot: statements that claim something about facts (for example, numbers that are or are not correct), would be factual, and opinions would be non- factual (NF). However, relevance for a broad public is key, and the distinction between FR and FNR is often not straightforward. Also, claims are made in different (surface) forms, including as pre- suppositions, and journalists need to critically investigate all of these. Therefore, a codebook needs to provide more than concept definitions and example sentences: it needs to help coders understand why an example sentence would be considered interesting and relevant enough to be checkworthy (or not), and how to detect this. This will equip coders with the skills for analysing the topical and linguistic structure of their material. (In a prior step, coders must be selected who have a solid knowledge of the social and political context and a solid competency of the language.) The following five examples illustrate some of the complexities that a journalistic, fact-checking-based approach entails. Care was taken to describe signals as bases for heuristics that demand holistic judgement rather than mechanistic patterns. S1: Together with 122 other countries, we have requested, in the General Assembly of the United Nations, that a ceasefire be declared in Aleppo.” Category: FNR Explanation: You can check whether the request did indeed involve 122 countries, but for many people a few more or less will not make a difference. In addition, the fact that our country requests a ceasefire is neither controversial nor counterintuitive. S2: As regards Canada, it is even 90%.” Category: FR Explanation: You cannot know what this is about. However, since a number is being mentioned together with the signal word even”, it can be a relevant factual claim. [Other typical phenomena and signal words indicating FR are comparisons: increasingly”, growing”, etc.] S3: They do not need paternalism.” S4: This is therefore a good thing.” Category: NF Explanation: These sentences may not sound like opinions (they do not contain I think/I find”), but they are expressions of opinions. You could easily add I think/I find” to the sentence without changing the meaning: I think that they do not need paternalism” and Therefore I find this a good thing.” S5: I consider it undesirable that 80% of the migrants are unemployed.” Category: FR Explanation: The sentence begins with an opinion: I consider”. What follows is a factual claim because it needs to be checked whether 80% of the migrants are indeed unemployed. This is also something that many people are likely to find interesting. Fig. 1. Basic FactRank procedure. B. Berendt et al. Online Social Networks and Media 22 (2021) 100113 53.3. Participants and procedure (2): the human actors in the FactRank pipeline Fig. 2 gives a more detailed view of the general FactRank procedure, naming actors and datasets. The expert coders were three of the authors of the current paper: Jan Jagers, Peter Burger, and Alexander Pleijter, who work as fact-checkers. The student coders were six students from Leiden University’s Mas- ter’s program Journalism and New Media. They had previously taken a course on fact checking, in which they also conducted a fact-check themselves. All six students labelled in phase 3. In phase 2, these stu- dents had been ‘scored’ by their percent agreement with the ground- truth labels given by the expert coders. The five students with the highest agreement were considered to be the best student coders, and they labelled also in phase 5. Our starting data also contained claims collected from fact-checks done by other professional fact-checkers/journalists from relevant Flem- ish and Dutch media. Finally, student coders using the FactRank website (different from the other student coders) voted on sentences considered check-worthy by the model. The purpose of this phase 9 was two-fold: to serve as a first formative test of a projected functionality for the real-life website and its professional users, and to test the usefulness for model quality of voting as a form of getting human ground-truth assessments, a form that is more convenient than labelling from scratch. 3.4. Data Sources The datasets are derived from a broad range of sources from Flemish and Dutch politics and news sites: BE Parliament Plenary, BE Commis- sion, Interviews BE, Interviews NL, NL Parliament Plenary, politicians’ Twitter accounts, NL fact-check websites, and Factchecks Knack. The sources themselves remained the same over all phases. For phases 1–7 in Fig. 1, a static dataset covering the time between March 2017 and March 2019 was used, whereas phases 8 and 9 draw on a dynamic dataset that is continuously extended by daily collection from the sources. The reason why we focussed on politicians, and for instance not on clickbait, is twofold. First, politicians embody representative de- mocracy. They hold the parliamentary debate that, in theory at least, leads to, or is part of, the decisions and legislation made. Since that legislation affects all the people politicians speak for and their everyday lives, in a functioning democracy, politicians should use correct facts in their arguments. Here, correct” means facts that are as undisputed as possible. Political views differ regarding what to do. But the facts on the table – the building blocks of discussion – should ideally be agreed upon. We thus focus on politicians because their words are the beating heart of democracy. Second and related to that first argument, monitoring politicians’ spoken – and written(-down) - words is very time-consuming. Politicians produce large volumes of text and arguments every day as well in parliament, via their direct communication channels – i.e. social media – and in interviews in news outlets on paper, radio, and TV. Considering the first argument above, we consider the need for journalists to receive assistance, a priority. Our selection of sources considers the traditional arena of political actors (parliament) as well as mainstream media (from which interviews were taken) and Twitter as the most recent arena. Especially on Twitter, there is no control over what is written and no gate-keeping. This is one of the reasons why – in the Flemish/Dutch environment described here as well as elsewhere – the need for fact-checking has increased tremendously over the past years. The sources we used are, in descending order of importance, (1) transcripts of plenary debates held in the Belgian federal and in the Dutch parliament; (2) interviews with politicians from different political parties in Flemish and Dutch newspapers; and (3) Flemish and Dutch politicians’ writings on their microblog Twitter. These data were sup- plemented with claims that had already been fact-checked by Dutch and/or Flemish media. Those claims came from politicians, but also from other pundits such as academics or experts cited in media coverage, and also from that coverage itself – for instance, a newspaper headline. The parliamentary debates are complete, as is the list of Fig. 2. Human actors, machine-learning model versions, and data (figure legend: see Fig. 1). B. Berendt et al. Online Social Networks and Media 22 (2021) 100113 6Twitter accounts from the parliamentarians. Sources are representative in the sense that the dataset covers utterances in parliament from elected politicians that represent a wide range of different political/ideological viewpoints. Since both Belgium and the Netherlands have proportional electoral systems and low electoral thresholds, the number of parties represented in parliament is relatively high (7 Flemish parties in Belgium and 13 parties in the Netherlands). The selections of interviews and fact-checks were done based on subjective assessment of interest- ingness by the experts in our team. Data are scraped or obtained via the API (Twitter); no data cleaning issues have arisen. The distribution over sources is reported in Section 3.5 for the main evaluation dataset (D4, explained further below). Datasets The static dataset Dtotal consists of 410,000 sentences. All datasets to be labelled were selected from Dtotal, in ways that aimed at selecting interesting” sentences, as described below. First, 300 sentences were selected from this set to be labelled by our fact-checking experts in step 1. To avoid creating a useless dataset consisting mostly of uninteresting NF sentences, the 300 were not cho- sen randomly. Instead, the machine-learning model from the earlier study [22] was applied to all 410,000 sentences, for each of the cate- gories FR, FNR and NF, sentences were ranked by their score (most likely to be FR, as judged by the model” etc.), and the top-ranking sentences selected such that the distribution over (likely) FR, (likely) FNR, and (likely) NF was uniform. This resulted in 217 sentences for which our three expert labellers agreed (see Section 3.3). After initially labelling each sentence and explaining their decision individually, our three expert labellers also agreed on one ‘explanation’ of each of these sentences. A further 150 sentences from other professional fact-checkers were added to this. Regardless of whether these sentences were judged to be true, false, half-true, etc. by the other professional fact-checkers, the fact that they had been selected for this test indicated that the other pro- fessional fact-checkers deemed them check-worthy. Our experts agreed with these judgments. The combined dataset of 367 labelled sentences (the Reference Dataset, RD in Fig. 2) was used to train the student coders in phase 2. Our experts continued labelling further sentences. The 2000 sentences chosen for labelling in phase 3 (D1 in Fig. 2) were selected as follows: We trained the SVM model from Laperre et al. [22] on the 517 sentences that the experts had agreed on so far (110 FR, 224 FNR, 183 NF). The model was trained in a binary setting, i.e. FR against the rest, and applied to 10,000 sentences from Dtotal. From the result, 1000 sen- tences with >50% confidence of being FR were taken, and a further 1000 randomly chosen. This choice of instances reflected a utility metric [7] focused on precision in the early phases of model learning: we wanted to reduce the uncertainty of instances considered FR by our model via obtaining judgments from human labellers. The two-class setting was used only as a step in generating datasets to be labelled; the models were evaluated with respect to the three-class setting (see Section 4 below). Each sentence in D1 was labelled by all 6 student coders (i.e. each coder labelled 2000 sentences), and the majority labels were taken to be the ground-truth labels for these sentences in D2 . This resulted in 702 sentences for which there was majority agreement. The 5000 sentences chosen for labelling in phase 5 (D3 in Fig. 2) were selected as follows: We trained the SVM from [22] on the 2622 sentences that the experts (in their continuing labelling process), or the student labellers in phase 3, had agreed on so far (328 FR, 1227 FNR, 1067 NF). The model was again trained in a binary setting and applied to Dtotal. From the result, 5000 sentences close to the decision boundary of the SVM (i.e. around 50%) were chosen, stratified by source (1000 each from interviews BE, interviews NL, plenary/commission transcripts, 2nd chamber NL, and Twitter). This choice in a more advanced phase of model learning re- flected a more general utility metric, that of choosing instances that the model is uncertain about. Each sentence in D3 was labelled by 2 out of the 5 ‘best student coders’, such that each coder labelled 2000 sentences, and the agreed- upon labels were taken to be the ground-truth labels in D4 . Sentences from D3 on which the two coders disagreed were excluded from further consideration. In the next step, the results from all previous steps of human labelling were used as the ground-truth dataset, consisting of a total of 7037 sentences (D4 in Fig. 2). All sources originally crawled were represented in the dataset (see Table 3). 1100 sentences had been labelled by other professional fact-checkers, and 5937 by our student and expert coders. This dataset is available at https://github.com/lejafar/FactRank/tree/ master/factrank/data/training. Current deployment results (D5 in Fig. 2) originate from continuous source monitoring and labelling of the new data. They are stored for future iterations of the FactRank pipeline. We then created a dataset D6 by asking three student coders (from Antwerp University and not involved in the earlier rounds) to vote on outputs from the live FactRank website, i.e. on D5. The coders were trained by one of the experts in a similar way as the earlier Leiden coders. First, we explained the goal of FactRank and gave a detailed explanation of the codebook, and we discussed the examples used in the codebook. Next, we performed a small test with all three coders, giving them the same 20 statements. Since the FactRank website only allowed a binary classification, we discussed the results with a clear focus on distinguishing between check-worthy and non-check-worthy state- ments. Coders in this test run of step 9 in Fig. 1 were instructed as fol- lows: Go to factrank.org and look at all sentences from the Flemish Parliament, the [Belgian] Federal Parliament, and the Dutch Parliament. [Each student concentrated on one source.] For every sentence, do the following: If you think this sentence is indeed check-worthy, vote it up with the upvote button. If you think the sentence is not check-worthy, vote it down with the downvote button. If you are unsure, do not vote.” From these, we selected the upvoted sentences. These sentences can be used directly as FR statements. (Downvotes are either FNR or NF, and they can only be used when shifting to a binary FR vs. not FR classifier, which is left for future development and not considered in the present paper.) A further motivation was to boost the number and variety of positive examples (considered check-worthy by humans, similar to phase 3, inspired by classical strategies of relevance feedback in inter- active text retrieval). This resulted in 1270 sentences with an upvote. The model V2 was re-trained using this set D6 with the new upvotes. An overview of the numbers of sentences in the human-labelled datasets is given in Table 1. These are at the same time the class dis- tributions in the input datasets used for model training. The outputs of model training are summarised in Table 4. 3.5. The FactRank machine-learning model The machine-learning model V0 and V1 was inspired by the method for claim detection” of Claimbuster. It used, like Hassan et al. [14,15] did, a support vector machine (SVM). A linear kernel was used because it gave the best classification quality in preliminary tests. The features of the SVM included uni- and bi-grams, POS tags derived using pattern6 and sentiment analysis scores also derived using pattern. The process is Table 1 Class distributions in the ground-truth datasets labelled by human annotators. Dataset FR FNR NF Total RD 110 224 183 517 D2 328 1227 1067 2622 D4","Berendt, B., Burger, P., Hautekiet, R., Jagers, J., Pleijter, A., & Van Aelst, P. (2021). FactRank: Developing automated claim detection for Dutch-language fact-checkers. Online Social Networks and Media, 22, 100113."
ML_DB_008,A machine learning based framework for detection of fake political speech,"Daily news is one of the primary needs of modern society to keep in touch with the world. Unfortunately, social media platforms have notably become a politicians’ tool for spreading propaganda campaigns and disparage opponents, which leads to side effects such as amplifying social discord. In order to thwart fake news, independent journalists have maintained a fact-checking organization and shared their checking results on political speeches on their website, which has raised public awareness for upholding democratic values. Meanwhile, researchers have proposed various types of machine learning and deep learning-based approaches as well as linguistic based approaches by using various types of information for the detection of fake news. Some of them have shown promising results on the detection of fake news. However, they focused on the detection of hoaxes, hateful speech, attractive headlines, political astroturfs, and satirical news or posts. In this paper, we propose an automated framework for the detection of fake political speech. It uses different classification methods for extracting features from political speech statement and its metadata including speech subject, location, speaker’s profile, speaker’s credibility, and speech context information. The features are then used to train a machine learning model with automatic feature selection and parameter tuning. On the ”Liar” dataset, our trained Support Vector Machine (SVM) model has achieved 74% detection accuracy. The evaluation results show that our framework is effective in the detection of fake political speech.",Computer Science,Proceeding,2020,Y,N,1,"Daily news is one of the primary needs of modern society to keep in touch with the world. In this data-driven age, people can easily retrieve news at their hands right away through various types of devices with the internet connection. Likewise, modern media platforms, including websites, blogs, streaming services, and social media, are broadcasting an extensive amount of information. The society benefits from media information doubtlessly. Unfortunately, social media platforms have notably become a politicians’ tool for spreading propaganda campaigns and disparage opponents, which leads to side effects such as amplifying social discord. For example, Twitter was reported to be used as a social manipulation instrument in Brazilian presidential election in 2014 [2]. In order to thwart fake news, independent journalists have maintained a fact-checking organization and shared their checking results on political speeches on their website, which has raised public awareness for upholding democratic values. However, all these checking processes need to be manually performed which is time and labor consuming. In the last decade, for the detection of fake news, researchers have proposed various types of machine-learning and deep-learning based approaches [5], [8], [14], [19], [27], [30] as well as linguistic based approaches [18], [21] by using various types of information including specific words, metadata, the author’s information, and the audience’s reaction. Some of them have shown promising results on detecting fake news. However, they focused on the detection of hoaxes, hateful speech, attractive headlines, political astroturfs, and satirical news or posts; while in this paper we focus on the detection of fake political speech by training a machine learning model with more types of metadata. In this paper, we propose an automated machine-learning based framework for the detection of fake political speech. It uses different classification methods to pre-process and extract linguistic features from political speech statements and from the metadata such as speech subject, location, speaker’s profile, speaker’s credibility, and speech context information. These features are then used to train a machine learning model. In order to achieve the best detection performance, our framework automatically selects the best accuracy from the well-known machine learning algorithms and optimizes classification models. Possible algorithms include Decision Tree, Naive Bayes, Support Vector Machine (SVM), Multilayer Perception Classifier (MLP) [10], Convolutional Neural Network (CNN) [31], Fasttext [11] and BERT [7]. We used a popular Liar” dataset [30] to evaluate the performance of our framework. The evaluation results show that our framework is effective on the detection of political fake news with 74% accuracy with the Support Vector Machine (SVM) model. The rest of the paper is organized as follows. Section II discusses related work about the detection of fake news. Section III presents the design of our framework including an overview as well as the details. Section IV evaluates the effectiveness of our framework. Section V discusses the limitation and the future work. Finally, Section VI makes a conclusion. II. RELATED WORK A. Machine- and deep-learning based fake news detection Researchers have proposed various types of machinelearning and deep-learning based approach for the detection of fake news. Ratkiewitz et. al. [19] proposed a machine-learning based framework which combines topological, content-based, and crowd-sourced features extracted from information diffusion networks on Twitter to detect the spreading of political misinformation. Although this approach achieved a 96% accuracy, it focuses on one social media platform only while our framework focuses on public political speech on various platforms. In order to facilitate the research on fake news detection, Wang et. al. [30] proposed a new benchmark dataset, Liar. Liar is based on twelve thousand manually labeled short statements collected over decades and released by a group of individual journalists from a fact-checking website PolitiFact.com The dataset consists of news statements and their metadata. The authors trained a set of convolutional neural network models on this Liar dataset and their accuracy was 27.7%. For the same dataset, Long et. al. [14] proposed a hybrid LSTM model to incorporate topic information and speaker’s profile for the detection of fake news. Their model achieved 41.5% accuracy only. Different from [14], [30], our framework considers more aspects of the data in Liar dataset, including but not limited to the topic information and speaker’s profile. Meanwhile, our framework performs pre-processing and classifications for the feature extraction while the models in [14], [30] used the raw data as features directly. Bhattacharjee et. al. [5] proposed a semi-supervised deep learning model for the detection of fake news. In this model, an initial classification model was constructed on a small set of annotated data. The model was then gradually updated by using a human-machine interactive approach to incorporate more samples from a large pool of unlabeled data. This approach solved the challenges of veracity detection in a big amount of data. In the evaluation, the authors trained a model with a small set of data randomly selected from the Harvard-Dataverse Twitter collection, and used the Liar dataset for testing purpose only. However, the performance of their model was highly dependent on the dataset balance and sparseness. In [8], for the detection of fake news, Dong et. al. proposed a semi-supervised deep learning model with two-path convolutional neural networks by using a limited amount of labeled data. The proposed model achieved 79% accuracy. Unlike their approaches, our framework is based on supervised machine learning models and the Liar dataset is used for both training and testing purposes in our evaluation. Even though neural networks are more sophisticated, there is no guarantee that neural network models always outperform traditional machine learning models. In addition, deep learning methods often require a huge amount of training data. In order to catch hoaxes on social media platforms such as Facebook, Tacchini et. al. [27] proposed two classification techniques. They used logistic regression and the adaptation of a boolean crowd-sourcing algorithm, respectively. Those techniques are not adhered to content or linguistic related methodologies. Based on users’ reactions to the posts on specific Facebook pages, hoaxes and non-hoax information on Facebook can be classified with 99% accuracy. However, our framework aims to detection general fake political speech. B. Linguistic-based fake news detection In order to characterize the news, some researchers have applied natural language processing related techniques to incorporate linguistic information into the detection of fake news. Satire is a type of deception that attracts the audience by in a jesting way while other types of deceptive news aim to convince their audience of false of truth. In order to detect satirical news, Rubin et. al. [21] proposed a SVM model with five predictive features: absurdity, humor, grammar, negative affect, and punctuation. The proposed model achieved 90% precision on 360 news articles. However, the lexical methods used by their model have limited success, especially on complex and long sentences. Meanwhile, different from our purpose, their model is not for the detection of fake political speech. Rashkin et al. [18] proposed a model, which was based on linguistic characteristics of untrustworthy text, to detect fake news. By leveraging Wiktionary, the model compares the language of the news with that of satire, hoaxes, and propaganda, and extracts linguistic characteristics. The extracted information was then used for training Long Short Term Memory (LSTM), Naive Bayes, and Maximum Entropy (MaxEnt) models. With samples from PolitiFact, Naive Bayes and LSTM models achieved 56% accuracy on the binary classification while MaxEnt achieved 22% accuracy on sixclasses classification. C. Fake news prevention Qayyum et. al. [17] have proposed a blockchain-based architecture to counter the spreading of fake news. The proposed architecture used smart contracts to assure the authenticity of the news and its sources. Different from it, our framework focuses on the detection of fake political speech. III. DESIGN A. Overview The overall design of our detection framework is shown in Figure 1. The framework has two phases for training and testing, respectively. It consists of four components, preprocessing, feature extraction, model training, and prediction, across the two phases. The pre-processing component preprocesses the speech content as well as the metadata including speech subject, location where it happened, speaker’s profile, speaker’s credibility, and speech context information. The feature extraction component then applies different classification methods for extracting features from the pre-processed speech content and metadata. The model training component trains a binary classification model based on the extracted features, and automatically performs algorithm and feature selection and parameter tuning for the best prediction accuracy. The prediction component uses the trained model to predict whether a public political speech is trustable or not. The dataset is the main body where we can learn knowledge for the design and evaluation of our framework. However, Fig. 1. Design Overview there are many datasets for the natural language processingbased detection of fake news as shown in Table I. Because this work specifically focuses on the classification of political speeches, we choose the Liar dataset which was released by a group of independent journalists on PolitiFact.com in 2017. It contains 12,791 political speeches from 2006 to 2017 as well as their metadata. The metadata includes speech subject, speech context, location of the speech, speaker’s name, political party affiliation, job occupation, and credit history. Contributors to the Liar dataset have made a huge effort to cover almost all types of media platforms such as TV programs, websites, social medias, and public meetings [30]. When compared to the Liar dataset, other datasets either are limited to specific platforms (e.g., BUZZFEEDNEWS [25] and PHEME [33]) or do not have a rich content (e.g., FAKENEWSNET [24]). Although the FEVER [28] dataset is also for the fact extraction and verification, its claims were not collected from real political speeches but generated by altering sentences extracted from Wikipedia. Note that the classification model can be re-trained whenever the dataset about the political speech is updated. B. Feature extraction The overall approach for our feature extraction is to categorize the text-based speech content and its metadata into a set of pre-defined labels based on the semantic information. A text pre-processing is first performed on content and metadata to remove unnecessary characters (e.g., white space) for the feature extraction. Different classification methods are then applied for extracting labels (i.e., features) for both statements and their metadata, including speech subject, location, speaker’s profile (e.g., name, job occupation and party affiliation), speaker’s credibility, and speech context information. TABLE I SUMMARY OF DATASETS RELATED TO THE DETECTION OF FAKE NEWS Name Input Data Size Label Annotator LIAR [30] Short 12,791 six- editor & claim grade journalist FEVER [28] Short 185,445 three- trained claim grade annotator BUZZFEED- Facebook 2282 four- journalist NEWS [25] post grade BUZZFACE [23] Facebook 2263 four- journalist post grade Some-like- Facebook 15,500 true/false - it-hoax [27] post PHEME [33] Tweet 330 true/false journalist CREDBANK [15] Tweet 60 vector worker million (size 30) FAKENEWSNET Article 23,921 fake/real editor [24] In this section, we will provide details about this feature extraction process. 1) Speech Subject: We manually categorize arbitrary values of the subject of a political speech into three labels, social”, applied”, and other”. The social” label includes all social science related topics such as those in education, economy, history, religion, governance, society, and legal. The applied” label includes all applied science related topics such as those in technology and engineering, biology, health and medicine, agriculture, sports, and weather. The other” label includes the rest topics such as those related to military and transportation. 2) Job Occupation: For the feature job, we define three labels, public”, state” and other”, to the position acquired by the speaker at the time of speech. The public” label includes public but non-federal positions such as chief executive officer (CEO) of a company, vice president and teachers in universities and schools, and any position in a public media. The state” label includes all the federal positions such as state governors, senators, mayors, congress members, and the president. The other” label includes all the rest positions such as the jobs of those freelancers. Correspondingly, a classification method is designed to categorize general job descriptions to these three labels. Specifically, we first manually label a small set of job descriptions to these three labels, then train a Universal Serial Encoder (USE) classifier [6] with the manually labeled job descriptions, and use the trained model for classifying any new job description to one of these labels. We choose the USE model because of several reasons: (1) it has the capability to address big data dependency problem in a natural language processing task, (2) it works efficiently with a small amount of training data, and (3) our micro-benchmark results show that USE can achieve higher accuracy on job classification when compared to other machine learning algorithms such as K-nearest Neighbor (KNN) [12]. 3) Location and Speaker’s Profile: Regarding the feature location, the physical location, where the public speech happened, is categorized into five regions (labels): west”, midwest”, south”, northeast”, and other”. We use the name of a speaker directly as a feature in our model. For the feature party, the party affiliation of a speaker is categorized into three labels: republic”, democrat”, and others”. In order to represent the credit history of a speaker, we use true speech count and false speech count for the label/feature extraction. The true speech count indicates how many times that a speaker made true statements in his/her public speeches, while the false speech count indicates how many times that a speaker made fake statements in his/her public speeches. If the true speech count is equal or greater than the false speech count, the label of feature credibility will be fair”; otherwise it will be liar”. 4) Speech Context: For the feature context, we use the same method for the feature job to classify context description into three labels, press”, direct”, and social”. The press” label includes all traditional public media platforms such as press agencies, TV programs, newspapers, and magazines. The direct” label includes all other forms of verbal speech such as meetings, public speeches, and debates. The social” label includes all social media such as message and video posts on websites or online social media platforms like Facebook and Twitter. 5) Speech Statement: The current communication behavior theories had already proved that deceptive storytellers communicate qualitatively different than truth-tellers; therefore, liars can be identified by their words and speaking style. For example, deceivers speak with more total number of words and more sense-based words (e.g., feel, see, and touch) but less self-oriented pronouns. In addition, they tend to use a few of cognitive complexity words and express more negative emotions. To certain extent, even human judgment can achieve 50-63% accuracy on detecting deceptions in the news without the proper knowledge [22]. In our framework, we perform a linguistic analysis to extract and count linguistic characteristics from the speech statement, and use the counts of linguistic characteristics as features. Specifically, we use Linguistic Inquiry and Word Count (LIWC) [16] to count the number of words in the speech statement for 93 categories of emotional, cognitive, and structural text components under 14 catalogs. As a result, we extract 93 linguistic features from a speech statement. Table II lists all these categories of text components. Besides LIWC, we have also considered other techniques adopted by existing work for extracting features from statements. (1) Part-Of-Speech Tagging. Part-Of-Speech (POS) tagging is widely used by many Natural Language Processing (NLP) tasks. It automatically tags words with categories (e.g., nouns, verbs, adjectives, pronouns, etc.) by grammar similarity. As researchers found, tagging entities, places, adjectives, verbs, and adverbs helps identify fake news [32]. Therefore, we used the NLTK library [13] to extract POS tags as features from the statements; however, our experiments show that it does not improve the final prediction accuracy. (2) Subject- Verb Analysis. Since POS tagging performs a more intensive analysis of specific words such as subject and verb, we modeled the relation between subjects and verbs in statements and used the relation as features instead of simply and directly using POS tags. Specifically, we extracted features of thing and human from subjects and possession and motion from verbs in statement texts. However, this approach did not help improve the final prediction accuracy. (3) Emotion Extraction. Emotion is one of the most significant factors that have been leveraged by fake news. Researchers have been studying it for a long time; they believe that a highly emotionalized environment is the logical outcome of modern fake news at online platforms and emotional contents are more likely to spread faster. Therefore, gauging emotions in texts and make it machine-understandable gives an opportunity to the detection of fake news [3]. In this direction, we used Scikit-learn python library and extracted 9,187 sadness and 3,604 happiness words as emotional features for political speech statements. However, our experiments show that they did not help improve the final prediction accuracy. C. Model Training In the model training, we have experimented with a set of machine learning algorithms in a popular machine learning package, Weka, as well as some deep-learning algorithms used by existing work for the detection of fake news. These algorithms include Decision Tree, Naive Bayes, Support Vector Machine (SVM), Multi-layer Perception Classifier (MLP) [10], Convolutional Neural Network (CNN) [31], Fasttext [11] and BERT [7]. In our experiments, we observed that using Sequential Minimal Optimization (SMO) for training a SVM classifier outperforms all other algorithms with default parameters for our task (Section IV-C). Meanwhile, the polynomial kernel helps the SVM model achieve its best performance. Due to the space limitation, we only present and analyze the evaluation results based on SVM classifier in the next section. Researchers from Facebook have developed a novel solution, Fasttext, to tackle the constraints of common methods (e.g., linear classification and Bag-of-word) for understanding text. Note that those common methods are often computationally expensive and they can not scale up to larger datasets. Researchers claimed that Fasttext can train one billion words in less than ten minutes with standard CPUs [11]. In BERT, Google promotes the use of pre-trained language corpus followed by a fine tuning for both supervised and unsupervised NLP classifications. Although millions of words are used in the pre-training, the computational cost can be reduced by the fine tuning thus making BERT effective on classification [7]. In the next section, we will also perform a simple comparison on the performance of these classification methods. D. Feature Selection Training a machine learning model with a large number of features and dimensions often requires intensive computation. Therefore, feature selection is an essential part in machine learning for improving the prediction accuracy while reducing the training effort if possible. Specifically, feature selection is for identifying the features that have the strongest positive impact to the prediction accuracy while excluding the least TABLE II CATEGORIES OF TEXT COMPONENTS IN LIWC Catalog Category Summary dimensions WC (word count), Analytic, Clout, Authentic, Tone, WPS (words/sentence), Sixltr (words > 6 letters), Dic (dictionary words) Punctuation marks Total punctuation marks period, comma, colon, semicolon, question mark, exclamation, dash, quote, apostrophe, parentheses, other punctuation marks Function words Total function words i, we, you, she/he, they, pronoun, possessive pronoun, interrogative pronoun, article, preposition, conjunction, negation, auxiliary verb, adverb Biological processes Total biological words body, health, sexual, ingest Time orientation present tense, past tense, future tense Relativity Total relativity words motion, space, time Affective process Total affective words positive emotion, negative emotion, anxiety, anger, sadness Cognitive process Total cognitive words insight, causation, discrepancy, tentative, certainty, differentiation Informal language Total informal words swear words, netspeak, assent, nonfluencies, filler Drives Total drive words affiliation, achievement, power, reward, risk Other grammar common verbs, common adjectives, comparisons, interrogatives, numbers, quantifiers Personal concerns work, leisure, home, money, religion, death Social process Total social words family, friend, female references, male references Perceptual process Total perceptual words see, hear, feel important features from the training and testing. Previous studies have already proposed various approaches that addressed dependency, redundancy, and correlation among features [20]. For every data sample (i.e., a political speech and its metadata) to be used for training or testing, our feature extraction component outputs total 100 features. 7 of them are from metadata while the rest 93 are extracted by LIWC from the speech statement. In order to reduce the total number of features and shorten the training time while preserving the prediction accuracy, an automatic feature selection will be performed during the model training. Principal Component Analysis (PCA) is one of the popular methods for feature selection for heterogeneous data. It analyzes the data table for identifying the inter-correlated features, then and represents them in new orthogonal variables (i.e., principal components). These variables can be used to significantly improve the model accuracy based on their similarities [1]. We have conducted several experiments about PCA for feature selection for our model; however, PCA does not improve the overall accuracy. Therefore, we decide to use the Extra Tree Classifier, a type of ensemble learning technique, for feature selection among the total 100 features. It aggregates the results of multiple decorrelated decision trees with a random sample of k features to determine the best set of features without losing stability or accuracy while avoiding the problem of over-fitting [4]. The result of the extra tree classifier is a set of scaled scores (from 0 to 1) for all the features, respectively. 0 indicates the least important features while the closer score to 1 indicates the strongest features. E. Parameter Tuning Parameter tuning is another of the significant pillars of machine learning. It aims to find optimal parameters for a given classification model for improving the prediction performance. In the model training component, we use both grid search [26] and Auto-Weka [29] for the parameter tuning on our SVM model. Specifically, in grid search we perform an exhaustive search on given ranges for SVM parameters, and use cross-validation to identify the best combination of hyper-parameters for the highest prediction accuracy. Besides grid search, we use Auto-Weka [29] (an extension of Weka) to iterate multiple classification configurations and search for the best classification model and its parameters because the prediction performance depends on both the model and its parameters (including the kernel). IV. EVALUATION In this section, we first introduce the data set used to evaluate our framework. We then discuss the performance of our automated feature extraction. The rest of this section will be used to discuss the overall detection performance and the optimization results. A. Dataset For our evaluation, we use the Liar dataset collected and released by a group of independent journalists from Politi- Fact.com. It has total number of 12,791 records. Every record consists of a public speech statement and the metadata. All the speech statements have been verified by human fact-checkers and annotated with six labels: true”, false”, half-true”, mostly-true”, barely-true” and pants-fire”. The metadata in the dataset includes speech subject with 142 unique values, job occupation of the speaker with 1,277 unique text values, state which is the location of speech and has 85 unique values, the speaker’s name, party affiliation of the speaker with 16 unique values, speech context with 5,075 unique text values, and the speaker’s credit history represented by five numbers, barely true” counts, false” counts, half true” counts, mostly true” counts, and pants on fire” counts. B. Automatic Feature Extraction Results For the feature job, we manually labeled 295 out of its 1,277 unique values in the Liar dataset into public”, private”, and others”, and trained a USE model (Section III-B2). The model achieved 93% accuracy and was used to classify the rest of job descriptions in the Liar dataset. Similarly, we manually labeled 350 out of 5,075 unique values for the feature context to press”, public”, and direct”, and trained a USE model (Section III-B4). The model achieves 92% accuracy and was used to classify the rest of speech context in the Liar dataset. For the speaker’s credit history (i.e., the feature of credibility), we sum the counts of pants fire”, false”, and barely true” as the value of false speech counts and sum the counts of half true” and mostly true” as the value of true speech counts (Section III-B3). The three features of speech subject, speech location, and the speaker’s party affiliation were manually classified. The six original labels in the Liar dataset are mapped into two labels, true” or false”, in our classification model. The true” label includes the original true”, half-true”, and mostly-true” labels, while the false” label includes the original false”, barely-true” and pants-fire” labels. C. Overall Results In order to find the best machine learning model for the detection of fake political speech with the Liar dataset, we used Auto-Weka for searching through all possible algorithms with 10-fold cross-validation and performed an automatic parameter tuning. We define a true positive (TP) as a true speech being correctly predicted as true, false positive (FP) as a fake speech being incorrectly predicted as true, true negative (TN) as a fake speech being correctly predicted as fake, false negative (FN) as a true speech being incorrectly predicted as fake. Based on these values, we further calculate TP rate, FP rate, precision, recall, and accuracy. Table III shows the overall performance of our model for the detection of fake political speech. Our SVM model achieved 73.8% accuracy with all the 100 features (7 features from the metadata and 93 features from the statement). Our detection model outperforms the models trained with the same dataset in [14], [30]. TABLE III PERFORMANCE OF SVM MODEL ON 12,791 DATA SAMPLES Model TP Rate FP Rate Precision Recall Accuracy SVM 0.87 0.427 0.72 0.87 0.739 The confusion matrix of our SVM model is shown in Table IV. 6,209 (87%) of the 7,134 true speeches were correctly classified, and 3,252 (57.3%) of the 5,657 fake speeches were correctly classified. This observation indicates that true speeches are more likely to be predicted correctly while fake speeches are prone to be predicted incorrectly to be true. It implies that the speaker tends to add ambiguousness to a speech rather than giving a 100% fake speech, which causes a big challenge in the detection of fake speech. TABLE IV CONFUSION MATRIX OF SVM MODEL _x0002__x0002__x0002__x0002__x0002__x0002_ Ground Truth _x0002__x0002__x0002__x0002__x0002_ Prediction False True False 3,241 (TN) 2,416 (FP) True 925 (FN) 6,209 (TP) Table V shows the performance of our SVM model on 93 features from the speech statement only, 7 features from the metadata only, and all the 100 features from the speech statement and metadata. The last column shows the performance of the optimized SVM model with all the 100 features. In addition, our model can still achieve 62.8% accuracy with all other features than the speaker’s credibility. These observations illustrate that the metadata plays a significant role for improving the performance of detection of fake political speech. TABLE V PERFORMANCE OF SVM MODEL ON STATEMENTS AND METADATA Statement Metadata Statement + Statement + Metadata Metadata + Optimization Accuracy 0.612 0.729 0.739 0.74 Besides the traditional machine learning models, we have also experimented with deep learning models on speech statements only in the Liar dataset (without LIWC). We preprocessed the statements by lowercasing and lemmatizing words as well as removing all the stop words. As shown in Table VI, deep learning models Multi-layer Perception Classifier (MLP) [10], Convolutional Neural Network (CNN) [31], Fasttext [11], and BERT [7] do not significantly improve the detection performance when compared to the performance of SVM model on statements only (i.e., 61.2% accuracy). The design of neural network heavily relies on data, and the data size plays a significant role in training the neural network [9]. Even though some of the deep learning methods can use pretrained language models [7], they still suffer from the problem of over-fitting. Therefore, we did not proceed with those deep learning models with the small-size Liar dataset. D. Model Optimization An extra tree classification was further performed on all the 100 features for the purpose of feature selection. This classifier identified top ten features: (1) speaker’s credibility (from metadata), (2) speaker’s name (from metadata), (3) TABLE VI ACCURACY OF DEEP LEARNING MODELS ON STATEMENTS ONLY FROM THE LIAR DATASET MLP CNN Fasttext BERT Accuracy 0.557 0.614 0.662 0.66 numbers (from statement), (4) party (from metadata), (5) authentic (from statement), (6) prepositions (from statement), (7) function words (from statement), (8) LIWC dictionary words (from statement), (9) relativity including motion, space, and time (from statement), and (10) words showing power in the text such as superior and bully (from statement). Seven of them are LIWC generated features from the speech statement while the rest three are from the speech metadata. We then trained an SVM model by using only these top 10 features. The model with the 10-fold cross-validation achieves the same accuracy of 73.9%. In order to explore positive or negative correlations among all the features, we generated a heatmap as shown in Figure 2. A green cell in the figure indicates a positive correlation between two features on x axis and y axis; a red cell indicates a negative correlation between two features on x axis and y axis. The analysis results on the heatmap indicate that (1) there is no strong correlation between the features from the metadata, except the speaker’s credibility, and the prediction result; (2) there is no strong correlation between LIWC features from the statement and the prediction result; and (3) there is no strong correlation between features from the metadata and LIWC features from the statement. We also tried the Backward Selection on LIWC features for the feature selection. It iteratively excludes a group of features (corresponding to a LIWC category) from the model training and testing, and observes the change on the accuracy during the process. Unfortunately, the back selection did not improve the overall performance on the detection of fake political speech. Table VII shows the accuracy of model trained and tested by using e","Purevdagva, C., Zhao, R., Huang, P. C., & Mahoney, W. (2020, December). A machine-learning based framework for detection of fake political speech. In 2020 IEEE 14th International Conference on Big Data Science and Engineering (BigDataSE) (pp. 80-87). IEEE."
ML_DB_011,A comparison of fake news detecting and fact-checking AI based solutions,"Scientific objective of this paper is to analyse how advanced are Artificial Intelligence (AI) tools to fight successfully information disorder. More specifically, this is an overview and ranking on existing tools based on AI in this specific area. Research method is comparative analytics. We compare the most developed and publicly available fake-news detecting and fact-checking AI based solutions (intelligent machines). The comparison is based on two key parameters: accuracy and comprehensiveness. Results and conclusions: Analyse show that a third of the examined AI systems are, in terms of comprehensiveness, in the top category, while the majority are in the medium category. As far as accuracy is concerned, very few AI machine developers are interested in providing further details about their products and functionalities for studies such as ours which raises suspicions about their actual performance. Surprisingly, one of the most discussed AI systems among EU leaders seems to actually belong to the least developed. Cognitive value: There is a need for a larger and more detailed study with involvement of AI specialists who would be able, and allowed, to test all available AI machines with their key features and functionalities.",Social Science,Article,2019,N,Y,3,"Scientific objective of this paper is to analyse how advanced are Artificial Intelligence (AI) tools to fight successfully information disorder. More speci fically, this is an overview and ranking on existing tools based on AI in this speci fic area. Research method is comparative analytics. We compare the most developed and publicly available fake-news detecting and fact-checking AI based solutions (intelligent machines). The comparison is based on two key parameters: accuracy and comprehensiveness. Results and conclusions: Analyse show that a third of the examined AI systems are, in terms of comprehensiveness, in the top category, while the majority are in the medium category. As far as accuracy is concerned, very few AI machine developers are interested in providing further details about their products and functionalities for studies such as ours which raises suspicions about their actual performance. Surprisingly, one of the most discussed AI systems among EU leaders seems to actually belong to the least developed. Cognitive value: There is a need for a larger and more detailed study with involvement of AI specialists who would be able, and allowed, to test all available AI machines with their key features and functionalities. KEYWORDS artificial intelligence, comparison, fact-checking, fake news, testing * This is partial deliverable of the H2020 CSA Project: COMPACT: From Research To Policy Through Raising Awareness of the State of the Art on Social Media and Convergence, Project Number 762128. The Commission is not responsible for any claims made in this article. 366 Andrej Školkay, Juraj Filin • A Comparison of Fake News Detecting and Fact-Checking AI Based Solutions Studia Medioznawcze 2019, Vol. 20, No. 4 (79), pp. 365–383 https://mediastudies.eu There are about 50 fake news detecting and fact-checking organisations in Europe, and double that number in other parts of the world (Funke, 2018). Fake news detecting, fact-checking and debunking organisations and initiatives rely almost exclusively on manual tracking of fake news systems (information disorder), and only rarely employ semi-automated tracking systems (Pavleska, Školkay, Zankova, Ribeiro,  Bechmann, 2018). This is a costly, inefficient, error- prone and slow process of making sense of information disorder (which includes deliberately and accidentally or unintentionally misleading information, unexpected offensive outcomes, hoaxes, and conspiracy theories) in both online and offline environments. Measured by volume, only about 0.25 percent of total content delivered by Google contains offensive or clearly misleading content, but this fraction is still considered to be potentially damaging to society.1 A possible solution appears to be the use of AI powered news and social discourse analysis for such a purpose. Obviously, AI can be used for the same (negative) purpose as a digital weapon in cyber wars using bots. It may be that several AI applications, such as algorithmic journalism, identifiaction of target-groups for specific disinformation campaigns, or the maintenance of user networks, may play a role in fake news distribution. Nonetheless, this article aims at exploring the most recent advances in this strategic research, focused only on the positive side of the use of AI tools in order to provide up-to-date knowledge and the first comparative assessment of state-of-the-art of AI solutions aimed at detecting and debunking fake news and carrying out fact-checking. Our comparison does not claim to be comprehensive, but is rather a contribution to the debate. In spite of some scepticism about the potential of AI (as we discuss below), including some contradictory gloomy forecasting of the AI negative impact (e.g. Shotter, 1997, and perhaps the most well-known Hawkins, see e.g. Cellan-Jones, 2014), the exploration of AI seems to be highly relevant to the current scientific discourse. For example, 40% of calls (100 of out of 250) for conferences published on the ‘easychair’ portal in March 2018 included AI among their key words. Yet only about 10 of these actually tackled fake news and/or social media as a major topic and, moreover, there is not a single paper that tackles the role of AI within information disorder in general and the effectiveness of AI tools using a comparative method in particular. Although one can agree with Chinnappa’s (2017) and Craft, Ashley and Maks’s (2017) arguments that the best way to combat the problem of fake news is to support the development and identi fication of high- quality online content, promoting media literacy, restricting the flow of money to deliberately misleading content, and ensuring that reporting and feedback tools are as effective as they can be, nevertheless, the AI contribution within this context can, and should be, explored in more detail. There is an important contribution to this debate but it is almost exclusively from experts within the AI – i.e. technology – field (e.g. Vlachos and Riedel 2016; Popat, Mukherjee, Strötgen,  Weikum, 2016; Hassan, Li,  Tremayne 2015; Zhao, Resnick,  Mei, 2015). There also is a paper by Özgöbek and Gullain (2017) in which they offer a brief state of the art overview of the automatic detection of fake news. However, they do not present any AI tools. Therefore, as highlighted by Babakar and Moy (2016, 19): ‘There is an urgent need for a thorough literature review of work on automated checking, including work outside academia. ","Školkay, A., & Filin, J. (2019). A comparison of fake news detecting and fact-checking AI based solutions. Studia Medioznawcze, 20(4), 365-383."
ML_DB_014,Automatically identifying fake news in popular twitter threads,"Information quality in social media is an increasingly important issue, but web-scale data hinders experts’ ability to assess and correct much of the inaccurate content, or fake news,” present in these platforms. This paper develops a method for automating fake news detection on Twitter by learning to predict accuracy assessments in two credibility-focused Twitter datasets: CREDBANK, a crowdsourced dataset of accuracy assessments for events in Twitter, and PHEME, a dataset of potential rumors in Twitter and journalistic assessments of their accuracies. We apply this method to Twitter content sourced from BuzzFeed’s fake news dataset and show models trained against crowdsourced workers outperform models based on journalists’ assessment and models trained on a pooled dataset of both crowdsourced workers and journalists. All three datasets, aligned into a uniform format, are also publicly available. A feature analysis then identifies features that are most predictive for crowdsourced and journalistic accuracy assessments, results of which are consistent with prior work. We close with a discussion contrasting accuracy and credibility and why models of non-experts outperform models of journalists for fake news detection in Twitter.",Social Computing,Proceeding,2017,Y,N,186,"Measuring accuracy and credibility in text are well-studied topics in disciplines from psychology to journalism[1], [2], [3]. The proliferation of large-scale social media data and its increasing use as a primary news source [4], however, is forcing a re-examination of these issues. Past approaches that relied on journalistically trained gatekeepers” to filter out low-quality content are no longer applicable as social media’s volume has quickly overwhelmed our ability to control quality manually. Instead, platforms like Twitter and Facebook have allowed questionable and inaccurate news” content to reach wide audiences without review. Social media users’s bias toward believing what their friends share and what they read regardless of accuracy allows these fake stories to propagate widely through and across multiple platforms[5]. Despite research into rumor propagation on Twitter [6], [7], [8], fake image sharing in disaster aftermath [9], and politically moti- vated astroturfing” [10], rumor and fake news” are becoming increasingly problematic. Computational methods have proven useful in similar contexts where data volumes overwhelm human analysis capabilities. Furthermore, regularities in bot behavior [11] and financially motivated sensationalists [12] suggest machine learning-based approaches could help address these quality issues. In this paper, we present a method for automating fake news” detection in Twitter, one of the most popular online social media platforms. This method uses a classification model to predict whether a thread of Twitter conversation will be labeled as accurate or inaccurate using features inspired by existing work on credibility of Twitter stories [13], [6]. We demonstrate this approach’s ability to identify fake news by evaluating it against the BuzzFeed dataset of 35 highly shared true and false political stories curated by Silverman et al. [14] and extracted from Twitter. This work is complicated by the limited availability of data on what is fake news” online, however, so to train this system, we leverage two Twitter datasets that study credibility in social media: the PHEME journalist-labeled dataset [15] and the CREDBANK crowdsourced dataset [5]. PHEME is a curated data set of conversation threads about rumors in Twitter replete with jour- nalist annotations for truth, and CREDBANK is a large-scale set of Twitter conversations about events and corresponding crowdsourced accuracy assessments for each event. Results show our accuracy prediction model correctly clas- sifies two-thirds of the Twitter fake news stories and outper- forms prior work in this area. Furthermore, accuracy models generated from crowdsourced workers outperform models trained on journalists in classifying potentially fake Twitter threads. Feature analysis also shows crowdsourced workers’ accuracy assessments are more influenced by network effects while journalists’ assessments rely more on tweet content and language. This work makes the following contributions: • An automated mechanism for classifying popular Twitter threads into true and fake news stories, • An analysis of the different features used by journalists and crowdsourced workers/non-experts in assessing ac- curacy in social media stories, and • An aligned collection of three datasets that capture accu- racy judgements across true and false stories. II. RELEVANT WORK AND DATASETS Social media’s explosions in popularity has enabled research into credibility in the online context, especially on microblog- ging platforms. Several previous efforts have proposed meth- arXiv:1705.01613v2 [cs.SI] 30 May 2018 ods for evaluating the credibility of a given tweet [8] or user [16] while others have focused more on the temporal dynamics of rumor propagation [6]. Most relevant to our paper, however, is the 2013 Castillo et al. work, which provides a compre- hensive examination of credibility features in Twitter [13]. This study was built on an earlier investigation into Twitter usage during the 2010 Chile earthquake, where Twitter played a significant role both in coordination and misinformation [17]. The later study developed a system for identifying newsworthy topics from Twitter and leveraged Amazon’s Mechanical Turk (AMT) to generate labels for whether a topic was credible, similar to CREDBANK but at a smaller scale. Castillo et al. developed a set of 68 features that included characteristics of messages, users, and topics as well as the propagation tree to classify topics as credible or not. They found a subset of these features, containing fifteen topic-level features and one propagation tree feature, to be the best performing feature set, with a logistic regression model achieving an accuracy of 64% for credibility classification. Given general users have difficulty judging correct and accurate information in social media [18], [7], however, crowdsourced credibility assessments like these should be treated with caution. The investigation presented herein builds on this past work by evaluating whether crowdsourced workers (as used in both CREDBANK and Castillo et al.) are valid accuracy assessment sources. A. The PHEME Rumor Dataset The PHEME rumor scheme data set was developed by the University of Warwick in conjunction with Swissinfo, part of the Swiss Broadcasting Company [15]. Swissinfo jour- nalists, working with researchers from Warwick, constructed the PHEME data set by following a set of major events on Twitter and identifying threads of conversation that were likely to contain or generate rumors. A rumor” in this context was defined as an unverified and relevant statement being circulated, and a rumor could later be confirmed as true, false, or left unconfirmed. During each rumor selected in the PHEME dataset, journal- ists selected popular (i.e., highly retweeted) tweets extracted from Twitter’s search API and labeled these tweets as rumor or non-rumor. This construction resulted in a set of 330 labeled rumorous source tweets across 140 stories. For each tweet in this labeled set, the authors then extracted follow- up tweets that replied to the source tweet and recursively collected descendant tweets that responded to these replies. This collection resulted in a tree of conversation threads of 4,512 additional descendant tweets. Journalists from Swissinfo labeled source tweets for each of these threads as true, false, or unverified. Once this curated set of labeled source tweets and their respective conversation threads were collected, the PHEME data set was then made available to crowdsourced annotators to identify characteristics of these conversation threads. This crowdsourced task asked annotators to identify levels of support (does a tweet support, refute, ask for more information about, or comment on the source tweet), certainty (tweet author’s degree of confidence in his/her support), and evidentiality (what sort of evidence does the tweet provide in supporting or refuting the source tweet) for each tweet in the conversation. Past work found disagreement and refutation in threads to be predictive of accuracy [13], and these annotations of whether a tweet supports or refutes the original tweet help quantify this disagreement, which we leverage later. Of the 330 conversation trees in PHEME, 159 were labeled as true, 68 false, and 103 unverified. B. The CREDBANK Dataset In 2015, Mitra and Gilbert introduced CREDBANK, a large- scale crowdsourced data set of approximately 37 million of which were unique. The data set covered 96 days starting in October of 2014, broken down into over 1,000 sets of event-related tweets, with each event assessed for accuracy by 30 annotators from AMT [5]. CREDBANK was created by collecting tweets from Twitter’s public sample stream, identi- fying topics within these tweets, and using human annotators to determine which topics were about events and which of these events contained accurate content. Then, the systems used Twitter’s search API to expand the set of tweets for each event. CREDBANK’s initial set of tweets from the 96-day capture period contained approximately one billion tweets that were then filtered for spam and grouped into one-million-tweet windows. Mitra and Gilbert used online topic modeling from Lau et al. [19] to extract 50 topics (a topic here is a set of three tokens) from each window, creating a set of 46,850 candidate event-topic streams. Each potential event-topic was then passed to 100 annotators on AMT and labeled as an event or non-event, yielding 1,049 event-related topics (the current version of CREDBANK contains 1,377 events). These event- topics were then sent to 30 additional AMT users to determine the event-topic’s accuracy. This accuracy annotation task instructed users to assess the credibility level of the Event” by reviewing relevant tweets on Twitter’s website (see Figure 5 in Mitra and Gilbert [5]). Annotators were then asked to provide an accuracy rating on a 5-point Likert scale of factuality” (adapted from Sauri et al. [20]) from [−2, +2], where −2 represented Certainly Inaccurate” and +2 was Certainly Accurate” [5]. Annotators were required to provide a justification for their choice as well. These tweets, topics, event annotations, and accuracy annotations were published as the CREDBANK dataset.1 Data provided in CREDBANK includes the three-word topics extracted from Twitter’s sample stream, each topic’s event annotations, the resulting set of event-topics, a mapping of event-topics’ relevant tweets, and a list of the AMT accuracy annotations for each event-topic. One should note that CRED- BANK does not contains binary labels of event accuracy but instead has a 30-element vector of accuracy labels. In CREDBANK, the vast majority (> 95%) of event accu- racy annotations had a majority rating of Certainly Accurate” 1Available online http://compsocial.github.io/CREDBANK-data/","Buntain, C., & Golbeck, J. (2017, November). Automatically identifying fake news in popular twitter threads. In 2017 IEEE International Conference on Smart Cloud (SmartCloud) (pp. 208-215). IEEE."
ML_DB_024,A Context Aware Approach for Detecting CheckWorthy Claims in Political Debate,"In the context of investigative journalism, we address the problem of automatically identifying which claims in a given document are most worthy and should be prioritized for fact-checking. Despite its importance, this is a relatively understudied problem. Thus, we create a new dataset of political debates, containing statements that have been factchecked by nine reputable sources, and we train machine learning models to predict which claims should be prioritized for fact-checking, i.e., we model the problem as a ranking task. Unlike previous work, which has looked primarily at sentences in isolation, in this paper we focus on a rich input rep resentation modeling the context: relationship between the target statement and the larger context of the debate, interaction between the opponents, and reaction by the moderator and by the public. Our experiments show state-of-the-art results, outperforming a strong rivaling system by a margin, while also confirming the importance of the contextual information.",Computer Science,Article,2020,Y,N,1,"The current coverage of the political landscape in the press and in social media has led to an unprece- dented situation. Like never before, a statement in an interview, a press release, a blog note, or a tweet can spread almost instantaneously and reach the public in no time. This proliferation speed has left little time for double-checking claims against the facts, which has proven critical in politics, e.g., during the 2016 presidential campaign in the USA, which was arguably impacted by fake news in social media and by false claims. Figure 1: Information verification pipeline. Investigative journalists and volunteers have been working hard trying to get to the root of a claim and to present solid evidence in favor or against it. Manual fact-checking has proven very time- consuming, and thus automatic methods have been proposed as a way to speed-up the process. For in- stance, there has been work on checking the factu- ality/credibility of a claim, of a news article, or of an information source (Castillo et al., 2011; Ba et al., 2016; Zubiaga et al., 2016; Ma et al., 2016; Hardalov et al., 2016; Karadzhov et al., 2017a,b; Nakov et al., 2017). However, less at- tention has been paid to other steps of the fact- checking pipeline, which is shown in Figure 1. The process starts when a document is made public. First, an intrinsic analysis is carried out in which check-worthy text fragments are identi- fied. Then, other documents that might support or rebut a claim in the document are retrieved from various sources. Finally, by comparing a claim against the retrieved evidence, a system can de- termine whether the claim is likely true or likely false. For instance, Ciampaglia et al. (2015) do this on the basis of a knowledge graph derived from Wikipedia. The outcome could then be pre- sented to a human expert for final judgment.1 1As of present, fully automatic methods for fact checking still lag behind in terms of quality, and thus also of credibility in the eyes of the users, compared to what high-quality man- ual checking by reputable sources can achieve, which means that a final double-checking by a human expert is needed. arXiv:1912.08084v1 [cs.CL] 14 Dec 2019 In this paper, we focus on the first step: predicting check-worthiness of claims. Our contributions can be summarized as follows: 1. New dataset: We build a new dataset of manually-annotated claims, extracted from the 2016 US presidential and vice- presidential debates, which we gathered from nine reputable sources such as CNN, NPR, and PolitiFact, and which we release to the research community. 2. Modeling the context: We develop a novel approach for automatically predicting which claims should be prioritized for fact- checking, based on a rich input representa- tion. In particular, we model not only the textual content, but also the context: how the target claim relates to the current segment, to neighboring segments and sentences, and to the debate as a whole, and also how the op- ponents and the public react to it. 3. State-of-the-art results: We achieve state-of- the-art results, outperforming a strong rival- ing system by a margin, while also demon- strating that this improvement is due primar- ily to our modeling of the context. We model the problem as a ranking task, and we train both Support Vector Machines (SVM) and Feed-forward Neural Networks (FNN) obtain- ing state-of-the-art results. We also analyze the relevance of the specific feature groups and we show that modeling the context yields a signifi- cant boost in performance. Finally, we also ana- lyze whether we can learn to predict which facts are check-worthy with respect to each of the in- dividual media sources, thus capturing their bi- ases. It is worth noting that while trained on po- litical debates, many features of our model can be potentially applied to other kinds of information sources, e.g., interviews and news. The rest of the paper is organized as follows: Section 2 discusses related work. Section 3 de- scribes the process of gathering and annotating our political debates dataset. Section 4 presents our supervised approach to predicting fact-checking worthiness, including the explanation of the model and the information sources we use. Section 5 presents the evaluation setup and discusses the re- sults. Section 6 provides further analysis. Finally, Section 7 presents the conclusions and outlines some possible directions for future research. 2 Related Work The previous work that is most relevant to our work here is that of (Hassan et al., 2015), who developed the ClaimBuster system, which assigns each sentence in a document a score, i.e., a num- ber between 0 and 1 showing how worthy it is of fact-checking. The system is trained on their own dataset of about eight thousand debate sentences (1,673 of them check-worthy), annotated by stu- dents, university professors, and journalists. Un- fortunately, this dataset is not publicly available and it contains sentences without context as about 60% of the original sentences had to be thrown away due to lack of agreement. In contrast, we develop a new publicly-available dataset,2 based on manual annotations of politi- cal debates by nine highly-reputed fact-checking sources, where sentences are annotated in the con- text of the entire debate. This allows us to explore a novel approach, which focuses on the context. Note also that the ClaimBuster dataset is an- notated following guidelines from (Hassan et al., 2015) rather than a real fact-checking website; yet, it was evaluated against CNN and PolitiFact (Has- san et al., 2016). In contrast, we train and evaluate directly on annotations from fact-checking web- sites, and thus we learn to fit them better. Beyond the document context, it has been pro- posed to mine check-worthy claims on the Web. For example, Ennals et al. (2010a) searched for linguistic cues of disagreement between the author of a statement and what is believed, e.g., falsely claimed that X”. The claims matching the patterns go through a statistical classifier, which marks the text of the claim. This procedure can be used to acquire a dataset of disputed claims from the Web. Given a set of disputed claims, (Ennals et al., 2010b) approached the task as locating new claims on the Web that entail the ones that have already been collected. Thus, the task can be conformed as recognizing textual entailment, which is analyzed in detail in (Dagan et al., 2009). Finally, Le et al. (2016) argued that the top terms in claim vs. non-claim sentences are highly overlapping, which is a problem for bag-of-words approaches. Thus, they used a Convolutional Neu- ral Network, where each word is represented by its embedding and each named entity is replaced by its tag, e.g., person, organization, location. 2The dataset and the source code are available in GitHub: https://github.com/pgencheva/claim-rank","Gencheva, P., Koychev, I., Màrquez, L., Barrón-Cedeño, A., & Nakov, P. (2019). A Context-Aware Approach for Detecting Check-Worthy Claims in Political Debates. arXiv preprint arXiv:1912.08084."
ML_DB_028,Linked credibility reviews for explainable misinformation detection,"In recent years, misinformation on the Web has become in creasingly rampant. The research community has responded by propos ing systems and challenges, which are beginning to be useful for (various subtasks of) detecting misinformation. However, most proposed systems are based on deep learning techniques which are finetuned to specific domains, are difficult to interpret and produce results which are not machine readable. This limits their applicability and adoption as they can only be used by a select expert audience in very specific settings. In this paper we propose an architecture based on a core concept of Credibility Reviews (CRs) that can be used to build networks of dis tributed bots that collaborate for misinformation detection. The CRs serve as building blocks to compose graphs of (i) web content, (ii) ex isting credibility signals –factchecked claims and reputation reviews of websites–, and (iii) automatically computed reviews. We implement this architecture on top of lightweight extensions to Schema.org and services providing generic NLP tasks for semantic similarity and stance detec tion. Evaluations on existing datasets of socialmedia posts, fake news and political speeches demonstrates several advantages over existing systems: extensibility, domainindependence, composability, explainability and transparency via provenance. Furthermore, we obtain competitive results without requiring finetuning and establish a new state of the art on the Clef’18 CheckThat! Factuality task.",Computer Science,Proceeding,2020,N,Y,10,"Technical solutions for computer-aided misinformation detection and fact- checking have recently been proposed[1,6] and are essential due to the scale of the Web. However, a lack of hand-curated data, maturity and scope of cur- rent AI systems, means assessing veracity[12] is not feasible. Hence the value of the current systems is not so much their accuracy, but rather their capacity of retrieving potentially relevant information that can help human fact-checkers, who are the main intended users of such systems, and are ultimately responsible for verifying/filtering the results such systems provide. Therefore, a main chal- lenge is developing automated systems which can help the general public, and influencers in particular, to assess the credibility of web content, which requires explainable results by AI systems. This points towards the need for hybrid ap- proaches that enable the use of the best of deep learning-based approaches, but also of symbolic knowledge graphs to enable better collaboration between large platforms, fact-checkers, the general public and other stakeholders like policy- makers, journalists, webmasters and influencers. In this paper, we propose a design on how to use semantic technologies to aid in resolving such challenges. Our contributions are: – a datamodel and architecture of distributed agents for composable credi- bility reviews, including a lightweight extension to schema.org to support provenance and explainability (section 3) – an implementation of the architecture demonstrating feasibility and value (section 4) – an evaluation on various datasets establishing state-of-the-art in one dataset (Clef’18 CheckThat! Factuality task) and demonstrating capabilities and limitations of our approach, as well as paths for improvements (section 5) 2 Related Work The idea of automating (part of) the fact-checking process is relatively recent[1]. ClaimBuster[6] proposed the first automated fact-checking system and its archi- tecture is mostly still valid, with a database of fact-checks and components for monitoring web sources, spotting claims and matching them to previously fact- checked claims. Other similar services and projects include Truly media2, invid3 and CrowdTangle4. These systems are mainly intended to be used by professional fact-checkers or journalists, who can evaluate whether the retrieved fact-check article is relevant for the identified claim. These automated systems rarely aim to predict the accuracy of the content; this is (rightly) the job of the journalist or fact-checker who uses the system. Many of these systems provide valuable REST APIs to access their services, but as they use custom schemas, they are difficult to compose and inspect as they are not machine-interpretable or explainable. Linked Credibility Reviews for Explainable Misinformation Detection 3 Besides full-fledged systems for aiding in fact-checking, there are also various strands of research focusing on specific computational tasks needed to identify misinformation or assess the accuracy or veracity of web content based on ground credibility signals. Some low-level NLP tasks include check-worthiness[11] and stance detection[15,14], while others aim to use text classification as a means of detecting deceptive language[13]. Other tasks mix linguistic and social me- dia analysis, for example to detect and classify rumours[20]. Yet others try to assess veracity of a claim or document by finding supporting evidence in (semi)structured data[18]. These systems, and many more, claim to provide im- portant information needed to detect misinformation online, often in some very specific cases. However without a clear conceptual and technical framework to integrate them, the signals such systems provide are likely to go unused and stay out of reach of users who are exposed to misinformation. The Semantic Web and Linked Data community has also started to con- tribute ideas and technical solutions to help in this area: perhaps the biggest impact has been the inclusion in Schema.org[5] of the ClaimReview markup5, which enables fact-checkers to publish their work as machine readable struc- tured data. This has enabled aggregation of such data into knowledge graphs like ClaimsKG [17], which also performs much needed normalisation of labels, since each fact-checker uses its own set of labels. A conceptual model and RDF vocabulary was proposed to distinguish between the utterance and propositional aspects of claims [2]. It allows expressing fine-grained provenance (mainly of an- notations on the text of the claim), but still relies on ClaimReview as the main accuracy describing mechanism. It is unclear whether systems are actually using this RDF model to annotate and represent claims as the model is heavyweight and does not seem to align well with mainstream development practices. In this paper, we build on these ideas to propose a lightweight model which focuses on the introduction of a new type of Schema.org Review that focuses on credibility rather than factuality6. The focus on credibility, defined as an estimation of factuality based on avail- able signals or evidence, is inspired by MisinfoMe[10,9] which borrows from social science, media literacy and journalism research. MisinfoMe focuses on credibility of sources, while we expand this to credibility of any web content and integrate some of MisinfoMe’s services in our implementation to demonstrate how our approach enables composition of such services. There is also ongoing work on W3C Credibilty Signals7, which aims to define a vocabulary to specify credibility indicators that may be relevant for assessing the credibility of some web content. To the best of our knowledge, this is still work in progress and no systems are implementing the proposed vocabularies. 5 https://www.blog.google/products/search/fact-check-now-available-google-search-and-news-around-world/ 6 In our opinion, current AI systems cannot truly assess veracity since this requires human skills to access and interpret new information and relate them to the world. 7 https://credweb.org/signals-beta/ 4 R. Denaux and JM Gomez-Perez 3 Linked Credibility Reviews This section presents Linked Credibility Reviews (LCR), our proposed linked data model for composable and explainable misinformation detection. As the name implies, Credibility Review s (CR) are the main resources and outputs of this architecture. We define a CR as a tuple 〈d, r, c, p〉, where the CR: – reviews a data item d, this can be any linked-data node but will typically refer to articles, claims, websites, images, social media posts, social media accounts, people, publishers, etc. – assigns a credibility rating r to the data item under review and qualifies that rating with a rating confidence c. – provides provenance information p about: • credibility signals used to derive the credibility rating. Credibility Signals (CS) can be either (i) CRs for data items relevant to the data item under review or (ii) ground credibility signals (GCS) resources (which are not CRs) in databases curated by a trusted person or organization. • the author of the review. The author can be a person, organization or bot. Bots are automated agents that produce CRs for supported data items based on a variety of strategies, discussed below. The credibility rating is meant to provide a subjective (from the point-of- view of the author) measure of how much the credibility signals support or refute the content in data item. Provenance information is therefore crucial as it allows humans —e.g. end-users, bot developers— to retrace the CRs back to the ground credibility signals and assess the accuracy of the (possibly long) chain of bots (and ultimately humans) that were involved in reviewing the initial data item. It also enables the generation of explanations for each step of the credibility review chain in a composable manner as each bot (or author) can describe its own strategy to derive the credibility rating based on the used credibility signals. Bot Reviewing Strategies CR bots are developed to be able to produce CRs for specific data item types. We have identified a couple of generic strategies that existing services seem to implement and which can be defined in terms of CRs (these are not exhaustive, though see figure 2 for a depiction of how they can collaborate): – ground credibility signal lookup from some trusted source. CR bots that implement this strategy will (i) generate a query based on d and (ii) convert the retrieved ground credibility signal into a CR; – linking the item-to-review d with n other data items d′ i of the same type, for which a CRd′ i is available. These bots define functions fr , fc and fagg. The first two, compute the new values ri and ci based on the original values and the relation or similarity between d and d′ i i.e. ri = fr (CRd′ i , d, d′). These produce n credibility reviews, CRi d, which are then aggregated into CRd = fagg({CRi d | 0 ≤ i < n}). Linked Credibility Reviews for Explainable Misinformation Detection 5 – decomposing whereby the bot identifies relevant parts d′ i of the item-to- review d and requests CRs for those parts CRd′ i . Like the linking bots, these require deriving new credibility ratings CRdi and confidences based on the relation between the whole and the parts; and aggregating these into the CR for the whole item. The main difference is that the parts can be items of different types. Representing and Aggregating Ratings For ease of computation, we opt to rep- resent credibility ratings and their confidences as follows: – r ∈ <, must be in the range of [−1.0, 1.0] where −1.0 means not credible and 1.0 means credible – c ∈ <, must be in the range of [0.0, 1.0] where 0.0 means no confidence at all and 1.0 means full confidence in the accuracy of r, based on the available evidence in p. This representations makes it possible to define generic, relatively straight- forward aggregation functions like: – f mostConfident which selects CRi which has the highest confidence value c – f leastCredible which selects the CRi which has the lowest value r 3.1 Extending schema.org for LCR While reviewing existing ontologies and vocabularies which could be reused to describe the LCR model, we noticed that schema.org[5] was an excellent start- ing point since it already provides suitable schema types for data items on the web for which credibility reviews would be beneficial (essentially any schema type that extends CreativeWork). It already provides suitable types for Review, Rating, as well as properties for expressing basic provenance information and meronymy (hasPart). Some basic uses and extensions compliant with the orig- inal definitions are: – Define CredibilityReview as an extension of schema:Review, whereby the schema:reviewAspect is credibility8 – use schema:ratingValue to encode r – add a confidence property to schema:Rating which encodes the rating confidence c. – use isBasedOn to record that a CR was computed based on other CRs. We also use this property to describe dependencies between CR Bots, even when those dependencies have not been used as part of a CR. – use author to link the CR with the bot that produced it The main limitation we encountered with the existing definitions was that CreativeWorks (including Reviews) are expected to be created only by Persons or Organizations, which excludes reviews created by bots. We therefore propose to extend this definition by: 8 Note that ClaimReview is not suitable since it is overly restrictive: it can only review Claims (and it assumes the review aspect is, implicitly, accuracy). 6 R. Denaux and JM Gomez-Perez – introducing a type Bot which extends SoftwareApplication – allowing Bots to be the authors of CreativeWorks. Finally, in this paper we focus on textual misinformation detection and found we were missing a crucial type of CreativeWork, namely Sentences. Recently, a Claim type was proposed, to represent factually-oriented sentences and to work in tandem with the existing ClaimReview, however, automated systems still have trouble determining whether a sentence is a claim or not, therefore, CR bots should be able to review the credibility of Sentences and relevant aspects between pairs of sentences such as their stance and similarity. The overall schema.org based data model is depicted in figure 1, focused on CRs for textual web content (we leave other modalities as future work). Fig. 1: Linked Credibility Review data model, extending schema.org. 4 acred – Deep Learning-based CR bots To demonstrate the potential of the Linked Credibility Review architecture, we have implemented a series of CR bots capable of collaborating to review articles, tweets, sentences and websites.9 We present the conceptual implementation in sections 4.1 to 4.4 and provide further details in section 4.5. 4.1 Ground Credibility Signal Sources Ultimately we rely on two ground credibility signal sources: – A database of ClaimReviews which provide accuracy ratings for factual claims by a variety of fact-checkers. 9 The source code is available at https://github.com/rdenaux/acred Linked Credibility Reviews for Explainable Misinformation Detection 7 – Third-party, well-established services for validating WebSites, such as News- Guard and Web Of Trust10, which rely on either expert or community-based ratings. 4.2 GCS Lookup Bots The two GCS sources are mediated via two GCS lookup bots. The LookupBotClaimReview returns a CR for a Claim based on a ClaimReview from the database. In order to derive a CR from a ClaimReview, the accu- racy rating in the ClaimReview need to be converted into equivalent credibil- ity ratings. The challenge here is that each fact-checker can encode their re- view rating as they see fit. The final review is typically encoded as a textual alternateName, but sometimes also as a numerical ratingValue. ClaimsKG al- ready performs this type of normalisation into a set of “veracity” labels, but for other ClaimReviews we have developed a list of simple heuristic rules to assign a credibility and confidence score. The LookupBotWebSite returns a CR for a WebSite. This is a simple wrapper around the existing MisinfoMe aggregation service [10], which already produces credibility and confidence values. 4.3 Linking Bots Although the GCS lookup bots provide access to the basic credibility signals, they can only provide this for a relatively small set of claims and websites. Misinformation online often appears as variations of fact-checked claims and can appear on a wide variety of websites that may not have been reviewed yet by a human. Therefore, to increase the number of sentences which can be reviewed, we developed the following linking bots (see Section 4.5 for further details). The LinkBotPreCrawled Sentence uses a database of pre-crawled sentences extracted from a variety of websites. A proprietary NLP system11 extracts the most rele- vant sentences that may contain factual information in the crawled documents. This is done by identifying sentences that (i) are associated with a topic (e.g. Politics or Health) and (ii) mentions an entity (e.g. a place or person). The CR for the extracted sentence is assigned based on the website where the sentence was found (i.e. by using the LookupBotWebSite). Since not all sentences published by a website are as credible as the site, the resulting CR for the sentence has a lower confidence than the CR for the website itself. The LinkBotSemSim Sentence is able to generate CRs for a wide variety of sentences by linking the input sentence si to sentences sj for which a CR is available (via other bots). This linking is achieved by using a neural sentence encoder fenc : S 7 → <d –where S is the set of sentences and d ∈ N +–, that is optimised to encode semantically similar sentences close to each other in an embedding space. The bot creates an index by generating embeddings for all the sentences reviewed 10 https://www.newsguardtech.com/, https://www.mywot.com/ 11 http://expert.ai 8 R. Denaux and JM Gomez-Perez by the LookupBotClaimReview and the LinkBotPreCrawled Sentence . The incoming sentence, si is encoded and a nearest neighbor search produces the closest matches along with a similarity score based on a similarity function fsim : <d × <d 7 → <[0,1]. Unfortunately, most sentence encoders are not polarity aware so that negations of a phrase are considered similar to the original phrase; therefore we use a second neural model for stance detection fstance : S × S 7 → SL, where SL is a set of stance labels. We then define fpolarity : SL 7 → {1, −1}, which we use to invert the polarity of rj if si disagrees with sj . We also use the predicted stance to revise the similarity score between si and sj by defining a function freviseSim : SL, <[0,1] 7 → <[0,1]. For example, stances like unrelated or discuss may reduce the estimated similarity, which is used to revise the confidence of the original credibility. In summary, the final CR for si is selected via f mostConfident from a pool of CRi,j for the matching sj ; where the rating and confidences for each CRi,j are given by: ri = rj × fpolarity(fstance(si, sj )) ci = cj × freviseSim ( fstance(si, sj ), fsim (fenc(si), fenc(sj ))) 4.4 Decomposing Bots By combining linking and GCS lookup bots we are already capable of review- ing a wide variety of sentences. However, users online encounter misinformation in the form of high-level CreativeWorks like social media posts, articles, im- ages, podcasts, etc. Therefore we need bots which are capable of (i) dissecting those CreativeWorks into relevant parts for which CRs can be calculated and (ii) aggregating the CRs for individual parts into an overall CR for the whole CreativeWork. In acred, we have defined two main types: – DecBotArticle reviews Articles, and other long-form textual CreativeWorks – DecBotSocMedia reviews SocialMediaPosts In both bots, decomposition works by performing NLP and content analysis on the title and textual content of the CreativeWork di. This results in a set of parts P = {dj } which include Sentences, linked Articles or SocialMediaPosts and metadata like the WebSite where d was published. Each of these can be analysed either recursively or via other bots, which results in a set of reviews {CRj } for the identified parts. We define a function fpart which maps CRj onto CRi,j, which takes into account the relation between di and dj as well as the provenance of CRj to derive the credibility rating and confidence. The final CRi is selected from all CRi,j via f leastCredible. Figure 2 shows a diagram depicting how the various CR bots compose and collaborate to review a tweet. 4.5 Implementation details Our database of ClaimReviews contains 45K claims and was based on public resources such as ClaimsKG [17] (32K), data-commons (9.6K) and our in-house Linked Credibility Reviews for Explainable Misinformation Detection 9 Fig. 2: Depiction of acred bots collaborating to produce a CR for a tweet. developed crawlers (4K). The database of pre-crawled sentences contained 40K sentences extracted from a variety of generic news sites on-line between april 2019 and april 2020. It consisted primarily in relatively well-regarded news sites like expressen.se, krone.at, zdf.de, which combined for about 35K of the sentences, and a long tail of other sites including theconversation.com and heartland.org. For reproducibility, we will publish the list of sentences along associated URLs. Our heuristic rules to normalise ClaimReviews are implemented in about 50 lines of python to map numeric ratingValues (in context of specified bestRating and worstRating values) and 150 lines of python to map about 100 alternateName values (e.g “inaccurate”, “false and misleading”, “this is exaggerated”) and about 20 patterns (e.g. “wrong.*”, “no, *”) into estimated c, r values. The sentence encoder, fenc in LinkBotSemSim Sentence is implemented as a RoBERTa- base [7] model finetuned on STS-B [4]. We employ a siamese structure as this enables us to perform encoding of the claims off-line (slow) and comparison on-line (fast) at the cost of some accuracy. Our model achieves 83% Pearson correlation on STS-B dev. The stance detector, fstance, is also a RoBERTa-base model trained on FNC- 1 [14], which assigns a stance label (either “agree”, “disagree”, “discuss” or “unrelated”) to pairs of texts: about 50K for training and about 25K for testing. Our model achieves 92% accuracy on the held-out test set. See our GitHub repository for links to model weights and jupyter notebooks replicating our fine- tuning procedure. Each implemented bot defines a set of templates to generate textual expla- nations. These reflect processing performed by the bot in a way that can be inspected by a user. Produced CRs use the schema:ratingExplanation prop- erty to encode the generated explanations and use markdown to take advantage of hypertext capabilities like linking and formatting of the explanations. Exam- ples are presented in table 1. Different CR bots are deployed as separate Docker images and expose a REST API accepting and returning JSON-LD formatted requests and responses. They are all deployed on a single server (64GB RAM, Intel i7-8700K CPU @ 3.70GHzn with 12 cores) via docker-compose. The ClaimReview and pre-crawled sentence databases are stored in a Solr instance. The index of encoded sentences is generated off-line on a separate server with a GPU by iterating over the claims and sentences in Solr, and loaded into memory on the main server at run-time. 5 Evaluation One of the main characteristics of the LCR architecture is that CR bots can be distributed across different organizations. This has the main drawback that it can be more difficult to fine-tune bots to specific domains since top-level bots do not have direct control on how lower-level bots are implemented and fine-tuned. Therefore in this paper we first evaluated our acred implementation, described above, on a variety of datasets covering social media posts, news articles and political speeches. Our rationale is that existing, non-distributed fact-checking approaches have an edge here as they can fine-tune their systems based on train- ing data and therefore provide strong baselines to compare against. We used the explanations, along with the provenance trace, to perform error analysis12 on the largest dataset, described in Section 5.2. This showed acred was overly con- fident in some cases. To address this, we introduced a modified version, acred+ with custom functions to reduce the confidence and rating values of two bots under certain conditions: DecBotArticle when based only on a website credibility; LinkBotSemSim Sentence when the stance is “unrelated” or “discuss”. 5.1 Datasets The first dataset we use is the Clef’18 CheckThat! Factuality Task [11] (clef18). The task consists in predicting whether a check-worthy claim is either true, half-true or false. The dataset was derived from fact-checked political debates and speeches by factcheck.org. For our evaluation we only use the English part of this dataset13 which contains 74 and 139 claims for training and testing. FakeNewsNet [16] aims to provide a dataset of fake and real news enriched with social media posts and context sharing those news. In this paper we only use the fragment derived from articles fact-checked by Politifact that have tex- tual content, which consists of 420 fake and 528 real articles. The articles were retrieved by following the instructions on the Github page14. 12 Note that usability evaluation of the generated explanations is not in the scope of this paper. 13 Our implementation has support for machine translation of sentences, however this adds a confounding factor hence we leave this as future work. 14 https://github.com/KaiDMML/FakeNewsNet, although we note that text for many of the articles could no longer be retrieved, making a fair comparison difficult. Linked Credibility Reviews for Explainable Misinformation Detection 11 Table 1: Example explanations generated by our bots. Bot Example explanation LookupBotClaimRev Claim ’Ford is moving all of their small-car productin to Mexico.’ is mostly not credible based on a fact-check by politifact with normalised numeric ratingValue 2 in range [1-5] LookupBotWebSite Site www.krone.at seems mostly credible based on 2 review(s) by ex- ternal rater(s) NewsGuard or Web Of Trust LinkBotPreCrawled Sentence Sentence Now we want to invest in the greatest welfare program in modern times. seems credible as it was published in site www.expressen.se. (Explanation for WebSite omitted) LinkBotSemSim Sentence Sentence When Senator Clinton or President Clinton asserts that I said that the Republicans had had better economic policies since 1980, that is not the case. seems not credible as it agrees with sentence: Obama said that ’since 1992, the Republicans have had all the good ideas...’ that seems not credible based on a fact-check by politifact with textual rating ’false’. Take into account that the sentence appeared in site www.cnn.com that seems credible based on 2 review(s) by external rater(s) NewsGuard or Web Of Trust LinkBotSemSim Sentence Sentence Can we reduce our dependence on foreign oil and by how much in the first term, in four years? is similar to and discussed by: Drilling for oil on the Outer Continental Shelf and in parts of Alaska will ’immediately reduce our dangerous dependence on foreign oil.’ that seems not credible, based on a fact-check by politifact with textual rating ’false’. DecBotArticle Article “Part 1 of CNN Democratic presidential debate” seems not credible based on its least credible sentence. (explanation for sentence CR omitted) DecBotSocMedia Sentence Absolutely fantastic, there is know difference between the two facist socialist powers of todays EU in Brussels, and the yesteryears of Nazi Germany in tweet agrees with: ’You see the Nazi platform from the early 1930s ... look at it compared to the (Democratic Party) platform of today, you’re saying, ’Man, those things are awfully similar.’’ that seems not credible based on a fact-check by politifact with textual claim-review rating ’false’” 12 R. Denaux and JM Gomez-Perez Finally, coinform25015 is a dataset of 250 annotated tweets. The tweets and original labels were first collected by parsing and normalising ClaimReviews from datacommons and scraping fact-checker sites using the MisinfoMe data collector [9,10]. Note that acred’s collection system is not based on MisinfoMe16. The original fact-checker labels were mapped onto six labels (see table 2) by 7 human raters achieving a Fleiss κ score of 0.52 (moderate agreement). The fine- grained labels make this a challenging but realistic dataset. Table 2: Mapping of credibility ratingValue r and confidence c for coinform250. label r c credible r ≥ 0.5 c > 0.7 mostly credible 0.5 > r ≥ 0.25 c > 0.7 uncertain 0.25 > r ≥ −0.25 c > 0.7 mostly not credible −0.25 > r ≥ −0.5 c > 0.7 not credible −0.5 > r c > 0.7 not verifiable any c ≤ 0.7 For each dataset our pre- diction procedure consisted in steps to (i) read sam- ples, (ii) convert them to the appropriate schema.org data items (Sentence, Article or SocialMediaPost), (iii) re- quest a review from the ap- proriate acred CR bot via its REST API; (iv) map the pro- duced CR onto the dataset la- bels and (v) optionally store the generated graph of CRs. For clef18 we set t = 0.75, so that r >= t has label TRUE, r <= −0.75 has label FALSE and anything in between is HALF-TRUE. See table 2 for coinform250 threshold definitions. 5.2 Results On clef18, acred establishes a new state-of-the-art result as shown in table 3, achieving 0.6835 in MAE, the official metric in the competition [11]. This result is noteworthy as, unlike the other systems, acred did not use the training set of clef18 at all to finetune the underlying models. With acred+, we further improved our results achieving 0.6475 MAE. Table 4: Results on FakeNewsNet Politifact com- pared to baselines that only use article content. System Accuracy Precision Recall F1 acred 0.586 0.499 0.823 0.622 acred+ 0.716 0.674 0.601 0.713 CNN 0.629 0.807 0.456 0.583 SAF/S 0.654 0.600 0.789 0.681 On FakeNewsNet, acred+ obtained state of the art results and acred obtained competitive results in line with strong baseline systems reported in the original pa- per [16], shown in table 4. We only consider as baselines systems which only use the article content, since acred does not use credibility re- views based on social context 15 https://github.com/co-inform/Datasets 16 acred’s data collector is used to build the ClaimReview database described in Sect. 4; it does not store the itemReviewed URL values; only the claimReviewed strings. Linked Credibility Reviews for Explainable Misinformation Detection 13 Table 3: Results on clef18 English test dataset compared to baselines. The bottom rows shows results on the English training set. system MAE Macro MAE Acc Macro F1 Macro AvgR acred 0.6835 0.6990 0.4676 0.4247 0.4367 acred+ 0.6475 0.6052 0.3813 0.3741 0.4202 Copenhagen[19] 0.7050 0.6746 0.4317 0.4008 0.4502 random[11] 0.8345 0.8139 0.3597 0.3569 0.3589 acred “training” 0.6341 0.7092 0.4878 0.4254 0.4286 acred+ “training” 0.6585 0.6386 0.4024 0.3943 0.4020 yet. Note that baselines used 80% of the data for training and 20% for testing, while we used the full dataset for testing. We performed a manual error analysis on the acred results for FakeNews- Net17: – 29 errors of fake news predicted as highly credible (r ≥ 0.5): 16 cases (55%) were due to acred finding pre-crawled sentence matches in that appeared in snopes.com, but not ClaimReviews for that article. A further 7 cases (24%) were due to finding unrelated sentences and using the WebSiteCR where those sentences appeared, while being over-confident about those credibili- ties. – 41 errors of fake news predicted with low-confidence (c <= 0.7). 30 of these cases (73%) are due to the FakeNewsNet crawler as it fails to retrieve valid content for the articles: GDPR or site-for-sale messages instead of the origi- nal article content. In these cases, acred is correct in having low-confidence credibility ratings. In the remaining 27% of cases, acred indeed failed to find evidence to decide on whether the article was credible or not. – 264 real articles were rated as being highly not credible (r < −0.5). This is by far the largest source of errors. We manually analysed 26 of these, chosen at random. In 13 cases (50%), the real stance should be unrelated, but is predicted as discussed or even agrees; often the sentences are indeed about a closely related topics, but still about unrelated entities or events. In a further 7 cases (27%) the stance is correctly predicted to be unrelated, but the confidence still passess the threshold. Hence 77% of these errors are due to incorrect or overconfident linking by the LinkBotSemSim Sentence. – 48 real articles were rated as being somewhat not credible (−0.25 < r ≤ 0.25), in a majority of these cases, the r was obtained from LinkBotPreCrawled Sentence rather than from LinkBotSemSim Sentence Finally, for the coinform250 dataset, acred+ obtains 0.279 accuracy which is well above a baseline of random predictions, which obtains 0.167 accuracy. The confusion matrix shown in figure 3f shows that the performance is in line with that shown for FakeNewsNet (fig.3d) and clef18 (fig.3e). It also shows 17 As stated above, we used the results of this analysis to inform the changes imple- mented in acred+ 14 R. Denaux and JM Gomez-Perez that acred tends to be overconfident in its predictions, while acred+ is more cautious. (a) FakeNewsNet (b) clef18 (test) (c) coinform250 (d) FakeNewsNet (e) clef18 (test) (f) coinform250 Fig. 3: Confusion matrices for acred (top) and acred+ (bottom row) in evalua- tion datasets. We use ≈ for mostly and ¬ for not in the coinform250 labels. 5.3 Discussion The Good Our approach obtains competitive results in challenging datasets; these results are especially impressive when you take into account that we do not train or fine-tune our underlying models on these datasets. With acred+, we also showed we can substantially improve results by performing some simple optimization of aggregation functions; however doing this in a systematic manner is not in the scope of this paper where we are focusing on validating the LCR design. Since the results were consistent across the different datasets, this shows that our network of bots have certain domain independence and validate our design for composable CR bots as our lookup and linking bots can successfully be reused by the high-level decomposing bots. We think this is largely due to our","Denaux, R., & Gomez-Perez, J. M. (2020, November). Linked credibility reviews for explainable misinformation detection. In International Semantic Web Conference (pp. 147-163). Springer, Cham."
ML_DB_029,TRUSTD: Combat Fake Content using Blockchain and Collective Signature Technologies,"The growing trend of sharing news/contents, through social media platforms and the World Wide Web has been seen to impact our perception of the truth, altering our views about politics, economics, relationships, needs and wants. This is because of the growing spread of misinformation and disinformation intentionally or unintentionally by individuals and organizations. This trend has grave political, social, ethical, and privacy implications for society due to 1) the rapid developments in the field of machine learning (ML) and deep learning (DL) algorithms in creating realistic looking yet fake digital content (such as text, images, and videos), 2) the ability to customize the content feeds and to create a polarized so called filter bubbles” leveraging the availability of the bigdata. Therefore, there is an ethical need to combat the flow of fake content. This paper attempts to resolves some of the aspects of this combat by presenting a high level overview of TRUSTD, a blockchain and collective signature based ecosystem to help content creators in getting their content backed by the community, and to help users judge on the credibility and correctness of these contents.",Computer Science,Proceeding,2020,N,Y,5,"Fake news has become increasingly one of the main threats to democracy, journalism, and freedom of expression. It has weakened public trust in governments and its potential impact on the contentious Brexit referendum and the equally divisive 2016 U.S. presidential election is yet to be realized [1]. Our economies are also affected by the spread of fake news, with fake news being connected to stock market fluctuations and massive trades. For example, fake news claiming that Barack Obama was injured in an explosion wiped out $130 billion in stock value 1. The news is a newly received or noteworthy information, especially about recent not-necessarily political events. De- ception can be described as an act of intentionally causing another person to have or continue to have a false belief that is truly believed to be false by the person intentionally causing 1https://www.forbes.com/sites/kenrapoza/2017/02/26/can-fake-news- impact-the-stock-market the false belief by bringing about evidence on the basis of which the other person has or continues to have that false belief [2]. With the ease of sharing information on social media platforms, the rapid accessibility of uploaded content on the World Wide Web, and the rapidly progressing fields of Artificial Intelligence (AI) and Machine Learning (ML), deception has far more potential in altering an individual’s perception and influencing their decisions and choices than it has ever had. In this context, various issues can be identified: 1 - Fake content can have a strong impact on listeners and readers: Experts distinguish between misinformation and disinformation [3]. Misinformation is a false or misleading piece of information —e.g., sharing a fraudulent content with- out verifying the authenticity of its source is misinformation. Disinformation is deliberately falsified information to obscure the truth —e.g., spreading false content with the intention to harm an individual’s reputation. In this paper we refer to the misinformation, disinformation and artificially generated content as deceptive artifacts or fake content. Fake content is a general term which would include the term fake news. Throughout the paper, we will use the terms fake news and fake content interchangeably. Repeated exposure to a piece of information makes it familiar, until it is eventually perceived as acceptable and valid. This phenomenon is known as the illusory truth effect [2]. 2- Individuals are victims of the filter bubbles: Social media websites, including Facebook, Twitter and LinkedIn are said to construct filter bubbles” that allows users to only view content they agree with or that aligns with their pre-existing beliefs [4]. A filter bubble is an individual’s personal and unique online space, the nature of which depends on their online identity (e.g. preferences, behaviour, believes, etc.). Individuals may not have full control on the construction of this identity. In addition, individuals have no say regarding what penetrates into the space and what gets filtered out. Therefore, individuals are now victims of the algorithms that arXiv:2008.13632v1 [cs.CR] 28 Aug 2020 2 these centralised platforms use to filter and sort the content (content feed). There is no guarantee that these algorithms are not designed to favor one content over another in order to maximise the revenue or achieve a specific objective (e.g. Cambridge Analytica scandal). 3- Lack of transparency and traceability: Anyone has the freedom to publish any content on any social media platform without any scrutiny about the correctness or the credibility of the content. Individuals do not have any mechanism to trace back the originator of the content. In addition, if the content is published in a news agency Website, individuals do not have any clue who has approved that content and what is the approval process. 4- Lack of empowering tools: There is a debate whether the centralised social media companies should intervene in controlling the political adverts on their platforms or not. Some people argue in favour of banning these adverts, while others argue that these private companies should not make the decision on behalf of the user and these adverts should be shown to the user, and it is up to the user to make their mind. We have two issues with this argument: 1) since it is an advertisement platform, their algorithm could be designed to allow these adverts to reach to the users in a way that maximises the profit and maybe maximises the intended impact on the users, 2) there is no helpful tools for the users to assess the credibility and the correctness of the content. The users are making their own judgement on any content in an ad-hoc manner. Verifying the authenticity and credibility of every content is a daunting task for the users. That is why some users prefer to trust a few sources of news/information. In order to remedy to the above issues we propose TRUSTD, an ecosystem that allows content readers (users) to assess the correctness and credibility of the content they receive. We believe that there is a need to empower the user with a tool that allows them to delegate the task of verifying the content to a set of trusted parties of their own choice. That way, users will be able to assess the credibility of the content themselves based on their level of trustiness of each of their trusted parties. In addition, since this is a global issue, TRUSTD is a decentralised and open platform where everyone is able to join the system. Today, Distributed Ledger Technologies (DLTs) and specifi- cally blockchain, the Decentralised Identifiers (DIDs) [5], and collective signature [6] present opportunities for stakeholders and policymakers as potential technologies that can help to combat digital deception. These technologies enable security and trust in a decentralised Peer-to-Peer (P2P) network without any central managing authority. There are only a few articles of the literature that use blockchain to combat digital deception and they are mostly focused on tracing the source of the information. To the knowledge of the authors, this is the first article that proposes a user-centred approach to empower the user with the a tool to identify digital deception using the above technologies. In this paper, we are focusing on the human element when addressing the fake content issue. In other words, it is true that the machines are able to detect the fake content to some extent but there is no replacement to the human intervention. In addition, TRUSTD does not inhibit the current existing automated tools to detect the fake content, these deep tools can be included as part of the user policy rather than rely only on the quality of these sophisticated tools. The rest of this paper is organized as follows. Section II reviews the state-of-the-art of current digital deception and the involved technologies and approaches used to combat it. Section III provides a background on the Schnorr signing and how it is used in the collective signing algorithm. In Section IV, we list the main driving requirements that drive the design of TRUSTD. Section V describes the details of the proposed architecture and its implementation. Finally, Section VI is devoted to conclusions and future work. II. RELATED WORK Recently, Wikipedia co-founder Jimmy Wales has launched WikiTribune2, a platform to combat the low-quality con- tent. This platform is designed for small, niche communities that can sustain themselves where almost everything on the platform is editable (similar to Wikipedia) and the users are responsible to correct the contents even those written by other users. However, this might not be an effective approach to combat the fake content as this platform can be turned into a source of fake news. Governments worldwide are taking various measures to prevent the widespread of fake news, which may be driven by different motivations and which may undermine national security [7]. These measures include the introduction of new legislation e.g. 1) new laws that would give governments more powers to hold technology companies (e.g., Facebook, Twitter and Google) and individuals accountable for the spread of fake news, and 2) new laws that would seek to counter the impact of automated social media accounts (bots). Although these laws are important and in the right direction, they hold the technology companies accountable of identifying the fake news on behalf of the content readers (users). In this respect, users have to trust that these companies implement adequate algorithms to remove the fake news as quickly as possible. However, this is not enough because the user has to trust the criteria followed by these algorithms to identify the fake news. The user should be empowered by tools to allow them to specify their trust policy in order to determine the credibility of the content. In response, technology companies are defending them- selves and are introducing mechanisms to detect and remove fake news. For example, Facebook Journalism Project aims to collaborate with content organizations and journalism experts to improve the quality of information shared on the platform [8]. Facebook tries to provide an improved ranking system for posts shown in the News Feed and expediting the reporting of misleading content. Google has presented a white paper outlining measures to prevent the spread of disinformation through their products [9]. The objective is to enhance its search result ranking system by ranking news on the basis of expertise, credibility and authority. Although these initiatives are necessary, they still do not position the user in the center. The user should be able to make their own judgement on the 2https://wt.social/ 3 content based on their own criteria. We believe the fake news problem cannot be solved by automated tools only; and these tools should be complemented by manual verification process conducted by humans. The IEEE Global Initiative has published a report titled Ethically Aligned Design” [10] on guiding and encouraging the development of autonomous systems that are primarily focused on human well-being and protecting human rights through preventing misuse of AI, ensuring system transparency and developing a framework for developer accountability. The goal of The IEEE Global Initiative is that Ethically Aligned Design will provide pragmatic and directional insights and recommendations, serving as a key reference for the work of technologists, educators and policymakers in the coming years. TRUSTD is inspired by the general principals of the above initiative such as human rights, well-being, data agency, transparency and accountability; and it uses these principals as a guideline to derive the driving requirements in the design of an open and decentralised ecosystem to combat the flow of fake content. The authors in [1], presented four perspectives to study fake content: knowledge-based, style-based, propagation-based and credibility based. In knowledge-based approach, one aims to analyze and/or detect fake news, using a process known as fact-checking. Manual fact-checking can be divided into (I) expert-based and (II) crowd-sourced fact-checking. Expert- based fact-checking relies on domain-experts to verify the given news contents, therefore, it leads to highly accurate results. Recently, many websites have emerged to provide expert-based fact-checking services. For example, PolitiFact provides the PolitiFact scorecard”, which presents statistics on the authenticity distribution of all the statements related to a specific topic. Another example is HoaxSlayer, which classifies the articles and messages into e.g., hoaxes, spams and fake news. A comprehensive list of fact-checking websites is provided by Reporters Lab at Duke University3, where over two hundred fact-checking websites across countries and languages are listed. Although these websites can provide ground-truth for the detection of fake content, these websites are still silo-ed and centralised and they have their own experts and their own methodology of identifying the fake news. Crowd-sourced fact-checking relies on a large number of fact-checking individuals and therefore, it is less credible and accurate due to the political bias of these individuals and their conflicting annotations. Hence, one often needs to (i) filter non-credible individuals and (ii) resolve conflicting fact- checking results. An example is Fiskkit4, where users can upload articles, and provide ratings and tags for sentences within articles. TRUSTD can be seen as a crowd-sourced fact- checking approach but it is the content creator who chooses the fact-checkers and it is the user who determines the content’s credibility based on their trust level in these fact-checkers. Automatic fact-checking techniques have been developed, heavily relying on Information Retrieval (IR) and Natural 3https://reporterslab.org/fact-checking/ 4https://fiskkit.com/ Language Processing (NLP) techniques. The overall automatic fact-checking process can be divided into two stages: (I) fact extraction (also known as knowledge-base construction) and (II) fact-checking (also known as knowledge comparison) [1]. Knowledge-based approaches aim to evaluate the authenticity of the given content, while style-based approaches aim to assess the intention behind publishing the content. Credibility- based approaches evaluate the content based on content- related and social-related information. The AI Foundation has developed an intelligent software called Reality Defender”, to detect potentially fake media in the digital world5. This soft- ware runs AI-driven analysis techniques to detect alterations in the scanned images, videos and other media, and allows for reporting suspected fakes. In TRUSTD, these deep tools can be included as part of the user’s trust policy, as will be seen later, rather than rely solely on the quality of these tools. Blockchain technology has been already leveraged to con- tribute solutions to address the fake news problem. For exam- ple, Shang et al. [11] trace the source of news by keeping a ledger of timestamps and the connections between the different blocks. Huckle et al. [12] introduced a blockchain- based application that is capable of indicating the authenticity of digital media. Using the trust mechanisms of blockchain technology, the tool can show the provenance of any source of digital media, including images used out of context in attempts to mislead. However, the authors mentioned that although their application has the potential to be able to verify the originality of media resources, that technology is only capable of providing a partial solution to fake content. We agree with the authors that this is because the blockchain technology is incapable of proving the authenticity of a content story and that requires human skills. III. BACKGROUND A. Schnorr signing In Schnorr signing, we can aggregate public keys of P participants into a single signing key [13], and uses the non-interactive version of the Fiat-Shamir heuristic 6. Using Elliptic Curve methods, to sign a message we take a random value (k) and a private key value (d) and a generator point G (G is a base point on an elliptic curve) and compute: Q = dG (1) and: R = kG (2) For it to be non-interactive we then calculate: e = H(R ‖ M ) (3) and then: s = k − ed (4) The signature is then (s, e). To verify we compute: rv = sG + eQ (5) 5https://aifoundation.com/responsibility/ 6https://en.wikipedia.org/wiki/Fiat-Shamir_heuristic 4 and then: ev = H(rv ‖ M ) (6) We then check that rv = ev . Each participant has a private key (ai) and a public key Ai = aiG. We can then determine the aggregate public key with: A = ∑ i∈P Ai (7) B. Collective Signing Within legal infrastructures, we might have several wit- nesses W , and we ask a number of the witnesses W ′ to verify that something is correct. If one of the witnesses cannot verity the information, we would highlight a problem. Let us say we have a controller on a network, and a number of selected trusted witnesses. Each of the witnesses can then check all of the messages sent by the nodes on the network, and if one of them determines a problem, they can tell the rest of the network. In this respect, every message (M ) is collectively signed by W witnesses. The collective signing (CoSi) algorithm is defined by Syta et al [6]. With CoSi (collective signing), there are four phases involving P participants and where the leader has an index value of zero. Each participant has a private key (ai) and a public key (Ai = aiG, and where G is a base point on an elliptic curve). We then determine the aggregated public key with [6]: A = ∑ i∈P Ai (8) Announcement: Initially the leader broadcasts a message (M ) that it wants the participants to sign. Commitment: Each node i will pick a random scalar (vi) and determines their commitment (Vi = [vi]G). Each commitment is then sent to the leader, who will wait for a specific amount of commitments (P ′) to be received. The leader then creates a participant bitmask and aggregates all the received commitments: V = ∑ j∈P ′ Vj (9) and creates a participation bitmask Z. The leader then broadcasts V and Z to the other participants. Challenge: Each of the participants computes the collective challenge (using the hash function H): c = H(V ||A||M ) (10) and send the following back to the leader: ri = vi + c × ai (11) Response: The leader will wait until the participants in P ′ have sent their responses. Once received, the leader computes the aggregated response: r = ∑ j∈P ′ rj (12) and publishes the signature of the message (M) as: (V, r, Z) (13) Each node can then check their own signature value and agree with the leader. In order to handle large numbers of participants during signature generation efficiently, CoSi protocol uses a tree- shaped network communication overlay [14]. Any tree used by CoSi should be a complete tree for performance reasons. The leader is the root node of the tree and is responsible for creating the tree. An intermediate node is a node who has one parent node and at least one child node. A leaf node is a node who has only one parent and no child nodes. The leader multicasts a message to his direct child nodes. Upon reception of a message, each node stores the message and multicasts it further down to its children node, except if the node is a leaf. Each node generates its response. Each leaf node sends its response to their parent and is allowed to leave the protocol. Each other node starts waiting for the responses of its children. When the root node receives all the responses from its children, it can generate the signature. C. Decentralised Identifier (DID) The self-sovereign identity (SSI) refers to an identity man- agement system which allows individuals to fully own and manage their digital identity [15]. The World Wide Web Con- sortium (W3C) working group on verifiable claims states that in a SSI system users exist independently from services [16]. This highlights the contrast to current identity management which either relies on a number of large identity providers such as Facebook and Google or the user has to create new digital identities at each individual service provider. SSI is enabled by the new development of blockchain technology. Through the trustless, decentralised database that is provided by a blockchain, classic Identity Management registration processes can be replaced [15]. A Decentralised Identifier (DID) is comprised of a scheme as well as a method and method specific identifier. The method closely resembles the namespace component of an Uniform Resource Name (URN) 7. Each distinct blockchain or rather each identity registry constitutes its own namespace while the blockchain specific identifiers specify the actual identity addressed by the DID. An example for such a DID path would be: did:examplechain:123456789. The DID document is the key to the decentralised identity. A DID is a unique identifier that can be resolved to a DID document [17]. This document contains cryptographic material and authentication suites enabling identification of the DID subject. They also contain service endpoints allowing secure encrypted communication to the DID subject. A public DID is a DID that is registered on a public distributed ledger meaning DID is resolvable to a document and hence verifiable by 7https://en.wikipedia.org/wiki/Uniform_Resource_Name 5 anyone. There should be some restrictions or process by which DIDs can be registered to a public ledger. Once registered it is possible to use that DID as a root of trust. Establishing pairwise private DID connections can be done by first contacting the endpoint (i.e. agent) resolved from the public DID document. Sending a new unique DID and DID document along with proof you are the DID subject. In response you should receive a new unique DID signed by the public key of the public DID. Together these two DIDs form a private unlinkable connection between two parties complete with private endpoints and the cryptographic material required to verify the origin and integrity of the communication. D. DLTs and blockchain capabilities DLTs like Tangle or blockchain are able to provide seamless authentication, efficient and secure data storage, robustness against attacks, scalability, transparency and accountability. Such features can play an effective role in combating fake content, considering that transactions cannot be tampered once they have been distributed, accepted and validated by a network consensus and stored in blocks [18]. Moreover, transactions are easily auditable by all the involved stakehold- ers. More detailed information on how to design a blockchain according to the business needs and deployment environment can be found, for example, in [19] [20]. IV. DRIVING REQUIREMENTS We believe that combating the fake content cannot be a pure technical solution and that a human element should always exist. In the proposed ecosystem, we can identify two types of users: - Individuals and organizations who produce the content. We call them content creators (CCs). These creators may have different agendas and they could be honest or dishonest in reporting any news or spreading any information. - Content readers: We call them users. These users are the consumers of the content. The following are the set of requirements that we believe TRUSTD needs to meet: A. Traceability of origin The ecosystem should allow the originator of the content to be identified at any point of time. That could be achieved by storing the content along with the originator ID in an immutable ledger that cannot be maliciously altered (i.e. blockchain). B. Traceability of approval In TRUSTD, every CC can request other actors to sign their content. This signature means that the signing actor agrees on the correctness of the content. This is necessary in order for the user to be able to assess the credibility of the content, as will be seen in the next sections. Once any actor provides their signature (witnessing the correctness of the content), this signature should be stored in a way that this actor cannot deny their action at any time. However, the ecosystem should allow any actor to change their mind and revoke their signature after, for example, they discover that the content was incorrect. For transparency purposes, the list of actions of any actor should be captured in the immutable ledger. C. Accountability There is a need to ensure that the signing actors cannot deny their involvement in signing off the content when the content is discovered to be false. D. System openness The ecosystem should allow any individual or organization to join and be part of the approval actors. For example, these individuals can be journalists, editors, activists, etc. who have different levels of expertise; and the organizations can be any news agency. However, since the fake content concerns all people and it affects everyone’s life, the system should be open to allow anyone to be part of the signing actors - we call them appraiser actors (AAs). This is especially important for the content that is originated from people (non-journalists) such as abnormal accidents or natural disasters, and that the only witnesses are the people seen at the accident location. E. The content should be evaluated by the user It is true that the ecosystem allows the invited AAs to sign off the contents they receive from CCs. However, the system should keep the user in the center and it should empower them with a mechanism to judge on the credibility of the content. For that objective, the user should be able to select the list of AAs and assign them a trust level value which is in the range [0, T ]. T is the maximum trustiness value. For example, a trust level of T means the user completely trusts that actor. A trust level of 0 means the user does not trust the actor. The user can determines the credibility of the content by calculating the following formula: C = ∑ i∈N Ti/(T ∗ N ) (14) T i: The trustiness value of the actor i. N : The length of the actor list added by the user. The result is a number in the range [0, 1]. This number indicates the level of credibility of the content. F. The content should be backed by actors Before publishing their content to the social media and news websites, the CCs can get an approval from a set of AAs of their choice. These AAs can be selected by the CCs themselves or by the system in automated way based on a specific criteria. For example, the system can extract the main keywords of the content and send it to all AAs that are tagged with these keywords. In addition, these AAs can specify a policy (using the DID system) to accept signature requests only from CCs who can prove that they have specific attributes, for example, holding a specific degree. These attributes should be signed by an entity trusted by the AA. 6 The approval comes in a form of a collection of signatures from these AAs indicating that they agree on the correctness of the content. We can imagine here different signature types. For example, the CC may obtain a collection of signatures from the AAs who share the CC’s opinion, or who support the CC’s call. In this paper, we focus on the signature that means that the AA agree on the correctness of the content. G. The ecosystem should be decentralised and scalable Since the fake news/content is a global issue, the system should not be centralised. Instead, the system should provide a protocol between the different users and actors which are reg- istered in different subsystems owned by different companies or organisations. However, these subsystems should have one common denominator: the identifier of the users and actors. For that aim, the decentralised identity (DID) [5] would be a perfect fit in this context. In this respect, the actors can create their own DIDs and publish them on their websites or their social media profiles. That DID will be used for signature verification as will be seen later. V. SYSTEM DESIGN Three main entities can be identified: 1) The content creator (CC), 2) the appraisal actor (AA) who signs off any content, and 3) the user who needs to assess the credibility of any content. Each entity should be represented by a DID agent. These agents should follow a specific protocol in order to get any content signed off by the relevant AAs. The users’ agents should be able to verify any content based on the trust policy specified by the user. Here is the proposed protocol as illustrated in Fig 1: 1- An AA creates its own DID and publishes it somewhere so that the CCs, users and other AAs know about it. 2- A CC creates the content in a form of a text, photo, video, sound, or any other digital format. 3- Depending on the content scope, the CC can choose a list of AAs that they are candidate to approve and sign off the content. Alternatively the system recommends specific AAs. The result is a set of DIDs which can be used to retrieve their corresponding public keys. 4- A collective signature is generated in five steps over two round trips between the agent of the CC and the agents of the selected AAs, as follows: AAs need not coordinate for the creation of their key- pairs beyond selecting a common elliptic curve, and verifiers can apply flexible acceptance policies beyond simple t-of-n thresholds. We use the following notation: B: Generator of the group of AAs. L: Order of the group generated by B. (ai, Ai): Each AAi generates their long term private-public key pair (ai, Ai) as in EdDSA. A: collective public key A generated from the public keys of AAs. N : denotes the list of AAs, the size of N is denoted by n. - Announcement Step: The CC broadcasts an announce- ment message to the AAs indicating the start of a signing process. This message contains the content itself. - Commitment Step: Upon the receipt of the announcement message, each AAi generates a random secret ri by hashing 32 bytes of cryptographically secure random data. Each ri must be re-generated until it is different from (0 mod L) or (1 mod L). Each AA then constructs the commitment Ri as the encoding of [ri]B, sends Ri to the CC and stores the generated ri for usage in the response phase. - Challenge Step: The CC waits to receive the commitments Ri from the other AAs for a certain configurable time frame. After the timeout, the CC constructs the subset M of AAs from whom he has received a commitment Ri and computes the sum: R = ∑ i∈M Ri (15) The CC then computes c = SHA512(R||A||M ) mod L (16) The CC broadcasts c to all AAs. - Response Step: Upon reception of c, each AA generates their response: si = (ri + c ∗ ai) mod L (17) and send it to the CC. - Signature Generation Step: The CC waits to receive the responses si from the AAs for a certain configurable time frame. After the timeout, the CC checks if he received responses from all AAs in M and if not he must abort the protocol. The CC then computes the aggregate response s = ∑ i∈M si mod L (18) and initializes a bitmask Z of size n to all zero. For each AAi who is present in N but not in M the CC sets the i − th bit of Z to 1, i.e., Z[i] = 1. The CC then forms the signature sig as the concatenation of the byte-encoded point R, the byte- encoded scalar s, and the bitmask Z. The resulting signature is of the form: sig = R||s||Z (19) 5- The CC sends a request to the underlying blockchain to store the hash of the content along with the collective signature in the blockchain. 6- In order to verify the credibility of a specific content, the user agent calculates the hash of the content and uses it as an input parameter to query the blockchain and to retrieve the corresponding collective signature (if any). 7- As aforementioned, each user should be able to choose a set of their trusted AAs. Each user agent maintains a list of the public keys of those actors along with their trust level values. Based on the retrieved collective signature, the user agent obtains the public keys of the AAs who signed off the content and then calculates the credibility value of the content as described in section IV.E. 7 Fig. 1. The TRUSTD approach VI. IMPLEMENTATION An initial prototype has been built (Fig. 2 to 4). We use Django version 2.2.7 to build a TRUSTD-based blog where the users can register to the system, create contents, specify the list of their trusted parties, assign a trust level to each actor in that trusted list, send the contents to a subset of their trusted parties list (Fig 2), view the names of the signed parties along with the trustworthiness level of the related content (Fig 4), and receive requests from other users to sign contents (Fig 3). We use GO programming language version 1.13.4 to implement CoSi8. VII. DISCUSSION A. Limitation of the TRUSTD approach In its current implementation, TRUSTD has some limita- tions that can be solved in future work: - The authors in [21] introduced mBCJ, a secure two- round multi-signature scheme. Their results show that mBCJ is only marginally less efficient than CoSi, so that any protocol based on the CoSi scheme should instead be built on the provably more secure mBCJ scheme. The TRUSTD approach is agnostic to the used collective signature scheme. We plan to adopt the mBCJ scheme as a mechanism to collectively sign any digital content. - In some extreme cases, the content could be signed by one AA who is not trusted by the user. For example, if the content is a video of a publicly known person and this person signs off this video as correct, but this person is not trusted by the user, in our current implementation, the system recommends to the user that","Jaroucheh, Z., Alissa, M., Buchanan, W. J., & Liu, X. (2020, July). TRUSTD: combat fake content using blockchain and collective signature technologies. In 2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC) (pp. 1235-1240). IEEE."
ML_DB_032,Automated fact-checking for assisting human factcheckers,"The reporting and the analysis of current events around the globe has expanded from professional, editor-lead journalism all the way to citizen journalism. Nowadays, politicians and other key players enjoy direct access to their audiences through social media, bypassing the filters of official cables or traditional media. However, the multiple advantages of free speech and direct communication are dimmed by the misuse of media to spread inaccurate or misleading claims. These phenomena have led to the modern incarnation of the fact checker — a professional whose main aim is to ex amine claims using available evidence and to assess their veracity. As in other text forensics tasks, the amount of information available makes the work of the factchecker more difficult. With this in mind, starting from the perspective of the professional fact-checker, we survey the available intelligent technologies that can support the human expert in the different steps of her fact-checking endeavor. These include identifying claims worth fact-checking, detecting relevant previously fact checked claims, retrieving relevant evidence to factcheck a claim, and actually verifying a claim. In each case, we pay attention to the challenges in future work and the potential impact on real-world fact-checking.",Social Science,Article,2021,Y,Y,28,"The spread of fake news, misinformation and disinformation on the web and in social media has become an urgent so- cial and political issue. Social media have been widely used not only for social good, but also to mislead entire commu- nities. To fight against such false or misleading informa- tion, several initiatives for manual fact-checking have been launched. Some notable fact-checking organizations include FactCheck.org,1 Snopes,2 PolitiFact,3 and FullFact.4 ∗Contact Author Such fact-checking organizations are also potential benefi- ciaries of and/or leaders in automated fact-checking research. As misinformation and disinformation have become major concerns globally, tech companies, as well as national and international agencies began work in this area. Recently, sev- eral international initiatives have also emerged such as the Credibility Coalition5 and EUfactcheck,6 and some tools have been made available such as Google Factcheck7 and Hoaxy.8 Moreover, fact-checking is a common task in settings that go beyond online misinformation, as the verification of content’s accuracy is a priority for many organizations [Karagiannis et al., 2020]. A large body of research has been devoted to develop- ing automatic systems for fact-checking [Li et al., 2016; Shu et al., 2017; Lazer et al., 2018; Vosoughi et al., 2018; Vo and Lee, 2018]. This includes datasets [Hassan et al., 2015; Augenstein et al., 2019], and evaluation campaigns [Thorne and Vlachos, 2018; Nakov et al., 2021a]. However, there are credibility issues with automated systems [Arnold, 2020], and thus a reasonable solution (i.e., human in the loop) is to facilitate human fact-checkers using automated systems. Yet, there has been limited work in this direction. Thus, to facilitate human fact-checkers, in this survey, we explore what fact-checkers want and what research has been done that can actually support them in their work. This is important because manual fact-checking is a time-consuming process, going through several manual steps. The study by Vlachos and Riedel [2014] describes the following typical se- quence of fact-checking steps: (i) extracting statements that are to be fact-checked, (ii) constructing appropriate questions, (iii) obtaining the pieces of evidence from relevant sources, and (iv) reaching a verdict using that evidence. In the current information ecosystem (including web and social media), there is a large volume of false claims not only in textual form, but also misleading or manipulated images and videos, including deepfakes,” and there has been a lot of recent work on fact-checking images and videos. However, here we limit our focus to automated fact-checking on text, as it remains the focus of most professional fact-checkers. arXiv:2103.07769v2 [cs.AI] 22 May 2021 There have been a number of surveys on fake news” [Shu et al., 2017; Lazer et al., 2018; Vosoughi et al., 2018; Alam et al., 2021], rumors [Zubiaga et al., 2018], fact- checking [Thorne and Vlachos, 2018; Kotonya and Toni, 2020], factuality [Li et al., 2016; Zannettou et al., 2019; Nakov et al., 2021b], and propaganda [Martino et al., 2020]. Unlike that work, here we study the desiderata of fact- checkers vs. the research attempts that aim to meet them. 2 What Fact-Checkers Want Recently, Full Fact carried out extensive interviews with pro- fessional fact-checkers from 24 organizations in 50 coun- tries [Arnold, 2020]. The report discussed key challenges they face where they believe technology can help. These include monitoring potentially harmful content, selecting claims to check, creating and distributing articles, and man- aging suggestions from readers (such as tip lines serving WhatsApp or Signal). The same report revealed that most fact-checkers do not be- lieve that tools to automate the verification of claims, i.e., the last step of a typical fact-checking pipeline [Vlachos and Riedel, 2014], will be used in the foreseeable future. Some believe that the required intuition and creativity can never be automated, even if some parts of their work can be supported. This sets up a twin challenge for Artificial Intelligence (AI) practitioners: first, to develop practical tools that solve the problems fact-checkers face, and second, to demonstrate their value to fact-checkers in their day-to-day work. In the mean- time, there is a recognised need for tools to help with finding claims, including previously fact-checked claims, and in pro- viding relevant evidence to help write fact-checking articles. 2.1 Finding Claims Worth Fact-Checking Choosing which claims to check is a complex process. Fact-checking is time-consuming and it often takes effort to determine whether a claim can even be checked, let alone whether it is misleading. Fact-checkers have to balance the potential harm that a misleading claim may cause (including risk to health, risk to democratic processes, and risk of ex- acerbating emergency situations) against the effort required to check a claim. Fact-checkers are also committed to being non-partisan, and thus it is important that such tools do not introduce any unfair bias. In many countries, governments choose not to publish reliable official statistics, thus making certain statistics-related claims virtually impossible to verify. While simple algorithms can often decide whether content is viral, it is much harder to estimate the checkworthiness” of a claim. For example, breaking news stories are often both popular and accurate. Given the limited resources of fact- checking organizations, many claims that are check-worthy nonetheless remain unchecked; thus, using historic lists of claims that were or were not checked is not a reliable indica- tion of whether similar claims are worth fact-checking. Claims may be found in many sources, including news websites, social media (text, audio, or video), and broadcast media. To monitor such a range of sources, fact-checkers of- ten use a variety of technologies, such as news alerts, auto- matic speech recognition and translation tools, all of which typically depend on underlying AI technologies. 2.2 Detecting Previously Fact-Checked Claims Misleading claims are often repeated in multiple channels, independently of any fact-checks or rebuttals.9 Once a claim has been established as misleading, the ongoing spread of re- peats or copies of the claim can be minimised by its rapid detection. In the simplest cases, these could be simple copy and paste” repeats that are relatively easy to detect, but more often they will be paraphrases of the original or endlessly evolving variations. Given the resources required to write fact-checking articles, it is preferable to respond to multiple repeats of a claim with a single fact-checking article. The number of fact-checking initiatives continues to grow. The Duke Reporters’ Lab lists 305 active fact-checking orga- nizations.10 While some of them have debunked just a couple of hundred claims, others such as PolitiFact, FactCheck.org, Snopes, and Full Fact have each fact-checked thousands or even tens of thousands of claims. Moreover, manual fact-checking often comes too late. It has been shown that fake news” can spread six times faster than real ones [Vosoughi et al., 2018], and that over half of the spread of some viral claims happens within the first ten minutes of their posting on social media [Zaman et al., 2014]. To counter this, quickly detecting that a new viral claim has already been fact-checked allows for a timely action that can limit the spread and the potential harmful impact. The prob- lem is made harder by the transient nature of many claims. For example, a claim about infection rates may be wrong to- day but correct next week, and thus re-using previous checks should be done carefully. For journalists, the ability to discover quickly whether a claim has been previously fact-checked could be revolution- izing as it would allow them to put politicians on the spot dur- ing live events. In such a scenario, automatic fact-checking would be of limited utility as, given the current state of tech- nology, it is not credible enough in the eyes of a journalist. Finally, false claims often originate in one language and then get translated to other languages. Tools that can spot repeated claims across languages would be useful to ad- dress this. More generally, multi-lingual tools can help fact- checkers around the world, even those with limited resources. 2.3 Evidence Retrieval Fact-checking is often limited by the time available: there are typically far more claims to verify than what is practically possible. Even if full automation remains out of reach (see the next section), tools that support fact-checkers in their manual verification process are to be welcomed. Tools that automatically retrieve relevant data from trusted sources may save fact-checkers a lot of time. This is espe- cially true if the evidence is hidden in large text documents, audio-visual recordings and streams, or is in a language that the fact-checker is not familiar with. Thus, combining au- tomatic transcription, summarization, translation, and search can make sources of evidence available to fact-checkers that would be impossible or impractical to access otherwise. 9President Donald Trump repeated one false claim over 80 times: Figure 1: A fact-checking pipeline. 2.4 Automated Verification On first consideration, the automated verification of claims seems like the ultimate application of AI to fact-checking. If such technologies can be developed and deployed, they would allow fact-checking organizations to be faster and to provide a more comprehensive coverage than manual fact-checking could ever achieve. However, many claims are not simply correct or incorrect, but may be partially correct, or correct but misleading without extra context, etc. One key role of professional fact-checkers is to help their audience gain full understanding of a claim, with all its nuances and complexity, rather than simply applying a binary classification. Fact-checkers can only have an impact if they are trusted by their readers. They therefore take great care to only publish fact-checks after meticulous research, and adhere to strict ed- itorial standards, as outlined, e.g., in the fact-checkers’ code of principles11 by the International Fact-Checking Network. This leads to a major hurdle before adopting fully automated verification methods: such methods will inevitably be im- perfect, and publishing incorrect fact-checks could seriously damage the reputation of the responsible fact-checking orga- nization. They may be more valuable as internal tools by pre- senting the evidence, reasoning and conclusion regarding a claim, before the (human) fact-checker writes and publishes their fact-checking article. 3 What Technology Currently Offers Fact-checking is not a straightforward or routine process. It requires a chain of steps that go from sensing media and spot- ting check-worthy claims all the way through to concluding whether the claim is true, partially-true, false, misleading, or perhaps impossible to judge. Figure 1 shows a typical fact- checking pipeline, partially derived from [Barr ́on-Cede ̃no et al., 2020]. Below, we discuss each step in this pipeline. 3.1 Finding Claims Worth Fact-Checking As fact-checkers are flooded with claims, they need to decide what is actually worth fact-checking. This has encouraged the development of AI solutions, e.g., as part of shared tasks such as the CLEF CheckThat! lab 2018-2021 [Nakov et al., 2018; Elsayed et al., 2019; Barr ́on-Cede ̃no et al., 2020; Nakov et al., 2021a], as well as inside dedicated fact-checking organi- zations such as Full Fact [Corney, 2019]. The problem is widely tackled as a ranking one, where the system has to produce a ranked list of claims coupled with check-worthiness scores. Such a score is important to in- crease the system’s transparency and to provide fact-checkers with the ability to prioritize or to filter claims. Fact-checkers can also provide feedback on how reflective this score is of the actual check-worthiness of a claim, which can be later used to tune the system. ClaimBuster [Hassan et al., 2017] is the first system for check-worthiness detection, and it was used by fact-checkers in the Duke Reporters’ Lab project.12 It was trained on a man- ually annotated dataset to distinguish between non-factual sentences, unimportant factual claims, and check-worthy fac- tual claims; it used features based on sentiment, named en- tities, part-of-speech tags, words, and claim length. Kon- stantinovskiy et al. [2021] developed a more detailed schema and dataset for check-worthiness annotation of TV shows. Gencheva et al. [2017] created a dataset of political debates, derived by observing which sentences were fact-checked by fact-checkers; they used a rich set of features modeling the sentence structure and the context of the claim. The dataset was used in the ClaimRank system [Jaradat et al., 2018], and was extended to multitask learning from nine fact-checking organizations [Vasileva et al., 2019]. Further extensions were used for the CLEF CheckThat! lab, where the partic- ipants developed models based on pre-trained transformers such as BERT and RoBERTa [Hasanain and Elsayed, 2020; Nikolov et al., 2020; Williams et al., 2020]. Finally, as observation-based annotations cannot give reliable negative examples, the task was also modeled using positive unlabeled learning [Wright and Augenstein, 2020]. During a recent general election, Full Fact used a fine- tuned BERT model to classify claims made by each political party, according to whether they were numerical claims, pre- dictions, personal beliefs, etc. [Corney, 2019] This allowed fact-checkers to rapidly identify the check-worthy claims, and thus to focus their efforts in the limited time available while voters are making their final decisions. Social media companies are also working on combating misinformation and disinformation on their platforms. Face- book described a proprietary tool to identify claims that should be fact-checked [Facebook, 2020]. They leverage flags by the users for a post indicating that it is potentially false, as well as features from the content of the replies, to predict whether the post contains false information. The model is updated using feedback from fact-checkers. 3.2 Detecting Previously Fact-Checked Claims Interestingly, despite the importance of detecting whether a claim has been fact-checked before, it has been explored only recently. Shaar et al. [2020] formulated the task, and re- leased two specialized datasets: (a) on tweets, which are to be compared to claims in Snopes, and (b) on political de- bates, to be matched to claims in PolitiFact. They further proposed a learning-to-rank approach based on a combination of BERT and traditional BM25, matching the input to the en- tire fact-checking article. Follow-up work explored the role of context for (b), including using neighboring sentences, co- reference resolution, and reasoning over the target text with Transformer-XH [Shaar et al., 2021]. The task was also fea- tured in the CLEF CheckThat! Lab [Barr ́on-Cede ̃no et al., 2020; Nakov et al., 2021a]. Vo and Lee [2020] explored a multi-modal setup, where tweets with claims about images were matched against the Fauxtography section of Snopes. Full Fact is currently trialling a similar tool internally. 12http://reporterslab.org/tech-and-check Recently, Google has released the Fact Check Explorer,7 which is an exploration tool that allows users to search a number of fact-checking websites, such that use ClaimReview from schema.org,13 for the mentions of a topic, a person, etc. However, the tool cannot handle complex claims, as it uses Google Search, which is not optimised for long queries. 3.3 Evidence Retrieval Evidence retrieval aims to find external evidence to help fact- checkers decide on the factuality of an input claim. When the input consists of a check-worthy claim and a (potentially closed) data collection, the process could finish in the pro- duction of a ranking of the relevant data —as in a standard retrieval scenario— or in the extraction of specific pieces of evidence, e.g., a text snippet or a recording. When dealing with a closed reference collection, the task can be addressed as a ranking problem, e.g., based on BM25 or on some kind of similarity over vectorial representations between the input claim and the documents in the collec- tion. Recent work has also combined document-level and sentence-level similarity to improve relevant document re- trieval [Akkalyoncu Yilmaz et al., 2019]. Once a relevant document has been found, it is possible to further extract relevant snippets representing arguments in favour or against the target claim, to be presented to the hu- man fact-checker [Alshomary et al., 2020]. It is also possible to further generate snippets to brief the fact-checkers with some relevant background knowledge about the target claim. Fan et al. [2020] achieved this by first generating and retrieving relevant passage briefs, then iden- tifying and retrieving documents based on entity briefs, and finally generating and answering question answering briefs decomposed from the claim. The CLEF-2013 INEX lab [Bellot et al., 2013] included a shared task that asked to retrieve evidence snippets from a pool of 50k books to confirm or to refute a claim. They found that entity matching was one of the most important features. The CLEF CheckThat! lab also featured tasks on claim evidence retrieval, at the document and also at the passage level, which was offered in Arabic [Elsayed et al., 2019; Barr ́on-Cede ̃no et al., 2020]. The Fact Extraction and Verification shared task (FEVER) focused on extracting an evidence sentence related to a claim from Wikipedia articles and determining whether it supports, refutes, or provides no enough information about the claim [Thorne et al., 2018]. As in INEX, named entities were among the key pieces of information, and they were often used to compose the queries to retrieve the most relevant articles [Malon, 2018; Hanselowski et al., 2018]. A typi- cal system to solve the task starts with document retrieval, e.g., using BM25, followed by sentence retrieval based on the similarity between the input claim and each sentence in the top-n retrieved documents, which can be measured using TF.IDF, Word Mover’s Distance, or BERT. Finally, it would use natural language inference to decide on the verdict. More recent work has used specialized neural semantic matching networks for each of these steps [Nie et al., 2019]. 13http://schema.org/ClaimReview Evidence retrieval might need to go beyond text, e.g., when verifying a claim about an image or a video. In such cases, reverse image search can help find other contexts where the multimedia content was used [Zlatkova et al., 2019]. This allows to check whether these contexts agree with the claim, and to detect out-of-context content, e.g., an image or a video from one event portrayed as being from a different event, as well as potentially manipulated images/videos. Popular tools for this include TinEye,14 Google Image Search, and Yandex Image Search. Relevant research tools are also being devel- oped in two EU projects: WeVerify15 and InVID.16 3.4 Automated Verification Automatic claim verification approaches can be divided into explainable and non-explainable. Explainable approaches, also known as reference-based approaches, are more relevant to assisting human fact- checkers. They verify the input claim against a trusted source such as tables [Chen et al., 2020] or a database [Ahmadi et al., 2019], or using inference over a knowledge graph, pos- sibly while also using Horn rules [Gad-Elrab et al., 2019]. This includes two approaches that we discussed above: find- ing previously fact-checked claims that can verify the in- put claim [Shaar et al., 2020], and fact-checking it against Wikipedia [Thorne et al., 2018; Nie et al., 2019]. Non-explainable approaches make a prediction based on the content of documents retrieved from the Web [Popat et al., 2016; Karadzhov et al., 2017; Augenstein et al., 2019], or on social media by modeling the message and its propagation, the users and their reactions over time, links to media sites, etc. [Castillo et al., 2011; Shu et al., 2017; Vosoughi et al., 2018; Nguyen et al., 2020]. This further includes analysis of the language used in the claims based on lexicons such as LIWC [Rashkin et al., 2017], or using perplexity analysis [Lee et al., 2021]. Fact-checking has also been done using masking in BERT-style transformers [Lee et al., 2020]. While automatic verification is hard, there are promising results for certain kinds of claims. For example, an explicit claim about a numerical value, such as In 2017, global elec- tricity demand grew by 3%.”, can be verified automatically using official statistics, even when this requires applying a complex formula [Karagiannis et al., 2020]. Success here depends on the availability of reliable data, presented in a consistent format, which varies widely between countries and fields. Similarly, simple claims can be verified with promis- ing accuracy when good evidence is available, e.g., for popu- lar entities on the Web [Augenstein et al., 2019]. While the accuracy and the scope of automated fact- checking algorithms keeps improving, two problems prevent their adoption in fact-checking organizations. First, even on the original datasets, their effectiveness is not high enough to allow automatic decisions. Second, most claims in the public realm are more complex, e.g., that COVID-19 vaccines have been developed too quickly and are still experimental.17 To verify such claims, fact-checkers might need to inter- view experts, to collaborate with other fact-checkers, to un- derstand the context and the framing of the claims, to track down and to verify multiple sources and pieces of evidence — all of which require human-levels intelligence. The gen- eral verification of arbitrary claims requires deep understand- ing of the real world that currently eludes AI. Indeed, most methods are designed to assist fact-checkers in their work with suggestions and assume that a human user will assess the verification output before assigning a true/false label. 3.5 Some Real-World Systems Below, we present a brief overview of some notable systems that cover multiple steps of the fact-checking pipeline, while also offering a suitable user interface. AFCNR: The system accepts a claim as an input, searches over news articles, retrieves potential evidence and presents to the user a judgment on the stance of each piece of evidence towards the claim and an overall rating of the claim’s veracity given the evidence [Miranda et al., 2019]. The system was extensively tested by eleven journalists from BBC. BRENDA: This is a browser extension, which allows users to fact-check claims directly while reading news articles [Bot- nevik et al., 2020]. It supports two types of input, either the full page opened in the browser, or a highlighted snippet in- side the page. In the first scenario, the system applies check- worthiness identification in order to decide which sentences in a page to fact-check. ClaimPortal: 18 After retrieving tweets in response to a query, the system [Majithia et al., 2019] scores them for check-worthiness using ClaimBuster and tries to verify each tweet using previously fact-checked claims from PolitiFact. Squash: The system is developed at the Duke Reporters’ lab, this system (i) listens to speech, debate and other events, (ii) transcribes them into text, (iii) identifies claims to check, and then (iv) fact-check them by finding matching claims al- ready fact-checked by humans [Adair, 2020]. Full Fact’s system is designed to support fact-checkers. It (i) follows news sites and social media, (ii) identifies and cat- egorizes claims in the stream, (iii) checks whether a claim has been already verified, and then (iv) enriches the claims with data to support the fact-checker. It is in daily use in the UK and several countries in Africa [Dudfield, 2020]. We believe that the prototypes presented above are good examples of the steps taken towards developing systems that cater to fact-checkers. More systems are now designed to efficiently identify claims originating from various types of sources (e.g., news articles, broadcast, and social media). Moreover, the fact-checker is now becoming a part of the system by providing feedback, rather than just being a con- sumer of its output. Finally, we see an increase in systems’ transparency by providing explainable decisions, thus mak- ing them more an assistive tool rather than a replacement for the fact-checker. However, there are several challenges left to tackle, as we present in the next sections. 18http://idir.uta.edu/claimportal/ 4 Lessons Learned The main lesson from our analysis is that there is a partial dis- connection between what fact-checkers want and what tech- nology has to offer. We provide more detail below. 1. Over time, many tools have been developed, either to au- tomatically fact-check claims or to provide facilities to the fact-checkers to support their manual fact-checking process. However, there are still limitations in both automated and manual processes: (i) credibility issue for automated systems, as they do not provide support- ing evidence, and (ii) scalability issue for manual fact- checking. 2. Automated fact-checking systems can help fact- checkers in different ways: (i) to find claims worth fact- checking, (ii) to find relevant previously fact-checked claims; (iii) to find supporting evidence (in the form of text, audio or video), translating (for multilingual con- tent) and summarising relevant posts, articles and docu- ments if needed, and (iv) to detect claims that are spread- ing faster to slow them down. 3. There is a lack of collaboration between researchers and practitioners in terms of defining tasks and devel- oping datasets to develop automated systems. In gen- eral, a human-in-the-loop can be an ideal setting for fact- checking, which is currently not fully explored. 5 Challenges and Future Forecasting Below we discuss some major challenges and we forecast some promising research directions: 5.1 Major Challenges • Leveraging multi-lingual resources: The same claim, with slightly different variants, often spreads over differ- ent regions of the world at almost the same or at differ- ent time periods. These may be international claims” such as medical claims about COVID-19, or stories that are presented as local, but with varied, false locations. Those claims might be fact-checked in one language, but not in others. Moreover, resources in English are abun- dant, but in low-resource languages, such as Arabic, they are clearly lacking. Aligning and coordinating the ver- ification resources and leveraging them across different languages to improve fact-checking is a challenge. • Ambiguity in the claims: Another reason why auto- matic fact-checking is challenging is related to the fact that often a claim has multiple interpretations. An ex- ample is The COVID death rate is rising.” Is this about mortality or about fatality rate? Does it refer to to- day/yesterday or to the last week/month? Does it re- fer to the entire world or to a specific area? In such cases, knowledge about the context is necessary in or- der to properly frame the claim and to filter out unlikely interpretations. After that, all remaining interpretations should be analyzed, which would further slow down the work of fact-checkers. One system that proposes a solu- tion to this problem is CoronaCheck.19 • System bias: The majority of existing systems are trained using datasets curated by a small group of peo- ple and often annotated by non-experts. This in turn results in systems biased towards how the system de- velopers perceive factuality and how the annotation task was described to the annotators. The dangers of bias in large language models is becoming increasingly obvi- ous [Bender et al., 2021], and should not be ignored just because the purpose of the system is benevolent. • Contextual information: The current state-of-the-art for automated fact-checking makes limited use of con- textual information, e.g., reader’s comments, linked sources of news articles, social network data for social media posts. Such information can provide useful sig- nals for enriching the current models. • Multimodality: Information is typically disseminated through multiple modalities such as text, image, speech, video, temporal, user profile, and network structure. Ad- dressing the problem based on a single modality can be a step towards failure. For example, it might be difficult to detect fake news pieces that are automatically generated using deep fakes and/or GPT-3-style text generation. To avoid such issues, multimodal approaches would be one way to go, if evidence can be gathered from multiple types of sources at the same time. This in turn requires multimodal datasets to develop suitable models. 5.2 Future Forecasting • Close collaboration between fact-checking platforms and researchers: We envision closer collaboration be- tween professionals from fact-checking platforms along- side researchers in the domain to discuss common inter- ests, existing solutions, and future directions, has been a challenge. • Integrated solutions: We also envision unified and open-source initiatives to develop resources for system development and benchmarking. • Usability: We further forecast more research on the sys- tem interface design, which would facilitate the adop- tion of AI by fact-checkers. It is important to de- velop systems that require minimal technical knowledge and reduce cognitive load. Such systems can help a larger number of fact-checkers and journalists in the fact-checking process. • Interpretability and explainability: Models should be designed in such a way that their outcomes are explain- able, unbiased, and more accountable to ethical consid- erations. • Efficient and real-time solutions: Finally, in order to tackle the velocity of the spread of fake news there is a need to develop systems that are efficient and scal- able for real-time solution. To be effective, such systems would need to be embedded within, or accessible by, so- cial networks and other big technology companies.","Nakov, P., Corney, D., Hasanain, M., Alam, F., Elsayed, T., Barrón-Cedeño, A., ... & Martino, G. D. S. (2021). Automated fact-checking for assisting human fact-checkers. arXiv preprint arXiv:2103.07769."
ML_DB_033,Community-Based Fact-Checking on Twitter's Birdwatch Platform,"Misinformation undermines the credibility of social media and poses significant threats to modern societies. As a countermeasure, Twitter has recently introduced Birdwatch,” a community-driven approach to address misinformation on Twitter. On Birdwatch, users can identify tweets they believe are misleading, write notes that provide context to the tweet and rate the quality of other users’ notes. In this work, we empirically analyze how users interact with this new feature. For this purpose, we collect all Birdwatch notes and ratings between the introduction of the feature in early 2021 and end of July 2021. We then map each Birdwatch note to the factchecked tweet using Twitter’s historical API. In addition, we use text mining methods to extract content characteristics from the text explanations in the Birdwatch notes (e. g., sentiment). Our empirical analysis yields the following main findings: (i) users more frequently file Birdwatch notes for misleading than not misleading tweets. These misleading tweets are primarily reported because of factual errors, lack of important context, or because they treat unverified claims as facts. (ii) Birdwatch notes are more helpful to other users if they link to trustworthy sources and if they embed a more positive sentiment. (iii) The social influence of the author of the source tweet is associated with differences in the level of user consensus. For influential users with many followers, Birdwatch notes yield a lower level of consensus among users and community created factchecks are more likely to be seen as being incorrect and argumentative. Altogether, our findings can help social media platforms to formulate guide lines for users on how to write more helpful fact checks. At the same time, our analysis suggests that community-based fact-checking faces challenges regarding opinion speculation and polarization among the user base.",Social Science,Article,2021,N,Y,4,"Misinformation on social media has become a major fo- cus of public debate and academic research. Previous works have demonstrated that misinformation is widespread on so- cial media platforms and that misinformation diffuses sig- nificantly farther, faster, deeper, and more broadly than the truth (e.g., Vosoughi, Roy, and Aral; Pr ̈ollochs, B ̈ar, and Feuerriegel; Pr ̈ollochs, B ̈ar, and Feuerriegel 2018; 2021b; 2021a). Concerns about misinformation on social media Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. have been rising in recent years, particularly given its po- tential impacts on elections (Aral and Eckles 2019; Bakshy, Messing, and Adamic 2015; Grinberg et al. 2019), public health (Broniatowski et al. 2018), and public safety (Starbird 2017). Major social media providers (e. g., Twitter, Face- book) thus have been called upon to develop effective coun- termeasures to combat the spread of misinformation on their platforms (Lazer et al. 2018; Pennycook et al. 2021). A crucial prerequisite to curb the spread of misinforma- tion on social media is its accurate identification (Penny- cook and Rand 2019a). Predominant approaches to identify misinformation on social media can be grouped into two categories. First, human-based systems in which (human) experts or fact-checking organizations (e. g., snopes.com, politifact.com, factcheck.org) determine the veracity (Has- san et al. 2017; Shao et al. 2016). Second, machine learning- based systems can automatically classify veracity (Ma et al. 2016; Qazvinian et al. 2011). Here machine learning mod- els are typically trained to classify misinformation using content-based features (e. g. text, images, video), context- based features (e. g., time, location), or based on propagation patterns (i. e., how misinformation circulates among users). Yet both approaches have inherent drawbacks: (i) expert’s verification tends to be accurate but is difficult to scale given the limited number of professional fact-checkers (Penny- cook and Rand 2019a). (ii) Machine learning-based detec- tion is scalable but the prediction performance tends to be unsatisfactory (Wu et al. 2019). Complementary approaches are thus necessary to identify misinformation on social me- dia both accurately and at scale. As an alternative, recent research has proposed to build on collective intelligence and the wisdom of crowds” to fact-check social media content (Micallef et al. 2020; Bhuiyan et al. 2020; Pennycook and Rand 2019a; Epstein, Pennycook, and Rand 2020; Allen et al. 2020; Allen et al. 2021; Godel et al. 2021). The wisdom of crowds is the phenomenon that when many individuals independently make an assessment, the aggregate user judgments will be closer to the truth than most individual estimates or even experts (Frey and van de Rijt 2020). Applying the con- cept of crowd wisdom to fact-checking is appealing as it would allow for large numbers of fact-checks that can be inexpensively and frequently acquired (Allen et al. 2021; Pennycook and Rand 2019a). However, research and earlier arXiv:2104.07175v3 [cs.SI] 14 Dec 2021 attempts to harness the wisdom of crowds to identify mis- leading social media content have so far produced mixed results. On the one hand, experimental evidence suggest that the assessment of even small crowds is comparable to those from experts (Allen et al. 2021; Bhuiyan et al. 2020; Pennycook and Rand 2019a). On the other hand, existing crowd-based fact-checking initiatives such as TruthSquad, Factcheck.EU, and WikiTribune had limited success and did not prove to be a model to produce high-quality fact- checks at scale (Bhuiyan et al. 2020). Due to quality is- sues with crowd-created fact-checks, these initiatives re- quired workarounds involving the assessment of experts – which again limited their scalability (Bhuiyan et al. 2020; Bakabar 2018). In sum, it has been found to be challenging to implement real-world community-based fact-checking platforms that uphold high quality and scalability. Informed by these findings, Twitter has recently launched Birdwatch,” a new attempt to address misinformation on social media by harnessing the wisdom of crowds. Differ- ent from earlier crowd-based fact-checking initiatives, Bird- watch is a community-driven approach to identify mislead- ing tweets directly on Twitter (see example in Fig. 1). The idea is that the user base on Twitter provides a wealth of knowledge that can help in the fight against misinforma- tion. On Birdwatch, users can identify tweets they believe are misleading (e. g., factual errors) or not misleading (e. g., satire) and write (textual) notes that provide context to the tweet. They can also add links to their sources of informa- tion. A key feature of Birdwatch is that it implements a rat- ing mechanism that allows users to rate the quality of other participants’ notes. These ratings should help to identify the context that people will find most helpful and raise its visi- bility to other users. While the Birdwatch feature is currently in pilot phase and only directly visible on tweets to pilot par- ticipants in the U. S., Twitter’s goal is that Birdwatch will be available to everyone on Twitter. Research goal: In this work, we provide a holistic anal- ysis of the Birdwatch pilot on Twitter. We empirically ana- lyze how users interact with Birdwatch and study factors that make community-created fact checks more likely to be per- ceived as helpful or unhelpful by other users. Specifically, we address the following research questions: • (RQ1) What are specific reasons due to which Birdwatch users report tweets? • (RQ2) How do Birdwatch notes for tweets categorized as being misleading vs. not misleading differ in terms of their content characteristics (e. g., sentiment, length)? • (RQ3) Are tweets from Twitter accounts with certain characteristics (e. g., politicians, accounts with many fol- lowers) more likely to be fact-checked on Birdwatch? • (RQ4) Which characteristics of Birdwatch notes are as- sociated with greater helpfulness for other users? • (RQ5) How is the level of consensus among users associ- ated with the social influence of the author of the source tweet? Methodology: To address our research questions, we col- lect all Birdwatch notes and ratings from the Birdwatch website between the introduction of the feature on January 23, 2021, and the end of July 2021. This comprehensive dataset contains 11,802 Birdwatch notes and 52,981 ratings. We use text mining methods to extract content characteris- tics from the text explanations of the Birdwatch notes (e. g. sentiment, length). In addition, we employ the Twitter his- torical API to collect information about the source tweets (i. e., the fact-checked tweets) referenced in the Birdwatch notes. We then perform an empirical analysis of the observa- tional data and use regression models to understand how (i) the user categorization, (ii) the content characteristics of the text explanation, and (iii) the social influence of the source tweet are associated with the helpfulness of Birdwatch notes. (a) Source tweet (b) Birdwatch note Figure 1: Screenshot of an exemplary community-created fact-check on Birdwatch. Contributions: To the best of our knowledge, this study is the first to present a thorough empirical analysis of Twit- ter’s Birdwatch feature. In contrast to earlier studies focus- ing on whether crowds are able to accurately assess social media content, we contribute to research into misinforma- tion and crowdsourced fact-checking by shedding light on how users interact with a community-based fact-checking system. Our work yields the following main findings: 1. Users file a larger number of Birdwatch notes for mis- leading than for not misleading tweets. Misleading tweets are primarily reported because of factual errors, lack of important context or because they treat unverified claims as fact. Not misleading tweets are primarily reported be- cause they are perceived as factually correct or because they clearly refer to personal opinion or satire. 2. Birdwatch notes filed for misleading vs. not misleading tweets differ in terms of their content characteristics. For tweets reported as misleading, authors of Birdwatch notes use a more negative sentiment, more complex language, and tend to write longer text explanations. 3. Tweets from influential politicians (on both sides of the political spectrum in the U. S.) are more likely to be fact- checked on Birdwatch. For many of these accounts, Bird- watch users overwhelmingly report misinformation. 4. Birdwatch notes reporting misleading tweets are per- ceived as being more helpful by other Birdwatch users. Birdwatch notes are perceived as being particularly help- ful if they provide trustworthy sources and embed a posi- tive sentiment. 5. The social influence of the author of the source tweet is associated with differences in the level of user consen- sus. For influential users with many followers, Birdwatch notes receive more total votes (helpful & unhelpful) but a lower helpfulness ratio (i. e., a lower level of consensus). Also, Birdwatch notes for influential users are particularly likely to be seen as incorrect and argumentative. Implications: Our findings have direct implications for the Birdwatch platform and future attempts to implement community-based approaches to combat misinformation on social media. We show that users perceive a relatively high share of community-created fact checks as being informa- tive, clear and helpful. Here we find that encouraging users to provide context (e. g., by linking trustworthy sources) and avoid the use of inflammatory language is crucial for users to perceive community-based fact checks as helpful. De- spite showing promising potential, our analysis suggests that Birdwatch’s community-driven approach faces challenges concerning opinion speculation and polarization among the user base – in particular with regards to influential accounts. Background Misinformation on Social Media Social media has become a prevalent platform for consum- ing and sharing information online (Bakshy, Messing, and Adamic 2015). It is estimated that almost 62% of the adult population consume news via social media, and this propor- tion is expected to increase further (Pew Research Center 2016). As any user can share information, quality control for the content has essentially moved from trained journalists to regular users (Kim and Dennis 2019). The inevitable lack of oversight from experts makes social media vulnerable to the spread of misinformation (Shao et al. 2016). Social media platforms have indeed been observed to be a medium that disseminates vast amounts of misinformation (Vosoughi, Roy, and Aral 2018). Several works have studied diffusion characteristics of misinformation on social media (Friggeri et al. 2014; Vosoughi, Roy, and Aral 2018), finding that misinformation spreads significantly farther, faster, deeper, and more broadly than the truth. The presence of misin- formation on social media has detrimental consequences on how opinions are formed and the offline world (Allcott and Gentzkow 2017; Bakshy, Messing, and Adamic 2015; Del Vicario et al. 2016; Oh, Agrawal, and Rao 2013). Mis- information thus threatens not only the reputation of indi- viduals and organizations, but also society at large. Previous research has identified several reasons why mis- information is widespread on social media. On the one hand, misinformation pieces are often intentionally written to mis- lead other users (Wu et al. 2019). It is thus difficult for users to spot misinformation from the content itself. On the other hand, the vast majority of social media users do not fact- check articles they read (Geeng, Yee, and Roesner 2020; Vo and Lee 2018). This indicates that social media users are often in a hedonic mindset and avoid cognitive reasoning such as verification behavior (Moravec, Minas, and Dennis 2019). A recent study further suggests that the current de- sign of social media platforms may discourage users from reflecting on accuracy (Pennycook et al. 2021). The spread of misinformation can also be seen as a social phenomenon. Online social networks are characterized by homophily (McPherson, Smith-Lovin, and Cook 2001), (po- litical) polarization (Levy 2021), and echo chambers (Bar- ber ́a et al. 2015). In these information environments with low content diversity and strong social reinforcement, users tend to selectively consume information that shares similar views or ideologies while disregarding contradictory argu- ments (Ecker, Lewandowsky, and Tang 2010). These effects can even be exaggerated in the presence of repeated expo- sure: once misinformation has been absorbed, users are less likely to change their beliefs even when the misinformation is debunked (Pennycook, Cannon, and Rand 2018). Identification of Misinformation Identification via expert’s verification: Misinformation can be manually detected by human experts. In traditional media (e. g., newspapers), this task is typically carried out by professional journalists, editors, and fact-checkers who verify information before it is published (Kim and Den- nis 2019). With the possibility for everyone to share in- formation, social media essentially transfers quality con- trol to platform users (Kim and Dennis 2019). This has given rise to a number of third-party fact-checking orga- nizations such as snopes.com and politifact.com that thor- oughly investigate and debunk rumors (Wu et al. 2019; Vosoughi, Roy, and Aral 2018). The fact-checking assess- ments are consumed and broadcast by social media users like any other type of news content and are supposed to help users to identify misinformation on social media (Shao et al. 2016). However, the experts’ verification approach has inherent drawbacks: (i) it does not scale to the volume and speed of content that is generated in social media. Users can create misleading content at a much faster rate than fact- checkers can evaluate it. Misinformation is thus likely to go unnoticed during its period of peak virality (Epstein, Penny- cook, and Rand 2020). (ii) Approximately 50% of Ameri- cans overall think that fact-checkers are biased and distrust fact-checking corrections (Poynter 2019). Hence even when users become aware of the fact-checks of fact-checking or- ganizations, their impact may be limited by lack of trust. In fact, Vosoughi et al. 2018 show that fact-checked false ru- mors are more viral than the truth. Identification using machine learning methods: Pre- vious research has intensively focused on the problem of misinformation detection via means of supervised ma- chine learning (Castillo, Mendoza, and Poblete 2011; Ducci, Kraus, and Feuerriegel 2020; Vosoughi, Mohsenvand, and Roy 2017). Researchers typically collect posts and their la- bels from social media platforms, and then train a classifier based on diverse sets of features. For instance, content-based approaches aim at directly detecting misinformation based on its content, such as text, images, and video. However, different from traditional text categorization tasks, misinfor- mation posts are deliberately made seemingly real and ac- curate. The predictive power of the actual content in detect- ing misinformation is thus limited (Wu et al. 2019). Other common information sources include context-based features (e. g., time, location), or propagation patterns (i. e., how mis- information circulates among users) (e.g., Kwon and Cha 2014). Yet also with these features, the lack of ground truth labels poses serious challenges when training a machine learning classifier – in particular in the early stages of the spreading process (Wu et al. 2019). Identification through wisdom of crowds: Recent re- search has proposed to outsource fact-checking of misinfor- mation to non-expert fact-checkers in the crowd (Micallef et al. 2020; Bhuiyan et al. 2020; Pennycook and Rand 2019a; Epstein, Pennycook, and Rand 2020; Allen et al. 2020; Allen et al. 2021). The idea is to identify misleading so- cial media content by harnessing the wisdom of crowds (Woolley et al. 2010). The wisdom of crowds has been re- peatedly observed in a wide range of settings, including online platforms such as Wikipedia and Stack Overflow, where the crowd ensures relatively trustworthy and high- quality accumulation of knowledge (Okoli et al. 2014). Ap- plying the concept of crowd wisdom to fact-checking on social media may have crucial advantages: (i) compared to the expert’s verification approach, which is limited by the number of professional fact-checkers, crowd-based ap- proaches allow to identify misinformation at a large scale (Pennycook and Rand 2019a). (ii) Community-based fact- checking addresses the problem that many users distrust pro- fessional fact-checks (Poynter 2019). (iii) The wisdom of crowds literature suggests that, even if the ratings of indi- vidual users are noisy and ineffective, in the aggregate user judgments may be highly accurate (Woolley et al. 2010). Experimental studies found that the crowd can in fact be quite accurate in identifying misleading social media con- tent. Here the assessment of even relatively small crowds is comparable to those from experts (Bhuiyan et al. 2020; Epstein, Pennycook, and Rand 2020; Pennycook and Rand 2019a). Yet, while the crowd may be generally able to accurately identify misinformation, not all users may always choose to do so (Epstein, Pennycook, and Rand 2020). Important challenges include manipulation attempts (Luca and Zervas 2016), lack of engagement in cognitive reasoning (Penny- cook and Rand 2019b), and politically motivated reason- ing (Kahan 2017). Each of these factors may hinder an ef- fective fact-checking system. For example, users could pur- posely try to game the fact-checking system by reporting so- cial media posts as being misleading that do not align with their ideology (irrespective of the perceived veracity) or to achieve partisan ends (Luca and Zervas 2016). Furthermore, the high level of (political) polarization of social media users (Conover et al. 2011; Barber ́a et al. 2015), can result in vastly different interpretations of facts or even entirely dif- ferent sets of acknowledged facts (Otala et al. 2021). Earlier crowd-based fact-checking initiatives such as TruthSquad, Factcheck.EU, and WikiTribune (O’Riordan et al. 2019; Florin 2010) indeed experienced significant quality-issues with community-based fact-checks (Bhuiyan et al. 2020; Bakabar 2018). These initiatives required workarounds or hybrid approaches involving final judgments by experts or delegating primary research to experts and secondary tasks to the crowd (Bhuiyan et al. 2020; Bakabar 2018). This suggests that it is challenging to implement real-world community-based fact-checking platforms that uphold high quality and scalability. More precisely, as pointed out by Ep- stein, Pennycook, and Rand (2020), the challenges that must be overcome in order to implement it successfully are social in nature rather than technical, and thus involve empirical questions about how people interact with community-based fact-checking systems. What is Birdwatch? On January 23, 2021, Twitter has launched the Birdwatch” feature, a new approach to address misinformation on so- cial media by harnessing the wisdom of crowds. In contrast to earlier crowd-based initiatives to fact-checking, Bird- watch is a community-driven approach to identify mislead- ing tweets directly on Twitter. On Birdwatch, users can iden- tify tweets they believe are misleading and write (textual) notes that provide context to the tweet. Birdwatch also fea- tures a rating mechanism that allows users to rate the quality of other users’ notes. These ratings are supposed to help to identify the context that people will find most helpful and raise its visibility to other users. The Birdwatch feature is currently in the pilot phase in the U. S. and only pilot par- ticipants can see community-written fact-checks directly on tweets when browsing Twitter. Users not participating in the pilot can access Birdwatch via a separate Birdwatch web- site1. Twitter’s goal is that community-written notes will be visible directly on tweets, available to everyone on Twitter. Birdwatch notes: Users can add Birdwatch notes to any tweet they come across and think might be misleading. Notes are composed of (i) multiple-choice questions that al- low users to state why a tweet might or might not be mis- leading; (ii) an open text field where users can explain their judgment, as well as link to relevant sources. The maximum number of characters in the text field is 280. Here, each URL counts as only 1 character towards the 280 character limit. After it’s submitted, the note is available for other users to read and rate. Birdwatch notes are public, and anyone can browse the Birdwatch website to see them. Ratings: Users on Birdwatch can rate the helpfulness of notes from other users. These ratings are supposed to help to identify which notes are most helpful and to allow Bird- watch to raise the visibility of the context that is found most helpful by a wide range of contributors. Tweets with Bird- watch notes are highlighted with a Birdwatch icon that helps users to find them. Users can then click on the icon to read all notes others have written about that tweet and rate their quality (i. e., whether or not the Birdwatch note is helpful). 1The Birdwatch website is available via birdwatch.twitter.com Source tweet Birdwatch note Ratings Categorization Text explanation Helpful Unhelpful #1 Paul Elliott Johnson (@RhetoricPJ): Rush Limbaugh had a regular radio segment where he would read off the names of gay people who died of AIDS and celebrate it and play horns and bells and stuff.” :::::: Misleading These segments are fictitious and have never happened.” 3 21 #1 Alexandria Ocasio-Cortez (@AOC): I am happy to work with Republicans on this issue where there’s common ground, but you almost had me murdered 3 weeks ago so you can sit this one out. Happy to work w/ almost any other GOP that aren’t trying to get me killed. In the meantime if you want to help, you can resign. [LINK]” Misleading While I can understand AOC being worried she may have been hurt by the Capitol Protesters she cannot claim Ted Cruz tried to have her killed without providing proof. That’s dangerous misinformation by falsely accusing Ted Cruz of a serious crime.” 47 12 #3 Adam Kinzinger (@RepKinzinger): The vast majority of Americans believe in universal background checks. As a gun owner myself, I firmly support the Second Amendment but I also believe we have to be willing to make some changes for the greater good. Read my full statement on #HR8 here: [LINK]” Not misleading Universal background checks enjoy high levels of public support; a 2016 representative survey found 86% of registered voters in the United States supported the measure. [LINK]” 7 0 #4 Richard Grenell (@RichardGrenell): Congratulations to @PeteButtigieg on becoming the second openly gay member of a President’s Cabinet. Welcome to the club! [LINK]” Not misleading This is correct. Ric Grenell was the first openly gay cabinet member. [LINK]” 5 2 Table 1: Examples of Birdwatch notes and ratings. The columns Helpful” and Unhelpful” refer to the number of users who have responded Yes” (No”) to the rating question Is this note helpful?” Wavy underlining indicates fact-checks that are clearly false (i. e., abuse of the fact-checking feature). Notes rated by the community to be particularly helpful then receive a currently rated helpful badge. Illustrative examples: Tbl. 1 presents four examples of Birdwatch notes and helpfulness ratings. The table shows that many Birdwatch notes report tweets that refer to a polit- ical topic (e. g., voting rights). We also see that the text ex- planations differ regarding their informativeness. Some text explanations are longer and link to additional sources that are supposed to support the comments of the author. There are also users trying to abuse Birdwatch by falsely assert- ing that a tweet is misleading. In such situations, other users can down-vote the note through Birdwatch’s rating system. As an example, the (false) assertion in Birdwatch note #1 re- ceived 3 helpful and 21 unhelpful votes. In some cases, users also fact-check tweets for which veracity may be seen as be- ing difficult to assess. For instance, Birdwatch note #2 ad- dresses the source tweet from Democratic Congresswoman Alexandria Alexandria Ocasio-Cortez as a factual assertion that members of the GOP were trying to get her killed during the Capitol riots on January 6, 2021. However, one may also argue that the tweet can be seen a hyperbolic claim to em- phasize that she believes that certain policy views or rhetor- ical stance encouraged the Capitol rioters. This is reflected in mixed helpfulness ratings: Birdwatch note #2 received 47 helpful and 12 unhelpful votes. Data Data Collection We downloaded all Birdwatch notes and ratings between the introduction of the feature on January 23, 2021, and the end of July 2021 from the Birdwatch website. This comprehen- sive dataset contains a total number of 11,802 Birdwatch notes and 52,981 ratings. On Birdwatch, multiple users can write Birdwatch notes for the same tweet. The average num- ber of Birdwatch notes per fact-checked tweet is 1.31. Each Birdwatch note has a unique id (noteId) and each rating refers to a single Birdwatch note. We merged the ratings with the Birdwatch notes using this noteId field. The result is a single dataframe in which each Birdwatch note corre- sponds to one observation for which we know the number of helpful and unhelpful votes from the rating data. Key Variables We now present the key variables that we extracted from the Birdwatch data. All of these variables will be empirically analyzed in the next sections: • Misleading: A binary indicator of whether a tweet has been reported as being misleading by the author of the Birdwatch note (= 1; otherwise = 0). • Text explanation: The user-entered text explanation (max 280 characters) to explain why a tweet is misleading or not misleading. • Trustworthy sources: A binary indicator of whether the author of the Birdwatch note has responded Yes” to the question Did you link to sources you believe most people would consider trustworthy?” (= 1; otherwise = 0). • HVotes: The number of other users who have responded Yes” to the rating question Is this note helpful?” • Votes: The total number of ratings a Birdwatch note has received from other users (helpful & unhelpful). Content Characteristics We use text mining methods to extract the following content characteristics from the text explanations of the Birdwatch notes: • Sentiment: We calculate a sentiment score that measures the extent of positive vs. negative emotions in Birdwatch notes. Our computation follows a dictionary-based ap- proach as in Vosoughi, Roy, and Aral (2018). Here we use the NRC emotion lexicon (Mohammad and Turney 2013), which classifies English words into positive and negative emotions. The fraction of words in the text expla- nations related to positive and negative emotions is then aggregated and averaged to create a vector of positive and negative emotion weights that sum to one. The sentiment score is then defined as the difference between positive and negative emotion scores. • Text complexity: We calculate a text complexity score, specifically, the Gunning-Fog index (Gunning 1968). This index estimates the years of formal education necessary for a person to understand a text upon reading it for the first time: 0.4 × (ASL + 100 × nwsy≥3/nw), where ASL is the average sentence length (number of words), nw is the total number of words, and nwsy≥3 is the number of words with three syllables or more. A higher value thus indicates greater complexity. • Word count: We determine the length of the text explana- tions in Birdwatch notes as given by the number of words. Our text mining pipeline is implemented in R 4.0.2 us- ing the packages quanteda (Benoit et al. 2018) in Version 2.0.1 and sentimentr (Rinker 2019) in Version 2.7.1. Twitter Historical API Each Birdwatch note addresses a single tweet, i. e., the tweet that has been categorized as being misleading or not mis- leading by the author of the Birdwatch note. We used the Twitter historical API to map the tweetID referenced in each Birdwatch note to the source tweet and collected the follow- ing information for the author of each source tweet: • Account name: The name of the Twitter account for which the Birdwatch note has been reported. • Followers: The number of followers, i. e., the number of accounts that follow the author of the source tweet. • Followees: The number of followees, i. e., the number of accounts whom the author of the source tweet follows. • Account age: The age of the author of the source tweet’s account (in years). • Verified: A binary dummy indicating whether the account of the source tweet has been officially verified by Twitter (= 1; otherwise = 0). Empirical Analysis Analysis of Birdwatch Notes (RQ1 & RQ2) In this section, we analyze how users categorize misleading and not misleading tweets in Birdwatch notes. We also ex- plore the reasons because of which Birdwatch users report tweets and how Birdwatch notes for misleading vs. not mis- leading differ in terms of their content characteristics. Categorization of tweets in Birdwatch notes: Fig. 2 shows that users file a larger number of Birdwatch notes for misleading than for not misleading tweets. Out of all Bird- watch notes, 90 % refer to misleading tweets, whereas the remaining 10 % refer to not misleading tweets. Fig. 2 further suggests that authors of Birdwatch notes are more likely to report that they have linked to trustworthy sources for mis- leading (75 %) than for not misleading tweets (64 %).Not misleading Misleading 0 2500 5000 7500 Trustworthy sources No trustworthy sources Figure 2: Number of users who responded Yes” to the ques- tion Did you link to sources you believe most people would consider trustworthy?” Why do users report misleading tweets? Authors of Birdwatch notes also need to answer a checkbox question (multiple selections possible) on why they perceive a tweet as being misleading. Fig. 3 shows that misleading tweets are primarily reported because of factual errors (32 %), lack of important context (30 %), or because they treat unverified claims as facts (26 %). Birdwatch notes reporting outdated information (5 %), satire (3 %), or manipulated media (2 %) are relatively rare. Only 3 % of Birdwatch notes are reported because of other reasons.Manipulated media Satire Other Outdated information Unverified claim as fact Missing important context Factual error 0 2000 4000 Number of Birdwatch Notes Figure 3: Number of Birdwatch notes per checkbox answer option in response to the question Why do you believe this tweet may be misleading?” Why do users report not misleading tweets? Fig. 4 shows that not misleading tweets are primarily reported be- cause they are perceived as factually correct (58 %), clearly refer to personal opinion (19 %), or satire (12 %). Birdwatch users only rarely report tweets containing information that was correct at the time of writing but is now outdated (1 %). 10 % of tweets are reported because of other reasons. Content characteristics: The complementary cumulative distribution functions (CCDFs) in Fig. 5 visualize how Bird- watch notes for misleading vs. not misleading tweets differ in terms of their content characteristics. We find that Bird- watch notes reporting misleading tweets use similarly com- plex language but embed a higher proportion of negative emotions than tweets reported as being not misleading. For tweets reported as being misleading, authors of Birdwatch notes also tend to write longer text explanations.Outdated but not when written Other Clearly satire Personal opinion Factually correct 0 200 400 600 Number of Birdwatch Notes Figure 4: Number of Birdwatch notes per checkbox answer option in response to the question Why do you believe this tweet is not misleading?” (a) Sentiment25 50 100 −1 −0.5 0 0.5 1 Sentiment CCDF (%) Not misleading Misleading (b) Text complexity0.01 0.1 1 10 100 0 10 20 30 40 Text complexity CCDF (%) Not misleading Misleading (c) Word count0.01 0.1 1 10 100 0 50 100 150 200 Word count CCDF (%) Not misleading Misleading Figure 5: CCDFs for (a) sentiment, (b) text complexity, and (c) word count in text explanations of Birdwatch notes. Analysis of Source Tweets (RQ3) We now explor","Pröllochs, N. (2021). Community-Based Fact-Checking on Twitter's Birdwatch Platform. arXiv preprint arXiv:2104.07175."
ML_DB_041,The Effects of Interactive AI Design on User Behavior: An Eyetracking Study of fact-checking COVID19 Claims,"We conducted a lab-based eye-tracking study to investigate how interactivity of an AI-powered fact-checking system affects user interactions, such as dwell time, attention, and mental resources involved in using the system. A within-subject experiment was con ducted, where participants used an interactive and a non-interactive version of a mock AI fact-checking system, and rated their perceived correctness of COVID-19 related claims. We collected web-page interactions, eye-tracking data, and mental workload using NASA TLX. We found that the presence of the affordance of interactively manipulating the AI system’s prediction parameters affected users’ dwell times, and eye-fixations on AOIs, but not mental workload. In the interactive system, participants spent the most time evaluat ing claims’ correctness, followed by reading news. This promising result shows a positive role of interactivity in a mixed-initiative AI-powered system",Computer Science,Proceeding,2022,N,Y,0,"As an important task in Information Retrieval (IR), fact-checking has various implications for both system-centered and user-centered IR. These include which results should be returned, how they should be presented, what models of interaction should be provided, and how can success be evaluated. Many new models for automatic fact- checking of claims were recently developed in machine learning and natural language processing literature [ 7, 8 , 10, 18 ]. These models, however, focused on fully-automated fact-checking, and maximizing model’s predictive accuracy. While accurate predictions are important, a user doubtful of on- line information is likely to remain skeptical of any fact-checking tool [ 12 ]. As with all AI systems, fact-checkers operate on limited resources and thus are failable and prone to errors. Users may ar- rive at a wrong decision influenced by model errors. Nguyen et al. [ 15 ] show that users might trust a fact-checking model even when the model is wrong. Further, Mohseni et al. [ 13 ] show that transparent systems prevent users from over-trusting model predic- tions. Effective human-AI teaming [3] might alleviate such issues in fact-checking models for which a system needs to be transparent. A transparent system could reveal to a user how it made a predic- tion to support a user’s understanding and calibrate trust [ 1 , 3, 4]. Moreover, individual claim assessments will certainly partially rely on the user’s prior worldviews concerning the perceived credibility of sources and claims. Thus, a fact-checking system needs to integrate user beliefs, and enable users to infuse their views and knowledge into the system. Additionally, such a system needs to be transparent, by communi- cating the prediction-uncertainty of its model and enabling users to perform their in-depth reasoning. Recent investigations have also suggested that search engines, like any technology, have the potential to harm their users [2 , 11 , 17 ]. Allowing users to interact with a tool might help in mitigating such harm. In this short paper, we present an eye-tracking and usability analysis of a human-AI mixed-initiative fact-checking system. We believe that such systems can potentially augment the efficiency and scalability of automated IR, with transparent and explainable AI [ 12 , 14, 15 ]. Starting with a claim as a query, the system retrieves relevant news articles. It then infers the degree to which each article arXiv:2202.08901v1 [cs.HC] 17 Feb 2022 CHIIR ’22, March 14–18, 2022, Regensburg, Germany Shi, et al. supports or refutes the claim (stance), as well as each news source’s reputation. The system then aggregates this evidence and predicts the claim correctness. It shows to the user how the information is being used, and what are the sources of the model’s uncertainty. Our focus in this paper is not to evaluate the fact-checking IR model per se, but rather to understand how users interact with the system via two different user interfaces: an interactive interface and a non-interactive interface. In the interactive interface (Figure 1), the model’s predicted source reputation and stance for each retrieved article, is shown to the user. These can be revised via simple sliders to reflect a user’s beliefs and/or to correct erroneousness model estimates. The overall claim prediction is then updated visually in real-time, as the user interacts with the system. In the non- interactive interface (Figure 2), the values of source bars, with no option to change any of the data presented. Therefore, the key research aim is to study the effects of the in- teractive AI design on user behavior in fact-checking systems. This paper investigates three aspects of the user behavior: dwell time, attention, and mental workload. We hypothesize that people would spend more time in the interactive interface, pay more attention to the interactive elements, and that the interactive interface would impose higher mental workload. 2 METHOD 2.1 Experimental design A controlled, within-subjects eye-tracking study was conducted in the Information eXperience usability lab at the University of Texas at Austin (N=40, 22 females). Voluntary participants interacted with two versions (user-interfaces) of a fact-checking system. Partici- pants were pre-screened for native-level English familiarity, 20/20 vision (uncorrected or corrected), and non-expert topic familiarity of the content being shown. A Tobii TX-300 eye-tracker was used to record the participants’ eye movements. Upon completion of the experiment, each participant was compensated with USD 25. 2.2 Tasks Each participant interacted with two versions (user-interfaces) of a fact checking system. In each interface, there were 12 trials. Each trial consisted of viewing a claim and, optionally, its corresponding news articles. Screenshots of single trials from the two versions of the system are shown in Figures 1 and 2. In each trial, a claim was shown at the top of the interface, and surrogates of five related news articles were presented below, each with its corresponding news source, source reputation, news headline, and the article’s stance towards the claim. Based on the article’s stance and news source reputation, the system provided a prediction of the claim’s correctness at the bottom. The news headlines were clickable and upon clicking opened the news article in a new tab. The claims and corresponding news-articles were on the topic of the COVID-19 pandemic. They were handpicked by the re- searchers to simulate a mock version of the fact-checking system for usability analysis. Each claim was selected so as to have a pre- assigned ground-truth correctness value of TRUE, FALSE, or UN- SURE (claims that are partially true, or not totally proven). The TRUE and UNSURE claims were handpicked from reputed web- sites in the medical domain, such as World Health Organization, WebMD, Mayoclinic, Johns Hopkins University, US State Govern- ment webpages, and others. The FALSE claims were selected by searching for coronavirus myths” on popular search engines. The supporting news articles for each claim were collected by manu- ally searching the web. The source reputations for news articles were collected from existing datasets [5 , 16 ], while the stance val- ues of each news article towards each claim was labelled by the researchers. Two example claims are wearing masks is not re- quired during exercising”, and asymptomatic people can transmit COVID-19”. In total there were 24 claims (8 TRUE, 8 FALSE, 8 UN- SURE), distributed equally between both interfaces. The order of the interfaces (interactive / non-interactive) was balanced, and the order of the claims were randomized. The list of claims and corre- sponding news articles used are shared in the GitHub repository: 2.3 Procedure The overall procedure of the experimental session is illustrated in Figure 3. Each session started with two training trials (one in interactive, one in non-interactive) for participants to get famil- iar with the two fact-checking interfaces. Then the participants started trials in one of the two interfaces (experiment blocks), which was randomly chosen and balanced across all participants. In each trial (viewing a claim) participants interacted with the interface freely without a time limit. Participants were also instructed to click on news headlines to open the underlying news articles in a new browser tab, and read it, if they considered it necessary for evaluating the claim. Before and after viewing each claim in the system, participants indicated their perceived-correctness of the claim (which is not analyzed in this short paper). After completing 12 trials in the first interface (block), participants reported their mental workload using the NASA-TLX questionnaire [6 ]. Then they were allowed to take a five-minute break before resuming the second block (in the other interface). NASA-TLX questionnaire was again administered at the end of the block. 2.4 Measures 2.4.1 Eye-tracking (ET) Measures. To study the effect of the dif- ferences in design of the user-interfaces on user’s information behavior, we collected eye-tracking data to reflect the participants’ attention, reading, and information processing. We divided the in- terfaces into six areas of interests (AOIs): (i) claim text (T); (ii) news source names (S); (iii) source reputation (R); (iv) news article head- lines (H); (v) stance of news article towards the claim (A); and (vi) predicted correctness of the claim (C). The ET measures comprise total fixation count, total fixation duration, and average fixation duration. For analysis, we discarded low-quality and non-reading eye tracking data, which are the fixations with durations shorter than 100 milliseconds or longer than 1500 milliseconds [9]. 2.4.2 Interface and News Dwell Times. To assess how much time participants spent in interacting with our system vis a vis how much time they spent on reading the original news articles (which opened in a new browser tab upon clicking the news headlines in the interface), we recorded the total dwell times while participants were viewing the interface (interface dwell time), and when they were reading news articles (news dwell time). Interactive AI for Fact-Checking CHIIR ’22, March 14–18, 2022, Regensburg, Germany Figure 1: Interactive version of the fact-checking system showing different areas of interest (AOIs) Figure 2: Non-interactive version of the system 2.4.3 Mental Workload. We used NASA-TLX questionnaire to mea- sure mental workload after task completion in each interface use. The questionnaire includes six questions which measure six work- load factors. 3 RESULTS 3.1 Dwell time As shown in Figure 4, the participants generally spent more time reading news than using the interface. Overall, in the interactive interface, participants spent more time using the fact-checking system (interface dwell time), as well as reading the underlying news articles (news dwell time). A Wilcoxon signed ranks test (T=255, n=40, p<.05) indicated that the total dwell time spent on reading the underlying news articles Figure 3: Non-interactive version of the system was significantly different between the two interfaces. The sum of the positive difference ranks (Í 𝑅+ = 564) was larger than the sum of the negative difference ranks (Í 𝑅− = 255), showing that people spent more time reading news in the interactive interface, than in the non-interactive interface. The effect size for the test was 0.33, signifying that presence of interactivity in the system had an effect on making users spend more time reading underlying news articles. 3.2 Fixation count and duration UI and ET. Figure 5 shows that total fixation count and total fixation duration have wider spread and higher maximums in the interactive interface. We employed the Wilcoxon signed ranks between the two interfaces to test for significant differences in total fixation count (T=99, n=40, p<.05) and total fixation duration (T=77, n=40, p<.05). The results indicated that the fixation measures differed CHIIR ’22, March 14–18, 2022, Regensburg, Germany Shi, et al. Figure 4: Non-interactive version of the system Figure 5: Non-interactive version of the system significantly between the two interfaces. In addition, the sums of the positive difference ranks for the total fixation count (Í 𝑅+ = 721) and total fixation duration (Í 𝑅+ = 743) were larger than the sums of the negative difference ranks for the total fixation count (Í 𝑅− = 99) and total fixation duration (Í 𝑅− = 77), respectively. Therefore, there were more fixations and overall longer eye-dwell time in the interactive interface. Moreover, the effect size for the matched-pair samples was 0.66 for total fixation count and 0.71 for total fixation duration, showing that presence of interactivity had a strong effect of attracting participants’ visual attention and processing. We did not find any significant differences in average fixation duration. AOIs, UI and ET. Figure 6 illustrates that most fixations and longer fixation durations were on the ‘news article headlines’ area of interest (AOI), followed by the ‘source reputation’, ‘news source’, and ‘stance of article’ AOIs. The ‘predicted correctness’ AOI had fewer fixations and shorter durations, while the ‘claim text’ AOI had the least. Figure 6: Non-interactive version of the system A two-way ANOVA was conducted to examine the effect of interface and AOI on eye-fixation metrics. There was a significant interaction between the effects of interface and AOI on total fixation count, F(5, 464) = 4.00, p < .05, total fixation duration, F(5, 464) = 4.41, p < .05, and average fixation duration, F(5, 464) = 3.42, p < .05. Simple main effects analysis showed that both interface and AOI had significant main effects on all three of the eye-fixation metrics. Figure 7: Non-interactive version of the system Claim correctness, UI and ET. Figure 7 shows the eye-tracking metrics for viewing TRUE, FALSE, and UNSURE claims in the two interfaces. The total fixation count and total fixation duration while viewing UNSURE claims are higher than when viewing TRUE or FALSE claims. To investigate further, a two-way ANOVA was con- ducted to examine the effect of interface and claim-correctness on eye-fixation metrics. Interface type had a significant main effect on all three eye-fixation metrics: total fixation count, F(1, 234) = 37.42, p < .05, total fixation duration, F(1, 234) = 42.18, p < .05, and average fixation count, F(1, 234) = 4.85, p < .05. Claim correctness had a significant main effect on total fixation count, F(2, 234) = 5.95, p < .05, and total fixation duration, F(2, 234) = 5.50, p < .05, but not on average fixation duration. Interaction effects were also not significant. 3.3 Mental workload The Wilcoxon signed ranks test (T=208, n=40, p>.05) indicated that the participants’ mental workload was not significantly different between the two interfaces. Therefore, the interactivity did not significantly influence the mental workload level. 4 DISCUSSION AND CONCLUSION We conducted a lab-based experiment to investigate how interactiv- ity of an AI-powered fact-checking system affects user interactions. We found that the interactivity of the system has an influence on the system dwell time, fixation count and duration, but not mental workload. Overall people tended to spend more time on reading the original news than looking at and interacting with the two systems. This indicated that they did not rely on the system unilaterally, but read the original news to help them make informed decisions. Further- more, people engaged more and spent more time reading the news in the interactive than in the non-interactive system. We found that the interactivity of the interface makes a difference in the fixation counts and durations regardless of the AOI type and claim condi- tion. People always paid more attention to the interactive interface when using the fact-checking system. The news headlines drew Interactive AI for Fact-Checking CHIIR ’22, March 14–18, 2022, Regensburg, Germany the most attention among all the interface elements. By reading the headlines, people decided which news to view. We also found that the fixation counts and fixation duration differed significantly between the news headlines, news source, source reputation, and article stance. Users tended to pay more attention to them, when the interface elements were interactive. The difference between the claim conditions showed that people paid more attention to the UNSURE claims. They needed more information from the system when they were dealing with the UNSURE claims. People’s men- tal workload was not influenced by the system interactivity. Even though they paid more attention to the interactive interface, the amount of perceived mental resources required was apparently not significantly changed. Thus, the interactivity did not increase the self-reported effort of using the system. These findings reveal that the system interactivity plays a pos- itive role in a mixed-initiative AI-powered system. The system interactivity encourages people to spend the most time evaluat- ing the claim correctness and then reading the news, while not imposing higher mental workload on users. Limitations of our work include using only researcher-assigned tasks and claims selected by researchers, which were assessed by participants in the lab environment rather than in the context of their natural information tasks and fact-checking. Additionally, we did not capture participants’ prior knowledge regarding the particular claims in the study. In practice, fact-checking is often performed with constrained time and high error costs. However, this study does not incentivize the participants for accuracy and efficiency. Future work will include using sets of claims on different topics and investigation of user interaction in the context of their naturally generated tasks. Surprisingly, we notice that users spend a signifi- cant chunk of time reading the linked articles. Future work could explore the effect of time constraints and incentivizing participants’ accuracy on engagement with interactive AI. Specifically, a time- constrained setup might encourage participants to engage with the AI outcomes more instead of reading all the articles themselves. We see that participants pay more attention to the interactive interface. However, it is unclear whether this increased attention stems from the apparent correlation between interactivity and the attention required for engagement with interactive interfaces. Future work could bolster the current findings by closely observing prolonged user interaction with the tool.","Shi, L., Bhattacharya, N., Das, A., Lease, M., & Gwizdka, J. (2022, March). The Effects of Interactive AI Design on User Behavior: An Eye-tracking Study of Fact-checking COVID-19 Claims. In ACM SIGIR Conference on Human Information Interaction and Retrieval (pp. 315-320)."
ML_DB_049,Fake News Detection in Social Networks via Crowd Signals,"Our work considers leveraging crowd signals for detecting fake news and is motivated by tools recently introduced by Facebook that enable users to flag fake news. By aggregating users’ flags, our goal is to select a small subset of news every day, send them to an expert (e.g., via a third-party fact-checking organization), and stop the spread of news identified as fake by an expert. The main objective of our work is to minimize the spread of misinformation by stopping the propagation of fake news in the network. It is especially challenging to achieve this objective as it requires detecting fake news with high confidence as quickly as possible. We show that in order to leverage users’ flags efficiently, it is crucial to learn about users’ flagging accuracy. We develop a novel algorithm, Detective, that performs Bayesian inference for detecting fake news and jointly learns about users’ flagging accuracy over time. Our algorithm employs posterior sampling to actively trade off exploitation (selecting news that maximize the objective value at a given epoch) and exploration (selecting news that maximize the value of information towards learning about users’ flagging accuracy). We demonstrate the effectiveness of our approach via extensive experiments and show the power of leveraging community signals for fake news detection.",Computer Science,Proceeding,2018,N,Y,177,"Fake news (a.k.a. hoaxes, rumors, etc.) and the spread of misinfor- mation have dominated the news cycle since the US presidential election (2016). Social media sites and online social networks, for example Facebook and Twitter, have faced scrutiny for being unable to curb the spread of fake news. There are various motivations for ∗Work performed while at ETH Zurich. This paper is published under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. WWW ’18 Companion, April 23–27, 2018, Lyon, France © 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY 4.0 License. ACM ISBN 978-1-4503-5640-4/18/04. generating and spreading fake news, for instance, making polit- ical gains, harming the reputation of businesses, as clickbait for increasing advertising revenue, and for seeking attention1. As a concrete example, Starbucks recently fell victim to fake news with a hoax advertisement claiming that the coffee chain would give free coffee to undocumented immigrants2. While Starbucks raced to deny this claim by responding to individual users on social media, the lightening speed of the spread of this hoax news in online social media highlighted the seriousness of the problem and the critical need to develop new techniques to tackle this challenge. To this end, Facebook has recently announced a series of efforts towards tackling this challenge [10, 11]. Detection via expert’s verification. Fake news and misinforma- tion have historically been used as tools for making political or business gains [ 9]. However, traditional approaches based on veri- fication by human editors and expert journalists do not scale to the volume of news content that is generated in online social networks. In fact, it is this volume as well as the lightening speed of spread in these networks that makes this problem challenging and requires us to develop new computational techniques. We note that such computational techniques would typically complement, and not replace, the expert verification process—even if a news is detected as fake, some sort of expert verification is needed before one would actually block it. This has given rise to a number of third-party fact-checking organizations such as Snopes3 and Factcheck.org4 as well as a code of principles [ 25 ] that should be followed by these organizations. Detection using computational methods. There has been a re- cent surge in interest towards developing computational methods for detecting fake news (cf., [ 7 ] for a survey)—we provide a more detailed overview of these methods in the Related Work section. These methods are typically based on building predictive models to classify whether a news is fake or not via using a combina- tion of features related to news content, source reliability, and network structure. One of the major challenges in training such predictive models is the limited availability of corpora and the subjectivity of labelling news as fake [ 27 , 33 ]. Furthermore, it is difficult to design methods based on estimating source reliability 1Snopes compiles a list of top 50 fake news stories: Journalism, Misinformation, Fact Checking TrackWWW 2018, April 23-27, 2018, Lyon, France517 and network structure as the number of users who act as sources is diverse and gigantic (e.g., over one billion users on Facebook); and the sources of fake news could be normal users who unin- tentionally share a news story without realizing that the news is fake. A surge of interest in the problem and in overcoming these technical challenges has led to the establishment of a volunteer- ing based association—FakeNewsChallenge5—comprising over 100 volunteers and 70 teams which organizes machine learning compe- titions related to the problem of detecting fake news. 1.1 Leveraging users’ flags. Given the limitation of the current state-of-the-art computational methods, an alternate approach is to develop hybrid human-AI methods via engaging users of online social networks by enabling them to report fake news. In fact, Facebook has recently taken steps towards this end by launching a fake news reporting tool in Germany [11 ], as shown in Figure 1. The idea of this tool is that as news propagates through the network, users can flag the news as fake. Figure 1: Facebook has launched tools in Germany to report fake news. Image source: [11]. As proposed by Facebook [11], the aggregated users’ flags as well as well as other available signals can be used to identify a set of news which potentially is fake. These news can then be sent to an expert for review via a third-party fact-checking organization. If an expert labels the news as fake, it could be removed from the network or marked as disputed making it appear lower in news-feed ranking. The contemporary work by Kim et al . [16] explored the idea of detecting fake news via leveraging users’ flagging activity by using the framework of marked temporal point processes. We highlight the key differences of their approach to ours in the next section. 1.2 Our Contributions In this paper, we develop algorithmic tools to effectively utilize the power of the crowd (flagging activity of users) to detect fake news. Given a set of news, our goal is to select a small subset of k news, send them to an expert for review, and then block the news which are labeled as fake by the expert. We formalize our objective as to 5http://www.fakenewschallenge.org/ minimize the spread of misinformation, i.e., how many users end up seeing a fake news before it is blocked. We design our algorithm Detective, which implements a Bayesian approach for learning about users’ accuracies over time as well as for performing inference to find which news are fake with high confidence. In short, our main contributions include: • We formalize the problem of leveraging users’ flagging ac- tivity for detection of fake news. We showcase the need to learn about users’ accuracy in order to effectively leverage their flags in a robust way. • We develop a tractable Bayesian algorithm, Detective, that actively trades off between exploitation (selecting news that directly maximize the objective value) and exploration (se- lecting news that helps towards learning about users’ flag- ging accuracy). • We perform extensive experiments using a publicly available Facebook dataset to demonstrate the effectiveness of our approach. We plan to make the code publicly available so that other researchers can build upon our techniques for this important and timely problem of detecting fake news. 2 RELATED WORK Contemporary results. Kim et al. [16] explored the idea of detect- ing fake news via leveraging users’ flagging activity. In particular, they introduce a flexible representation of the above problem using the framework of marked temporal point processes. They develop an algorithm, Curb, to select which news to send for fact-checking via solving a novel stochastic optimal control problem. The key technical differences of the approach by Kim et al . [16] to ours are: (1) we learn about the flagging accuracy of individual users in an online setting; in contrast, they consider all users to be equally reliable and estimate the flagging accuracy of the population of users from historical data; (2) our algorithms are agnostic to the actual propagation dynamics of news in the network; they model the actual propagation dynamics as a continuous-time dynamical system with jumps and arrive at an algorithm by casting the prob- lem as an optimal control problem; and (3) we use discrete epochs with a fixed budget per epoch (i.e., the number of news that can be sent to an expert for reviewing); they use continuous time and consider an overall budget for their algorithm. Computational methods for detecting fake news. There is a large body of related work on rumor detection and information credibility evaluation (with a more recent focus on fake news de- tection) that are applicable to the problem of detecting fake news. These methods are typically based on building predictive models to classify whether a news is fake. At a high-level level, we can cat- egorize these methods as follows: (i) based on features using news content via natural language processing techniques [ 13 , 31, 34 , 38]; (ii) via learning models of source reliability and trustworthiness [ 20, 22, 28]; (iii) by analyzing the network structure over which a news propagated [ 6 ]; and (iv) based on a combination of the above- mentioned features, i.e., linguistic, source, and network structure [ 1, 17 , 18, 35 ]. As we pointed out in the Introduction, there are several key challenges in building accurate predictive models for identifying fake news including limited availability of corpora, sub- jectivity in ground truth labels, and huge variability in the sourcesTrack: Journalism, Misinformation, Fact Checking TrackWWW 2018, April 23-27, 2018, Lyon, France518 who generate fake news (often constituting users who do it unin- tentionally). In short, these methods alone have so far proven to be unsuccessful in tackling the challenge of detecting fake news. Leveraging crowd signals for web applications. Crowdsourcing has been used in both industrial applications and for research stud- ies in the context of different applications related to web security. For instance, [ 23] and [ 5 ] have evaluated the potential of leveraging the wisdom of crowds for assessing phishing websites and web security. Their studies show a high variability among users—(i) the participation rates of users follows a power-law distribution, and (ii) the accuracy of users’ reports vary, and users with more experience tend to have higher accuracy. The authors also discuss the potential of voting fraud when using users’ reports for security related applications. Wang et al . [32] performed a crowdsourcing study on Amazon’s Mechanical Turk for the task of sybil detection in online social networks. Their studies show that there is a huge variability among crowd users in terms of their reporting accura- cies that needs to be taken into account for building a practical system. Chen et al . [3], Zheleva et al . [39] present a system sim- ilar to that of ours for the task of filtering email spam and SMS spam, respectively. The authors discuss a users’ reputation system whereby reliable users (based on history) can be weighted more when aggregating the reports. However, their work assumes that users’ reputation/reliability is known to the system, whereas the fo- cus of our paper is on learning users’ reputation over time. Freeman [12] discusses the limitations of leveraging user feedback for fake account detection in online social networks—via data-driven stud- ies using Linkedin data, the authors show that there is only a small number of skilled users (who have good accuracy that persists over time) for detecting fake accounts. Crowdsourcing with expert validation On a technical side, our approach can be seen as that of a semi-supervised crowdsourcing technique where users’ answers can be validated via an external expert. Hung et al . [14] , Liu et al . [21] present probabilistic models to select specific news instances to be labeled by experts that would maximize the reduction in uncertainty about users’ accuracy. With a similar flavor to ours, Zhao et al . [36] presents a Bayesian approach to aggregate information from multiple users, and then jointly infer users’ reliability as well as ground truth labels. Similar to our approach, they model users’ accuracy via two separate parameters for false positive and false negative rates. However, their approach is studied in an unsupervised setting where no expert validation (ground truth labels) are available. 3 THE MODEL We provide a high-level specification of our model in Protocol 1. There is an underlying social network denoted as G = (U , E) where U is the set of users in the network. We divide the execution into different epochs denoted as t = 1, 2, . . . ,T , where each epoch could denote a time window, for instance, one day. Below, we provide details of our model—the process of news generation and spread, users’ activity of flagging the news, and selecting news to get ex- pert’s labels. 3.1 News Generation and Spread We assume that new news, denoted by the set X t , are generated at the beginning of every epoch t (cf., line 4).6 In this paper, we consider a setting where each news has an underlying label (un- known to the algorithm) of being “fake"" (f ) or “not fake"" (  ̄f ). We use random variable Y ∗(x) to denote this unknown label for a news x and its realization is given by y∗(x) ∈ { f ,  ̄f }. The label y∗(x) can only be acquired if news x is sent to an expert for review who would then provide the true label. We maintain a set of “active"" news At (cf., line 5) which consists of all news that have been generated by the end of epoch t but for which expert’s label have not been acquired yet. Each news x is associated with a source user who seeded this news, denoted as ox (cf., line 4). We track the spread of news in the set At via a function π t : At → 2U . For a news a ∈ At , the function π t (a) returns the set of users who have seen the news a by the end of epoch t. During epoch t, let ut (a) ⊆ U \ π t −1(a) be the set of additional users (possibly the empty set) to whom news a ∈ At propagates in epoch t, hence π t (a) = π t −1(a) ∪ ut (a) (cf., line 9). 3.2 Users’ Activity of Flagging the News In epoch t, when a news a ∈ At propagates to a new user u ∈ ut (a), this user can flag the news to be fake. We denote the set of users who flag news a as fake in epoch t via a set lt (a) ⊆ ut (a) (cf., line 10). Furthermore, the function ψ t (a) returns the complete set of users who have flagged the news a as fake by the end of epoch t.7 For any news x and any user u ∈ U , we denote the label user u would assign to x via a random variable Yu (x). We denote the realization of Yu (x) as yu (x) ∈ { f ,  ̄f } where yu (x) = f signifies that user has flagged the news as fake. In this paper, we consider a simple, yet realistic, probabilistic model of a user’s flagging activity as discussed below. User abstaining from flagging activity. Reflecting the behavior of real-world users, user u might abstain from actively reviewing the news content (and by default, does not flag the news)—we model this happening with a probability γu ∈ [0, 1]. Intuitively, we can think of 1 − γu as the engagement of user u while participating in this crowdsourcing effort to detect fake news: γu = 1 means that the user is not participating at all. User’s accuracy in flagging the news. With probability (1 − γu ), user u reviews the content of news x and labels the news. We model the accuracy/noise in the user’s labels, conditioned on that the user is reviewing the content, as follows: • αu ∈ [0, 1] denotes the probability that user u would not flag the news as fake, conditioned on that news x is not fake and the user is reviewing the content. • βu ∈ [0, 1] denotes the probability that user u would flag the news as fake, conditioned on that news x is fake and the user is reviewing the content. User’s observed activity. Putting this together, we can quantify the observed flagging activity of user u for any news x with the 6For simplicity of presentation, we consider every news generated in the network to be unique. In real-world settings, the same news might be posted by multiple users because of externalities, and it is easy to extend our model to consider this scenario. 7Note that as per specification of Protocol 1, for any news x , the source user ox doesn’t participate in flagging x .Track: Journalism, Misinformation, Fact Checking TrackWWW 2018, April 23-27, 2018, Lyon, France519 Protocol 1: High-level specification of our model 1 Input: social network graph G = (U , E); labeling budget per epoch k. 2 Initialize: active news A0 = {} (i.e., news for which expert’s label is not acquired yet). 3 foreach t = 1, 2, . . . ,T do /* At the beginning of epoch t */ 4 News X t are generated with ox ∈ U as the origin/source of x ∈ X t . 5 Update the set of active news as At = At −1 ∪ X t . ∀x ∈ X t , do the following: 6 Initialize users exposed to the news x as π t −1(x) = {}. 7 Initialize users who flagged the news x as ψ t −1(x) = {}. /* During the epoch t */ 8 News At continue to propagate in the network. ∀a ∈ At , do the following: 9 News a propagates to more users ut (a) ⊆ U \ π t −1(a); i.e., π t (a) = π t −1(a) ∪ ut (a). 10 News a is flagged as fake by users lt (a) ⊆ ut (a); i.e., ψ t (a) = ψ t −1(a) ∪ lt (a). /* At the end of epoch t */ 11 Algorithm Algo selects a subset St ⊆ At of up to size k to get expert’s labels given by y∗(s) ∈ { f ,  ̄f } ∀ s ∈ St . 12 Block the fake news, i.e., ∀s ∈ St s.t. y∗(s) = f , remove s from the network. 13 Update the set of active news as At = At \ St Note that news s ∈ S t s.t. y∗(s) =  ̄f remain in the network, continue to propagate, and being flagged by users following matrix defined by variables (θu,  ̄f , θu, f ): [ θu,  ̄f 1 − θu, f 1 − θu,  ̄f θu, f ] = γu [1 1 0 0 ] + (1 − γu ) [ αu 1 − βu 1 − αu βu ] where    θu,  ̄f ≡ P (Yu (x) =  ̄f | Y ∗(x) =  ̄f ) 1 − θu,  ̄f ≡ P (Yu (x) = f | Y ∗(x) =  ̄f ) θu, f ≡ P (Yu (x) = f | Y ∗(x) = f ) 1 − θu, f ≡ P (Yu (x) =  ̄f | Y ∗(x) = f ) The two parameters (αu , βu ) allow us to model users of different types that one might encounter in real-world settings. For instance, • a user with (αu ≥ 0.5, βu ≤ 0.5) can be seen as a “news lover"" who generally tends to perceive the news as not fake; on the other hand, a user with (αu ≤ 0.5, βu ≥ 0.5) can be seen as a “news hater"" who generally tends to be skeptical and flags the news (i.e., label it as fake). • a user with (αu = 1, βu = 1) can be seen as an “expert” who always labels correctly; a user with (αu = 0, βu = 0) can be seen as a “spammer” who always labels incorrectly. 3.3 Selecting News to Get Expert’s Label At the end of every epoch t, we apply an algorithm Algo—on behalf of the network provider—which selects news St ⊆ At to send to an expert for reviewing and acquiring the true labels y∗(s) ∀s ∈ St (cf., line 11). If a news is labeled as fake by the expert (i.e., y∗(s) = f ), this news is then blocked from the network (cf., line 12). At the end of the epoch, the algorithm updates the set of active news as At = At \ St (cf., line 13). We will develop our algorithm in the next section; below we introduce the formal objective of minimizing the spread of misinformation via fake news in the network. 3.4 Objective: Minimizing the Spread of Fake News Let’s begin by quantifying the utility of blocking a news a ∈ At at epoch t—it is important to note that, by design, only the fake news are being blocked in the network. Recall that |π t (a)| denotes the number of users who have seen news a by the end of epoch t. We introduce |π ∞(a)| to quantify the number of users who would eventually see the news a if we let it spread in the network. Then, if a news a is fake, we define the utility of blocking news a at epoch t as valt (a) = |π ∞(a)| − |π t (a)|, i.e., the utility corresponds to the number of users saved from being exposed to fake news a. If an algorithm Algo selects set St in epoch t, then the total expected utility of the algorithm for t = 1, . . . ,T is given by Util(T , Algo) = TÕ t =1 E [ Õ s ∈S t 1{y∗(s)=f }valt (s) ] (1) where the expectation is over the randomness of the spread of news and the randomness in selecting St ∀t ∈ {1, . . . ,T }. In this work, we will assume that the quantity valt (a) in Equation 1 can be estimated by the algorithm. For instance, this can be done by fitting parameters of an information cascade model on the spread π t (a) seen so far for news a, and then simulating the future spread by using the learnt parameters [8, 26, 37]. Given the utility values valt (·), we can consider an oracle Oracle that has access to the true labels y∗(·) for all the news and maxi- mizes the objective in Equation 1 by simply selecting k fake news with highest utility. In the next section, we develop our algorithm Detective that performs Bayesian inference to compute y∗(·) using the flagging activity of users as well as via learning users’ flagging accuracy {θu,  ̄f , θu, f }u ∈U from historic data. 4 OUR METHODOLOGY In this section we present our methodology and our algorithm Detective. We start by describing how news labels can be inferredTrack: Journalism, Misinformation, Fact Checking TrackWWW 2018, April 23-27, 2018, Lyon, France520 for the case in which users’ parameters are fixed. Next, we consider the case in which users’ parameters are unknown and employ a Bayesian approach for inferring news labels and learning users’ pa- rameters. Given a prior distributions on the users’ parameters and a history of observed data (users’ flagging activities and experts’ labels obtained), one common approach is to compute a point esti- mate for the users’ parameters (such as MAP) and use that. However, this can lead to suboptimal solutions because of limited exploration towards learning users’ parameters. In Detective, we overcome this issue by employing the idea of posterior sampling [24, 29]. 4.1 Inferring News Labels: Fixed Users’ Params We take a Bayesian approach to deal with unknown labels y∗(·) for maximizing the objective in Equation 1. As a warm-up, we begin with a simpler setting where we fix the users’ labeling parameters (θu,  ̄f , θu, f ) for all users u ∈ U . Let’s consider epoch t and news a ∈ At for which we want to infer the true label y∗(a). Let ω be the prior that a news is fake; then, we are interested in computing: P(Y ∗(a) = f | {θu,  ̄f , θu, f }u ∈U , ω,ψ t (a), π t (a)) ∝ ω · Ö u ∈ψ t (a) P ( Yu (a) = f | Y ∗(a) = f , θu, f ) · Ö u ∈π t (a)\ψ t (a) P ( Yu (a) =  ̄f | Y ∗(a) = f , θu, f ) = ω · Ö u ∈ψ t (a) θu, f · Ö u ∈π t (a)\ψ t (a) (1 − θu, f ) where the last two steps follow from applying Bayes rule and as- suming that users’ labels are generated independently. Note that both users’ parameters {θu,  ̄f , θu, f }u ∈U affect the posterior proba- bility of a news being fake as the normalization constant depends on both P(Y ∗(a) = f | ·) and P(Y ∗(a) =  ̄f | ·). At every time t ∈ {1, . . . ,T }, we can use the inferred posterior prob- abilities to greedily select k news St ⊆ At , |St | = k that maximize the total expected utility, i.e., Õ s ∈S t P(Y ∗(s) = f | ·) · valt (s). (2) This greedy selection can be performed optimally by selecting k news with the highest expected utility. This is implemented in our algorithm TopX, shown in Algorithm 2. 4.2 Inferring News Labels: Learning Users’ Params In our setting, the users’ parameters {θu,  ̄f , θu, f }u ∈U are unknown and need to be learnt over time. Learning about users. We assume a prior distribution over the users’ parameters (Θ  ̄f , Θf ) shared among all users. For each user u ∈ U , we maintain the data history in form of the following matrix: Dt u =       dt u,  ̄f |  ̄f dt u,  ̄f |f dt u, f |  ̄f dt u, f |f       . The entries of this matrix are computed from the news for which experts’ labels were acquired. For instance, dt u,  ̄f |  ̄f represents the Algorithm 2: Algorithm TopX 1 Input: • Active news At ; information valt (·), lt (·), π t (·) • budget k; news prior ω • users’ parameters {θu,  ̄f , θu, f }u ∈U . 2 Compute p(a) for all a ∈ At as P(Y ∗(a) = f | {θu,  ̄f , θu, f }u ∈U , ω, lt (a), π t (a)) 3 Select St = arg maxS ⊆At , |S | ≤k Ía ∈S p(a)valt (a) 4 Return: St count of how often the user u labeled a news as not fake and the acquired expert’s label was not fake. Given Dt u , we can compute the posterior distribution over the users’ parameters using Bayes rules as follows: P(θu,  ̄f | Θ  ̄f , Dt u ) ∝ P(Dt u | θu,  ̄f ) · P(θu,  ̄f | Θ  ̄f ) = (θu,  ̄f )dt u,  ̄f |  ̄f · (1 − θu,  ̄f )dt u, f |  ̄f · P(θu,  ̄f | Θ  ̄f ) Similarly, one can compute P(θu, f | ·). Inferring labels. We can now use the users’ parameters posteriors distributions to infer the labels, for instance, by first computing the MAP parameters θ MAP u,  ̄f = arg max θu,  ̄f P(θu,  ̄f | Θ  ̄f , Dt u ) (and θ MAP u, f similarly) and invoking the results from the previous section.8 Then, at every epoch t we can invoke TopX with a point es- timate for the users’ parameters to select a set St of news. However this approach can perform arbitrarily bad compared to an algorithm that knows the true users’ parameters (we refer to this algorithm as Opt) as we show in our analysis. The key challenge here is that of actively trading off exploration (selecting news that maximize the value of information towards learning users’ parameters) and exploration (selecting news that directly expected utility at a given epoch). This is a fundamental challenge that arises in sequential decision making problems, e.g., in multi-armed bandits [ 2], active search [4, 30] and reinforcement learning. 4.3 Our Algorithm Detective In this section, we present our algorithm Detective, shown in Algorithm 3, that actively trades off between exploration and ex- ploitation by the use of posterior sampling aka Thompson sam- pling [ 24, 29 ]. On every invocation, the algorithm samples the users’ parameters from the current users’ posterior distributions and invokes TopX with these parameters. Intuitively, we can think of this approach as sampling users’ parameters according to the probability they are optimal. Analysis. We analyze our algorithms in a simplified variant of Protocol 1, in particular we make the following simplifications: 8Note that a fully Bayesian approach for integrating out uncertainty about users’ parameters in this case is equivalent to using the mean point estimate of the posterior distribution.Track: Journalism, Misinformation, Fact Checking TrackWWW 2018, April 23-27, 2018, Lyon, France521 Algorithm 3: Algorithm Detective 1 Input: • user priors Θf , Θ  ̄f ; users’ histories {Dt u }u ∈U . 2 Sample θu,  ̄f ∼ P(θu,  ̄f | Θ  ̄f , Dt u ), θu, f ∼ P(θu, f | Θf , Dt u ) 3 St ← Invoke TopX with parameters {θu,  ̄f , θu, f }u ∈U 4 Return: St (1) There are M sources o1, . . . , oM , each generating news every epoch t. (2) For any news x seeded at epoch t, valτ (x) > 0 only for τ = t. This means that news x reaches it maximum spread at the next timestep t + 1, hence the utility of detecting that news drops to 0. To state our theoretical results, let us introduce the regret of an algorithm Algo as Regret(T , Algo) = Util(T , Opt) − Util(T , Algo). We can now immediately state our first theoretical result, highlight- ing the necessity of exploration. Proposition 1. Any algorithm Algo using deterministic point estimates for the users’ parameters suffers linear regret, i.e., Regret(T , Algo) = Θ(T ). Proof sketch. The proof follows by considering a simple prob- lem involving two users, where we have perfect knowledge about one user with parameters (0.5 + ε, 0.5 + ε) and the other user either has parameters (1, 1) or (0, 0) (expert or spamer). The key idea here is that any algorithm using point estimates can be tricked into al- ways making decisions based on the first user’s flagging activities and is never able to learn about the perfect second user. _x0003_ The above result is a consequence of insufficient exploration which is overcome by our algorithm Detective, as formalized by the following theorem. Theorem 1. The expected regret of our algorithm Detective is E[Regret(T , Detective)] = O(C√M ′T log(CM ′T )), where M ′ = (M k ) and C is a problem dependent parameter. C quantifies the total number of realizations of how M news can spread to U users and how these users label the news. Proof sketch. The proof of this theorem follows via interpret- ing the simplified setting as a reinforcement learning problem. Then, we can apply the generic results for reinforcement learning via pos- terior sampling of Osband et al . [24]. In particular, we map our problem to an MDP with horizon 1 as follows. The actions in the MDP correspond to selecting k news from the M sources, the re- ward for selecting a set of news S is given by Equation 2 (evaluated using the true users’ parameters). _x0003_ Given that the regret only grows as O(√T ) (i.e., sublinear in T ), this theorem implies that Detective converges to Opt as T → ∞. However, as a conservative bound on C could be exponential in |U | and M, convergence may be slow. Nevertheless, in practice we observe competitive performance of Detective compared to Opt as indicated in our experiments. Hence, Detective overcomes the issues in Proposition 1, and actively trades off exploration and exploitation. 5 EXPERIMENTAL SETUP Social network graph and news generation. We consider the social circles Facebook graph [ 19], consisting of 4,039 users (nodes) U and 88,234 edges, computed from survey data collected by using a Facebook app for identifying social circles. Every user can be the seed of news as described shortly and to every user a probability is assigned with which it (hypothetically) generates fake news in case it seeds news. In particular, 20% of the users generate fake news with probability 0.6, 40% of the users generate fake news with probability 0.2 and the remaining 40% of the users generate fake news with probability 0.01 (the class of a user is assigned randomly). For determining the seeds of news, we partition the users into users Un which commonly spread news and users Ur = U \Un which only occasionally spread news. That is, in every iteration of Protocol 1, we select M = 25 users for generating news, where users in Un are selected with probability 0.5 |Un | and users in Ur are selected with probability 0.5 |Ur | . Hence, in our experimental setup this corresponds to a prior for seeding fake news of about 20%, i.e., ω ≈ 0.2. News spreading. In our experiments, news spread according to an independent cascade model [ 15 ], i.e., the diffusion process of every news is a separate independent cascade with infection probability 0.1 + U[0, 0.1] (fixed when the news is seeded). In every epoch of Protocol 1, we perform two iterations of the independent cascade models to determine the news spread at the next epoch. We estimate the number of users who would eventually see news a, i.e., |π ∞(a)|, by executing the independent cascade models for each news for 600 iterations. Users’ parameters. In our experiments we consider three types of users, i.e., good users (αu = βu = 0.9), spammers (αu = βu = 0.1) and indifferent users (αu = βu = 0.5). Unless specified otherwise, each user is randomly assigned to one of these three types. Also, we set γu = 0 unless specified otherwise (note that 1 −γu quantifies the engagement of a user). Algorithms. We execute Protocol 1 for T = 100 epochs. In every epoch of Protocol 1, the evaluated algorithms select k = 5 news to be reviewed by an expert. In our experiments we compare the performance of Detective, Opt (unrealistic: TopX invoked with the true users’ parameters), Oracle (unrealistic: knows the true news labels). In addition, we consider the following baselines: • Fixed-CM. This algorithm leverages users’ flags without learning about or distinguishing between users. It uses fixed users parameters θu,  ̄f = θu, f = 0.6 for invoking TopX. • No-Learn. This algorithm does not learn about users and does not consider any user flags. It greedily selects those news with highest valt (·), i.e., St = arg max S ⊆At , |S |=k Õ s ∈S valt (s). • Random. This algorithm selects a random set St ⊆ At , |St | = k of active news for labeling by experts.Track: Journalism, Misinformation, Fact Checking TrackWWW 2018, April 23-27, 2018, Lyon, France522 1 20 40 60 80 100 time t 0.0 0.2 0.4 0.6 0.8 1.0 1.2 average utility ×10−2 RANDOM ORACLE NO-LEARN DETECTIVE OPT(a) Learning about users0.0 0.2 0.4 0.6 0.8 1.0 engagement 0.0 0.2 0.4 0.6 0.8 1.0 utility RANDOM ORACLE NO-LEARN DETECTIVE OPT (b) Users’ engagement in flagging.0.0 0.2 0.4 0.6 0.8 1.0 fraction of good users 0.0 0.2 0.4 0.6 0.8 1.0 utility RANDOM ORACLE NO-LEARN DETECTIVE OPT FIXED-CM (c) Robustness against spammers Figure 2: Experimental results. (a) Learning about users: Detective achieves average utility competitive compared to that of Oracle (which knows the true news labels). The average utility of Detective converges to that of Opt as Detective progressively learns the users’ parameters. (b) Users’ engagement in flagging: even with low engagement Detective can effectively leverage crowd signals to detect fake news. (c) Robustness against spammers: Detective is effective even if the majority of users is adversarial, highlighting the importance of ","Tschiatschek, S., Singla, A., Gomez Rodriguez, M., Merchant, A., & Krause, A. (2018, April). Fake news detection in social networks via crowd signals. In Companion proceedings of the the web conference 2018 (pp. 517-524)."
ML_DB_051,A Content Management Perspective on Fact-Checking,"Fact checking has captured the attention of the media and the public alike; it has also recently received strong attention from the computer science community, in particular from data and knowledge management, natural language processing and information retrieval; we denote these together under the term content management”. In this paper, we identify the fact checking tasks which can be performed with the help of content management technologies, and survey the recent research works in this area, before laying out some perspectives for the future. We hope our work will provide interested researchers, journalists and fact-checkers with an entry point in the existing literature as well as help develop a roadmap for future research and development work.",Computer Science,Proceeding,2018,Y,N,61,"Journalism is about reporting the state of the world and events that affect society. Journalists’ ethics aim to accuracy, objectivity, impartiality and accountability. However, these are notoriously hard to ensure, as individuals perceive the world subjectively through the prism of their personal mix of our experiences and values. Thus, while the perception of some events may reach a certain degree This paper is published under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. of consensus (e.g. ongoing natural disaster), others are violently disagreed upon (e.g. claims made about candidates’ past in an election campaign). The tension between fact reporting and value- and emotion-based interpretation is also visible in the variety of journalist roles, with reporter and fact checker at one end of the spectrum focusing on facts, and analysts, pundits and columnists at the other end, each interpreting and distilling such reports into a narrative their audience appreciates them for. The Internet has also changed the landscape in an unprecedented way. Paradoxically, in an era where information has never been so widely available, access to relevant, trustworthy and accurate infor- mation is hard to get. In some (mostly online) media sphere, facts widely supported with science are being ignored or marginalized in favor of narratives supported by influence groups; human impact on climate change is the foremost example. Skepticism towards journalists or more generally media has always existed; the Internet is making it easier to lead a social life from which contradiction is absent (bubble effect”) and to be exposed only to content one already agrees with (echo chamber”) and piercing such bubbles with facts is becoming increasingly hard. Also, the multiplication of online media outlets and the possibility to automate misinforma- tion spreading through social bots puts this task out of the reach of human journalists; computerized tools are called for, and are increasingly explored by researchers from many scientific commu- nities. Since 2015, we have been collaborating with journalists from Le Monde, France’s leading national newspaper, in an RD project named ContentCheck, focused on content management technolo- gies for journalistic fact checking [ 38 ]. The goal of this paper is to analyze and understand what role content management technologies, seen as data and knowledge management, information extraction, natural language processing, and their interactions, can play toward facilitating, speeding up and improving fact-checking work, whether performed by journalists or other fact-checkers.1 We stress that we do not believe it is feasible to automate” fact-checking or journalis- tic work; nor is it desirable, since journalistic content is authored by humans who understand, shape and drive the interests and knowl- edge of their fellow humans. Instead, we are interested in the role 1While most fact-checkers are journalists by trade, others are concerned citizens focusing on a special topic, e.g. water pollution in a given city, scientists working on a topic where politics hotly disputes scientific truth etc.Track: Journalism, Misinformation, Fact Checking TrackWWW 2018, April 23-27, 2018, Lyon, France565 content management can play as a provider of models, algorithms and tools. Below, we characterize the perimeter of fact checking-work we are interested in, delimit it from other scientific and engineering areas, and highlight its complexity (Section 2); we survey existing research in the realm of fact checking, or related to it, in recent years (Section 3); finally, we identify some open problems, promising research directions, and requirements requirements for systems and platforms that will be developed in the area, still within a content management perspective. 2 COMPUTATIONAL FACT CHECKING We define the scientific area we consider (Section 2.1), analyze its ingredients (Section 2.2) and finish our outline by acknowledging some of its limitations (Section 2.3). 2.1 Definition and perimeter Originally, fact checking designated the correctness checking of all facts in news article before they are published; this is a core part of the journalistic work. Increasingly though, the term refers to the analyses of claims a posteriori, after a certain article (or tweet, or speech etc.) is published, and often by people unaffiliated with the authors of the original content. Such fact checking work is performed by NGOs maintaining active, high-profile Web sites, such as FactCheck or PolitiFact in the US and FullFact in the UK, by specialists within established news organizations, such as the Fact Checker of the Washington Post2 in the US, the Décodeurs3 with Le Monde and Désintox4 within Libération, both major newspapers in France, and many others. Definition: This paper focuses on a posteriori fact checking, which we define as the investigative process consisting in: (1) extracting claims from some discourse, (2) searching for the facts the claims are based on, (3) assessing the accuracy of the claim with regards to those backing facts, and (4) providing perspective to claims for which there is no straight- forward settlement. Other definitions and design principles proposed in prior work include [ 3 , 14 , 63 ]. We believe our definition is both general and precise enough to encompass specialized systems capable of solving one of the above tasks, as well as end-to-end systems, providing solu- tions to several tasks. Broadly speaking, a full-fledged fact checking system enables the construction of specialized knowledge bases, lifting ambiguities inherent in natural language, correcting honest mistakes as well as purposeful misinformation, and archiving the result of such analysis in persistent, searchable, open repositories. Note that some related scientific areas are left out of the focus of the present work. First, audio, image and video processing have been very dynamic on this subject, notably through the field of multimedia forensics (image [ 6 , 57 , 70] or video [ 42 , 55 ]), leading to verification systems and services such as RevEye, Tineye or In- VID [ 61]; we do not cover their specific techniques and instead point 2https://www.washingtonpost.com/news/fact-checker 3http://www.lemonde.fr/les-decodeurs/ 4http://www.liberation.fr/desintox to the above references. Second, the area of fake news detection is closely related to fact checking, yet we view it as slightly distinct: the former includes any method for classifying a news item as true or false, including methods that do not consider the actual content of the news but instead focus on aspects such as its linguistic style or its pattern of propagation in a social network [ 49, 50, 54 ]. We consider a technique falls in the scope of fact checking as soon as it examines the facts (content) of a claim, usually against some reference information. Clearly, fact checking can be seen a worthy component of fake news detection, to which it can bring the dis- tinctive advantage of an explainable (transparent) analysis, based on the reference facts. 2.2 Fact checking ingredients From the above definition, we can derive a set of steps involved in a fact check, up to the possible successful outcome (the fact is confirmed). This is the case if: (i) the fact leaves little to no room for alternative interpretations, (ii) it is backed by sufficient references to sources, (iii) the sources are reliable5, and (iv) the claim is consistent with the sources. If the claims is too vague, leading to too many distinct interpre- tations, it is hard or impossible to check it effectively. If the fact checker lacks access to sufficient reliable sources, there is not enough background against which to check. If the reliability of the sources is in question, the fact check conclusion is hard to trust. Even in the presence of sufficient and reliable sources, evaluating claim accu- racy is tricky, due in part to the need to contextualize the claim, i.e., understand the precise setting (e.g., in which country, city, at which time) the claim is made; its validity can only be assessed in a spe- cific context. Some claims may be crafted to mislead, i.e., look valid given some context or source that is either irrelevant or flawed. For any step in the process not to be perceived as biased, trans- parency is key. Indeed, this is apparent in the principles instated by the International Fact-Checking Network (IFCN), an organization sponsored by the Poynter Institute to promote excellence in fact checking”:6 (1) A commitment to non-partisanship and fairness. (2) A commitment to the transparency of sources. (3) A commitment to the transparency of function and organiza- tion. (4) A commitment to the transparency of methodology. (5) A commitment to open and honest corrections. Hence, whichever aspect of the fact checking method research tackles, transparency appears as a horizontal feature that must taken into account, for the fact checking output to be useful as opposed to adding more noise to an already noisy landscape. 2.3 The limits of fact checking While technology can make fact checking easier and faster, its effectiveness is challenged by psychological or cultural barriers. Confirmation bias” is the well-known psychological observation 5There can be disagreement on what is a reliable source. In this paper, we consider a source reliable to the extent that the fact checker trusts it, and/or there is significant consensus (e.g. by a very large majority) on its trustworthiness. 6https://www.poynter.org/international-fact-checking-network-fact-checkers-code- principlesTrack: Journalism, Misinformation, Fact Checking TrackWWW 2018, April 23-27, 2018, Lyon, France566","Cazalens, S., Lamarre, P., Leblay, J., Manolescu, I., & Tannier, X. (2018, April). A content management perspective on fact-checking. In Companion Proceedings of the The Web Conference 2018 (pp. 565-574)."
ML_DB_054,Automated fact checking in the newsroom,"Fact-checking is an essential task in journalism; its importance has been highlighted due to recently increased concerns and efforts in combating misinformation. In this paper, we present an automated fact-checking platform which given a claim, it retrieves relevant textual evidence from a document collection, predicts whether each piece of evidence supports or refutes the claim, and returns a final verdict. We describe the architecture of the system and the user interface, focusing on the choices made to improve its user friendliness and transparency. We conduct a user study of the fact-checking platform in a journalistic setting: we integrated it with a collection of news articles and provide an evaluation of the platform using feedback from journalists in their workflow. We found that the predictions of our platform were correct 58% of the time, and 59% of the returned evidence was relevant.",Computer Science,Proceeding,2019,Y,Y,13,"Grounding information on reliable sources is a daunting experience, given the increasing amount of information circulating the web and other media platforms. Nevertheless, checking the veracity of claims is crucial for preserving trust in news sources. Manual verification of claims is a tedious task, that consumes a lot of time and effort from journalists and professional fact-checkers, and it typically requires searching for specific entities and content over large amounts of unstructured text [2, 4]. The rising interest in fact checking has led to the development of a number of approaches and tools automating the task or parts of it, with the motivation of facilitating the work of journalists, and interested readers more broadly [ 3, 9]. The most popular ef- fort, ClaimBuster [ 5], proposed a fact checking platform which detects factual claims that are worth checking and then uses APIs to query search engines and databases (Google and Wolfram Al- pha respectively). It also compares claims against previously fact checked ones in its database. The latter approach is also used by the system developed by Full Fact, Live, which is used to fact check repeated or paraphrased claims.1 Neither of these approaches is able to fact check previously unchecked claims, while the queries through existing commercial APIs are not tailored to fact checking, thus retrieving information that is not necessarily relevant. Other approaches rely instead on the detection of rumours based on the spread of readership over social media [7]. However, a rumorous claim is not necessarily false, and vice versa [16]. There is also work that checks claims against tables, such as those released by Claim: Tesla builds car factory in Shanghai. Evidence: Electric carmaker Tesla has signed an agreement with Chinese authorities to build a factory in Shanghai. We hope it will be completed very soon,” Tesla chief Elon Musk said. Figure 1: Example of claim and evidence. Extracted named entities in bold: organizations (purple), locations (blue) and people (green). the World Bank2, using trained classifiers to select the appropriate tuple [10 ]. However such approaches are typically restricted to fact checking numerical claims against tabular sources. Finally, some approaches aim at providing a pipeline of tools for information retrieval [ 14], but do not go as far as to provide an actual fact check mechanism. In this work, we propose an automated fact checking platform that checks claims by identifying sentences providing evidence in a large document collection. These sentences are classified as supporting, refuting or only related to the claim, and then com- bined into a final verdict using a state-of-the-art neural network- based approach [15 ]. The model is trained on the recently released FEVER dataset [ 11 ], a large scale fact checking dataset derived from Wikipedia comprising 185K claims. Unlike previous work, our model is able to check novel claims without relying on a database of fact checks. Also, the evidence retrieved and classified as sup- porting or refuting provides a justification of the verdict, which is likely to be relevant not only to assess the overall correctness of the platform, but also as part of the fact checking research conducted by journalists. Our retrieval model considers more information about a specific claim than generic search engines by leveraging information from words and named entities present in the dataset to obtain the best matching evidence for each claim (see Figure 1). We thoroughly evaluate the platform through user testing by journalists from the British Broadcasting Corporation (BBC) in the context of their workflow. Of the total 488 evidence passages retrieved by the system, the journalists reported that 58% were rele- vant and 59% were accurately classified as supports/refutes/other. Our platform can be applied to document collections beyond the ones used in this paper and our findings should help inform future research in automated fact checking and computational journalism more broadly. 2 FACT CHECKING SYSTEM Our fact-checking system comprises three main components: a document retrieval step, a sentence ranking, and a classification model, as shown in Figure 2. Initially we retrieve documents from a collection of news arti- cles, using a customized search engine based on inverse indexing and retrieval using the Okapi-BM25 algorithm [8 ]. Data is incre- mentally indexed from word and entity-level inverted indexes. The document retrieval component searches for documents whose fea- tures best match the claim. In the end of this step, we end up with approximately 10K documents related to the claim. w l lemmas e entities words heads features documentsd Document Retrieval Sentence Ranking Natural Language Inference (Hexa-F) ≈ 5k docs ≈ 25 evidence sentences Claim evidence 1 evidence 2 evidence 3 evidence 4 sentence l sentence p sentence n sentence m [title di , sentencel ] [title dk , sentencep ] [title dj , sentencen ] [title di , sentencem ] rank 1: rank 2: rank 3: rank 4: Claim Label: Figure 2: Fact-checking model comprised of three compo- nents: a) Document retrieval (top left), b) Sentence ranking (middle right), c) NLI prediction model (bottom). Following this, we rank the sentences in these documents accord- ing to predefined token and feature matching rules. We compute the cosine similarity of the claim with each best candidate sentence, using word embeddings trained on the One Billion Word Bench- mark corpus [1], and select those above a threshold tuned during development. This step aims at providing only sentences with high relevance to the claim, reducing the number of potential evidence that may be only marginally related with the claim. In the final component, we classify each of the extracted can- didate evidence in terms of whether they support, refute or are just related to the claim (other). We employ the natural language inference (NLI) model from the Hexa-F system [ 15 ] (one of the best performing systems in the FEVER shared task [12]) to classify the relation between the selected evidence sentences and the claim, one of supports/refutes/other, and a similar label which expresses whether the combined set of evidence sentences supports, refutes or is simply related to the claim. 2.1 Engineering Considerations In the first document retrieval component, we use inverted indexes to extract relevant documents, in Figure 3. We select all documents that match features in the claim, such as lemmas, words and ex- tracted named entities [6 ]. This document retrieval step takes about 50 ms to retrieve around 5k documents out of a dataset with about 445k documents. In the second component, we rank all the sentences from the 5k documents based on how well each sentence feature φ (si )j matches the claim φ (c)j , using a positional ranking approach. We use a ranking score based on ordered distances between N matching features in the sentence i: S1 (si , c) = ∑N j=1 exp (−di, j ), where di, j = pos(φ (si )j ) − pos(φ (si )j−1 ) represents the word distance between the position ( pos) of each j-th feature. This score is maximized if all words in the sentence match the claim exactly; it decreases exponentially with the word distance between matches. This part takes on average 336 ms. Next, we filter the sentences based on3580 the following rules: we keep only those sentences with length < 500 words; those that contain all the entities mentioned in the claim, as well as novel words that were not previously seen in previously selected sentences (less than 90% overlap with all words previously encountered). The novelty filter increases the diversity in the evidence passed to the entailment step. In the end of the second component, we re-rank the sentences by averaging the feature matching score of each sentence S1 and the cosine similarity between the claim and the sentence embeddings S2 = cos(si , c). We obtain these embeddings via a weighted average of all words in the sentence, weighted by term-frequency/inverse document frequency. To improve the performance of the sentence re-ranking component, we added the title of the document to each sentence and considered the weighted sum of all words in the title and sentence combined. This part takes about 652 ms, although it could be easily parallelizable. On average, 76 sentences are retrived per claim after these steps. We further removed all sentences with an averaged similarity score below a given threshold ((S1 + S2 ) /2<0.6), to ensure high quality evidence. In the end, about 25 sentences on average are selected. After re-ranking, all selected sentences are used as input in the NLI model (third component) which takes about 738 ms to predict labels for each of the sentences and the overall label for the claim. 3 USER INTERFACE The interface allows end-users to input a claim (black bar at the top of Figure 3), and receive a set of evidence sentences as output. The evidence is displayed in three columns: the top five ranking sentences that are in favour of the claim on the left, the top five that are against the claim in the middle, and the top five other sentences related to the claim on the right. In the bottom, a final overall label is presented to the user as either other, supporting or refuting the claim (label at the bottom). The interface allows users to directly evaluate it, providing feedback for evidence sentence w.r.t. the correctness of the label (correct label?”) and its relevance (relevant?”), as well as the correctness of the overall prediction. We provide a video with an example of a user interacting with the platform via the interface in. 4 USER EVALUATION DESIGN 11 BBC journalists provided feedback on the overall system and on the classification model. The journalists were asked to interact with the system by providing factual claims and evaluating the output of the model. For each claim, up to 15 evidence sentences were presented to them, 5 per category, each classified as supports, refutes or other. For each sentence, the journalists provided feedback on two aspects: relevance and correctness. To assess correctness, for each evidence sentence returned by the system, the journalist inputs the label which, according to his/her research on the subject, would be the correct one via the buttons in the correct label” box (see Figure 3). This measures primarily the accuracy of the entailment component of the system, assuming that the sentences returned are all related. For the final classification of the model (see Figure 3 (bottom)) they also assessed the overall prediction, to whether the claim was globally supported or refuted considering all the retrieved evidence. Precision Class supports refutes other all Relevant 71% 69% 49% 59 % Evidence Correctness 48% 27% 70% 58 % Global Correctness 56% 26% 31% 42 % Table 1: User evaluation on the full dataset. To assess relevance, the journalists were also asked to provide feedback as to whether they found each sentence returned relevant (see the buttons in the relevant ?” box in Figure 3). This part of the feedback aims to evaluate the quality of the retrieved evidence: whether it helps the journalists fact-check the input claim, regard- less of the classification label attributed by the system. It also serves as a proxy to measure the precision of the retrieval component, as all sentences shown to the journalists should be relevant (ideally). The journalists also had access to (i) a Question Answering (QA) [ 13 ] tool that could serve as an additional source of information, (ii) the full document’s text with annotated entities [6 ]. The QA implemented in the platform was used by the journalists, but was not evaluated in this paper. We provide an example of the addi- tional information, available for each extracted evidence sentence in Figure 4. 5 RESULTS Table 1 summarizes the results of the system is the user evaluation conducted. We show precision for each class (row:Relevant) mea- suring the proportion of the retrieved evidence sentences that were deemed relevant by the journalists, and precision for the predictions of the platform for the relation of each evidence sentence to the claim (row:Evidence) and the global prediction for the claim taking all the evidence into account (Overall result presented in Figure 3) (row:Global). The platform was evaluated on 67 claims in total, with a total of 488 evidence sentences retrieved: 30% supporting, 14% refuting and 56% related to the claim, as classified by our platform. In 71% of the claims checked by the journalists, it was reported that the evidence shown in the supports column was relevant, and so was for 69% of the evidence in the refutes columns. We consider these results to be very encouraging, given the difficulty of the task. Retrieving evidence that contradicts a given claim, is not usually as simple as retrieving related evidence by feature matching. Intro- ducing different information, e.g. entities, dates, actions, etc., that could refute the claim requires more complex language understand- ing methods. We further observe that the precision in predicting evidence as supports is 48% in the full dataset and increases to 67% in the subset of evidence deemed relevant (not shown in the table). The same trend is observed for the refutes label from 27% to 39%. This result suggests that the retrieval component still requires some improvement, especially for retrieving evidence refuting the claim. Strategies beyond feature matching are needed to improve the retrieval of relevant but opposing arguments. The precision of the classifier predicting the global label of the claim given the evidence also requires further improvement. Additionally, we received textual feedback from the journalists about the overall quality of the platform. They mentioned that it is helpful for fact checking, despite not being always accurate, both in the retrieval of relevant evidence and in their evaluation. An interesting remark mentioned possible improvements for handling3581 Figure 3: Example of the fact checking interface. Claim: Russia meddled with US elections” (top). Five maximum evidence sentences for each column: supports/refutes/other (middle). Example of final system decision and feedback buttons (bottom). Figure 4: Example of additional information for each evi- dence. Original document (right) with entities in bold. time-frames and dates. For instance, claims using the present tense should refer to current events, while those mentioned in the past tense together with dates should refer to that specific time-period only. Also they suggested that presenting the evolution of results over time would be very helpful to substantiate the claim. Han- dling time constraints in the retrieval process is a very interesting and challenging research direction. On the whole, the journalists reported that the system has a lot of potential to help their work. This user testing was extremely useful both for BBC journalists to experiment with state-of-the-art technology and for us to receive feedback to improve our platform in the future. 6 CONCLUSIONS AND FUTURE WORK This paper introduces a novel fact checking platform aimed to assist journalists in their investigative work-flow. Our platform can be used for search of supporting and refuting evidence regarding fac- tual claims. We evaluated using on a journalistic corpus with testing by eleven journalists, which found it to yield relevant results in 59% of the retrieved evidence. The performed user study provided very fruitful feedback to direct future work in automated fact checking. Suggested improvements such as handling temporal remarks, pose an interesting issue that we found very relevant to advance research in the field of information retrieval for fact checking.","Miranda, S., Nogueira, D., Mendes, A., Vlachos, A., Secker, A., Garrett, R., ... & Marinho, Z. (2019, May). Automated fact checking in the news room. In The World Wide Web Conference (pp. 3579-3583)."
ML_DB_055,Learning from fact-checkers: analysis and generation of fact-checking language,"In fighting against fake news, many fact-checking systems com prised of human-based fact-checking sites (e.g., snopes.com and politifact.com) and automatic detection systems have been developed in recent years. However, online users still keep sharing fake news even when it has been debunked. It means that early fake news detection may be insufficient and we need another complementary approach to mitigate the spread of misinformation. In this paper, we introduce a novel application of text generation for combating fake news. In particular, we (1) leverage online users named fact-checkers, who cite fact-checking sites as credible evidences to fact-check information in public discourse; (2) analyze linguistic characteristics of fact-checking tweets; and (3) propose and build a deep learning framework to generate responses with fact-checking intention to increase the fact-checkers’ engagement in fact-checking activities. Our analysis reveals that the fact-checkers tend to refute misinformation and use formal language (e.g. few swear words and Internet slangs). Our framework successfully generates relevant responses, and outperforms competing models by achieving up to 30% improvements. Our qualitative study also confirms that the superiority of our generated responses compared with responses generated from the existing models.",Computer Science,Proceeding,2019,N,Y,31,"Our media landscape has been flooded by a large volume of falsified information, overstated statements, false claims, fauxtography and fake videos1 perhaps due to the popularity, impact and rapid infor- mation dissemination of online social networks. The unprecedented amount of disinformation posed severe threats to our society, de- graded trustworthiness of cyberspace, and influenced the physical world. For example, $139 billion was wiped out when the Associated 1https://cnnmon.ie/2AWCCix Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR ’19, July 21–25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 what General Pershing of the United States did to terrorists when caught. There was no more Radical Islamic Terror for 35 years! @user Stop spreading fake news. This Pershing story has long been debunked! Original tweet Fact-checking tweet Original Poster Fact checker Figure 1: A real-life fact-checking activity where the fact- checker refutes misinformation in the original tweet. Press (AP)’s hacked twitter account posted fake news regarding White House explosion with Barack Obama’s injury. To fight against fake news, many fact-checking systems ranging from human-based systems (e.g. Snopes.com), classical machine learning frameworks [ 20 , 34 , 38 ] to deep learning models [ 29, 39 , 56 , 57 ] were developed to determine credibility of online news and information. However, falsified news is still disseminated like wild fire [ 31 , 59 ] despite dramatic rise of fact-checking sites worldwide [ 21 ]. Furthermore, recent work showed that individuals tend to selectively consume news that have ideologies similar to what they believe while disregarding contradicting arguments [ 8, 35 ]. These reasons and problems indicate that using only fact-checking systems to debunk fake news is insufficient, and complementary approaches are necessary to combat fake news. Therefore, in this paper, we focus on online users named fact- checkers, who directly engage with other users in public dialogues and convey verified information to them. Figure 1 shows a real-life conversation between a user, named original poster, and a fact- checker. In Figure 1, the original poster posts a false claim related to General Pershing. The fact-checker refutes the misinformation by replying to the original poster and provides a fact-checking article as a supporting evidence. We call such a reply a fact-checking tweet (FC-tweet). Recent work [ 52 ] showed that fact-checkers often quickly fact-checked original tweets within a day after being posted and their FC-tweets could reach hundreds of millions of followers. Additionally, [ 9] showed that the likelihood to delete shares of fake news increased by four times when there existed a fact-checking URL in users’ comments. In our analysis, we also observe that after receiving FC-tweets, 7% original tweets were not accessible because of account suspension, tweet deletion, and a private mode. Due to the fact-checkers’ activeness and high impact on dissem- ination of fact-checked content, in this paper, our goal is to furtherSession 3C: Fact-checking, Privacy and LegalSIGIR ’19, July 21–25, 2019, Paris, France335 support them in fact-checking activities toward complementing existing fact-checking systems and combating fake news. In par- ticular, we aim to build a text generation framework to generate responses2 with fact-checking intention when original tweets are given. The fact-checking intention means either confirming or re- futing content of an original tweet by providing credible evidences. We assume that fact-checkers choose the fact-checking URLs by themselves based on their interests (e.g., in Figure 1). Therefore, we focus on generating responses without au- tomatically choosing specific fact-checking URLs, which is beyond the scope of this paper. To achieve the goal, we have to solve the following research prob- lems: (P1) how can we obtain a dataset consisting of original tweets and associated fact-checking replies (i.e., replies which exhibit fact- checking intention)?; (P2) how can we analyze how fact-checkers communicate fact-checking content to original posters?; and (P3) how can we automatically generate fact-checking responses when given content of original tweets? To tackle the first problem (P1), we may use already available datasets [ 15 , 52 , 54 ]. However, the dataset in [ 15 ] contains rela- tively small number of original tweets (∼5,000) and many FC-tweets (∼170K). Since FC-tweet generation process depends on contents of original tweets, it may reduce diversity of generated responses. The dataset in [ 54 ] is large but fully anonymized, and the dataset in [ 52 ] does not contain original tweets. Therefore, we collected our own dataset consisting of 64,110 original tweets and 73,203 FC-tweets (i.e., each original tweet receives 1.14 FC-tweet) by using Hoaxy system [46] and FC-tweets in [52]. To understand how fact-checkers convey credible information to original posters and other users in online discussions (P2), we conducted data-driven analysis of FC-tweets and found that fact- checkers tend to refute misinformation and employ more imper- sonal pronouns. Their FC-tweets were generally more formal and did not contain much swear words and Internet slangs. These ana- lytical results are important since we can reduce the likelihood to generate racist tweets [14], hate speeches [7] and trolls [4]. To address the third problem (P3), we propose a deep learn- ing framework to automatically generate fact-checking responses for fact-checkers. In particular, we build the framework based on Seq2Seq [49] with attention mechanisms [28]. Our contributions are as follows: • To the best of our knowledge, we are the first to propose a novel application of text generation for supporting fact-checkers and increasing their engagement in fact-checking activities. • We conduct a data-driven analysis of linguistic dimensions, lexical usage and semantic frames of fact-checking tweets. • We propose and build a deep learning framework to generate responses with fact-checking intention. Experimental results show that our models outperformed competing baselines quan- titatively and qualitatively. • We release our collected dataset and source code in public to stimulate further research in fake news intervention3. 2We use the term fact-checking tweets (FC-tweets)”, fact-checking responses”, and fact-checking replies” interchangeably. 3https://github.com/nguyenvo09/LearningFromFactCheckers 2 RELATED WORK In this section, we briefly cover related works about (1) misinfor- mation and fact-checking, and (2) applications of text generation. 2.1 Misinformation and Fact-checking Fake news is recently emerging as major threats of credibility of information in cyberspace. Since human-based fact-checking sites could not fact-check every falsified news, many automated fact- checking systems were developed to detect fake news in its early stage by using different feature sets [ 10 , 15 , 40 , 54, 59 ], knowledge graph [ 47] and crowd signals [ 16 , 17 , 34 ], and using deep learning models [30 , 39 , 57 ]. In addition, other researchers studied how to fact-check political statements [ 56 ], mutated claims from Wikipedia [50] and answers in QA sites [32]. Other researchers studied intention of spreading fake news (e.g. misleading readers, inciting clicks for revenue and manipulating public opinions) and different types of misinformation (e.g. hoaxies, clickbait, satire and disinformation) [ 42, 53 ]. Linguistic patterns of political fact-checking webpages and fake news articles [ 13 , 42 ] were also analyzed. Since our work utilizes FC-tweets, analyzing users’ replies [ 9, 15 , 40, 54 ] are closely related to ours. However, the prior works had limited attention on analyzing how fact-checkers convey fact-checked content to original posters in public discourse. Additionally, researchers investigated topical interests and tem- poral behavior of fact-checkers [ 52 ], relationship between fact- checkers and original posters [ 11 ], how fake news disseminated when fact-checked evidences appeared [ 9], and whether users were aware of fact-checked information when it was available [ 15 ]. Our work is different from these prior works since we focus on lin- guistic dimensions of FC-tweets, and propose and build a response generation framework to support fact-checkers. 2.2 Applications of Text Generation Text generation has been used for language modeling [ 33], ques- tion and answering [ 12 ], machine translation [ 1 , 28 , 49 ], dialogue generation [ 43 – 45, 51 , 55 ], and so on. Recently, it is employed to build chat bots for patients under depression4, customer assistants in commercial sites, teen chat bots [ 14 ], and supporting tools for teachers [ 3]. Text generation has been also used to detect fake re- view [58 ], clickbait headlines [ 48], and fake news [ 41]. Our study is the first work that generates responses based on FC-tweets as a supporting tool for fact-checkers. Our work is closely related with dialog generation in which there are three main technical directions: (1) deterministic models [ 43 , 45 , 51, 55 ], (2) Variational Auto-Encoders (VAEs) [44 ], and (3) Generative Adversarial Net- works (GANs) [ 25]. Although recently VAEs and GANs showed promising results, deterministic models are still dominant in litera- ture since they are easier to train than VAEs and GANs, and achieve competitive results [ 22 ]. Thus, we propose a response generation framework based on Seq2Seq and attention mechanism [28]. 3 DATASET In this section, we describe our data collection and preprocessing process. We utilized the dataset in [ 52 ] and the Hoaxy system [ 46 ] 4https://read.bi/2QZ0ZPnSession 3C: Fact-checking, Privacy and LegalSIGIR ’19, July 21–25, 2019, Paris, France336 to collect FC-tweets, which contained fact-checking URLs from two popular fact-checking sites: snopes.com, and politifact.com. Totally, we collected 247,436 distinct fact-checking tweets posted between May 16, 2016 and May 26, 2018. Similar to [11 , 52 ], we removed non-English FC-tweets, and FC- tweets containing fact-checking URLs linked to non-article pages such as the main page and about page of a fact-checking site. Then, among the remaining fact-checking pages, if its corresponding original tweet was deleted or was not accessible via Twitter APIs because of suspension of an original poster, we further filtered out the fact-checking tweet. As a result, 190,158 FC-tweets and 164,477 distinct original tweets were remained. To further ensure that each of the remaining FC-tweets reflected fact-checking intention and make a high quality dataset, we only kept a fact-checking tweet whose fact-checking article was rated as true or false. Our manual verification of 100 random samples confirmed that fact-checking tweets citing fact-checking articles with true or false label contained clearer fact-checking intention than fact-checking tweets with other labels such as half true or mixture. In other words, FC-tweets associated with mixed labels were discarded. After the pre-processing steps, our final dataset consisted of 73,203 FC-tweets and 64,110 original tweets posted by 41,732 distinct fact-checkers, and 44,411 distinct original posters, respectively. We use this dataset in the following sections. 4 LINGUISTIC CHARACTERISTICS OF FACT-CHECKING TWEETS Since our goal is to automatically generate responses with fact- checking intention, it is necessary to analyze what kind of linguistic characteristics FC-tweets have, and verify whether FC-tweets in our dataset have the fact-checking intention. To highlight linguistic characteristics of FC-tweets, we compare FC-tweets with Normal Replies, which are direct responses to the same 64,110 original tweets without including fact-checking URLs, and Random Replies, which do not share any common original tweets with FC-tweets. Initially, we collected 262,148 English Nor- mal Replies and 97M English Random Replies posted in the same period of the FC-tweets. Then, we sampled 73,203 Normal Replies and 73,203 Random Replies from the initial collection to balance the data with our FC-tweets. All of the FC-tweets, Normal Replies and Random Replies were firstly preprocessed by replacing URLs with url and mentions with @user, and by removing special characters. They were tokenized by the NLTK tokenizer. Then, we answer the following research questions. Note that we sampled 73,203 Ran- dom Replies and 73,203 Normal Replies four times more, and our analysis was consistent with the following results. Q1: What are underlying themes in FC-tweets? To answer this question, we applied the standard LDA algorithm to each of the three types of replies, so we built three independent LDA models. Table 1 shows 5 topics extracted from each of the three LDA topic models with associated keywords. Firstly, FC-tweets ex- hibit clear fact-checking intention with keywords such as debunked, snopes, read, stop, check, and lie. Secondly, keywords of Normal Replies show awareness of misinformation. However, fact-checking intention is not clear compared with FC-tweets. The keywords of Random Replies are commonly used in daily conversations. Based Table 1: 5 LDA topics and associated keywords of FC-tweets, Normal Replies and Random Replies. Types Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 FC-tweets read fake facts false true stop news check snopes know try that’s debunked lie story Normal Replies one u fake president like liar like news trump really know f*** trump get stop Random Replies love good like like thank u thanks one would oh know yes people right im on the analysis, we conclude that the main themes of FC-tweets are about fact-checking information in the original tweets. Q2: What are the psycholinguistic characteristics of FC-tweets? We employed LIWC 2015 [ 37 ], a standard approach for mapping text to 73 psychologically-meaningful categories, for understanding psychological characteristics of FC-tweets. Given each of FC-tweets, we counted how many words of the FC-tweet belonged to each LIWC category. Then, we computed a normalized score for the category by diving the count by the number of words in the FC- tweet. Finally, we report the average scores μ and variances σ 2 for each LIWC category based on |FC-tweets|. The same process was applied to Normal Replies and Random Replies. We examined all LIWC categories and report only the most significant results. (A) FC-tweets have the highest usage of impersonal pronouns and the least utilization of personal pronouns. In Figure 2(a), we can see that FC-tweets exhibit the highest usage of impersonal pronouns (e.g. it, this, that) (μ = 0.071, σ 2 = 0.01) in comparison with Normal Replies (μ = 0.049, σ 2 = 0.009) and Random Replies (μ = 0.054, σ 2 = 0.005). This observation is statistically significant in Mann Whitney one sided U-test (p < 0.001). Examples of FC- tweets containing impersonal pronouns (called iprons in LIWC) are (i) @user This has been debunked repeatedly - url please stop spreading the lie, thanks!, and (ii) @user it is a wonderful quote but Lewis never said it : url and url. Differently, Normal Replies show the highest mean score in 2nd person pronouns (named you category) (μ = 0.071, σ 2 = 0.007, p < 0.001) in comparison to FC-tweets (μ = 0.037) and Random Replies (μ = 0.047). Note that you in this context may refer to the original posters. In 1st person pronouns (named I category), Random Replies have highest score because they contain daily personal conversa- tions between online users. Finally, FC-tweets have the smallest usage of we (μ = 0.003, σ 2 = 0.000) and they (μ = 0.004, σ 2 = 0.000) among three groups of replies (p < 0.001). (B) FC-tweets have a tendency to refute content of original tweets. Figure 2(b) shows the mean scores of adj, negate, differ and certain categories. Specifically, FC-tweets exhibit the highest mean score in adjectives category (μ = 0.080, σ 2 = 0.024, p < 0.001) in comparison to Normal Replies (μ = 0.052, σ 2 = 0.008) and Random Replies (μ = 0.056, σ 2 = 0.015). Prevalent adjectives in FC-tweets are fake, wrong, dump, false, and untrue. FC-tweets also tend to refute information of original tweets. Their mean score in negate category is 0.054, which is about two times higher than the mean score of Normal Replies (p < 0.001). FC-tweets haveSession 3C: Fact-checking, Privacy and LegalSIGIR ’19, July 21–25, 2019, Paris, France337 ipron you i they we LIWC Categories 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 Score FC-Tweets Normal Replies Random Replies(a) Pronounsadj negate differ certain LIWC Categories 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 Score FC-Tweets Normal Replies Random Replies (b) Negationinformal swear netspeak assent LIWC Categories 0.00 0.01 0.02 0.03 0.04 0.05 0.06 Score FC-Tweets Normal Replies Random Replies (c) Informalityfocuspresent focuspast focusfuture LIWC Categories 0.00 0.02 0.04 0.06 0.08 0.10 0.12 Score FC-Tweets Normal Replies Random Replies (d) Temporal Dimensions Figure 2: LIWC category scores of FC-tweets, Normal Replies and Random Replies. In the figure, we follow LIWC category abbreviations as labels [37]. For example, ipron and adj mean impersonal pronouns and adjectives, respectively. also the highest usage of words in differ category (e.g. actually, but, except) among the three groups (p < 0.001). In certain category (e.g. never, ever, nothing, always), FC-tweets’ mean score (μ = 0.036) also doubles the average score of Normal Replies significantly (p < 0.001). Examples of FC-tweets are: (i) @user wrong. never happened. url, (ii) @user except he didn’t. that tweet has been proven fake: url, and (iii) @user I sure hope you’re joking. url. (C) FC-tweets are usually more formal and have low usage of swear words. In Figure 2(c), FC-tweets have lower mean score in informal category (μ = 0.034, σ 2 = 0.010) than Normal Replies (μ = 0.038, p < 0.001) and Random Replies (μ = 0.066). FC-tweets also use the least swear words (μ = 0.013, σ 2 = 0.005, p < 0.005) among the three groups. In terms of netspeak category (i.e. Internet slangs), FC-tweets (μ ≈ 0.011) generally have smaller average score than Random Replies (μ = 0.035). Furthermore, FC-tweets do not contain much words in assent category (e.g. OK, yup, okey) (μ = 0.006, σ 2 = 0.002) compared with Random Replies (μ = 0.013, σ 2 = 0.005, p < 0.001). Regarding formality of FC-tweets, we conjecture that fact-checkers try to persuade original posters to stop spreading fake news, leading to more formal language, less usage of swear words. An example of FC-tweets is @user url I’m sure you’ll still say it’s true- but it simply isn’t. Google for facts and debunks please. (D) FC-tweets emphasize on what happened in the past whereas Normal Replies and Random Replies focus on present and future. In Figure 2(d), FC-tweets usually employ verbs in past tense to mention past stories to support their factual corrections. Thus, the average score of focuspast category of FC-tweets is the high- est (μ = 0.036, σ 2 = 0.005, p < 0.001) among the three groups of replies, whereas Normal Replies and Random Replies emphasize on present and future. Particularly, FC-tweets have the least score in focuspresent (μ = 0.117, σ 2 = 0.015, p < 0.001) and focusfu- ture (μ = 0.006, σ 2 = 0.000, p < 0.001) categories. An example of FC-tweets is @user yeah, she merely said something that was only slightly less absurd. url. Q3: How are semantic frames represented in FC-tweets? So far, we examined every word independently without consid- ering its dependencies with other words (e.g. surrounding words), which is helpful in understanding its meaning. Thus, we now em- ploy SEMAFOR model [6 ], trained on FrameNet data5, to extract rich structures called semantic frames based on syntactic trees of 5https://framenet.icsi.berkeley.edu/fndrupal/frameIndexArtificiality Statement Existence Desirability Temporal collocation Morality evaluation0 1 2 3 4 5 6 Percentage (%) FC-Tweets Normal Replies Random Replies Figure 3: Distribution of semantic frame types in FC-tweets. sentences. For example, a frame Statement consists of a noun and a verb where the noun indicates a speaker and the verb implies the act of conveying a message. We measured the distribution of semantic frames of FC-tweets by firstly counting the number of occurrences of every frame type across all FC-tweets, and normalized it by the total number of detected frames in all FC-tweets. The same process was applied to Normal Replies and Random Replies. Figure 3 shows the percentage of different types of frames detected by SEMAFOR. We have the following observations: (A) FC-tweets display high usage of Artificiality, Statement and Existence. In Figure 3, FC-tweets have the highest utilization of Artificiality (e.g. wrong, lie, fake, false, genuine, phoney) among three groups of replies (p < 0.001 according to one-sided z-test). This frame accounts for 5.71% detected frames in FC-tweets com- pared with Normal Replies (0.90%) and Random Replies (0.11%). FC-tweets also have the highest proportion of the frame Statement (3.96%, p < 0.001) among three groups of replies. Words that evoke frame Statement in FC-tweets are said, says, claims, report, told, talk, and mention. Examples of FC-tweets are (i) @user You’re the one who has no clue. She never said this: url, and (ii) @user Snopes reports this rumor as false. url. To refer to verified information, FC-tweets employed frame Existence (1.38%, p < 0.001) compared with Normal Replies (0.71%) and Random Replies (0.70%). The most popular phrases evoking frame Existence were real, there is, there are, exist, there were, and there have been. Examples of FC-tweets are: (i) @user There is no trucker strike in Puerto Rico url, (ii) @user That town doesnt exist url.Session 3C: Fact-checking, Privacy and LegalSIGIR ’19, July 21–25, 2019, Paris, France338 −0.6 −0.4 −0.2 0.0 0.2 0.4 0.6 Consine similarity 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Density(a) FC-tweets vs. articles (b) |Shares | vs. |tokens |Reasons 0 10 20 30 40 50 60 Percentage (%) User Suspension Not Found Status Not Authorized (c) Irretrievable tweets Figure 4: (a) Similarity between FC-tweets and fact-checking articles, (b) |Shares| vs. |Tokens| of FC-tweets and (c) Distri- bution of irretrievable original tweets. (B) FC-tweets exhibit the highest Morality_evaluation and have less usage of Desirability. As shown in Figure 3, FC-tweets contain the highest proportion of frame Morality_evaluation (1.06%, p < 0.001) among three groups. The most popular words in frame Morality_evaluation are wrong, evil, dishonest, despicable, unethi- cal, and immoral. Another supporting evidence of this observation is the lower usage of frame Desirability (e.g. good, better, bad, great, okay, cool) in FC-tweets (1.19%, p < 0.001) than Normal Replies (1.89%) and Random Replies (2.5%). An example of FC-tweets is: @user you’re such an evil, despicable creature. url (C) FC-tweets do not use much Temporal_collocation. FC-tweets show lower usage of Temporal_collocation (1.08%, p < 0.001) than Normal Replies (1.74%) and Random Replies (2.00%). The most com- mon words that evoke this frame in FC-tweets are when, now, then, today, current, recently, future. It seems these words are mainly about the present and the future. This result supports the same observation that FC-tweets tend to focus on the past. Q4: Do fact-checkers include details of fact-checking arti- cles? We firstly derived latent representations of FC-tweets and articles by training two Doc2Vec models [ 23] - one for FC-tweets and the other one for fact-checking articles. The embedding size is 50. Then, we measured cosine similarity between a FC-tweet and the fact-checking article embedded in the FC-tweet as shown in Figure 4(a). Interestingly, most FC-tweets do not have high similar- ity with FC-articles, suggesting that fact-checkers rarely include details from fact-checking articles in FC-tweets. However, there were several enthusiastic fact-checkers who extracted information from fact-checking articles to make FC-tweets more persuasive, as shown in two tails of the curve in Figure 4(a). Q5: Is there any connection between |tokens| of FC-tweets and |shares|? Since sharing FC-tweets by retweets and quotes is important for increasing the visibility of credible information on online social networks, we examined correlation between |tokens| of FC-tweets and their |shares|. We only focus on |tokens| because it could help us to decide length of a generated response. Figure 4(b) shows a scatter plot of FC-tweets’ |tokens| and |shares| (i.e., quotes and retweets). Generally, most FC-tweets had |shares|=0. However, FC-tweets with |tokens| ∈ [10; 20] usually received more attention. To verify this, we created two lists – one containing |shares| of FC-tweets with |tokens| ∈ [0; 9] and another one for |shares| of FC-tweets with |tokens| ∈ [10; 20] –, and then conducted Mann Whitney one-sided U-test. We found that the latter one had significantly larger numbers than the former one (p = 2.91 × 10−11 ). We conclude that very short FC-tweets may be not informative enough to draw readers’ attention, and lengthy FC-tweets may be too time-consuming to read, leading to small number of shares. Therefore, a reasonable length of FC-tweets is more preferable when we generate a response. Q6: Is there any signal suggesting positive effect of FC-tweets? We examined what happened to original tweets after receiving fact- checking tweets. In Oct 2018 (i.e., five months after collecting our dataset), we re-collected original tweets via Twitter APIs to see if all of the original tweets were retrievable. Interestingly, 4,516 (7%) original tweets were not retrievable. There are three reasons: (i) User Suspension, (ii) Not Found Status (i.e. deleted status), and (iii) Not Authorized (i.e. original tweets are in the private mode). In Figure 4(c), User Suspension accounted for 58.30% of the irre- trievable original tweets. Although there may be many factors that potentially explain suspension (e.g. original posters may have other abusing behaviors that triggered Twitter security system), one ob- vious observation is that fact-checkers tended to target bad users (e.g. content polluters [ 24 ]), who usually have abusing behavior on social platform. It means that fact-checkers are enthusiastic about checking credibility of information on social networks. Regarding two reasons Not Authorized and Not Found Status, perhaps original posters were either aware of the wrong information they posted or were under pressure due to criticisms they received from other users, leading to deletion or hiding their original tweets. In summary, our analysis reveals fact-checkers refuted content of original tweets, and their FC-tweets were more formal than Normal Replies and Random Replies. To provide supporting evidences, FC- tweets utilized semantic frames Existence and Statement. These results confirm that FC-tweets exhibit clear fact-checking intention. 5 RESPONSE GENERATION FRAMEWORK In the previous section, we analyzed common topics, lexical us- ages, and distinguishing linguistic dimensions of FC-tweets com- pared with Normal Replies and Random Replies. Our analysis re- vealed that FC-tweets indeed exhibited clear fact-checking inten- tion, which is the property that we desired. Now, we turn our attention to proposing and building our framework, named Fact- checking Response Generator (FCRG), in order to generate responses with fact-checking intention. The generated responses are used to support fact-checkers and increase their engagement. Formally, given a pair of an original tweet and a FC-tweet, the original tweet x is a sequence of words x = {xi |i ∈ [1; N ]} and the FC-tweet is another sequence of words y = {yj |j ∈ [1; M]}, where N and M are the length of the original tweet and the length of FC-tweet, respectively. We inserted a special token <s> as a starting token into every FC-tweet. Drawing inspiration from [ 28 ], we propose and build a framework as shown in Figure 5 that consists of three main components: (i) the shared word embedding layer, (ii) the encoder to capture representation of the original tweet and (iii) the decoder to generate a FC-tweet. Their details are as follows: 5.1 Shared Word Embedding Layer For every word xi in the original tweet x, we represent it as a one- hot encoding vector xi ∈ RV and embed it into a D-dimensional vector xi ∈ RD as follows: xi = We xi , where We ∈ RD×V isSession 3C: Fact-checking, Privacy and LegalSIGIR ’19, July 21–25, 2019, Paris, France339 <s> @user another lie @user another lie url url </s> he said this url GRU GRU GRU GRU GRU GRU GRU Shared Word Embeddings 𝒉𝑖 Context vector 𝒄𝒋 𝒉𝑗 + Vector concat. Activation Layer Softmax Original Tweet Fact-checking tweet GRU GRUFigure 5: Our proposed framework to generate responses with fact-checking intention. an embedding matrix and V is the vocabulary size. We use the same word embedding matrix We for the FC-tweet. In particular, for every word yi (represented as one-hot vector yi ∈ RV ) in the FC-tweet y, we embed it into a vector yi = Weyi . The embedding matrix We is a learned parameter and could be initialized by either pre-trained word vectors (e.g. Glove vectors) or random initial- ization. Since our model is designed specifically for fact-checking domain, we initialized We with Normal Distribution N (0, 1) and trained it from scratch. By using a shared We , we could reduce the number of learned parameters significantly compared with [ 28 ]. This is helpful in reducing overfitting. 5.2 Encoder The encoder is used to learn latent representation of the original tweet x. We adopt a Recurrent Neural Network (RNN) to represent the encoder due to its large capacity to condition each word xi on all previous words x<i in the original tweet x. To overcome the vanishing or exploding gradient problem of RNN, we choose Gated Recurrent Unit (GRU) [ 5 ]. Formally, we compute hidden state hi ∈ RH at time-step it h in the encoder as follows: hi = GRU (xi , hi−1 ) (1) where the GRU is defined by the following equations: zi = σ (xi Wz + hi−1Uz ) ri = σ (xi Wr + hi−1Ur )  ̃hi = tanh (xi Wo + (ri ⊙ hi−1 )Uo ) hi = (1 − zi ) ⊙  ̃hi + zi ⊙ hi−1 (2) where W[z,r,o], U[z,r,o] are learned parameters.  ̃hi is the new up- dated hidden state, zi is the update gate, ri is the reset gate, σ (.) is the sigmoid function, ⊙ is element wise product, and h0 = 0. After going through every word of the original tweet x, we have hidden states for every time-step X = [h1 ⊕ h2 ⊕ ... ⊕ hN ] ∈ RH ×N , where ⊕ denotes concatenation of hidden states. We use the last hidden state hN as features of the original tweet x = hN . 5.3 Decoder The decoder takes x as the input to start the generation of a FC- tweet. We use another GRU to represent the decoder to generate a sequence of tokens y = {y1, y2, .., yM }. At each time-step jt h , the hidden state hj is computed by another GRU: hj = GRU (yj , hj−1 ) where initial hidden states are h0 = x. To provide additional con- text information when generating word yj , we apply an attention mechanism to learn a weighted interpolation context vector cj dependent on all of the hidden states output from all time-steps of the encoder. We compute cj = Xaj where each component aji of aj ∈ RN is the alignment score between the jt h word in the FC-tweet and the it h output from the encoder. In this study, aj is computed by one of the following ways: aj =    so f tmax (XT hj ) Dot Attention so f tmax (XT Wa hj ) Bilinear Attention (3) where softmax(.) is a softmax activation function and Wa ∈ RH ×H is a learned weight matrix. Note that we tried to employ other attention mechanisms including additive attention [ 1] and con- cat attention [ 28 ] but the above attention mechanisms in Eq. 3 produced better results. After computing the context vector cj , we concatenate hT j with cT j to obtain a richer representation. The word at jt h time-step is predicted by a softmax classifier: ˆyj = so f tmax (Ws tanh (Wc [cT j ⊕ hT j ]T ) ) (4) wher","Vo, N., & Lee, K. (2019, July). Learning from fact-checkers: analysis and generation of fact-checking language. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 335-344)."
ML_DB_056,A Conceptual Model for Approaching the Design of Anti-disinformation Tools,"With the increasing amounts of mis-and disinformation circulating online, the demand for tools to combat and contain the phenomenon has also increased. The multifaceted nature of the phenomenon requires a set of tools that can respond effectively, and can deal with the different ways in which disinformation can present itself, such as text, images, and videos, the agents responsible for spreading it, and the various platforms on which incorrect information is prevalent. In this paper, after consulting independent fact-checkers to create a list, we map the landscape of the most known tools that are available to combat different typologies of mis and disinformation on the basis of three levels of analysis: the employment of policy-regulated strategies, the use of co-creation, and the preference for manual or automated processes of detection. We then create a model in which we position the different tools across three axes of analysis, and show how the tools distribute across different market positions. The most crowded positions are characterized by tools that employ automated processes of detection, varying degrees of policy implementation, and low levels of co-creation, but there is an opening for newly developed tools that score high across all three axes. The interest in co-creative efforts in the challenge towards addressing mis-and disinformation could indeed be an effective solution to cater to the need of the users, and respond effectively to the amounts and variety of mis and disinformation spreading online. ",Social Science,Proceeding,2021,N,Y,0,"A Conceptual Model for Approaching the Design 67 study from 1946 shows [1], the social media era has made it more pervasive. The 2000s has witnessed a rapid development of social media that has facilitated a spread of both information and misinformation regarding everything from local neighbourhoods to global issues. Studies analysing misinformation on social media platforms, have found that misinformation and disinformation travel faster than trustworthy information [20]. That has put social media platforms and their architecture at the centre of discussion on the spread of misinformation. While platforms like Facebook, Twitter and Instagram have started collaborating with fact-checkers to flag incorrect information, and giving users the possibility to report items for fact-checking, the tools provided by these platforms do not yet responding effectively to the demands. Furthermore, the echo-chamber-like structure of the networks within social media platforms [8, 17], and the personalization process carried out by algorithms, reinforce existing biases within the users [18], who tend to be exposed mostly to information that reinforces their pre-existing beliefs. Research on misinformation in the media has been progressing rapidly, with studied contextualizing misinformation within different realms and events, such as within journalism [5], and in the context of elections [21], and also trying to predict and identify future challenges [7]. Other studies have focused on taxonomizing the phenomenon [4]. The taxonomy of Giglietto et al. [9] is based on factors such as perceptions of the source, the story, the context, and the decisions of the audience and the propagator. The authors propose a taxonomy of pure disinformation” where both the original author and the propagator are aware of the false” nature of information, but they nevertheless decide to share it. A different situation is the one called misinformation propagated through disinformation” where information is originally produced as true” and then shared by a propagator who believes it is false”. Finally, disinformation propagated through misinformation” is the situation in which information is devised as false” by a creator but is perceived as true” by a propagator. On the ground the fact-checking work, i.e. the act of taking up published information, examining it for factualness and veracity and the re-publishing it, has also been carried out intensively in the past years. Given the demand for fact-checking, several tools have been developed to help the users navigate the information landscape within social media. It is these tools that are the object of research for this paper. 2 Aim The aim for the proposed model in this paper, is to map the current anti-disinformation tools landscape, by analysing the architectural choices that govern the tools’ functioning and response to disinformation. We see tools as software developed with the intention to detect and in some way judge and give the user a notice of mis/disinformation. In this way, we provide a framework for understanding what is available to the users, what is missing within the current landscape, and in which direction the tools should evolve in order to provide diversified options catering to the users’ needs, as well as to effectively address different typologies of disinformation in different online environments. 68 M. Svahn and S. C. Perfumi 3 Literature Review on the Qualities of Anti-disinformation Tools The work on combating disinformation can take on many shapes, at least as many as dis- information itself. Farrell and colleagues [6] outlined some of the specific problems that misinformation detection has to address. The authors also explain some different models of how misinformation spreads that are relevant for detection, and provide a typology of anti-misinformation tools such as style-based, knowledge-based, propagation-based or credibility-based tools. They position tools within the misinformation ecosystem, with regard to how, when and what kinds of misinformation they handle. However, the study does not focus on the users’ perspective, a gap that is taken up by the work of Komendatova et al. [11]. The authors reviewed disinformation tools in the perspective of design approaches, putting them into a perspective of value driven design. They found that design qualities of a lean back character i.e. not favouring active engagement are preferred by stakeholders, if compared to approaches favouring user engagement. This multitude of shapes and qualities that anti-disinformation tools can assume, shapes the ontology of the theoretical model we propose. Still some qualities seem to be present across many of the tools, and refer to how they are constructed, how they function, and what they allow. Within the main qualities that have been identified, the tools we examined in this study can rely on either automated or manual fact-checking, which means that the veracity of the information is assessed either by algorithms who rely on the existing databases of fact-checked information, or by humans who check the claims against other verified sources of information. They can be either proactive or reactive (i.e., what we will call policy” versus ad hoc), meaning that they either set up strategies to prevent the circulation of misinformation, ex ante or they react to misinformation at the point when it is detected in the system, ex post. Or finally, the tools can to varying degrees rely on collaborative, co-creative efforts. This means that some tools allow the broader community of users to provide information in different forms, such as flagging, fact-checking or feedback on the fact-checking process. Our object of research is the ontology of design qualities, that make up the con- ceptualization space of anti-disinformation tools. We do that by organising a theoretical model of the conceptualization space. With this we take a perspective of critical realism and constructionism [15]. 3.1 Aspects of Anti-disinformation Tools As highlighted in the previous paragraph, three main qualities of the existing anti- disinformation tools indicate how these approaches disinformation (proactively or reac- tively, namely establishing policies a priori, or establishing ad-hoc strategies to respond to specific cases), which modality they use to analyse it (manual or automated) and to what extent they allow joint efforts (degree of co-creation). We have also found Babakar [2] inspiring, as they postulate that fact-checking exists in a triangular trade off where the angles are; Speed: how quickly the task can be done; complexity, and how difficult the task is to perform. Babakar [2] can be seen as a precursor to the model of this study. Their triangle corner of Speed versus Difficulty relates to this study’s axis of AI/Handicraft and their third angle of Complexity can relate to this study’s axis of policy. Their notion of how a systems designer can only optimise for two of these at a time relates to how A Conceptual Model for Approaching the Design 69 our studies’ cube model postulates and illustrates inherently opposed trade-offs. On the basis of all these identified qualities, we proceed to build our model. Degree of Policy. One of the axes of our model is dedicated to assessing to what extent a tool is driven by policy. The degree of policy is measured in high and low, where policy” represents the high end of the spectrum, and ad-hoc” represents the opposite, low end. With the degree of policy” we measure to what extent the various anti-disinformation tools on one hand establish rules that regulate the definition and circulation of mis- information ex ante”, and an instance of misinformation detected according to these policies triggers the tool. Or on the other hand tools that evaluate circumstantially the single cases. Hence a high-end place on the spectrum represents a tool design based on an ex ante policy that drives the judgements of the tool, a low end is a situation of a more flexible character that evaluates instances of misinformation sui generis, post facto. Research in the area of policy has shown that policy work is crucial in a user-focused conceptualization of anti-disinformation tools [10, 19] Furthermore, the many and var- ied taxonomies of disinformation contribute to showing that an awareness of policies, may be advantageous when devising a user centred approach. Degree of AI. The second axis of our model is dedicated to the assessment of the extent to which the misinformation detection process is carried out by means of manual or automated fact-checking, applying AI and machine-learning. The amount of information that requires fact-checking produces an increasing demand for a number of tools and fact- checking services that rely on the use of AI to assess the veracity of the information. This approach can either be used as the sole method of information scanning, or in conjunction with manual fact-checking. Therefore, in our model we measure the degree of AI, where AI/Machine learning” represents the high end of the spectrum, and manual” represents the opposite, low end of the spectrum. Degree of Co-creation. In the last few years co-creation has spread rapidly in the busi- ness sector as a way of engaging with stakeholders and building knowledge. The appli- cation of co-creation methods is more recent in the public sector, particularly for policy development, and multiple challenges still need to be overcome [16]. It has been sug- gested that co-creation of anti-misinformation work in the public sector can be a way of meeting the multifaceted complexity of the task of anti-misinformation work [12]. Therefore, in our model, we measure the level of co-creation, where high degree of co-creation” represents the high end of our third axis, and low degree of co-creation (solo)” represents the opposite, low end of the same spectrum. In our model co-creation is intended as the extent to which the tools allow contributions and inputs from larger communities of users while carrying out the fact-checking work. 4 A Model for Evaluating Anti Disinformation Tools for Conceptual Purposes The presented qualities can be imagined as axes and presented as a model in the form of a cube. This form of model also illustrates how we postulate the three qualities to be communicating buckets. We apply the examples that follow below as a means of validation (Fig. 1). 70 M. Svahn and S. C. Perfumi Fig. 1. Organization of the axes in the cube 5 Review of Anti-disinformation Tools In the past few years, several tools that rely on both automated and manual” fact- checking, or on a combination of the two modalities have been developed to help users navigate the information encountered online. The overarching aim of these tools is to fact-check different aspects of the disinfor- mation content, such as the information architecture (e.g. the claims, the sources, the authors and the platforms), and aspects related to the typology of content (e.g., images, headlines). These tools can be employed as a way to prevent the spread of disinformation, or as a way for a user to react to it. Among the tools that more generally assess the content are; NewsGuard1 [14], Claim- Buster, 2 The Factual,3 CredEye, 4 Public Editor,5 and Newstrition. 6 Within these, tools like Cyabra7 employ more advanced technologies to detect deep-fakes, and the Co- Inform Dashboard8 is designed to help professionals such as journalists, fact-checkers and policy-makers in their everyday job. Some social media platforms have developed their own fact-checking systems, often in collaboration with external fact-checking organizations. Examples of these are A Conceptual Model for Approaching the Design 71 the Facebook Fact Checking Program, 9 Twitter Birdwatch, 10 and the Whatsapp IFCN chatbot.11 Besides the tools developed in collaboration with the platforms, there are also platform-specific tools developed externally, like CaptainFact,12 designed for YouTube, Foller.me, 13 the Co-Inform plug-in14 and Hoaxy, 15 designed for Twitter, FakeSpot, 16 specific for e-commerce platforms, and tools like Botometer17 that are not only platform- specific, but also agent-specific: Botometer provides information about the probabil- ity that a Twitter account is a bot. Other tools are content-specific, for example Tin- Eye18 focuses on detecting fake and decontextualized images, and the WeVerify/InVID plug-in19 is created for the assessment of videos. Finally, tools are being developed not only to detect, signal and correct misinfor- mation, but also to work on the critical thinking and analytical abilities of the users, and one of these is Fiskkit. 20 To make sure that fact-checking work is carried out trans- parently and to counter the rise of counter-fact-checking initiatives, the International Fact-Checking Network21 provides information and assessments on the credibility of the various fact-checkers and fact-checking initiatives [14]. 6 A Model for Evaluating the Concept Space of Anti-disinformation Tools We have proposed a model to be used to categorize and define anti-misinformation tools from the perspective of the functionalities encountered by the user. Here we map a number of chosen examples to the cubic model. We made a theoretical sample and in February 2021 we tested the content validity of the choice against a panel of nine experts from the IFCN network.21 The chosen tools are placed within the cube in a place that represents their position in relation to the three axes. The bottom horizontal axis represents the degree of policy, moving from left (lowest degree) to right (highest degree). The vertical left axis represents the degree of co-creation, moving from the bottom (lowest degree) upwards (highest degree). Finally, the third and last dimension, which indicates the degree of AI/Machine-learning, is placed at the conjunction point with the policy axis and is visualized as the cube depth axis”, which moves starting 72 M. Svahn and S. C. Perfumi from the conjunction point with the policy axis (lowest degree), and runs alongside the bottom of the right cube face until the next conjunction point (highest degree). In the previous section we proposed a model to be used to categorize, define and evaluate converging media to be considered for use when approaching anti disinforma- tion tools. The cubic structure of the model, based on the three aspects or dimensions discussed above and shown in Fig. 2, makes it salient how the axes are connected vessels. To be evaluated, a disinformation tool is measured with a design analysis and positioned in the cube with respect to the three dimensions, and then evaluated from its position. In the following final section of the chapter we apply the proposed model to a number of examples in order to show whether it can be used as intended. Fig. 2. Distribution of the mapped tools across the cube 7 Application of the Proposed Model on Anti-disinformation Tools The model presented above, whilst being theoretical, presents several practical implica- tions. We can see a crowded market position in the fifth corner with Hoaxy, Factmata Foller.me, Botometer, and Newstrition. These are all characterized by high degrees of AI/Machine-learning, and low degrees of policy and co-creation. The second most-crowded market position we find in the middle between the fifth and the sixth corners, it is represented by tools like The Factual, FakerFact, Tin-Eye and Cyabra. These tools score similarly across our axes, and they are characterized by a high degree of AI/Machine-learning, a medium degree of policy, meaning that they use a mixed approach, and a low degree of co-creation. A Conceptual Model for Approaching the Design 73 We can observe that the third corner is empty. It is defined by a high degree of co-creation, a low degree of policy and a mostly manual process. However, it would be unlikely to see an anti-disinformation tool in that market position, as the functioning and usability of such a tool would be impaired by these very characteristics. To provide an example, a platform that perhaps represents this position is Reddit, but it is not within the realm of anti-disinformation tools. The other tools appear to be more diversified across the various axes, with the exception of WeVerify (InVID), the Co-inform plug-in, and the Co-inform dashboard, which are roughly grouped in the same area round the second corner: the Co-inform plug-in and WeVerify are both characterized by high degrees in all the axes (AI/Machine-learning, policy and co-creation), while the Co-Inform dashboard also has high degrees of policy and co-creation, but medium degrees of AI/Machine- learning. These are among the few that meet up to the idea of applying co-creation to meet the demands of combatting disinformation (cf. 3.2.3). It is unlikely that the tools in the crowded fifth corner defined by being high in AI, low in policy and low in co-creation will develop away from that corner. This is due to them being designed as stand-alone tools that are not part of any ecosystem, and that is an intrinsic design quality in these that is unlikely to change in the short term. Birdwatch is an interesting case, it occupies a unique position with being high in human processing and co-creation while being highly dependent on policy. Its future position is dependent on the way it manages to over time keep and implement policy, we can right now in March 2021 see that the design qualities of Birdwatch may be moving it towards the empty corner of a high degree of co-creation, a low degree of policy and a mostly manual process. This is a conceptual study, a natural next step would be to bring users into the picture, and examine their perceptions exploring e.g. how design qualities are categorized into product definitions, c.f. e.g. [19, 20], to deeper explore user needs. Acknowledgements. This work has been partially funded by the Co-Inform project (770302), under the Horizon 2020 call H2020-SC6-CO-CREATION-2016–2017 (CO-CREATION FOR GROWTH AND INCLUSION)” of the European Commission. We would also like to express our special thanks and gratitude to Allan Leonard and Orna Young from FactCheckNI for their valuable contribution in identifying and analyzing the chosen tools, and for their help in reaching out to the broader community of fact-checkers within the International Fact Checking Network and also to Myrsini Glinos of egovlab at Stockholm University. 74 M. Svahn and S. C. Perfumi Appendix Here follows a table that lists tools onto the three axes that make up the model. Some tools of these are in competition some are complementary (Table 1). Table 1. Selected tools and axes evaluation Tool Policy: (reactive vs proactive) (low/medium/high) Process: (manual vs AI/machine learning) (low/medium/high) Creation: (solo vs co-creation) (low/medium/high) Notes Public editor Medium Low Community/high Botometer (by OSoMe) Low High Low CaptainFact (France) (plugin) High Low High Focus on inserting fact checks in videos Claimbuster (human input) Medium Medium Medium Human assisted matching learning to identify potential claims Co-Inform (plugin) High High High Co-Inform (dashboard) High Medium High Cyabra Medium High Low Decodex (Le Monde) (France) (plugin); run by journalists) (how) (why) Low Low Low Focus on source credibility Facebook fact-checking program Medium Medium Medium Facebook Third-Party Fact Checking Program with IFCN-verified partners Factmata Low High Low FakerFact (plugin) Medium High Low Fiskkit High Low High Works on the critical thinking and analysis skills Foller.me Low High Low Similar to MisinfoMe (continued) A Conceptual Model for Approaching the Design 75 Table 1. (continued) Tool Policy: (reactive vs proactive) (low/medium/high) Process: (manual vs AI/machine learning) (low/medium/high) Creation: (solo vs co-creation) (low/medium/high) Notes Hoaxy (by OSoMe) Low High Low Similar to MisinfoMe NewsGuard (plugin) Medium Low Low Focus on source credibility; run by journalists Newstrition Low High Low TheFactual (plugin) (how) (who) Medium High Low? Focus on article credibility TinEye replace AI and crowdsourcing as opposites Medium High Low Reverse image search Twitter Birdwatch Medium Low High Crowd-sourced fact checking WeVerify (InVID) (plugin) High HIGH High Focused on videos Whatsapp IFCN Chatbot Low HIGH Medium CredEye Low Medium Low Web page. Produces credibility score of a given text","Svahn, M., & Perfumi, S. C. (2021, September). A Conceptual Model for Approaching the Design of Anti-disinformation Tools. In International Conference on Electronic Participation (pp. 66-76). Springer, Cham."
ML_DB_057,Social Computing for verifying social media content in breaking news,"Social media is the place to go for both journalists and the general public when news events break, offering a real-time source of eyewitness images and videos through platforms like YouTube, Instagram, and Periscope. Yet, the value of such content as a means of documenting and disseminating breaking news is compromised by the increasing amount of content misuse and false claims in social media. To this end, cost-effective social-computing solutions for user-generated content verification are crucial for retaining the value and trust in social media for breaking news.",Social Computing,Article,2018,Y,Y,14,"Social media is the place to go for both journalists and the general public when news events break, offering a real-time source of eyewitness images and videos through platforms like YouTube, Instagram, and Periscope. Yet, the value of such content as a means of documenting and disseminating breaking news is compromised by the increasing amount of content misuse and false claims in social media. To this end, cost-effective social-computing solutions for user-generated content verification are crucial for retaining the value and trust in social media for breaking news. Most people have a smartphone in their pocket today, so eyewitnesses experiencing an event like a terror attack will often post real-time claims, such as the numbers dead or injured in a location, to Twitter or Facebook. Eyewitness images and videos will also be uploaded to sites like YouTube and Instagram or even streamed live to sites like Periscope. For events such as the 2015 Paris shootings, 1 the first eyewitness videos of the various shootings were posted within 5 to 10 minutes of the event happening. This was followed about 20 to 30 minutes later with veri- fied news reports from sources such as Le Figaro, the BBC, and CNN. In other cases, verifying eyewitness or user-generated media and claims can take much longer, from hours to even days, as, for instance, in the case of Malaysia Airlines Flight 17, which was shot down on 17 July 2014. In many cases, as soon as a breaking news event starts trending on Twitter, it is accompanied by considerable numbers of false claims and content misuse.2 This involves the use of multimedia for misinforming the public and misrepresenting people, organizations, and events. Misuse prac- tices range from publishing content that has been digitally tampered using photo-editing software to falsely associating content with an unfolding event. The paper Detection and Resolution of Rumours in Social Media: A Survey”2 contains an extensive discussion on the problem of rumor detection in social media. Stuart E. Middleton University of Southampton Symeon Papadopoulos Centre for Research and Technology Hellas Yiannis Kompatsiaris Centre for Research and Technology Hellas 83 IEEE Internet Computing Published by the IEEE Computer Society 1089-7801/18/$33.00 USD ©2018 IEEEMarch/April 2018 Authorized licensed use limited to: Centre for Research and Technology (C.E.R.T.H.). Downloaded on March 13,2020 at 13:45:38 UTC from IEEE Xplore. Restrictions apply. IEEE INTERNET COMPUTING Given the grave societal and economic impact of having misused content and false claims fea- tured in mainstream news, it becomes extremely important for news organizations to be able to verify eyewitness media in very short time. To this end, journalists are turning to social-compu- ting approaches to automatically analyze and verify 3 user-generated content in real time. The eventual hope is that cost-effective social computing can reduce the time spent on verification to time scales nearer to real time. SOCIAL-MULTIMEDIA FORENSICS AND SUPERVISED VERIFICATION Methods from the field of digital forensics are often used for assessing the veracity of multime- dia items (images or videos) posted online. Some methods focus on the analysis of information encoded in the metadata of multimedia content, such as EXIF (Exchangeable Image File) infor- mation, which is often associated with JPEG and TIFF images. Several types of digital manipu- lation, such as the use of photo-editing software, leave traces in the form of metadata unless special care is taken to remove them, and analysis of these traces can detect manipulations. Un- fortunately, several of the most popular social media platforms, including Facebook and Twitter, automatically remove much of the metadata from posted content, rendering metadata-based methods useless for content obtained from these platforms. Other forensics-based methods aim at uncovering traces of manipulation in the visual content itself. In images, such methods4 can detect cases of splicing and copy–move operations—for ex- ample, inpainting of a part of one image into a second, or replication of a part of an image within the same image. Methods can leverage the uniqueness of noise patterns introduced by the captur- ing device in order to detect whether an image contains traces from another image captured by a different device. Other methods focus on patterns associated with the color filter arrays of mod- ern image-capturing equipment. Splice detection methods exploit traces left by the JPEG com- pression process, working on the basis that the splicing of two different images and the subsequent recompression will leave detectable traces in the final JPEG file. While all of the above methods yield satisfactory results when applied on well-controlled test samples, they have been found to exhibit poor performance in real cases.5 One of the reasons that state-of-the-art methods fail to detect manipulations in media content published on the web is that such content is often the result of numerous intermediate operations, including resizing, cropping, and recompression, which have an obfuscating effect on the traces of digital manipula- tion. Examples are Twitter and Facebook, both of which automatically resize and recompress all images uploaded to them.6 Recent work in the FP7 REVEAL project (see Figure 1) addresses the poor performance of indi- vidual tampering-detection methods7 by generating tampering probability heat maps based on a number of complementary forensic-analysis algorithms. The inclusion of multiple image-foren- sics algorithms and side-by-side comparisons gives a powerful means to journalists to under- stand where possible digital tampering has occurred. The problem of identifying digital manipulations in video content is even more challenging compared to the case of images, and it is further exacerbated in cases where such content is sourced from video-sharing and social-net- working platforms such as YouTube and Facebook. The Horizon 2020 progam’s InVID project is looking into resilient approaches for video forensics building upon the TUNGSTENE com- mercial forensics engine. 84March/April 2018 www.computer.org/internet Authorized licensed use limited to: Centre for Research and Technology (C.E.R.T.H.). Downloaded on March 13,2020 at 13:45:38 UTC from IEEE Xplore. Restrictions apply. SOCIAL COMPUTING Figure 1. A digital-forensics platform for image verification (screenshot taken from the Media Verification Assistant). Some cases of content misuse are not detectable by using forensics analysis—for example, when an image from a past event is reposted as being associated with an unfolding event. These re- quire other methods that attempt to detect misuse by analyzing contextual cues from social me- dia sources. 8 A typical approach adopted by such methods is to extract a variety of trust-oriented features from social media posts, and the accounts generating these posts, and to use them for training machine-learning models based on historically labeled cases of fake and real posts. Such methods have been shown to yield very high fake-post-detection accuracy.9 EYEWITNESS MEDIA AND FACT EXTRACTION The standard workflow for automated fact checking10 involves the monitoring of data sources, fact identification, fact extraction, and fact checking. The challenges for social computing mostly involve fact identification and extraction. Once a fact is extracted, it can be checked either manu- ally or automatically against databases from sites such as PolitiFact, FactCheck.org, Snopes, and Wikipedia. Factual claims come in many forms. The most important for social computing are factual asser- tions, contextual statements associated with a fact, and contextual statements associated with the trustworthiness of the fact. Factual assertions themselves can be true, false, half-truths, or exag- gerations. Contextual statements can allow a true representation of a fact or misrepresent it by suggestions of a false location, actor, or time stamp. Contextual text can also introduce ideologi- cal cues and loaded language to bias the interpretation of the fact. Finally, contextual statements can suggest trustworthiness, such as attribution to a trusted source or claims of previous verifica- tion that themselves might be subject to falsehoods or deliberate bias. Fact identification approaches, especially for news-related sources, try to classify sentences into nonfactual, unimportant factual, and check worthy” factual statements11 so that they can be fil- tered prior to fact extraction. Fact extraction is a type of information extraction problem that runs alongside information extraction techniques for concepts such as event, topic, location, and time. In the past, approaches such as argumentative zoning12 were applied successfully to extract fac- tual statements on well-structured and trustworthy scientific documents. However, the text in web and social media sources is often neither well structured nor trustworthy, so new approaches are being explored. Early work in this area focused on verb phrase patterns (e.g., was elected to”) to extract facts via systems such as OLLIE (Open Language Learning for Information Extraction).13 These used part-of-speech (POS) tagging, dependency parsing, and distant supervision coupled with seed 85March/April 2018 www.computer.org/internet Authorized licensed use limited to: Centre for Research and Technology (C.E.R.T.H.). Downloaded on March 13,2020 at 13:45:38 UTC from IEEE Xplore. Restrictions apply. IEEE INTERNET COMPUTING attributes and bootstrapping to provide unsupervised fact extraction. In particular, they were able to capture the long tail” of factual statements, which is very important for the contextual inter- pretation (e.g., Putin made a deal with the separatists”). Later advances,14 motivated by the need to answer queries in search engines, added noun phrase patterns (e.g. Obama’s wife”) very suc- cessfully. Typically, such approaches exploit large databases of attribute names and noun phrases such as Freebase and DBpedia. Automated fact checkers use either domain-specific databases (e.g., PolitiFact) or web-scale da- tasets (e.g., DBpedia). Recently, there has been a trend of real-time crowdsourcing of fact check- ing during events such as US political rallies, with the Trump–Clinton presidential debates being the latest example. Fake news sites have also been increasing in number and can easily mislead readers15 into trusting misinformation based on a credible but false source attribution. The iCheck system16 is a good example where domain-specific heuristics extract fact types that are visualized via a crowdsourcing interface for users to check claims and up- or down-vote them. Work from the REVEAL project17 has taken these ideas one step further to help journalists ver- ify breaking news. Automated fact extraction using semantic grammars, seeded with linguistic phrases originating from journalists, is used to extract evidence from social media content about news events such as incident reports, facts about damage, the numbers of the dead and injured, people, locations, and attributed sources. User-generated content from the scene of a breaking event, not yet syndicated via news organizations, is particularly important for journalists. Super- vised-learning algorithms are employed within REVEAL to identify and filter posts containing eyewitness images and videos. This type of social computing is coupled with real-time visualiza- tions (see Figure 2), allowing journalist to quickly find contextual content such as original men- tions of claims for subsequent verification. Figure 2. An interactive real-time visualization mapping extracted facts and eyewitness media in posts about the December 2016 Malta plane hijacking (screenshot taken from the Journalist Decision Support System). CONCLUSION We highlighted in this article the potential of employing social-computing approaches for speed- ing up the task of verifying user-contributed information and content sourced from social media platforms. The problem is complex and calls for a variety of approaches, each targeting different challenges stemming from the characteristics of user-generated content, including high volume, inconsistent quality, and a lack of provenance information. Multimedia forensics targets the ac- 86March/April 2018 www.computer.org/internet Authorized licensed use limited to: Centre for Research and Technology (C.E.R.T.H.). Downloaded on March 13,2020 at 13:45:38 UTC from IEEE Xplore. Restrictions apply. SOCIAL COMPUTING tual content of multimedia. Supervised verification is best suited to cases where contextual fea- tures can be extracted and labeled training sets of fake and real examples are available. Fact ex- traction and visualization approaches target text-based sources that contain references to different elements of an event, such as people, times, and locations. The REVEAL project is one of the first efforts to bring together those technologies under a sin- gle platform that could provide comprehensive verification support to professional users; details on the successful user evaluation of pilot prototypes can be found in D7.2 User Evaluation Re- port.” 18 However, there is still a long way to go before such tools are widely used by newsrooms and journalists day to day. Figure 3 provides an overview of the projects and datasets useful to researchers interested in automating verification tasks for social media and news-related content. Figure 3. The research and dataset landscape for researchers interested in verification of social media and news-related content. One key challenge involved in delivering such an integrated solution is the lack of an appropriate human–computer interaction (HCI) approach that would empower users (e.g., journalists) to make optimal use of the technologies described above. Given the extensive use of algorithms, an effective HCI approach would need to build the trust of users by providing intuitive control and a clear explanation of the results. Ultimately, users are in charge of the whole process and will 87March/April 2018 www.computer.org/internet Authorized licensed use limited to: Centre for Research and Technology (C.E.R.T.H.). Downloaded on March 13,2020 at 13:45:38 UTC from IEEE Xplore. Restrictions apply. IEEE INTERNET COMPUTING make the final decision with respect to whether a piece of user-generated content should be con- sidered authentic or not. Moreover, support for collaborative work among teams of journalists is another key social-computing challenge that is missing from existing news provider in-house so- lutions, which instead employ general-purpose communication and messaging platforms such as Slack and WhatsApp. In conclusion, the problem of real-time verification of user-generated content is expected to re- main unsolved in the near future, but marked improvements have already been achieved on indi- vidual parts of the verification process thanks to social-computing approaches incorporating intelligent information processing. In the future, we anticipate considerable progress on this problem by incorporating the latest advances from deep learning. One example would be em- ploying generative adversarial networks19 to build highly accurate and robust models for visually distinguishing between tampered-with and untampered-with regions in multimedia content. An- other example would be novel HCI approaches focusing on the explainability of automatically generated results and the collaborative aspects of the verification process.","Middleton, S. E., Papadopoulos, S., & Kompatsiaris, Y. (2018). Social computing for verifying social media content in breaking news. IEEE Internet Computing, 22(2), 83-89."
ML_DB_058,Fake news” is not simply false information: A concept explication and taxonomy of online content,"As the scourge of fake news” continues to plague our information environment, attention has turned toward devising automated solutions for detecting problematic online content. But, in order to build reliable algorithms for flagging fake news,” we will need to go beyond broad definitions of the concept and identify distinguishing features that are specific enough for machine learning. With this objective in mind, we conducted an explication of fake news” that, as a concept, has ballooned to include more than simply false information, with partisans weaponizing it to cast aspersions on the veracity of claims made by those who are politically opposed to them. We identify seven different types of online content under the label of fake news” (false news, polarized content, satire, misreporting, commentary, persuasive information, and citizen journalism) and contrast them with real news” by introducing a taxonomy of operational indicators in four domains—message, source, structure, and network— that together can help disambiguate the nature of online news content.",Social Science,Article,2021,Y,N,106,"Fake news,” or fabricated information that is patently false, has become a major phe- nomenon in the context of Internet-based media. It has received serious attention in a variety of fields, with scholars investigating the antecedents, characteristics, and con- nature of misinformation contained in false news, so that we can better detect it and distinguish it from real news. Others focus on the susceptibility of users—why we fall for false news and how we can protect ourselves from this vulnerability. Both are geared toward improving media literacy to protect consumers from false information. Websites like Snopes and Politifact explicitly address the issue by verifying infor - mation in the news cycle with the help of an army of human fact-checkers. However, human fact-checking can be time consuming and subject to human foibles such as subjectivity and being limited by prior experiences (Vorhies, 2017). An alternative that has been proposed is the use of machine algorithms to facilitate the detection of false information (Conroy, Rubin,  Chen, 2015; Wang, 2017). Given the enormity of the fabricated news problem, machine-based solutions seem inevitable for tackling the scope and speed with which it is created and disseminated, especially around the time of elections, disasters, crises, and other developing stories. However, in order to develop reliable algorithms for detecting fabricated news, we have to be very disci- plined in defining it and differentiating it from legitimate news. Several scholars have led the way in defining and classifying fake news.” Southwell, Thorson, and Sheble (2017) pinpoint conceptual differences between mis- information and disinformation, and discuss in detail the grounding of truth necessary for defining misinformation. Lazer et al. (2018) reminds us of the importance of pro- cess and intent when conceptualizing fake news,” while Jack (2017) further disen- tangles the conceptual differences and similarities among an array of concepts associated with problematic information, ranging from disinformation to propaganda. Tandoc, Lim, and Ling (2018), on the other hand, analyze how the term fake news” specifically has been used by scholars, and developed a typology based on facticity and intention to deceive. They proceeded to place different kinds of online content on these two dimensions, with Propaganda, for example, scoring high on both, whereas fabrication is low on facticity but high on deceptive intention and satire being high on facticity and low on intention to deceive. Such classification of different types of fake news” is a useful starting point in enhancing our understanding of the phenomenon. However, we need more such distinguishing characteristics and dimensions, espe- cially those that can be usefully incorporated in automated detection algorithms. Facticity is useful for fact-checking news stories, but cannot be relied on in the case of breaking news about emergent events where no previous information is available. The intent to deceive may probably be inferred by knowing the pedigree of the news source, but can be difficult to establish in a dispositive manner. Thus, for machine learning purposes, we need a more comprehensive definition that can not only distin- guish between a variety of fake news” but also lend itself to operationalization at a very granular level for machine detection. Previous research has provided conceptual differences and similarities between the many terms associated with fake news.” The purpose of the current research is to go one step further by identifying specific operational features or indicators that can be fed into a machine-learning algorithm to reliably differentiate between different types of content that are associated with the broad label of fake news.” With supervised machine learning, the goal is to develop algorithms that reason from externally supplied label","Molina, M. D., Sundar, S. S., Le, T., & Lee, D. (2021). “Fake news” is not simply false information: A concept explication and taxonomy of online content. American behavioral scientist, 65(2), 180-212."
ML_DB_066,Factsheet: Understanding the promise and limits of automated fact-checking,"The last year has seen growing attention among journalists, policymakers, and technology companies to the problem of finding effective, large-scale responses to online misinformation. The furore over so-called ‘fake news’ has exacerbated long-standing concerns about political lying and online rumours in a fragmented media environment, sharpening calls for technological solutions to what is often seen as a technological problem. This factsheet gives an overview of efforts to automatically police false political claims and misleading content online, highlighting central research challenges in this area as well as current initiatives involving professional fact-checkers, platform companies, and artificial intelligence researchers.",Journalism Studies,Article,2018,Y,N,8,"The last year has seen growing attention among journalists, policymakers, and technology companies to the problem of finding effective, large-scale responses to online misinformation. The furore over so-called ‘fake news’ has exacerbated long-standing concerns about political lying and online rumours in a fragmented media environment, sharpening calls for technological solutions to what is often seen as a technological problem. This factsheet gives an overview of efforts to automatically police false political claims and misleading content online, highlighting central research challenges in this area as well as current initiatives involving professional fact-checkers, platform companies, and artificial intelligence researchers. The influence of ‘fake news’ in different parts of the world remains poorly understood. Initial evidence from the US and Europe suggests that the share of online users who visit false news sites directly is quite limited, and that people exposed to these sites visit mainstream news sources far more (Allcott and Gentzkow 2017; Guess et al. 2018; Fletcher et al. 2018). However, the same studies indicate fabricated news stories may draw disproportionate attention on social networks, outperforming conventional news, and some partisans (e.g. Trump voters in the US) appear to be regular users of false news sites. Little is known about the dynamics by which individual viral stories may influence the opinions and behaviour of specific, targeted audiences around particular events or issues. In the US and Europe, concern about commercially or politically motivated misinformation online – in particular about mounting evidence of sophisticated, state-backed campaigns operating from Russia – has fuelled a vigorous debate over policy options. These include a raft of proposals to regulate platform companies like Facebook and Google in new ways, a question under review in the European Commission. Several countries, notably Germany, France, and Ireland, have passed or are considering legislation that penalises the distribution of false information. These concerns have also drawn new attention to the potential of various automated fact- checking (AFC) technologies to combat false information online. However, deciding the truth of public claims and separating legitimate views from misinformation is difficult and often controversial work (see Graves 2016), challenges that carry over into AFC. Based on a review of current efforts and interviews with both fact-checkers and computer scientists working in this area, this survey of the AFC landscape finds that: Much of the terrain covered by human fact-checkers requires a kind of judgement and sensitivity to context that remains far out of reach for fully automated verification. Rapid progress is being made in automatic verification of a narrow range of simple factual claims for which authoritative data are available. Even here, though, AFC systems will require human supervision for the foreseeable future. Both researchers and practitioners agree that the real promise of AFC technologies for now lies in tools to assist fact-checkers to identify and investigate claims, and to deliver their conclusions as effectively as possible. FACTSHEET: Understanding the Promise and Limits of Automated Fa... https://www.digitalnewsreport.org/publications/2018/factsheet-underst... 2 sur 12 13-03-22, 13:25 So far independent, nonprofit fact-checking organizations have led the way in developing and implementing AFC, with little activity from traditional media outlets. Some individual AFC tools have been built inexpensively by fact-checking groups. However, advancing capabilities and developing large-scale systems requires continuing support from foundations, universities, and platform companies. Overview AFC initiatives and research generally focus on one or more of three overlapping objectives: to spot false or questionable claims circulating online and in other media; to authoritatively verify claims or stories that are in doubt, or to facilitate their verification by journalists and members of the public; and to deliver corrections instantaneously, across different media, to audiences exposed to misinformation. End-to-end systems aim to address all three elements – identification, verification, and correction (see chart). The first proposals to automate online fact-checking appeared nearly a decade ago. Over the last several years a growing research literature has embraced AFC as an interesting problem in artificial intelligence, intersecting with practical experiments by fact-checkers.1 Two recent programming competitions, the ‘Fast  Furious Fact Check Challenge’ and the ‘Fake News Challenge’, allowed research teams from around the world to test different AFC techniques on common problem sets.2 Dr Andreas Vlachos, a lecturer at University of Sheffield, remarks on the increased attention: We published our first paper in 2014. To us, apart from our interest in politics, we thought it was a great challenge for artificial intelligence to actually work on this problem. [But] for better or worse, Trump’s election increased the interest. Meanwhile, real-world AFC initiatives have enjoyed a wave of additional funding in the last two years. Full Fact, a London-based fact-checking charity, began developing AFC tools in 2016 with a €50,000 grant from Google and recently announced £500,000 additional funding from the Omidyar Foundation and the Open Society Foundations. The Duke Reporters Lab, based at Duke University, received $1.2m in late 2017 to launch the Tech  Check Cooperative, a hub for AFC projects, from the Knight Foundation, the Facebook Journalism Project, and the Craig Newmark Foundation. In January, Factmata, a London-based startup developing an AFC platform, announced $1m in seed funding. FACTSHEET: Understanding the Promise and Limits of Automated Fa... https://www.digitalnewsreport.org/publications/2018/factsheet-underst... 3 sur 12 13-03-22, 13:25","Graves, L. (2018). Factsheet: Understanding the promise and limits of automated fact-checking. Reuters Institute for the Study of Journalism."
ML_DB_069,"BeLink: Querying Networks of Facts, Statements and Beliefs","An important class of journalistic fact-checking scenarios [2 ] involves verifying the claims and knowledge of different actors at different moments in time. Claims may be about facts, or about other claims, leading to chains of hearsay. We have recently pro-posed [ 4] a data model for (time-anchored) facts, statements and beliefs. It builds upon the W3C’s RDF standard for Linked Open Data to describe connections between agents and their statements, and to trace information propagation as agents communicate. We propose to demonstrate BeLink, a prototype capable of storing such interconnected corpora, and answer powerful queries over them relying on SPARQL 1.1. The demo will showcase the exploration of a rich real-data corpus built from Twitter and mainstream media, and interconnected through extraction of statements with their sources, time, and topics.",Information Science,Proceeding,2019,Y,N,1,"Fact-checking journalists oftentimes need to check who said what when. Such an analysis may be made to determine where a public figure up for (re)election stands with respect to a given issue (a famous example is John Kerry’s Senate voting history on the war in Irak1, or the public positions of members of a whole political party on an issue (e.g., the projected Wall between the US and Mexico). Statements are made by individuals or organizations, on certain topics, and typically claim to refer to (real-world) facts. Different actors often make different statements about the same fact or about each other statements. An actor may also make different statements about the same thing at different points in time. Professional stan- dards of journalistic work lead to a high interest in modeling and being able to show statement sources, which extends our (informal) definition of data of interest to: who said what when where. The source can be public (e.g., a speech whose transcript is available on the Web, or a tweet) or it can be private (e.g., an email that journal- ists acquire through their sources, or a transcript of a conversation with a source). Many current tools allow analyzing online media to answer spe- cific questions, for instance, CrowdTangle allows to monitor social media and extract events, Twitonomy and TwitterAnalytics are specifically devoted to analyzing Twitter content etc. We propose to demonstrate BeLink, a tool for extracting and analyzing (timed) 1http://tiny.cc/92s43y Figure 1: BeLink architecture. facts, statements and beliefs, out of a set of varied data sources. At the core of BeLink is a generic data model for real-world facts, statements, and beliefs we recently introduced [4], including (but not limited to) those expressed on social networks. Time plays an important role, since we must capture when events occur (or when facts hold), and when different statements are made about them; this serves, for instance, to track position reversals in time, or to keep track of promises2; thus, our model incorporates classi- cal temporal database concepts to attach time to facts, statements, and communications. Further, we take inspiration from classical AI techniques for modeling agents’ beliefs in order to capture the connections between actors and their statements. The model is based on W3C’s Resource Description Framework (RDF) concrete graph data model. This makes instances of our data model easy to share and combine (link) with any other RDF dataset, e.g., one that classifies actors according to their political opinions, connections to companies etc., to enable even more analyses. Beyond being white-box” (as opposed to models not publicly shared, used by existing media analysis tools), the biggest advantage of our model is to be comprehensive (modeling all the above aspects: facts, agents, beliefs, and information propagation), interoperable (being RDF), extensible (other data sources can be turned into instances of our model) and endowed with formal semantics. Also, adopting RDF al- lows us querying instances of our model with powerful SPARQL 1.1 queries, notably featuring property paths; these capture information propagation along paths of unbound length. Figure 1 outlines the architecture of BeLink. It comprises a set of feeds which gather content from public mainstream and social media; a set of extractors to identify statements in this content, todetermine their topics, the statement time etc. The structured data 2http://tiny.cc/nys43y ","Cao, T. D., Duroyon, L., Goasdoué, F., Manolescu, I., & Tannier, X. (2019, November). BeLink: Querying Networks of Facts, Statements and Beliefs. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management (pp. 2941-2944)."
ML_DB_071,Claimbuster: The first-ever end-to-end fact-checking system,"Our society is struggling with an unprecedented amount of falsehoods, hyperboles, and half-truths. Politicians and organizations repeatedly make the same false claims. Fake news floods the cyberspace and even allegedly influenced the 2016 election. In fight- ing false information, the number of active fact-checking organizations has grown from 44 in 2014 to 114 in early 2017. 1 Fact- checkers vet claims by investigating relevant data and documents and publish their verdicts. For instance, PolitiFact.com, one of the earliest and most popular fact-checking projects, gives factual claims truthfulness ratings such as True, Mostly True, Half true, Mostly False, False, and even Pants on Fire”. In the U.S., the election year made fact-checking a part of household terminology. For example, during the first presidential debate on September 26, 2016, NPR.org’s live fact-checking website drew 7.4 million page views and delivered its biggest traffic day ever. The challenge is that the human fact-checkers cannot keep up with the amount of misinformation and the speed at which it spreads. One of the reasons for this is that fact-checking is an intellectually demanding, laborious, and time-consuming process.",Computer Science,Article,2017,Y,Y,182,"Our society is struggling with an unprecedented amount of false- hoods, hyperboles, and half-truths. Politicians and organizations repeatedly make the same false claims. Fake news floods the cy- berspace and even allegedly influenced the 2016 election. In fight- ing false information, the number of active fact-checking organi- zations has grown from 44 in 2014 to 114 in early 2017. 1 Fact- checkers vet claims by investigating relevant data and documents and publish their verdicts. For instance, PolitiFact.com, one of the earliest and most popular fact-checking projects, gives factual claims truthfulness ratings such as True, Mostly True, Half true, Mostly False, False, and even Pants on Fire”. In the U.S., the election year made fact-checking a part of household terminology. For example, during the first presidential debate on September 26, 2016, NPR.org’s live fact-checking website drew 7.4 million page views and delivered its biggest traffic day ever. The challenge is that the human fact-checkers cannot keep up with the amount of misinformation and the speed at which it spreads. One of the reasons for this is that fact-checking is an intellectu- ally demanding, laborious, and time-consuming process. This chal- lenge creates an opportunity for automated fact-checking systems. On the other hand, fact-checking technology is clearly falling be- hind, as there is simply no existing system that truly does auto- mated fact-checking. Today’s professional fact-checkers diligently perform their work as an art, following good practices in data and investigative journalism. A recent white paper [2] surveys existing tools that can be integrated. Although the relevant tools and tech- niques can assist fact-checking in various areas, a full-fledged, end- to-end solution does not exist. There have been some attempts,2 but those efforts did not lead to such fact-checking systems. ∗Work performed while at UT-Arlington. 2T. Wilner. Fail and move on: Lessons from automated fact-checking ex- periments. Poynter, September 7, 2016. This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Proceedings of the VLDB Endowment, Vol. 10, No. 12 Copyright 2017 VLDB Endowment 2150-8097/17/08. Starting in December 2014, we have been building ClaimBuster, an end-to-end system that uses machine learning, natural language processing, and database query techniques to aid in the process of fact-checking. It monitors live discourses (e.g., interviews, speeches and debates), social media, and news to identify factual claims, de- tect matches with a curated repository of fact-checks from profes- sionals, and deliver those matches instantly to the audience (e.g., by displaying a pop-up warning if a presidential candidate makes a false claim during a live debate). For various types of new claims not checked before, ClaimBuster automatically translates them into queries against knowledge databases and reports whether they check out. For claims where humans must be brought into the loop, it provides algorithmic and computational tools to assist lay persons and professionals in understanding and vetting the claims. Its use will be expanded to verify both political and non-political claims in many types of narratives, discourses, and documents such as sports news, legal documents, and financial reports. While the development of the full-fledged system is still on- going, several components of ClaimBuster are integrated and de- ployed in the real-world. One of its most mature components, the claim spotter, discovers factual claims that are worth checking. Given the plethora of discourses and narratives we are constantly exposed to, ClaimBuster gives each sentence a score that indicates how likely it contains an important factual claim that should be checked. This essentially provides a priority ranking that helps fact-checkers efficiently focus on the top-ranked sentences without painstakingly sifting through a large number of sentences. ClaimBuster was tested in real-time during the live coverage of all primary and general election debates throughout the 2016 U.S. election. Closed captions of the debates on live TV broadcasts, captured by a decoding device, were fed to ClaimBuster, which immediately scored each sentence spoken by the candidates and posted top-scored claims to the project’s website ClaimBuster) and Twitter account (@ClaimBusterTM). Post-hoc anal- ysis of the claims checked by professional fact-checkers at CNN, PolitiFact.com, and FactCheck.org reveals a highly positive corre- lation between ClaimBuster and journalism organizations in decid- ing which claims to check [5]. ClaimBuster has also been continu- ously monitoring Twitter and retweeting the check-worthy factual claims it finds in tweets from politicians and organizations (twitter. com/ClaimBusterTM). Recently it started to monitor Hansard” 3 – the transcripts of proceedings of the Australian parliament Claim Monitor Claim Spotter match found match not found Claim Checker claims keywords, queries ... social media debates Hansard Repository fact-checks claims PolitiFact.com CNN.com knowledge bases other sources other sources Website Twitter Slackbot API Fact-check Reporter Claim Matcher WebFigure 1: System architecture of ClaimBuster. ClaimBuster already produces true-or-false verdicts for certain types of factual claims. Given a factual claim which is scored highly by the claim spotter component, ClaimBuster may reach a verdict by two methods. One method is to translate the factual claim into questions and their accompanying answers. It then sends the questions to question-answering systems and compares the re- turned results with the aforementioned answers. It produces a ver- dict based on the presence/absence of a discrepancy between these two sets of answers. The other method is to search in a repository for similar or identical claims that have already been fact-checked by professionals and to use the verdicts from the professionals. In the case that ClaimBuster is not able to produce a verdict, it pro- vides processed search results from a general search engine to assist vetting the claim. The ClaimBuster project has received wide recognition in the fact-checking community and substantial media coverage. (See for a list of media outlets and the stories in which they cover ClaimBuster.) The aforementioned white paper calls ClaimBuster a tool with the most advanced generalised automatic claim spotting.” [2] Others considered it perhaps the biggest development to date” in ranking claims 4 and a pretty use- ful guide for journalists and those members of the public who wish to spend time using an algorithm to help find facts.” 5 ClaimBuster, upon completion, can benefit a large base of poten- tial users. It directly benefits citizens and consumers by improving information accuracy and transparency. It helps news organizations speed up their fact-checking and ensure the accuracy of their news stories. Businesses can use ClaimBuster to identify falsehoods in their competitors’ and their own reports and press releases. It can also assist professionals such as lawyers in verifying documents. 2. SYSTEM OVERVIEW The ClaimBuster system is hosted at and its features are being constantly expanded. Figure 1 depicts its system architecture. The claim monitor interfaces various data sources (social media, broadcasted TV programs, and websites) with ClaimBuster. The claim spotter identifies check-worthy fac- tual claims in verbose text from the data sources. The claim matcher 4K. Moreland and B. Doerrfeld. Automated Fact Checking: The Holy Grail of Political Communication. Nordic APIs, February 25, 2016. 5 P. Fray. Is that a fact? Checking politicians’ statements just got a whole lot easier. The Guardian, April 18, 2016. finds existing fact-checks that are closely-related or identical to the discovered claims. In this way, we fully leverage well-researched fact-checks from professional fact-checkers. This is particularly useful, because oftentimes the same false claims are repeated. 6 When a matching fact-check cannot be found, the claim checker queries external knowledge bases and the Web to vet the factual claims. The fact-check reporter compiles the evidence from the claim matcher and the claim checker, and presents fact-check re- ports to users through various channels, such as the project website, its Twitter account, a Slackbot, and a public API. Below we explain these components in more detail. Claim Monitor: This component continuously monitors and re- trieves texts from a variety of sources, upon which claim spotting is applied to discover important factual claims. At present, the system monitors the following sources. Broadcast Media: ClaimBuster uses a decoding device to extract closed captions in broadcasted TV programs. This was used for our live coverage of all twenty-one primary election debates and four general election debates of the 2016 U.S. presidential election. One challenge in delivering the live coverage of such events is the lack of speaker identity in the closed captions. ClaimBuster timely derives the speaker of a sentence using the Twitter Streaming API. 7 The idea is based on the premise that, during a popular live event, active Twitter users tend to mention the speaker while tweeting a statement the speaker made. Details of speaker identification in ClaimBuster can be found in [7]. Social Media: ClaimBuster has been continuously monitoring a list of 2220 Twitter accounts (U.S. politicians, news and media organizations) using the Twitter streaming API. It filters out non politics-related tweets using an SVM classifier [1]. Websites: ClaimBuster also gathers data from websites. For in- stance, as mentioned in Section 1, it monitors the transcripts of proceedings of the Australian parliament. Claim Spotter: Given a sentence, ClaimBuster gives it a score between 0.0 and 1.0. The higher the score, the more likely the sen- tence contains check-worthy factual claims. The lower the score, the more non-factual, subjective and opinionated the sentence is. ClaimBuster’s score is based on a classification and scoring model. The model was trained using tens of thousands of sentences from past general election debates that were labeled by human coders. 6A. D. Holan. All Politicians Lie. Some Lie More Than Others. The New York Times, December 11, 2015. Figure 2: The user interface of ClaimBuster when it is applied on a debate. Its features include the tokens in sentences and the tokens’ part- of-speech (POS) tags. The recall and precision in detecting check- worthy factual claim are 74% and 79%, respectively [3, 4]. The claim spotter has been applied on sentences from all the aforementioned sources, including the closed captions of the pres- idential debates, tweets, and Hansard. Post-hoc analysis of the claims from the primary debates for the 2016 U.S. presidential elec- tion checked by professional fact-checkers at CNN, PolitiFact.com and FactCheck.org reveals a highly positive correlation between ClaimBuster and journalism organizations in deciding which claims to check and the topics of the selected claims [5]. Although its scoring and ranking model was trained using a labeled dataset of presidential debates, we find that the model achieved strong results on politics-related tweets and Hansard as well. Claim Matcher: Given an important factual claim identified by the claim spotter, the claim matcher searches a fact-check reposi- tory and returns those fact-checks matching the claim. The reposi- tory was curated from various fact-checking websites. The system has two approaches to measuring the similarity between a claim and a fact-check. One is based on the similarity of tokens and the other is based on semantic similarity. An Elasticsearch 8 server is deployed for searching the repository based on token similarity, while a semantic similarity search toolkit, Semilar [8], is applied for the search based on semantic similarity. We combine the search results from both in finding fact-checks similar to the given claims. 8https://github.com/elastic/elasticsearch Claim Checker: Given a claim, the claim checker collects sup- porting or debunking evidence from knowledge bases and the Web. With regard to knowledge bases, it uses a question generation tool [6] to generate many questions based on the claim and select those good questions which are then sent to the question answering en- gine Wolfram Alpha via an API. 9 Then the answers from Wolfram Alpha are extracted. Simultaneously, it sends the aforementioned questions to Google via HTTP requests and extracts the answers from Google’s answer boxes in the HTML responses. If any clear discrepancies between the returned answers and the claim exist, then a verdict may be derived and presented to the user. Meanwhile, the factual claim itself is sent to Google as a general search query. The claim checker then parses the search result and downloads the web page for each top result. Within each such page, it finds sentences matching the claim. The matching sentences and a few of their surrounding sentences are then grouped together into a context. The contexts, answers returned from Wolfram Alpha and Google answer boxes, as well as any verdicts derived from those answers form the supporting or debunking evidence for the claim. The evidence is reported to the user, as follows. Fact-check Reporter: The fact-check reporter synthesizes a re- port by combining the aforementioned evidence and delivers it to users through the project website. Furthermore, ClaimBuster also delivers the claim spotter scores on claims through a variety of Figure 3: The homepage of ClaimBuster website. channels, including its website, Twitter account, API, and Slack- bot. Its Twitter account (@ClaimBusterTM) retweets the highly- scored tweets from politicians and organizations and posts highly- scored claims from live events such as the presidential debates. To this date, @ClaimBusterTM has retweeted and posted about 13K check-worthy factual claims. A Slackbot has been developed for users to supply their own text (i.e., directly as input or through a shared Dropbox folder) and receive the claim spotter score and fact- check report for that piece of text. The Slackbot has been published in the public Slack App directory and can also be installed by click- ing the ClaimBuster Slackbot” button in Figure 3. We also made available a public ClaimBuster API (note the button in Figure 3) to allow developers create their own fact-checking applications. 3. USER INTERFACE AND DEMONSTRA- TION PLAN We will demonstrate the user interface features of ClaimBuster’s website, Twitter account and Slackbot. Figure 3 is a screenshot of ClaimBuster’s homepage. It allows a user to apply ClaimBuster on their own text or view its results on the 2016 U.S. presidential debates and the Australian Hansard (cf. Section 2). The homepage also embeds tweets from the ClaimBuster Twitter account. Figure 2 is a screenshot of ClaimBuster applied on an archived presidential debate. The interface for the Australian Hansard is similar. Besides the basic information of the debate (e.g., title, date), the interface shows five panels. (1) The transcript panel displays the transcript of the debate. (2) The fact-check report panel displays supporting or debunking evidence collected by claim matcher and claim checker. (3) The social discussion panel allows users to discuss factual claims while collaboratively vetting them. (4) The video panel (omitted due to space limitations) has an em- bedded player which plays the debate video from YouTube. (5) The visualization panel shows a word cloud (omitted) and a claim spotter score chart for the sentences in the transcript. Sentences in the transcript panel are highlighted using different shades of blue proportional to their claim spotter scores. The web- site allows a user to sort the sentences by time or score and to use a slider to specify the minimum score for sentences to be highlighted. Every sentence can be annotated. An annotated sentence is under- lined in red. Users can discuss it, powered by the Genius platform , while collaboratively vetting it. When a user selects a sentence in the transcript panel, the fact- check report panel displays the supporting or debunking evidence for the selected sentence. Specifically, it shows three types of ev- idence. The leftmost column displays similar fact-checks (along with the verdicts) from the fact-check repository, if any. The middle column shows answers extracted from Wolfram Alpha and Google answer boxes. The rightmost column displays the related search re- sults from Google. The fact-check report is also directly available for any input sentence when the user clicks the button End-to-end Fact-checking” in Figure 3. The website further visualizes the content of a transcript using a word cloud and a claim spotter score chart. In the score chart, the data points represent the sentences and are color-coded by speak- ers. The x-axis of a point represents the corresponding sentence’s sequential position in the transcript, and the y-axis corresponds to the claim spotter score of the sentence. When a user adds the ClaimBuster Slackbot to their slack group, the user can ask the bot to fact-check a statement by using two commands. The /getscore command returns the claim spotter score of a sentence, e.g., Figure 4. The /factcheck command on a sentence returns its fact-check report. Figure 4: The ClaimBuster Slackbot","Hassan, N., Zhang, G., Arslan, F., Caraballo, J., Jimenez, D., Gawsane, S., ... & Tremayne, M. (2017). Claimbuster: The first-ever end-to-end fact-checking system. Proceedings of the VLDB Endowment, 10(12), 1945-1948."
ML_DB_073,Claimviz: Visual analytics for identifying and verifying factual claims,"Verifying a factual claim made by public figures, aka fact-checking, is a common task of the journalists in the newsrooms. One critical challenge that fact-checkers face is-they have to swift through a large amount of text to find claims that are check-worthy. While there exist some computational methods for automating the fact-checking process, little research has been done on how a system should combine such techniques with visualizations to assist fact-checkers. ClaimViz is a visual analytic system that integrates natural language processing and machine learning methods with interactive visualizations to facilitate the fact-checking process. The design of ClaimViz is based on analyzing the requirements of real fact-checkers and our case studies demonstrate how the system can help users to effectively spot and verify claims.",Computer Science,Proceeding,2020,Y,N,2,"Fact-checking is a task in journalism where the goal is to assess the veracity of claims made by public figures; especially politicians. Although it is a common task in all newsrooms, due to the large amount of misinformation and to stay away from political biases, independent fact-checking organizations such as PolitiFact.com, *e-mail: mrony@umd.edu †e-mail: enamulh@yorku.ca ‡e-mail: nhassan@umd.edu Factcheck.org have started to form and flourish in recent years. Ac- cording to Duke Reporters Lab, the number of fact-checking organi- zations in the world has increased five times since 2014 [26]. Due to the large volume of content to be checked, fact-checking has been the subject of calls from the journalism community to develop tools to automate this task [7, 10]. Researchers are working to automate the fact-checking process; mostly by combining machine learning, natural language processing and information retrieval methods to automatically spot the factual claims in text corpora (e.g. debates, interviews, tweets) and subsequently verifying the claim based on relevant information sources [14, 36, 37]. While most of these works have focused on computation methods, there remains a knowledge gap of how to effectively design for the users (e.g., fact-checkers, media analysts, journalists) to take advantage of these methods. In this paper, we conducted a design study with fact-checkers to understand how combining natural language processing with visual- ization techniques could support the user in spotting and verifying claims. After identifying the challenges and requirements of these fact-checkers, we designed and evaluated ClaimViz, a visual analytic system, to support fact-checkers in finding and verifying factual claims. ClaimViz (Figure 1) supports the user to explore the po- tentially check-worthy claims in a large transcript of conversations such as debates and interviews through multiple facets ( e.g. topics, speakers) and then select claims for further verification. Our contri- butions include (1) a visual analytic system called ClaimViz which makes novel integration between natural language processing and interactive visualization techniques to support fact-checkers and (2) the evaluation of the tool with four professional fact-checkers which provides some initial evidence of the potential utility of ClaimViz and provides future directions for developing visual analytic tools in the area of computational journalism. 2 RELATED WORK Automation in fact-checking: The concepts of automated fact- checking and misinformation detection are related yet cover two separate regions on a common information disorder spectrum [38]. While misinformation detection usually focuses on assessing credi- bility of articles, automated fact-checking focuses on assisting jour- nalists in vetting and verifying factual statements [10]. Hassan et al. primarily delineated the vision of automating the fact-checking process and identified the computational and journalistic challenges that are involved in this process [11]. One of the critical computa- tional challenges, assessing check-worthiness of claims, has been addressed by researchers using machine learning models [13, 21, 31]. Another challenge- verification, has been addressed by computation methods that can be divided into two categories by and large- AI based and Database based. The AI based systems primarily focus on the patterns and features from the data to predict the veracity using supervised learning [18,23,40]. While these methods are good at predicting veracity for new claims, the assumption of existence of latent patterns to differentiate fake news from real news may not be true in carefully curated disinformation contents. The Database based approaches assume that knowledge bases have enough rele- vant information to check claims [6]. While these approaches may explain the result better than the AI based solutions, these methods are incapable of assessing fresh claims. ClaimBuster [12, 14] pre- sented the first end-to-end prototype of an automated fact-checking system that uses the Database approach. It identifies the check- worthy claims and then predicts the veracity of claims by querying knowledge bases such as Wolfram Alpha and Freebase. Ciampaglia et al. [6] uses Wikipedia as a knowledge network and uses path enumeration between entities to assess the veracity of a claim. There are some works which combine both AI and Database approaches. For instance, Shi et al. [36] leverage knowledge graphs for retriev- ing information about entities in a claim and predict links between entities to quantify the truthiness of the claim. Others attempted to fact-check visual images instead of text. For instance, DejaVi supports journalists in identifying misinformation from social media images [27] by allowing them to flag images collaboratively, so that others can find those images as well as their near-duplicate images. Visualizations for fact-checking: Although there has been much research on computationally modeling the fact-checking pro- cess, limited work has been done on the integration of these auto- mated solutions in the newsrooms among the fact-checkers. Existing systems like ClaimBuster [14] and ClaimPortal [24] only provide limited interactive features like filtering based on claim score and do not provide adequate visualization support to spot and verify claims. Nguyen et al. [30] present a mixed-initiative approach for presenting the fact-checking predictions to help users understand how the underlying model arrived at its prediction. Again, their interface provides some filtering options to help users interact with the features of the model but it does not provide a visual overview of the fact-check worthy claims. Kurdani et al. [19, 20] present a visual analytic system to support the identification of misinformation in social media through analysis of text, social network, image, and lan- guage feature. In this paper, we closely work with the professional fact-checkers and design, ClaimViz, a visual analytic system for assisting the fact-checking process by leveraging natural language processing and visualization methods. We focus on fact-checking instead of misinformation detection and deal with speech transcripts (e.g. meetings, interviews, debates) as opposed to social media texts which are often constrained by character limits. Visual text analysis for multi-party conversations: There has been growing interest in combining automatic text analysis and interactive visualizations to support the exploration of multi-party conversations. Researchers developed interactive visualizations to show automatically extracted topics and sentiments from conversa- tions [8, 15, 17] as well as to show conversational dynamics using temporal episodes [34]. There have also been attempts to visualize argumentation patterns to support analysis of a debate’s deliberative content [8, 9, 32]. While the above body of work are helpful in understanding multi-party conversations they are not designed to deal with specific journalistic tasks of fact-checking. 3 THE FACT-CHECKING DOMAIN 3.1 Requirement Gathering In order to design ClaimViz, we followed visualization design study methodologies [29, 33] where we started by interviewing three pro- fessional fact-checkers from Politifact.com and Duke Reporters Lab. The interview was open-ended in nature and was run in a focus group setting so that it could open up a diverse range of issues and possible solutions. Before the interview, we demonstrated the early version of ClaimViz (which was based on Claimbuster [14], a system that all three fact-checkers were familiar with) so that the interviewees get a sense of the intended goal. Following that, we gathered feedback on the early mockup and further requirements. The interviewees provided important insights into their current work practices and challenges. They mentioned that they primarily focus on spoken texts where people are often forced to express what they have in mind as opposed to social media where the texts are short and speakers are usually more careful in what they express. They have tested ClaimBuster in their claim identification process in a limited scope. The workflow started by ClaimBuster monitoring news sources that are predefined by the organizations. It identified check-worthy claims from these sources and prepared an email newsletter with highly check-worthy claims and sent the letter to all fact-checkers around the country. While they felt that without ClaimBuster-like tool it would be very difficult to constantly monitor the new sources for spotting claims they also pointed out several tasks that are not currently well supported such as the lack of ability for human to provide input and absence of context information [3]. 3.2 Tasks Analysis Our conversation with domain experts revealed several important analytical tasks as follows: T1: Spot and read claims in context: The fact-checkers want to get a visual overview of how check-worthy claims are distributed across a transcript so that they can quickly spot the interesting claims. At the same time, just presenting the sentence containing the claim is not enough, they need the ability to read a few sentences before or after the claim to understand the proper context of the claim. T2: Understand topics of factual claims: Since a transcript may involve several topics of discussion, interviewees wanted to understand how claims are distributed across different topics and when topic transitions occur. They also wanted topics represented with meaningful short-phrases and the ability to merge similar topics together whenever necessary. T3: Find speaker attribution: Fact-checkers want to know who made the claim under examination. Conversely, they may want to find all claims made by a specific speaker of interest. T4: Annotating claims: Fact-checkers need to mark down claims and record them for further verification. They also want to forward the selected claims to others, preferably through emails. T5: Find relevant evidence for a given claim: After spotting claims in a transcript, fact-checkers must analyze external sources and knowledge-base to verify the truthfulness of the claims. For achieving this goal, they need to find all the relevant sources that may provide supporting or opposing evidence to the claim. 3.3 Text Analytics Our system applies several text analysis techniques on the tran- scribed document and then visualizes the results to the user (Figure 2). To support the fact-checking tasks, the system predicts: (1) the check-worthiness and (2) the claim type. We also applied techniques for topic and sentiment analysis as well as evidence mining.","Rony, M. M. U., Hoque, E., & Hassan, N. (2020, October). ClaimViz: Visual analytics for identifying and verifying factual claims. In 2020 IEEE Visualization Conference (VIS) (pp. 246-250). IEEE."
ML_DB_078,Understanding the Promise and Limits of Automated Fact-Checking,"The last year has seen growing attention among journalists, policymakers, and technology companies to the problem of finding effective, large-scale responses to online misinformation. The furore over so-called ‘fake news’ has exacerbated long-standing concerns about political lying and online rumours in a fragmented media environment, sharpening calls for technological solutions to what is often seen as a technological problem. This factsheet gives an overview of efforts to automatically police false political claims and misleading content online, highlighting central research challenges in this area as well as current initiatives involving professional fact-checkers, platform companies, and artificial intelligence researchers. The influence of ‘fake news’ in different parts of the world remains poorly understood. Initial evidence from the US and Europe suggests that the share of online users who visit false news sites directly is quite limited, and that people exposed to these sites visit mainstream news sources far more (Allcott and Gentzkow 2017; Guess et al. 2018; Fletcher et al. 2018). However, the same studies indicate fabricated news stories may draw disproportionate attention on social networks, outperforming conventional news, and some partisans (e.g. Trump voters in the US) appear to be regular users of false news sites. Little is known about the dynamics by which individual viral stories may influence the opinions and behaviour of specific, targeted audiences around particular events or issues. In the US and Europe, concern about commercially or politically motivated misinformation online – in particular about mounting evidence of sophisticated, state-backed campaigns operating from Russia – has fuelled a vigorous debate over policy options. These include a raft of proposals to regulate platform companies like Facebook and Google in new ways, a question under review in the European Commission. Several countries, notably Germany, France, and Ireland, have passed or are considering legislation that penalises the distribution of false information. These concerns have also drawn new attention to the potential of various automated fact-checking (AFC) technologies to combat false information online. However, deciding the truth of public claims and separating legitimate views from misinformation is difficult and often controversial work (see Graves 2016), challenges that carry over into AFC. Based on a review of current efforts and interviews with both fact- checkers and computer scientists working in this area, this survey of the AFC landscape finds that: • Much of the terrain covered by human fact- checkers requires a kind of judgement and sensitivity to context that remains far out of reach for fully automated verification. • Rapid progress is being made in automatic verification of a narrow range of simple factual claims for which authoritative data are available. Even here, though, AFC systems will require human supervision for the foreseeable future. • Both researchers and practitioners agree that the real promise of AFC technologies for now UNDERSTANDING THE PROMISE AND LIMITS OF AUTOMATED FACT-CHECKING| 2 | lies in tools to assist fact-checkers to identify and investigate claims, and to deliver their conclusions as effectively as possible. • So far independent, nonprofit fact-checking organizations have led the way in developing and implementing AFC, with little activity from traditional media outlets. • Some individual AFC tools have been built inexpensively by fact-checking groups. However, advancing capabilities and developing large- scale systems requires continuing support from foundations, universities, and platform companies.",Journalism Studies,Article,2018,Y,N,121,"AFC initiatives and research generally focus on one or more of three overlapping objectives: to spot false or questionable claims circulating online and in other media; to authoritatively verify claims or stories that are in doubt, or to facilitate their verification by journalists and members of the public; and to deliver corrections instantaneously, across different media, to audiences exposed to misinformation. End-to- end systems aim to address all three elements – identification, verification, and correction (see chart). The first proposals to automate online fact-checking appeared nearly a decade ago. Over the last several years a growing research literature has embraced AFC as an interesting problem in artificial intelligence, intersecting with practical experiments by fact- checkers.1 Two recent programming competitions, the ‘Fast  Furious Fact Check Challenge’ and the ‘Fake News Challenge’, allowed research teams from around the world to test different AFC techniques on common problem sets. 2 Dr Andreas Vlachos, a lecturer at University of Sheffield, remarks on the increased attention: We published our first paper in 2014. To us, apart from our interest in politics, we thought it was a great challenge for artificial intelligence to actually work on this problem. [But] for better or worse, Trump’s election increased the interest. Meanwhile, real-world AFC initiatives have enjoyed a wave of additional funding in the last two years. Full Fact, a London-based fact-checking charity, began developing AFC tools in 2016 with a €50,000 grant from Google and recently announced £500,000 additional funding from the Omidyar Foundation and the Open Society Foundations. The Duke Reporters Lab, based at Duke University, received $1.2m in late 2017 to launch the Tech  Check Cooperative, a hub for AFC projects, from the Knight Foundation, the Facebook Journalism Project, and the Craig Newmark Foundation. In January, Factmata, a London-based startup developing an AFC platform, announced $1m in seed funding. 1 Useful research overviews are in Cohen et al. 2011; Hassan et al. 2017; Vlachos and Riedel 2014. 2 See https://www.herox.com/factcheck/guidelines; http://www.fakenewschallenge.org CORE ELEMENTS OF AUTOMATED FACT-CHECKING identification • monitoring media  political sources • identifying factual statements • prioritising claims to check verification • checking against existing fact-checks • checking against authoritative sources • unstructured (e.g. credibility scoring) correction • flagging repeated falsehoods • providing contextual data • publishing new fact- checks UNDERSTANDING THE PROMISE AND LIMITS OF AUTOMATED FACT-CHECKING| 3 | Approaches to AFC Real-world AFC efforts begin with systems to monitor various forms of public discourse – speeches, debates, commentary, news reports, and so on – online and in traditional media. This is a difficult problem that may involve scraping transcripts and other material from media or political pages, monitoring live subtitle feeds, or using automatic transcription.3 Once monitoring is in place, the central research and design challenge revolves around the closely linked problems of identifying and verifying factual claims, explored below. A tension exists in that success in the first complicates the second, widening the range of claims that must be verified. In practice, AFC implementations constrain the problem by drawing on the work of human fact-checkers and/or by sharply limiting the kinds of claims being checked. Identifying Claims The greatest success in AFC research has come in the area of extracting discrete factual claims from a text such as a speech or an article. The most common approach relies on a combination of natural language processing and machine learning to identify and prioritise claims to be checked. For instance, ClaimBuster, an AFC platform developed at the University of Texas-Arlington, at a cost of roughly $150,000 so far, trained on about 20,000 sentences from past US presidential debates, classified by paid human coders, to learn to distinguish ‘check-worthy’ factual claims from opinions and boring statements (Hassan et al. 2017). In a test during a US primary debate in 2016, more than 70% of actual claims checked by fact-checkers at PolitiFact and CNN were among the top fifth of statements identified by ClaimBuster.4 A number of fact-checking outlets around the world have begun relying on software to help spot claims to check. In the US, for instance, the Duke Reporters Lab recently deployed a tool that uses ClaimBuster to deliver potentially interesting claims to fact-checkers at PolitiFact, FactCheck.org, the Washington Post, and the Associated Press (see the box). However, so far these systems can only identify simple declarative statements, missing implied claims or claims embedded in complex sentences which humans recognise easily. This is a particular challenge with conversational sources, like discussion programmes, in which people often use pronouns and refer back to earlier points. It is also important to note that the ‘ground truth’ established by training algorithms on human work is neither universal not permanent. For instance, ClaimBuster has been optimised to detect debate claims and does somewhat less well harvesting statements on Twitter. More broadly, the meaning and One hub for automated fact-checking projects is the Duke Reporters Lab at Duke University. Tech  Check Alerts, in beta testing since early 2018, automatically generates a daily email newsletter neatly listing 15 promising political claims harvested from transcripts of CNN programming. The programme uses the ClaimBuster API but identifies the speaker and strips out statements by journalists; modules are being developed to pull claims from the Congressional Record, the California legislature, and the Facebook feeds of candidates in contested congressional races. Today the email goes out at 10 a.m. EST to PolitiFact, FactCheck.org, the Washington Post, and the Associated Press. Another new project, FactStream, offers live, ‘second- screen’ fact-checking of major political events via a mobile app. Fact-checkers use the platform to respond to dubious claims in real time, issuing either a capsule summary from a previous fact-check, or, for new claims, a ‘quick take’ adding context. Its first public test came during the 2018 State of the Union address, fed by alerts from PolitiFact, FactCheck.org, and the Washington Post; reportedly more than 3,000 people used the app at some point during the speech. (See: https://www.poynter.org/news/review-live-fact-checking- takes-center-stage-state-union and http://www.niemanlab.org/2018/01/ factstream-debuted-live-fact-checking-with-last-nights-sotu-howd-it-go.) 3 A seminal discussion of monitoring and other core AFC challenges which informs this report is in Babakar and Moy 2016. 4 See https://www.poynter.org/news/holy-grail-computational-fact-checking-and-what-we-can-do-meantime. A longer term comparison is reported in Hassan et al. 2017.","Graves, D. (2018). Understanding the promise and limits of automated fact-checking. Reuters Institute for the Study of Journalism."
ML_DB_082,Untrue.News: A New Search Engine For Fake Stories,"In this paper, we demonstrate Untrue News, a new search engine for fake stories. Untrue News is easy to use and offers useful features such as: a) a multi-language option combining fake stories from different countries and languages around the same subject or person; b) an user privacy protector, avoiding the filter bubble by employing a bias-free ranking scheme; and c) a collaborative platform that fosters the development of new tools for fighting disinformation. Untrue News relies on Elasticsearch, a new scalable analytic search engine based on the Lucene library that provides near real-time results. We demonstrate two key scenarios: the first related to a politician ˆa looking how the categories are shown for different types of fake stories ˆa and a second related to a refugee ˆa showing the multilingual tool. A prototype of Untrue News is accessible via http://untrue.news",Computer Science,Article,2020,N,Y,5,"A Web Search Engine (WSE) is an Information Retrieval System used to discover web pages relevant to specific queries (Brin and Page, 2012). Multipurpose web search engines, such as Google, Yahoo, Bing among others, have been employed for retrieving a wide range of information from the Internet, ranging from news articles to scientific papers. Normally, search results are composed of thou- sands of items that are commonly ranked based on some criteria, for example: the combination of popularity and rel- evancy. Some WSEs also include the user’s activity history in their ranking schema, creating personalized results. Some studies(Pariser, 2011; Machado et al., 2018) have shown that those characteristics tend to lead users to a spe- cial type bias called Filter Bubble. Since this problem has been identified, other systems, like the web browser Duck- DuckGo1 , have become popular with the pledge to avoid this problem by not tracking, or ”bubbling”, users. How- ever, this measure was not enough to stop the proliferation of disinformation on the web. We see the lack of dedicated solutions as a barrier to the mitigation of disinformation. For example, when a person wants to make an exploratory search about fake stories involving a particular subject or a politician, the Multipurpose WSEs provides an array of un- related information turning the search into a thorny chore for the user. In this context of abundant information and disinformation we introduce Untrue News as a new type of search engine; designed exclusively for retrieving fake stories and their authors, filling an important gap left by the multipurpose tools. An essential aspect of it is the ability to enrich the re- sults with articles in different languages whilst avoiding the ”filter bubble” of personalized results. For instance, analyz- ing the fake stories about Greta Thunberg in Austria or in Brazil and retrieving the results in the user’s language and from different sources. Also, our system can provide the veracity of each rumor recovered directly in the retrieval web interface. In summary, the contribution of this project is as follows: 1. The first web search engine for fake news; 2. Open-source web crawler for gathering of fake articles on the Internet; 3. Natural Language Processing pipeline for data enrich- ment of fake articles in different languages, composed of automatic and semi-automatic strategies, e.g. text translation for multiple languages, recognition, and linking of entities (i.e. person, organizations, loca- tions, and more) to structured open semantic knowl- edge bases (DBpedia); 4. An information retrieval strategy for avoiding the filter bubble of personalized results. The rest of this paper is organized as follows: section 2. introduces the features, section 3. describes the design of our architecture, and section 4. gives further details about our knowledge base. on section 5. our workflow for col- laboration is exposed, and on section 6. two scenarios are presented to illustrate the usability of the platform. Finally, section 7. summarizes our perspectives and final consider- ations. 2. Features In addition to concerns about a classification that is coher- ent, rich, and accurate, Untrue News presents features that emerge from the social needs of the users. The features highlighted below show functionalities that not only differ from conventional web search engines, but also enhance the user’s experience. These are: a) multilingual system for translation of different stories; b) protection of user’s pri- vacy; and c) the building of an academic friendly platform. 2.1. Multi languages What are the fake stories being spread in Austria or Brazil about Greta Thunberg? How are these stories being in- terpreted in those two countries that speak different lan- guages? Untrue News combines fake stories in different languages around the same subject or person providing unique results on the user’s own language. That is, it will not only select the most relevant information around a cho- sen topic in the appropriate language (the user’s language), but it will actually translate the information into the user’s own language whenever necessary. This is a key feature since many of the rumors at the national level are not re- ported abroad and thus, not translated ˆa or at least not trans- lated well enough to be coherent and match the exact words typed by the users. 2.2. Privacy Untrue News distinguishes itself from other search engines by not profiling users; it shows everyone the same search results. User’s privacy is very important to us, so Untrue News makes sure that searches are concealed and protected, whilst avoiding the filter bubble of personalized search re- sults. It is now known how public opinion has been per- suaded on polemic topics, such as the 2016 US elections and the Brexit referendum, by the profiling of users accord- ing to their perceived views(Ward, 2018). That is, users with more conservative views are shown more articles and stories (even if fake ones) that support these views; like- wise, a more liberal-leaning users are shown stories that match these views. This tendency to search for, interpret or believe in information that confirms one’s prior beliefs is known as Confirmation Bias(Nickerson, 1998). This method of political persuasion based on user-profiling cre- ates a toxic social environment of polarised opinions and disinformation, which does not in any way contribute to healthy discussions on important issues. To the contrary, it opens the door for impersonation, manipulation, deception, distraction and intolerance. This is where Untrue News comes in, as an unbiased and independent fact-checking tool for the promotion of democracy and the truth that does not create ”filter bubbles”. In practical terms, Untrue News will only ever collect the data it absolutely needs to in order to improve its service; it will not create any type of user profile based on their search history because this data will only be stored for a short pe- riod of time and with great care to protect users’ privacy. Moreover, Untrue News does not use any external track- ing tool to optimize its service, which means that users can rest assure their searches are not being tracked, but in ad- dition, users are able to switch off all tracking, being able to browse completely anonymously in a private window. User’s searches are encrypted in order to protect them from potential eavesdroppers, and in doing so, we can guarantee that there will never be anyone between Untrue News and its users watching their searches. Finally, Untrue News will never sell your data or your searches to advertising compa- nies, this goes directly against our ethos and the purpose of the platform. 2.3. Academic Friendly We envision Untrue News as an academic umbrella for stu- dents and researchers interested in investigating different branches of knowledge; for example, User Experience, Big Data Analytics, Open Data, Machine Learning, and Lan- guage Technologies. Additionally, the source code of Un- true.news 2 is available on the internet as an Open Source project, for fostering collaboration in the development of new tools for fighting disinformation. The Open Source character is one of the underpinnings of this project. As a collaborative platform, Untrue News aims to build a community around itself which will enable it to grow collectively and thus, have a greater potential for over- coming the challenges a project of this magnitude will in- evitably encounter; i.e.: the language barrier. Besides pro- viding free tools, information, and advice, Untrue News compile a rich multilingual knowledge base that can be used for fostering other initiatives in fighting disinforma- tion (Mohtaj et al., 2019). 3. System’s Architecture The overall architecture of Untrue News is illustrated in Figure 1 showing the system working mainly in two phases. The first one is the data collection from a list of reliable fact-checking agencies. This has the role of gathering new rumors, classifying their authenticity, structuring the data, and indexing it in an Information Retrieval System. The second phase has the role of creating the interaction be- tween the platform and the users. This phase uses a frame- work that provides the user interface and also communi- cates with the Information Retrieval System in order to de- liver the rumors’ information about a query submitted to the system by the user. 1. Web Crawler. Normally, search engines maintain real-time information by running an algorithm com- monly called ’web crawler’. We have designed a web crawler exclusively for finding information about fake stories. For each distinct source included in our white- list of reliable websites, a template was carefully de- veloped to navigate through the website and extract only the metadata that is publicly available, such as: the claim, date, source, amongst others. 2. Data Enrichment This comprises several natural language-processing tasks to enrich the collected con- tent in order to enable faceted browsing in the context of the text, and to enhance search capabilities. For in- stance, we use an automatic machine translation that enables the user to overcome language barriers and to be informed about rumors in different languages. Fur- thermore, Untrue News employs Google translation API3 , as it has presented the best translations for this Figure 1: Untrue’s Ecosystem sort of documents. Lastly, we have employed named entity recognition and linking the documents to open semantic databases, like DBpedia Spotlight (Mendes et al., 2011) has been used4 . 3. Knowledge Base. This is a technology used to store complex structured and unstructured information. We have used Elasticsearch to store our data, which is a scalable analytic search engine based on the Lucene library, that provides near real-time results. In our sys- tem, it is responsible for indexing all documents col- lected by the Web Crawler during the data collection phase, and retrieve them by using a schema-free JSON document, when requested during the User interface phase. 4. Workflow Manager. We have employed Apache Air- flow 5 for orchestrating the data workflow in our sys- tem. It is an open-source platform that enables a sys- tem to programmatically schedule tasks and its depen- dencies as a Directed Acyclic Graphs (DAGs). In our system, it is employed for monitoring, alerting, and clustering the management of the data from the very early process of the data gathering to the indexing pro- cedures into the Knowledge Base. 5. User Interface. The user interface, depicted in 2, has a role in managing the interaction with the user. We have employed the Django framework6 since it follows the model-template-view architectural pattern that allows rapid and clear development. It is re- sponsible for handling all the requests from users, making the connection to the database and generat- ing HTML dynamically by using database-driven tem- plates. The templates contain the static parts of the desired HTML output, as well as some special syntax describing how dynamically the content was obtained from the database and display all of it to the user. 6. Automatic Ranking. This plays a central role in many information retrieval systems, especially in search engines(Woloszyn and Nejdl, 2018). The main aim of a ranking is to optimally sort the result list, showing the most relevant results on the top and the least relevant on the bottom. Typically, learning to rank is a machine learning problem of predicting a score judgment, i.e.: how relevant or irrelevant a cer- tain content is. Usually, to obtain this relevancy the judgment of humans are used as a benchmark for train- ing machine learning models. 4. Knowledge Base The Knowledge Base of rumors relies only in a highly rep- utable source or information. For instance, one valuable instrument to verify the truthfulness of a claim is ’fact- checking’. Usually, a fact-checking agency is a non-profit organization that provides independent investigations about questionable facts. We rely on sources considered by the fact-checking community as trustworthy according to In- ternational Fact-Checking Network’s code of principles 7 . From the fact-checker, we use only metadata publicly avail- able on the internet. 4.1. Rating Normalization When publishers write an article, they can add an spe- cial web markup (schema.org/ClaimReview) to their arti- cles containing structured data about the fact checked. For example, what was the claim being assessed, who made the claim, what was the verdict. The schema.org/ClaimReview markup establishes five different categories of the verdict: 1= ”False”, 2 = ”Mostly false”, 3 = ”Half true”, 4 = ”Mostly true”, 5 = ”True”. However, not all fact-checking agencies have adopted such standardization, being necessary a sec- ond step of normalization to make them comparable. In (a) Landing Page (b) Search Results Figure 2: User Interface order to tackle this issue, we have adopted 4 categories for data analysis (Tchechmedjiev et al., 2019): • TRUE: statements completely accurate; • FALSE: statements completely false; • MIXED: statements partially accurate with some ele- ments of falsity; • OTHER: special articles that do not provide a clear verdict or do not match any other categories. 4.2. Statistics Currently, the data collected from fact-checking agencies are composed of about 30.000 documents. This number is growing every day as new articles are published and the web crawler collects and indexes them. Table 1 presents the distribution of documents indexed by Untrue News accord- ing to the language. Most of the documents are obtained in English because it has a higher amount of news published everyday. Figure 3 (a) presents the number of documents by year and language indexed by Untrue News and shows that our web crawler can extract English fact-checking reports published since 1995. However, for Portuguese and German, the doc- uments obtained are from 2015 the earliest, since websites in those countries are newer. Finally, Figure 3 (b) presents the rating of distribution of these documents according to the classification of the 4 aforementioned categories. We are constantly improving our Knowledge Base by adding new sources of information. For instance, our short- term goal is to include all fact-checking agencies listed as signatories from the International Fact-Checking Network 8 . 5. Collaboration Workflow There is an active collaboration between the academic com- munity, students, and designers, working together to de- velop Untrue News. In this section we illustrate the work- flow of collaboration between these groups, see Figure 4. Table 1: Fact-checkers indexed by Untrue News. URL Language # fullfact.org English 26236 snopes.com politifact.com truthorfiction.com checkyourfact.com piaui.folha.uol.com.br/lupa/ Portuguese 3473 aosfatos.org/aos-fatos-e-noticia/ apublica.org/checagem/ g1.globo.com/e-ou-nao-e e-farsas.com dpa.com German 256 correctiv.org (a) Documents by year (b) Ratings Figure 3: Knowledge Base Statistics. As depicted in the figure 4, there are three different groups who play different roles in the system and push their cod- ing to the GitHub of the project. There is also a project leader who merges requests and make changes on the mas- ter branch of the project. To achieve continuous integra- tion in the project, the Jenkins triggers has been used to check any changes in the GitHub, pushing the changes in the server side as soon as any change is made in the reposi- tory. Figure 4: Collaboration Workflow 6. Demonstration Scenarios The following two scenarios are cases that can be consulted and shown in the demo section of Untrue News. The first is regarding the search for a politician’s name where the re- sults can show rumors to be true, false or, most commonly, a mix of both. The second case highlights the multi-language functionality showing the search for a refugee-related issue that has been published and checked in English but can be read in the user’s chosen language. 6.1. Searching for rumors related to a politician Rumors about politicians have become a big strategy in election periods and the use of the internet for spreading false information, against or in favor of a candidate, can be decisive during that period. (Machado et al., 2018) reasons that fake news were decisive in the 2018 election of Pres- ident Bolsonaro in Brazil. Compared to other candidates, ”Bolsonaro supporters spread the widest range of known junk news sources” and their strategy of using fake news to attack opponents generated new statutory laws and invest- ment in systems for fighting disinformation by the Brazil- ian government. The search for politicians on Untrue News results in different inquiries related to their names, which can be about rumors that: a) were spoken about them; b) they said about others; and c) that cites their names in the content. Accordingly, the results page shows the number of news that were found and how long the query of the database took. Each result of the list displays an icon re- lated to the category information, the title of the article by the fact-checking agency, the date of the publication, the country in which the news was checked, a link to access the original fact, and an excerpt of it. The news that a Brazil- ian politician posted, about the transfer of 8 million dol- lars to the Ministry of Education, for example, will show with a symbol of indecision because it was confirmed by a fact-checking agency that it is true that these funds were transferred from the government, however, it involved other ministries besides education. Therefore, this story is clas- sified as mixed because it involves true information ˆa the transfer of federal funds ˆa and false stories ˆa which depart- ment the funds actually went to. In this case the user could go to the fact-checking agency’s website to find out more about the details of the story. 6.2. Refugees checking out rumours in their mother tongue The spread of fake stories on the Internet affects oppressed social groups the most (Peters, 2018). In the case of refugees, this happens mainly due to the process of cul- tural adaptation where barriers such as language and obliv- iousness of local laws and customs are grounds for conflict. Being aware of stories about themselves is an important el- ement for improving their integration into society and the search for this kind of information on Untrue News can fa- cilitate this process. Searching the keyword ”refugees” on Untrue News will list a number of facts from various coun- tries and written in an array of languages. The development of the multilingual tool enables the identification and trans- lation of the listed content to be presented in the language chosen by the user; i.e.: Donald Trump’s declaration say- ing that ”Crime in Germany is up 10% plus since migrants were accepted”. In the results list, Untrue News will show the title of the article with a red symbol beside it, symbol- ically representing the complete falseness of the informa- tion. It also shows the USA flag used to identify where the checked article was published, which can be used as an ini- tial reference for the platform’s multilingual system. The multilingual tool will allow the user to choose a language for the title and article to be translated to, regardless of the language in which they were published. Full translation functionality for fake stories is a work in process. 7. Final Considerations and Perspectives In a time when rumors and false stories are decisive in building the collective thinking of society, Untrue News presents itself as a platform for fighting disinformation. As the first search engine for fake news, the platform has been developed to present orderly, relevant and accurate results, and its main features involve tools that enhance the user experience. Referring to the key features of the platform, the multilingual system for the identification and translation of results aims to aggregate users from different countries so that communication is offered to everyone equally. The users privacy is protected, providing consistent results but not standardized, whilst acting in the deconstruction of the filter bubbles. Finally, as an academic umbrella, the plat- form aims to enable other researchers and professionals to combat misinformation by working together based on true and checked information. Untrue News uses open-source platforms such as a work- flow manager to automate the tasks, an exclusive web crawler for collecting fake news, and a data enrichment process; all of them to build a consolidated knowledge base. The results showed through the user interface are ranked automatically by relevance and are organized by a collaborative workflow encompassing academy, students and designers. Currently, Untrue News is ranking about 30.000 documents between English, Portuguese and Ger- man. Since there is no standardization employed by the fact-checking agencies, Untrue News classifies the articles in four categories: true, false, mixed and other. This data normalization makes it possible to show on the search re- sults page which category the story is in, providing a clear information for the user. The perspectives for Untrue News are based on three points: a) technical amelioration in the searches; b) im- provement in platform usability; and c) development of collaborative processes. The first concerns new types of searches such as searching for images and videos or search- ing for news-related entities. This improvement will bring new types of results with more comprehensive information about the stories, the agencies and the false images. The second deals with the user experience on Untrue News, both regarding visual aspects and accessibility of informa- tion. Usability tests are being developed to understand best practices for displaying information to each user, whether by lists, symbols or plots generated for each result. Finally, collaborative process development includes a content pro- duction section on important topics such as misinformation and false stories, as well as curatorial processes where col- laborators will be able to suggest edits ˆa especially for re- sults in other languages. Building a collaborative, reliable and open-source platform for collective use is the main fo- cus of Untrue News","Woloszyn, V., Schaeffer, F., Boniatti, B., Cortes, E., Mohtaj, S., & Möller, S. (2020). Untrue. news: a new search engine for fake stories. arXiv preprint arXiv:2002.06585."
ML_DB_088,Human-in-the-loop artificial intelligence for fighting online misinformation: challenges and opportunities,"The rise of online misinformation is posing a threat to the functioning of democratic processes. The ability to algorithmically spread false information through online social networks together with the data-driven ability to profile and micro-target individual users has made it possible to create customized false content that has the potential to influence decision making processes. Fortunately, similar data-driven and algorithmic methods can also be used to detect misinformation and to control its spread. Automatically estimating the reliability and trustworthiness of information is, however, a complexproblem and it is today addressed by heavily relying on human experts known as fact-checkers. In this paper, we present the challenges and opportunities of combining automatic and manual fact-checking approaches to combat the spread on online misinformation also highlighting open research questions that the data engineering community should address.",Computer Science,Article,2020,N,Y,12,"As the amount of online information that is generated every day in news, social media, and the Web increases exponentially, so does the harm that false, inaccurate, or incomplete information may cause to society. Experts in fact-checking organizations are getting overwhelmed by the amount of content that requires investigation,1 and the sophistication of bots used to generate and deliberately spread fake news and false information (i.e., disinformation) is only making the tasks carried out by experts—i.e., identifying check-worthy claims and investigating the veracity of those statements—less manageable. The aim of this paper is to discuss the main challenges and opportunities of a hybrid approach where Artificial Intelligence (AI) tools and humans—including both experts and non-experts recruited on crowdsourcing platforms—work together to combat the spread of online misinformation. The remainder of this paper is organized as follows. Section 2 presents an overview of human-in-the-loop AI methods. Section 3 introduces the main challenges in identifying misinformation online. Section 4 summarizes recent work on machine learning methods applied to automatic truthfulness classification and check-worthiness. Section 5 describes recent advances on crowdsourcing one of the key activities in the fact-checking process, i.e., Copyright 2020 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE. Bulletin of the IEEE Computer Society Technical Committee on Data Engineering 65 judging the truthfulness or veracity of a given statement. Section 6 proposes a hybrid human-AI framework to fact-check information at scale. We conclude in Section 7 by summarizing the main take-away messages. 2 Human-in-the-loop AI Human-in-the-loop AI (HAI) systems aim at leveraging the ability of AI to scale the processing to very large amounts of data while relying on human intelligence to perform very complex tasks— for example, natural language understanding—or to incorporate fairness and/or explainability properties into the system. Example of successful HAI methods include [ 8, 9 , 15, 30]. Active learning methods [31] are another example of HAI where labels are collected from humans, fed back to a supervised learning model, and used to decide which data items humans should label next [32]. Related to this is the idea of interactive machine learning [ 2 ] where labels are automatically obtained from user interaction behaviors [20]. While being more powerful than pure machine-based AI methods, HAI systems need to deal with additional challenges to perform effectively and to produce valid results. One such challenge is the possible noise in the labels provided by humans. Depending on which human participants are providing labels for the AI component to learn from, the level of data quality may vary. For example, making use of crowdsourcing to collect human labels from people online either using paid micro-task platforms like Amazon Mechanical Turk or by means of alternative incentives like, e.g., ‘games with a purpose’ [37] is in general different from relying on a few experts. There is often a trade-off between the cost and the quality of the collected labels. On the one hand, it may be possible to collect few high-quality curated labels that have been generated by domain experts, while, on the other hand, it may be possible to collect very large amounts of human-generated labels that might be not 100% accurate. Since the number of available experts is usually limited, to obtain both high volume and quality labels, the development of effective quality control mechanisms for crowdsourcing is needed. Another challenge that comes with HAI systems is the bias that contributing humans may create and/or amplify in the annotated data and, consequently, in the models learned from this labelled data [16, 25]. Depending on the labelling task, bias and stereotypes of contributing individuals may be reflected into the generated labels. For example, an image labelling task that requires to identify the profession of people by looking at a picture, may lead to a female individual depicted in medical attire to be labelled as ‘nurse’ rather than as ‘doctor’. For such type of data collection exercises, it becomes important to measure and, if necessary, control the bias in the collected data so that the bias in the AI models trained with such data is managed and controlled as well, if not limited or avoided altogether. Possible ways to control such bias include working on human annotator selection strategies by, for example, including pre-filtering tasks to profile annotators and to then select a balanced set of human annotators to generate labels for an AI to learn from. Once manually labelled data has been collected, trained AI models may reflect existing bias in the data. An example of such a problem is that of ‘unknown unknowns’ (UU) [ 3 ], that is, data points for which a supervised model makes a high-confidence classification decision, which is however wrong. This means that the model is not aware of making mistakes. UUs are often difficult to identify because of the high-confidence of the model in its classification decision and may create critical issues in AI.2 The problem of UU is usually caused by having a part of the feature space being under-represented in the training data (e.g., training data skewed towards white male images may result into AI models that are not performing well on images of people from other ethnicities and of other genders). Thus, such AI models are biased because of the unbalanced training data they have been trained on. Possible ways to control for such bias include making use of appropriate data sampling strategies to ensure that training datasets are well balanced and cover well the feature space also for features that may not have been originally identified or used. 2A classic example of this is the Google gorilla mistake, see. 66 When incorporating humans into an HAI system, they become the efficiency bottleneck. While purely machine-based AI systems can learn from very many data points and, once trained, perform decisions in real-time, making use of a human component makes the system less scalable and less efficient. For this reason, it becomes important to decide how to best employ these less efficient and limited human resources and, instead, how to best leverage the scalability of machine-based methods in order to get the best out of the two worlds. The problem becomes even more complex when considering different types of human contributors which come with varying quality, availability, and cost levels. We discuss this in more depth in Section 6.2. Related to the previous problem, deciding what data points should be manually labelled by human annotators is another challenge. Given a usually very limited manual annotation budget, it becomes important to select the best data items to label in order to maximise their value with respect to the improvements of the trained AI models. Questions of this type are in particular relevant to systems relying on active learning strategies. Such improvements, however may relate not only to effectiveness, but also to other model properties like, for example, fairness. Another benefit of involving humans in HAI system is the ability to leverage their skills to improve the interpretability and explainability of AI models. Human contributors may be leveraged to, for example, add natural language explanations about why a certain supervised classifier decision has been made. Thus, in order to design and develop an high-quality HAI system, researchers have to look at a multi- dimensional problem which includes aspects like efficiency, accuracy, interpretability, explainability, and fairness. Human and machine components of an HAI system can contribute and possibly threaten each of these dimensions. Based on these issues, the overarching question in HAI systems is about deciding what should humans do and what should AIs do in order to optimally leverage the capabilities of both methodologies. In the remainder of this paper we discuss these challenges and opportunities in the context of fighting online misinformation. We use this problem as a showcase of HAI methods and discuss the potential of such methodology when applied to this context. 3 The Problem of Online Misinformation 3.1 An Interdisciplinary Challenge The spread of misinformation online is a threat to our safety online and risks to damage the democratic process. For instance, bots and trolls have been spreading disinformation across a number of significant scenarios, such as the election of US President Donald Trump in 2016 [ 5 ], the debate in the UK over Brexit [ 19 ], and, more recently, exaggerating the role of arson to undermine the link between bushfires in Australia and climate change.3 The World Health Organization (WHO) has referred to the problem of large amount of misinformation spreading during the COVID-19 pandemic as an “infodemic”4 [ 1]. Therefore, fact-checking information online is of great importance to avoid further costs to society. Because of the importance, impact, and interdisciplinarity of the issue, a number of different research areas have focused on understanding and stopping misinformation spreading online. This includes research in political sciences [ 22], communication science [ 40], computational social science [7 ], up to computer science including the fields of human-computer interaction [ 33], database [21], and information retrieval [28]. While different research methodologies are being applied, the overarching goal is to understand how misinformation is spreading, why people trust it, and how to design and test systems and processes to stop it. From a data engineering point of view, online misinformation poses some of the same common challenges observed in modern data management: i) volume: large amounts of data to be processed efficiently and in a scalable fashion; ii) velocity: processing data and making misinformation classification decisions in a timely 67 fashion also in conditions when data to be checked comes as a stream (e.g., Twitter propaganda bots generating and propagating misinformation in social networks; iii) variety: misinformation comes in multiple formats, from textual statements in news articles, to images used in social media advertising, to deep-fake videos artificially generated by AI models; iv) veracity: the core question of truthfulness classification often translates in deciding which data source can be trusted and which not. Thus, the data engineering community not being new to dealing with such challenges, can surely provide solutions, systems, and tools able to support the fight to online misinformation. We however still believe that this is an interdisciplinary challenge, and in the remainder of this paper we present a framework that goes beyond data engineering by including humans in the loop and by considering human factors as well. 3.2 Misinformation Tasks From the existing scientific literature about misinformation, we can see that there are a number of more specific tasks that need to be addressed to achieve the overarching goal of fighting online misinformation. The first task that comes to mind is truthfulness classification, that is, given a statement decide its truth level, in a scale from completely true to completely false. Fully automated approaches [ 23] as well as crowdsourcing-based approaches [28] have been proposed to address this task. However, other tasks related to online misinformation exist. For example, it is also important to decide about the check-worthiness of online content. As there are way too many statements and claims that could possibly be fact-checked, before expert fact checking can take place, a pre-processing filtering step needs to be completed to identify which statements should be going through a complete fact-checking process, out of a large collection of potential candidates. Criteria to be considered for such a selection process include: the potential harm that a certain statement being false could create, the reach of that statement, the importance and relevance of the topic addressed by the statement, etc. Automated methods for check-worthiness have been proposed in the literature [ 11], but are far from being effective enough to be deployed in practice and replace expert fact-checkers on this task.5 Another task related to misinformation is source identification. Being able to detect the origin of online information can provide additional evidence to information consumers about its level of trustworthiness. More than just either manual or automatic approaches to address these tasks, an additional way is to combine them together in order to optimize processes and leverage the best properties of each method. 4 Machine Learning for Fighting Online Misinformation For each of the misinformation tasks described in the previous section, there have been attempts to develop machine learning methods to tackle them. In this section we provide a summary of such research. For the problem of truthfulness classification, benchmarks on which to compare the effectiveness of different approaches have been developed. A popular benchmark for truthfulness classification is the LIAR dataset [39] that makes use of expert fact-checked statements from the PolitiFact website. More than 12K expert-labeled statements are used as ground truth to train and evaluate automatic classification systems effectiveness, so that system quality can be compared. Even larger than that is the FEVER dataset [ 34] that contains 180K statements obtained by altering sentences extracted from Wikipedia. Other earlier and smaller truthfulness classification benchmark datasets include [36, 14]. A lot of effort has been made within the AI research community not only to obtain accurate classification decision, but also to provide explainable results. Supervised methods for this task have looked at which features are the most indicative of truthfulness [27]. Recent approaches have designed neural networks that aim at combining evidence fragments together to inform the truthfulness classification decision [ 43]. Such evidence 68 can then be used to explain the automatic classification decisions. Other studies looking at the explainability dimension of this problem have observed that different features may be indicators for different types of fake news and can be used to cover different areas of the feature space [26]. Adversarial neural networks have shown to improve the effectiveness in identifying distinctive features for truthfulness classification [42]. Methods to automatically decide on check-worthiness [ 11] have looked at how to assign a score to a sentence and to predict the need for it to be checked by experts using supervised methods and training data. While some methods make use of contextual information, that is, of the surrounding text, to decide on the check-worthiness of a sentence [ 13], the most effective ones consider each sentence in isolation and use domain specific word embeddings within an LSTM network [17]. Metadata about information sources presented to social media users have an effect on the perceived truthfulness of the information [ 24]. Providing news source and contextual metadata may help users to make informed decisions [12]. Related to this, the New York Times R&D group has started a project to provide provenance metadata around news using blockchain technology to track the spread of news online and to provide contextual information to news readers.6 5 Crowdsourcing Truthfulness More than just machine learning-based methods, crowdsourcing can be used as a way to label data at scale. In the context of misinformation, crowdsourcing is a methodology that can provide, for example, truthfulness classification labels for statements to be fact-checked. While experts may not be directly replaced by crowd workers (see work by Bailey et al. [4]), by deploying appropriate quality control mechanisms, crowdsourcing can provide reliable labels [10]. In a recent research on crowdsourcing truthfulness classification decisions we have looked at how to scale the collections of manual labels and at the impact of the annotators’ background on the quality of the collected labels specifically looking for the impact of the annotator political bias with respect to the assessed statement and of the scale used to express the truthfulness judgment [ 28]. In another follow-up study, we have then looked at the impact of the timeliness of the assessed statements on the quality of the collected truthfulness labels. Results show that even more recent statements can still reliably be fact-checked by the crowd [29]. More in detail, we looked at how the crowd assessed the truthfulness of COVID-19 true and false statements during the pandemic, finding an agreement with expert judgments comparable to that in the previous study. Another common challenge for expert fact-checkers, due to the limited available resources, is deciding which items should be fact-checked among very many candidates. More than just leveraging crowdsourcing to decide on truthfulness, the crowd may also be able to support expert fact-checkers in performing the task of deciding about the ‘check-worthiness’ of content, that is, asking the crowd to decide whether or not a given piece of content would benefit from being fact-checked by experts. Several factors affect the decision of selecting a statement to undergo a fact-checking process. The crowd may be involved in validating these factors which include, for example, the level of public interest of the assessed content, the possible impact of such content not being true, and the timeliness of the content. In this way, it would be possible to manually filter more content for fact-checking (the effectiveness of fully automated check-worthiness approach is still very low [11]) thus allowing expert fact-checkers to focus on actual fact-checking rather than on filtering and deciding what needs to be fact-checked. 69 AI Tools Crowdsourced Workers Fact-Checking Experts - Machine Learning - Search and Recommendation - Natural Language Processing - Check-worthiness - Truthfulness Judgments - High-quality fact-checking - Guarantee of: nonpartisanship and fairness transparency on sources, funding, and methodology open and honest correction policy - News and Media - Social Media - Parliamentary sessions - ...Figure 1: Human-in-the-loop AI framework for fighting online misinformation. 6 A Hybrid Human-AI Framework for Fighting Online Misinformation 6.1 Combining Experts, AI, and Crowd Given the limitations of both automated and human-based methods for fact checking, we rather envision a hybrid human-AI approach to fight online misinformation. Such an approach has the benefit of leveraging the positive aspects of each of the different approaches, that is, the scalability of AI to efficiently process very large amounts of data, the ability of expert fact-checkers to correctly identify the truthfulness level of verified statements in a transparent and fair way, and the ability of crowdsourcing to manually process significantly large datasets. We are starting to see the appearance of hybrid approaches for fact-checking, like, for example, the work presented by Karagiannis et al. [21] . The proposed system is an example of how to efficiently use human fact checking resources by having a machine-based system supporting them to find the facts that need to be manually checked out of a large database of possible candidates. The combination of these methods may not only result in more efficient and effective fact-checking processes, but also lead to improved trust on the outcomes over purely AI-based methods and may also leverage the embedded human dimension to increase the level of transparency of the truthfulness labels attached to news (i.e., explaining why a certain piece of news has been labelled as fake, like fact-checkers do already, but something that AI-based methods still struggle to provide). Such an approach may also lead to resource optimization, where the more expensive and accurate expert fact checkers may be intelligently deployed only on the few most important and challenging verification tasks, while the crowd and AI can work together to scale-up the execution of very large amounts of fact-checking tasks. We thus envision a waterfall model where different levels of cost/quality trade-offs can be applied at different stages by means of appropriate task allocation models. 6.2 The Framework The existence of numerous challenges and constraints that need to be resolved concurrently leads us to the proposal of a solution that not only combines humans and machines, but that in doing so leverages different types and levels of engagement in the process of fighting misinformation. Our proposed framework consists of three main actors: fact-checking experts, AI methods, and crowdsourcing workers (see Figure 1). Fact-checking experts are the protagonists of the framework and are the ones who make use of the other two components to optimize the efficiency of the fact-checking process and maintain high-quality standards. Also, 70 Scale Cost Accuracy + + +- - - Explainability +- Crowdsourced Workers Fact-Checking ExpertsAI Tools Control of Bias +-Figure 2: Trade-offs between the actors of the framework. they are the only ones who can guarantee that this HAI system meets the three principles7 of (i) non-partisanship and fairness, (ii) transparency on sources, funding, and methodology; and (iii) open and honest correction policy. AI tools consist of automatic methods that fact-checkers can use to deal with the large amount of (mis)information produced through different channels such as news and media, parliamentary sessions, or social media [6]. Al- though AI tools are able to process data at scale, automatic predictions are typically not free from errors. For instance, machine learning methods used in systems such as ClaimBuster [18] or check-worthiness systems for the CheckThat! Lab at CLEF [11] are far from being 100% accurate. Moreover, it is not clear whether these tools would perform at the same level of accuracy in other scenarios, e.g., predicting check-worthiness of statements related to non-American politics. In summary, although state-of-the-art machine learning can compete—and even surpass—experts when data scale and costs are measured, as of today they are far from reaching human experts when considering the level of accuracy, explainability, and fairness. Crowd workers somehow lie in between experts and AI on all the five above mentioned dimensions (scale, cost, accuracy, explainability, and control of bias) and can be deployed on-demand based on changing requirements and trade-offs. Figure 2 summarizes the strong and weak points of the actors involved in the proposed framework. The proposed framework comes with several benefits: • Cost-quality trade-offs: it comes with the ability to trade-off and optimize between required cost and quality of the label collection process where human experts (i.e., fact checkers) come with the highest quality and cost and AI comes with the lowest cost; • Load management: it allows to deal with peaks of fact-checking tasks that may be otherwise impossible to deal with for expert fact-checkers working under constrained resource conditions. In such situations, they may be able to leverage the more scalable crowd and AI tools to deal with a sudden increase in annotation workload; • Trustworthiness: it can serve as a way to make AI technology accepted in well-established traditional journalistic environments that would not see positively an ‘AI taking over their job’. In such an intertwined framework, the key question becomes who should do what. Given a workload of misinformation tasks, a deadline, and required constraints like a minimum level of quality and a maximum 7https://ifcncodeofprinciples.poynter.org/know-more/the-commitments-of-the-code-of- principles 71 cost, the problem becomes to identify a task allocation solution that satisfies the constraints with maximum value. This can be addressed with a cascade model [ 35, 41, 38] with humans-in-the-loop, where AI tools, crowd workers and fact-checking experts cooperate to maximize value. For example, looking at the trade-off between urgency and quality, as soon as a statement is identified as requiring fact-check, an AI model can first be adopted to very efficiently provide a truthfulness label which could then possibly be replaced later on once a team of expert fact-checkers has concluded their forensic investigation of the available evidence in favour or against the statement being true. Such cascade of annotation tasks where many (or all) labels are quickly estimated automatically, only a small subset of those is sent to the crowd for a quick (but non-real time) validation of their truthfulness, and then only very few remaining statements are sent to experts to investigate in depth is the core idea of the proposed framework that leverages different levels of the size-quality-cost trade-offs that the different methodologies provide. One dimension that impacts task allocation decisions is the cost and scale of the annotation problem. In order to leverage the best of the automated and manual methods, AI and crowdsourcing can be used to scale up the annotation effort to very many statements thus being able to possibly provide truthfulness labels for every single statement being published online. Expert fact-checkers can then be parsimoniously deployed on statements that are either difficult to label by AI or crowdsourcing methods (e.g., selected by means of low algorithmic confidence or low annotator agreement within the crowd), or important to label accurately due to the possibly wide implications of the statement being false or due to the importance of the speaker who made the statement and its potential reach. Another open research question is on understanding how experts would actually work when embedded in this new framework: they would need to change consolidated and validated fact-checking processes and, instead, adapt to an environment in which their work is being complemented by AI and non-experts. This would necessarily require a certain level of trust in the HAI system that, on its side, is making decisions on what expert fact-checkers should do and on which statements they should work on. This translates into experts giving up a certain level of control on the process to the HAI system that has to decide what they do not get access to. For this to work, there needs to be a certain level of trust in the system that could possibly be achieved by the employment of self-explainable AI tools. This is also critical as as the fact-checking experts need at the end to be able to guarantee transparency on the process and methods used for fact-checking. 7 Take-Away Messages In this paper we discussed the problem of online misinformation and proposed a hybrid human-AI approach to address it. We proposed a framework that combines AI, crowdsourcing, and expert fact-checkers to produce annotations for statements by balancing annotation cost, quality, volume, and speed thus providing information consumers (e.g., social media users) with timely and accurate fact-checking results at scale. The proposed HAI approach aims at combining different methods to leverage the best properties of both AI and human-based annotation. Moreover, involving humans in the loop allows to better deal with the interdisciplinary nature of the misinformation problem by also providing human support on issues like explainability, trust, and bias. The model presented in this paper envisions a complex collaborative scheme between different humans and different AIs where the open research question moves to the optimization of these complementary resources and on how to decide which task should be allocated to which element of the HAI system. A human-in-the-loop solution to misinformation can also provide increased transparency on fact-checking processes leveraging together algorithms and AI and, in the end, provide more evidence and power to the end users to make informed decisions on which online information they should and which they should not trust.","Demartini, G., Mizzaro, S., & Spina, D. (2020). Human-in-the-loop Artificial Intelligence for Fighting Online Misinformation: Challenges and Opportunities. IEEE Data Eng. Bull., 43(3), 65-74."
ML_DB_089,Using NLP for Fact Checking: A Survey,"In recent years, disinformation and fake news” have been spreading throughout the internet at rates never seen before. This has created the need for fact-checking organizations, groups that seek out claims and comment on their veracity, to spawn worldwide to stem the tide of misinformation. However, even with the many human-powered fact-checking organizations that are currently in operation, disinformation continues to run rampant throughout the Web, and the existing organizations are unable to keep up. This paper discusses in detail recent advances in computer science to use natural language processing to automate fact-checking. It follows the entire process of automated fact-checking using natural language processing, from detecting claims to fact-checking to outputting results. In summary, automated fact-checking works well in some cases, though generalized fact-checking still needs improvement prior to widespread use.",Computer Science,Article,2021,N,Y,0,"Fact checking has been becoming more popular since the early 2000′s, however it has grown in popularity greatly in recent years. As of 2016, there were 113 active fact checking groups across the globe, 90 of which were established post 2010 [ 1]. With the rise of social media and fake news” spreading throughout the web [ 2], fast and accurate fact checking is now more imperative than ever. This is difficult, however, as the claims that need to be checked are written in human languages. Hence, by using Natural Language Processing (NLP), claims can be fact checked autonomously. Methods already exist to do this, and they are implemented in production and can receive good reviews from their users [ 3 ]. The current state of the art approach appears to be Claim Buster, as other papers compare themselves to this application in regard to performance. Claim Buster is available to use for free [4] and an open API is also available. However, a simple query for the web-based fact-checker of From 2016–2020 Donald Trump was President of the United States of America” returned no results that indicate that the query is true, most of the results being related to his age. This is an undesirable result for a simple query. This paper examines how NLP is used to fact check claims in written text, from the start of defining claims and determining those worthy of verification, to how parsed claims are verified against facts that are known to be true. 2. Review Studies on Using NLP for Fact Checking This section is subdivided into several sections. They are related to background information, claim identification and extraction, defining sources of evidence, and various processes of claim verification itself. 2.1. Background As mentioned above, fact checking has entered the foreground of public thought in recent years. To be clear, fact checking directly engages in adjudicating factual disputes by Designs 2021, 5, 42. Designs 2021, 5, 42 2 of 22 publicly deciding whose claim is correct or incorrect” [5]. This has become more important in recent years, as false information has become easier to spread and false statements and fake news” can be actively created to advertise [ 2 ] or cause dissent. Currently, there are approximately 160 fact checking organizations globally [3], and several online based fact checking applications, however false information continues to spread on the internet. Of the fact checking organizations, many launched in recent years. Between the years 2014 and 2015 alone over 50 were launched [ 1]. However, these groups are not always able to respond in time to new false stories that are spreading on social media, so there is a need to automate the process. NLP Methods Possibly the simplest and most common method used in NLP tasks is to use n-grams. N-grams are used in statistical methods and are pairings of n words. Bigrams (pairs of 2 words) and trigrams (sets of 3 words) are used most frequently. In a similar vein, entire sentences and parsed sentences can be grouped like this, as well. Neural Networks are also popular with NLP tasks. These are black box methods that require extensive training data to effectively work on real data. Layers of simple perceptrons that take in a 0 or a 1 and output the same are created with connections between each perceptron in adjacent layers. Weights between the perceptrons in each layer are learned, then over time the entire system learns to classify input. NLP has seen the advent of bi-directional long short-term memory neural networks in recent years. These networks take the input normally and reversed and have the ability to use information from previous inputs when generating their output. Broadly speaking, NLP methods work on matrices and vectors. Text is tokenized where each word is a token and, with this, sentences, phrases, and other groupings of words are converted into vectors. Combining the vectors allows for the creation of matrices. Combining vectors creates two dimensional matrices that represent token occurrences over multiple word groupings and facilitates the comparison of one word grouping to another. 2.2. Claim Identification and Extraction Some methods of automated fact checking assume that a claim is given as input, however in the context of fully automated fact checking this will not be the case. The three sections below discuss using machine learning to search for check-worthy claims in text and extracting meaningful data from them, then converting them into machine-readable formats, as well as a mathematical method that achieves both steps when working with statistical properties. 2.2.1. Searching for Claims This first step is not always implemented in autonomous fact checkers. For example, researchers in [6 ] assume that their model is provided claims to check, feeding them directly into a long short-term memory (LSTM) neural network. Claims are the first input to their system. However, this cannot always be the case, as some autonomous fact checking applications are designed to work on articles or on comment sections of social media, so the first step must be to determine what is and is not a claim, and then whether or not that claim is worthy of being fact checked. Inconveniently, even though research has gone into claim detection, there is no formal definition of what a claim is yet. Instead, the definition of a claim is defined by researchers on a per-project and organization basis [3 ]. This causes it to be difficult to replicate the work of other researchers. Full Fact is one fact checking organization that collaborated with researchers to work on a claim detecting algorithm. As of the 2015 UK election, their definition of a claim was an assertion about the world that can be checked” [ 3]. This definition is too broad when developing an autonomous fact checker, so the researchers defined a typology that would Designs 2021, 5, 42 3 of 22 capture the different types of claims. There are seven major categories of claims, each with several subcategories in their model. Annotated claims based on the classification system defined in Table 1 were provided by volunteers that were keen on fact checking [ 3]. These categories reduce into binary Claim/Not A Claim to ease and speed up initial classification, as seen in Table 2. Table 1. Claim typology with examples adopted from [3]. Main Category Subcategory Example Not a claim Give it all to them, I really don’t mind.” Other Other Molly gives so much of who she is away throughout the film.” Support/policy He has advocated for a junk food tax.” Quote The Brexit secretary said he would guarantee free movement of bankers.” Trivial claim It was a close call.” Voting record She just lost a parliamentary vote.” Public opinion A poll showed that most people who voted Brexit were concerned with immigration.” Definition The unemployed are only those actively looking for work.” Quantity Current Value 1 in 4 people wait longer than 6 weeks to see a doctor.” Changing quantity Comparison Ranking Prediction Hypothetical statements The IFS says that school funding will have fallen by 5% by 2019.”Claims about the future Personal experience Uncheckable I can’t save for a deposit” Correlation/causation Correlation Tetanus vaccine causes infertility”Causation Absence of a link Laws/rules Public Institutional procedures The UK allows a single adult to care for fewer children than other European countries.”Rules/rule changes Table 2. Reducing 7 categories into 2 classes. Claim Not a Claim Quantity Personal experience Correlation/causation Other Laws/rules Not a claim Prediction Researchers used sentence embeddings captured from InferSent, which differ from normal sentence embeddings as they factor word order [3]. Part of speech (POS) and named entities were retained in each sentence embedding, represented as the count of each in the corpus. Several classifiers were tested, all with default settings in Python’s scikit-learn library. Logistic regression performed the best, however POS and named entity features did not make a difference in performance. The F1 score using only logistic regression was 0.83 [3]. Similar work was done by researchers in [7], who developed Claim Buster. The dataset was created using sentences from U.S. general election presidential debates, with three categories. These categories were: Non-Factual Sentences (NFS), Unimportant Factual Designs 2021, 5, 42 4 of 22 Sentences (UFS), and Check-worthy Factual Sentences (CFS). Examples of each sentence type are available in Table 3. Table 3. Claim categories and examples adopted from [7]. Sentence Category Example NFS But I think it’s time to talk about the future. You remember the last time you said that? UFS Next Tuesday is Election day. Two days ago we ate lunch at a restaurant. CFS He voted against the first Gulf War. Over a million and a quarter Americans are HIV-positive. The training dataset contained sentences from debates between U.S. presidential candidates prior to a general election, starting from the 1960 debate. The sentences were labelled by humans to fit into one of the three categories. Several features were also extracted to the sentences and added to the data, such as sentiment scores (positivity vs. negativity in a sentence), sentence word count, word frequency, POS tags for words, and named entity types. Using a random forest classifier to determine the most important features, the top 30 were selected for use in training a model [7]. Several supervised learning methods were tested; however, a support vector machine (SVM) had the best performance overall. On the CFS class alone, the SVM correctly classified the sentence 72% of the time. 2.2.2. Claim Extraction The above section explains how claims are found, but not how they can be parsed to be comparable to whatever the researchers consider a valid source of truth is. Popular reductions are to convert claims into triplets such as {London, capital of, UK} or sets of short sentences as these tend to represent the claims themselves better [ 8 ]. In the case of the latter, the claims themselves can be used as input moving forward, assuming that they are short enough. However, converting the detected claims to triplets requires some extra work. The basic form of a triplet is: {subject, predicate, object}. Seemingly, the most efficient triplet extracting algorithms require that sentences be parsed into tree structures, where POS tags are applied to words and syntactic structure is added. Sentences are represented by tree parsers as a root with three children: a noun phrase (NP), a verbal phrase (VP) and the full stop (.). The root of the tree is denoted as S [9]. To extract this information, researchers in [ 9] utilized existing parsers, namely the Stanford Parser and OpenNLP. The algorithm searches for the first noun in the NP and assigns that as the main noun, then it associates attributes to it based on available adjectives and related subtrees and uses this as the subject. Next, it searches for the VP predicate by finding the deepest verb in the VP subtree, again associating adjectives and related subtrees as attributes. This becomes the predicate in the triplet. Finally, the object is defined by searching by looking for NP, participle phrases (PP), and adjective phrases (ADJP) that are siblings to the VP. Attributes are extracted and applied here, as well. With the above algorithm, the sentence A rare black squirrel has become a regular visitor to a suburban garden” results in the triplet {squirrel, become, visitor} with attributes found in Figure 1. Designs 2021, 5, 42 5 of 22Designs 2021, 5, x 5 of 23 Figure 1. Triplet conversion of the sentence A rare black squirrel has become a regular visitor to a suburban garden”. 2.2.3. Mathematical Method for Statistical Properties Researchers in [10] developed a method to identify and extract simple claims about statistical properties, such as the population of a nation. The claim identification portion relies on discovering textual patterns between an entity and a numerical value used to express the property of interest [10]. When attempting to extract information about the population of a nation, several different wordings are valid and can be seen in the left column of Table 4. If these were examples from textual sources, the values from those sources appear in the right column of Table 4. Table 4. Various phrasings and example data for the population of several nations. Pattern Data The population of X is y France: 66,000,000 Russia: 140,000,000 Iceland: 325,000 X’s population is estimated at y France: 66,030,000 Russia: 140,000,000 X has y inhabitants Russia: 140,000,000 Iceland: 300,000 To train their mathematical method, a set of entity-value pairs are required that act as a knowledge base (KB) containing the actual values of the properties, in this case pop- ulation. At the time of their experimenting, these values were: France: 66,028,467, Russia: Figure 1. Triplet conversion of the sentence A rare black squirrel has become a regular visitor to a suburban garden”. 2.2.3. Mathematical Method for Statistical Properties Researchers in [10 ] developed a method to identify and extract simple claims about statistical properties, such as the population of a nation. The claim identification portion relies on discovering textual patterns between an entity and a numerical value used to express the property of interest [10 ]. When attempting to extract information about the population of a nation, several different wordings are valid and can be seen in the left column of Table 4. If these were examples from textual sources, the values from those sources appear in the right column of Table 4. Table 4. Various phrasings and example data for the population of several nations. Pattern Data The population of X is y France: 66,000,000 Russia: 140,000,000 Iceland: 325,000 X’s population is estimated at y France: 66,030,000 Russia: 140,000,000 X has y inhabitants Russia: 140,000,000 Iceland: 300,000 To train their mathematical method, a set of entity-value pairs are required that act as a knowledge base (KB) containing the actual values of the properties, in this case Designs 2021, 5, 42 6 of 22 population. At the time of their experimenting, these values were: France: 66,028,467, Russia: 143,700,000, Iceland: 325,600. A set of textual patterns with their respective values are also required, this is the data in Table 4 as well as others, such as the pattern X’s inflation rate is y” with data France: 0.9, Iceland: 4.0” [ 10]. Because the values can be estimated in the text, the method aims to find patterns that predict the values well rather than finding patterns that state their exact values. The algorithm ranks all text patterns according to how well they predict the values for each property, then greedily selects those patterns until the aggregate predictions by the patterns stop improving. To rank the patterns for various properties, the predicted entity-value (EV′) pairs are compared against the actual entity-value (EV) pairs using the mean absolute percentage error (MAPE) as seen in the following equation: MAPE(EV, EV′) = 1 |E| ∑ e∈E |ve − v′ e| |ve| (1) where E and E′ refer to the entity sets in EV and EV′ respectively. The v and v′ refer to the values for each entity, e. This equation only utilizes the values for entities in both E and E′, so when calculating with the phrase X has y inhabitants” from Table 4 only the values for Russia and Iceland are used. In order to alleviate an issue where the MAPE equation would improperly score a wrong pattern highly, such as X has had y tourists this year” with France: 66,000,000, the MAPE equation is adjusted to take into account the number of values used in the calculation so that the end user or designer of a system can adjust how important this factor is. This adjusted MAPE equation is seen the following equation: adjustedMAPE(EV, EV′) = c c + N MAPE(EV, EV′) (2) where N is the number of values used in calculating the original MAPE and c is the adjustable parameter. Setting c to a lower value puts higher importance on the number of values, making the eventual algorithm seen in Figure 2 more reliable [10]. The algorithm that uses the MAPE equation can be seen in Figure 2. To start, it makes a best guess at the value given the KB and the textual pattern. This guess is the mean of the training data, their median, or 0, whichever produces a lower MAPE score. Next, all patterns are ranked based on their MAPE score for the property in question, and this ranking is iterated over. During each iteration, the MAPE score is calculated for each pattern combined with the entity in question and a list is populated of patterns that predict the value well. As soon as the MAPE value is increased with a possible pattern, the algorithm exits, and the selected list of patterns is returned [10]. Pattern generation for this method can be accomplished as a batch process. Using Freebase, now known as WikiData, as a KB, queries can be generated for statistical prop- erties that exist for many entities, and then these queries can be sent to search engines. The top results can be downloaded and parsed with an NLP toolkit and patterns can be extracted from them. This method was used by researchers in [ 10 ] when developing this model. With their testing, this algorithm achieved an overall precision score of 0.6, with strong variation across different properties [10]. 2.3. Sources of Evidence Various research projects into automated fact checking have used several different sources of evidence to compare claims to. This section enumerates these methodologies (listed below) as well as explaining how they have been curated by various researchers. The types of evidence discussed here are fact databases, the internet, other external sources, as well as the originator of the claim and the language that they use. Each source receives its own section below. Designs 2021, 5, 42 7 of 22Designs 2021, 5, x 7 of 23 Figure 2. Claim identification algorithm adopted from [10]. 2.3. Sources of Evidence Various research projects into automated fact checking have used several different sources of evidence to compare claims to. This section enumerates these methodologies (listed below) as well as explaining how they have been curated by various researchers. The types of evidence discussed here are fact databases, the internet, other external Figure 2. Claim identification algorithm adopted from [10]. 2.3.1. Fact Databases Fact databases store pre-checked claims or world knowledge and can sometimes be augmented by claims from trustworthy sources. The stored facts can be stored in numerous methods, from triplet representations to unaugmented, previously fact checked claims. Designs 2021, 5, 42 8 of 22 Triplet representations in databases have been used to perform fact checking on simple, numerical, claims by researchers in [10 , 11]. Researchers in [12] generated their machine learning dataset by downloading the subset of the Freebase database that relates to the statistical_region” entity type. Next, for each region they formed a web query and downloaded the top 50 results, then extracted statistical triplets from the pages that relate to the region currently being processed. Researchers in [11 ] extended the researchers of [12] by including temporal tags to factor the changing of statistics with time. Previously fact checked claims are another way that knowledge can be stored. This is the case for Claim Buster, where part of their fact checking process involves searching for closely related or identical claims that have already been fact checked by professionals [7 ]. These were curated from various fact checking websites, such as PolitiFact and AZCentral. Researchers in [8] developed a dataset similar to this named FEVER, based on the in- troduction sections of Wikipedia pages. Their dataset was generated by human annotators who, given a sentence from Wikipedia, were asked to generate a set of claims containing a single piece of information, focusing on the entity that the page is about. They also generated mutations of those claims, the types of mutations being: paraphrasing, negation, entity substitution, and making the claim more general or specific [ 8]. This dataset is labeled for machine learning applications. Each claim is labeled as supported, refuted, or not-enough-info. The first two labels are in relation to any Wikipedia page an annotator could find. Text, such as encyclopedia articles, verified news, and scientific papers can also be utilized in the fact checking process and be stored in a fact database. For example, researchers in [ 13 ] developed a dataset containing articles and journals. They then proceed to only utilize the headlines in their task of rumor debunking. Entire corpora are available, as well. In the first Fake News Challenge, researchers in [12] processed entire articles in their task of stance detection. Stance detection, in this case, being the process of determining how much an article agrees with its headline. 2.3.2. The Internet Like searching entire corpora for information, the internet itself can be used as a source of evidence when fact checking claims. Claim Buster utilizes web results to augment its fact checking when its existing claim database does not contain a similar enough claim. To implement this, a question generation tool is used, then valid questions are sent to the question answering engine Wolfram Alpha. Simultaneously, the questions are sent to Google and the snippet result is used. Google is also sent the claim itself, and the resulting pages are searched for similar claims. This information is sent directly to the end user [7]. 2.3.3. Other External Sources It is possible to achieve fact checking relying only on external sources, specifically just the snippets returned from Google and Bing searches. With the methods implemented by researchers in [6], parsing web pages yielded no better results than parsing only the snippet returned by searching a shortened version of the claim. Wikidata is an online KB with a public API that allows users to query for information. Since it launched, it became one of the most active Wikimedia projects. As with all Wikimedia projects, data is updated and managed by the community. This is crowd sourcing data acquisition. Data is stored in property-value pairs, each property relating to an entire Wikidata page containing labels, aliases, and descriptions [14]. 2.3.4. Originator and Language While not evidence towards truthfulness, the originator of a claim and the language that is used in it can be used to augment the process by which a claim is determined to be true or false. Including the metadata of speaker’s truthfulness history along with the claim itself has been shown to increase the accuracy of convolutional neural networks in spotting false statements [15]. The language of the claim itself is also useful in predicting veracity. Designs 2021, 5, 42 9 of 22 Trusted sources are more likely to use assertive words and less likely to use hedging words than untrusted sources. They are also more precise when describing events [16]. 2.4. Claim Verification The majority of methods used for automated fact checking are based on supervised machine learning [ 8]. This chapter is broken down into sections that each describes a different method of determining whether or not a claim is likely to be true or false. The methods explored are the language of the claim, comparing the claim to fact databases, comparing the claim to pre-checked claims, and comparing the claim to external sources. 2.4.1. Language Analysis As described above, the language of a claim can indicate its veracity. Analysis indicates that false claims often arise from differences in phrasing rather than outright fabrications, so language itself is useful to analyze. For example, subjective words can be used to dramatize or sensationalize a story [ 16]. First and second person pronouns are used more often in less reliable news sources, as news writers generally try to appear indifferent. Words used to exaggerate are also used more by fake news”, whereas concrete figures are found more often in truthful news [ 16 ]. This information has been used to develop several models that take a claim and term frequency-inverse document frequency (TF-IDF) data as input and predicts the PolitiFact rating, a scale consisting of completely false, false, mostly false, half true, mostly true, and true. A similar set of models were designed that only outputs a true or false verdict on a claim. These models were trained on PolitiFact and similar websites. The models in question are an LSTM, Naïve Bayes, and Maximum Entropy. Results of the testing by researchers in [ 16] is seen below in Table 5. These models are not reliable when dealing with PolitiFact’s 6 classes of truthfulness, however when simply determining if a statement is likely true or false the models perform better. Table 5. F1 scores of various models by [16] when classifying claims based on their language. True/False 6 Classes of Truthfulness Text Text + TF-IDF Text Text + IDF Naïve Bayes 0.44 0.58 0.16 0.21 Maximum Entropy 0.55 0.58 0.20 0.21 LSTM 0.58 0.57 0.21 0.22 Researchers in [ 17 ] take this further by including references to claims and how claims are cited. For example, if one source cites another and the second source is truthful, the citing source’s likelihood of being valid is increased. The inverse also applies, as false stories will propagate through related sources whereas true ones will likely do the same. Their method utilizes a triplet format for claims and outputs believability scores. Specifi- cally, these are subject-predicate-object (SPO) triplets, where the predicate in the triplet is a verb or verb phrase. Synonyms to these triplets are used going forward, using alternate wordings of the same statement. These are referred to as fact candidates. These candidates are assigned a believability score, which is a combination of its objectivity score and its co-mention score. The objectivity score is defined as: O( fi ) = log|Di |· ∑dk eDi O(dk) |Di | (3) where fi is the fact candidate, Di is a set of documents that the candidate is mentioned in, and each document where d e Di has objectivity O(d). The document objectivity score O(d) is calculated using a logistic regression classifier, and it can only be between 0 and 1, where 0 shows that the source is subjective [17]. Taking the logarithm of the absolute value of Di ensures that many mentions of a candidate in low objectivity sources do not end up Designs 2021, 5, 42 11 of 22 exact equivalence to completely unrelated, so these researchers grade similarity on a scale from 0 to 5, with 5 being the most similar. This is seen in Table 6.Designs 2021, 5, x 11 of 23 Figure 3. Pseudocode adopted from [17] for determining believability of a claim based on its lan- guage and its existence in other sources. 2.4.2. Comparing to Fact Databases Working with their FEVER dataset, researchers in [8] developed a system to perform fact checking based on it. This system consists of three components: document retrieval, sentence-level evidence selection, and textual entailment. Document retrieval was imple- mented by the existing DrQA system, a k-Nearest-Neighbor algorithm based on unigram and bigram TF-IDF vectors. Sentence selection was implemented using a ranking system relying on TF-IDF vectors, as well. Textual entailment was implemented using an existing state of the art system that had open-source code and did not require input text be syntactically parsed. As introduced above, researchers in [8] use a KB to check numerical claims. This knowledge base consists of a set of un-normalized tables, translated into Entry-Predicate- Value triplets. The process of fact checking is broken down into three steps: linking named entities in the claim to entities in the KB, filtering entries with the same entities using re- lation matching, and then determining if the claim is relevant. Finally, if the claim is rele- vant, the triplet from the KB is compared to the claim to determine truthfulness. 2.4.3. Comparing to Pre-Checked Claims A KNN algorithm can be used to assess semantic similarity between statements, as well [18]. This was implemented, not for fact checking, but simply for determining seman- tic textual similarity (STS) by researchers in [19]. Textual similarity can range from exact equivalence to completely unrelated, so these researchers grade similarity on a scale from 0 to 5, with 5 being the most similar. This is seen in Table 6. Two scores were generated during their research with two separate methods. The first, as with Claim Buster, is generated merely by comparing token similarity. The second involves generating XML for each sentence containing named entities, entity classifications, and dates. The similarity score was then calc ulated over several dimensions of the data. General similarity was generated via the cosine similarity of the TF-IDF of the tokens from all fields [19]. TF-IDF is a numerical statistic intended to reflect how important a word is to Figure 3. Pseudocode adopted from [17] for determining believability of a claim based on its language and its existence in other sources. Table 6. Semantic similarity ratings for sentences adopted from [19]. Similarity Rating Sentences 5 The bird is bathing in the sink. Birdie is washing itself in the water basin. 4 In May 2010, the troops attempted to invade Kabul. The US army invaded Kabul on 7 May last year, 2010. 3 John said he is considered a witness but not a suspect. I am not a suspect anymore,” John said. 2 They flew out of the nest in groups. They flew into the nest together 1 The woman is playing the violin. The young lady enjoys listening to the guitar. 0 John went horseback riding at dawn with a whole group of friends. Sunrise at dawn is a magnificent view to take in if you wake up early enough for it. Two scores were generated during their research with two separate methods. The first, as with Claim Buster, is generated merely by comparing token similarity. The second involves generating XML for each sentence containing named entities, entity classifications, and dates. The similarity score was then calculated over several dimensions of the data. General similarity was generated via the cosine similarity of the TF-IDF of the tokens from all fields [19]. TF-IDF is a numerical statistic intended to reflect how important a word is to a document or a corpus [ 20]. Author similarity was the cosine similarity of the TF-IDF vectors for the authors. Involved persons, time period, and physical location similarity were calculated with the cosine similarity of TF-IDF vectors of those fields. Event similarity Designs 2021, 5, 42 12 of 22 was the cosine similarity of TF-IDF vectors of all verbs. Subject and description were the cosine similarity of TF-IDF vectors of their respective fields. In the TF-IDF vectors, the IDF values were calculated from a subset of the overall data used for the entire project [19]. There are downsides to comparing input claims to previously checked ones. To start, previously checked claims must already exist and be similar enough to be detected. This involves storing checked claims and having a suitable algorithm for searching through them. It also requires that the checked claims already exist, generally having been checked by a human. This leaves the issue of not being able to keep up with quickly generated false statements on social media and requiring humans to perform the initial fact checking. 2.4.4. Comparing to External Sources NLP based fact checking can be achieved using only external sources, without existing fact databases, as demonstrated by researchers in [ 6 ]. Their system takes a claim and searches for support information on the Web. The steps involved in this are: external support retrieval, text representation, and veracity prediction. External support consists of generating a query from the input claim and sending it to a search engine. This query is a shortened version of the input claim, as claims tend to be two sentences long on average [6]. To accomplish this, words are ranked using TF-IDF. The IDF values were computed based on a 2015 Wikipedia dump and the English Gigaword. Only the verbs, nouns including named entities, and adjectives are considered. The generated queries are 5–10 tokens long and are sent to the Google and Bing search engines. Examples of these generated queries are visible in Table 7. Finally, the snippets at the top of the results and the URLs from reliable domains are ext","Lazarski, E., Al-Khassaweneh, M., & Howard, C. (2021). Using NLP for Fact Checking: A Survey. Designs, 5(3), 42."
ML_DB_095,"Examining the roles of automation, crowds and professionals towards sustainable fact-checking","This study explores an online fact-checking community called political fact-checking on reddit.com that relies on crowdsourcing to find and verify check-worthy facts relating to U.S. politics. The community embodies a network journalism model in which the process of finding and verifying check-worthy facts through crowd sourcing is coordinated by a team of moderators. Applying the concepts of connective journalism, this study analyzed the posts (N = 543) and comments (N = 10, 221) on the community’s Reddit page to understand differences in roles of the community members and the moderators. A mixed-method approach was used to analyze the data. The authors also developed an automated argument classification model to analyze the contents and identify ways to automate parts of the process. The findings suggest that a model consisting of crowds, professionals, and computer-assisted analysis could increase efficiency and decrease costs in news organizations that involve fact-checking.",Computer Science,Proceeding,2019,N,Y,8,"Many professional fact-checkers are suspicious of the idea of crowd- sourced fact-checking in which users verify factual claims [ 7 ]. Pro- fessionals often claim that users lack required skills, and are biased. We argue that contribution of crowds to fact-checking is essential in the networked media ecosystem where information is abundant This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. WWW ’19 Companion, May 13–17, 2019, San Francisco, CA, USA © 2019 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License. ACM ISBN 978-1-4503-6675-5/19/05. and rumors spread like wildfire with resources for investigative journalism steadily plummeting [6, 14 ]. A sustainable model for fact-checking platforms would consist of crowds, professional fact- checkers, and automated assisting tools. Crowds can perform many mundane but important tasks under the guidance of profession- als while programmers build tools to find credible sources and make sense of a large amount of user inputs. Crowds can also help better identify facts that people care about and identify sources that may lack credibility but still are popular. Professionals can play the ‘moderator’ and ‘seminar leader’ role in this process [ 16]. The purpose of this study is to explore such a crowdsourced fact- checking model where users check facts under the guidance of moderators. This study adds ‘automation’ to that model and identi- fies roles of each component of this model. It examines posts and comments in a crowd curated political fact-checking platform of reddit.com/politicalfactchecking to find answers to the following questions- • What claims do people want to see fact-checked? • What role does crowd play in fact-checking a claim? • What roles do moderators play in crowdsourced fact-checking? • How can computation play a role in scaling-up crowdsourced fact-checking? To understand roles of the crowd in finding factual claims, we ana- lyzed the posts in reddit/politicalfactcheck and identified popular topics that drew more user participation (e.g., number of comments), original sources of claims (e.g., mainstream media, alternative me- dia, blogs), and type of claims (e.g., stat, figure). To understand the roles of crowd in fact-checking, we analyzed the comments and identified types of arguments users provide, originality of sources that users present to support or reject claims, and relation between crowd actions and professional fact-checking activities. To iden- tify roles of moderators, we conducted a qualitative analysis on all posts and comments of moderators. We’ve also developed an automated argument classification model and identified some steps in fact-checking, which could be automated. The findings suggest Reddit’s model can lay a foundation for building a sustainable model for fact-checking organizations that lack resources. Roles of the crowd and automated assisting tools in fact-checking can increase efficiency and decrease expense and thus will enhance sustainability. Models solely reliant on professionals1001 are not sustainable given the amount of resource they require to survive. 2 RELATED WORK Professionals and scholars described fact-checking as a complicated process consisting of various steps that can be grouped into three major categories–(i) selection of facts to check, (ii) collection of evidence, and (iii) decision [3, 5 , 7]. Each of these steps comprises of multiple sub-steps. For instance, fact selection includes choosing claims from countless public utterances”, separating facts from opinion, separating newsworthy/check-worthy facts from all facts, and filtering verifiable facts [ 7]. In the literature on news media, fact-checking is narrowly defined and refers to as verification[5]. It is believed to be a job of trained professionals. Graves noted that professional fact-checkers practice a type of intertextual and annotative journalism” that lies within the framework of a larger political critique” [7]. Fact-checking sites run by professionals op- erate very self-consciously as hybrids of old and new media practice, organizing veteran print and broadcast reporters around a modern- izing genre meant to update political journalism” [7]. With the advent of digital technologies, research on fact-checking started to broaden its horizon. Mark Little [ 15 ] asked journalists to get comfortable with risk, transparency and collaboration and find the wisdom in the crowd”. Cohen et al. [ 2] envisioned a sys- tem called a cloud for the crowd, which combines computational resources as well as human expertise to support more efficient and effective investigative journalism. In a recent white paper 1, Mevan et al. presented the state of automated fact-checking and described how fact-checking can be scaled up dramatically using existing technologies. Hassan et al. suggested a Holy Grail towards computational fact-checking by depicting the roles of professional fact checkers and automation [8 ]. They also studied how auto- mated claim detection techniques perform against professional fact-checkers [11 ]. A number of tools have recently been proposed to fully automate fact-checking using natural language processing, machine learning, knowledge graph query, and question-answering techniques [ 9 , 10 , 12 , 20 ]. However, there is a lack of research on studying the active role of crowd in fact-checking. According to the best of our knowledge, this is the first work which systematically studies fact-checking in a crowdsourced environment. 3 METHODOLOGY 3.1 Dataset In this work, we study the crowd curated online political fact- checking platform reddit/politicalfactchecking 2. By and large, the platform functions in the following way– i) a user creates a post to submit a claim, ii) the community presents evidence and argu- ments related to the claim using the platform’s comment and reply features, iii) a group of moderators decides the appropriate flair for the claim based on the accumulated evidence and provides a justification. Note, both the post creator and the moderator group are members of the community and are eligible to participate in the second phase. A claim’s flair can be modified later in case there is new evidence. Below, we provide a short description of each of the Table 1: Distribution of the flairs Flair # Posts Avg. # Comments+Replies per Post Confirmed 32 20 Mostly True 21 23 Half-True 18 27 Mostly False 13 18 False 70 23 Unverifiable 39 27 Partisan Bias 16 18 Please Verify 111 18 Mod Post/Meta 44 43 No Flair 179 13 flairs. Detailed explanations are available in the platform’s website. We say a claim is fact-checked if the corresponding post is assigned any of the flairs except Please Verify and Mod Post/Meta. Confirmed: This post has enough evidence to support the claim. Mostly True: The claim is accurate but needs clarification/context. Half True: The claim is not entirely accurate, leaves out important information, or is out of context. Mostly False: The claim contains some elements of truth but ig- nores critical facts that would change the reader’s impression. False: The claim is blatantly false. Partisan Bias: This claim contains obvious political bias intended to edify one party or make another party look bad. Unverifiable: This claim contains more opinion than fact. It can easily become a debate. This tag also covers doublespeak, instances of incorrect terminology, or other miscellaneous claims that cannot be proved true or false. Please Verify: This post needs to be researched more in order to determine the validity of the claim. Mod Post/Meta: This is a post by a moderator or a discussion about improving reddit/politicalfactchecking. Using PRAW (Python Reddit API Wrapper) 3, a Python package which allows access to reddit’s official API, we scraped all (every- thing before July, 2017) the posts, comments and their metadata (timestamp, author). In total, there are 543 posts, 2, 835 comments, and 7, 386 replies to the comments. Table 1 shows number of posts and the average number of comments and replies per post for each flair category. There are 209 posts which have been fact-checked, 111 posts in Please Verify category, 179 posts which didn’t receive any flair, and 44 posts in Mod Post/Meta category. Figure 1 shows number of posts created in each quarter since the platform’s creation in September 2012 till July, 2017. It appears that the platform was more active at the initial stage. There were 10, 083 members, 2, 019 unique commentators, and 5 moderators in the community. About 80% of the comments and replies were created by 28% of the commentators. The average lengths of comments and replies are 477 and 331, respectively. 3.2 Coding and Analysis We manually coded the posts and comments to answer research questions. The posts were coded for five variables: post topic, post Figure 1: Number of posts per quarter type, source of claim, nature of the claim, and fact-checked entity. The comments were coded for three variables: commenter action, argument type, and source of evidence. We used a combination of analytical approaches to identify these variables and categories, as we did not find any study that analyzed Reddit posts for the purpose of understanding roles of crowds in fact-checking. We used a combination of deductive and inductive approaches [18 ]. We initially applied a deductive approach to identify the variables and the categories through a review of professional and scholarly literature on media content and typologies [13 , 21 ]. Then, we used an inductive approach to refine those categories to fit the purpose of this study. The authors have had several training sessions to discuss and finalize the coding categories. One coder coded most of the posts while the comments were coded by four coders, three of whom were journalists with over 30 years of experience combined. Inter-coder reliability (Krippendorff’s α)[4 ] ranged between 0.92 and 0.96 for comments, with an average of 0.94. 3.2.1 Posts. The first variable, post topic, included 12 categories: Economy, Education, Election, Environment, Foreign Affairs, Health, Immigration, Equality, Media Bias, Religion, Security, and Other. The second variable, post type, included three categories: Fact-check Request, Seeking Information, and Judgement/Opinion. We identify six categories for source of claim: News/Information Media, Organi- zation, Alternative Media–websites where majority of contents is created by contributors, User Generated Content (UGC) Platforms (e.g., social media), Personal Contact, and Others. The fourth variable, nature of the claim, was coded for four categories: Non-statistical (e.g., quotes), Statistical, Media (e.g., photo or video), and Other. The fifth variable, fact-checked entity, included Person, Organization, Event, Policy/Issue, and Other. Note that a post may be coded with multiple categories for a variable. 3.2.2 Comments. The same categories used for the variable post type and source of claim in posts were used for the variable argument type and source of evidence, respectively. The commenter action variable was coded for six categories: Providing Argument, Seek Clarification, Check Verifiability, Contact Claim Source, Assign Flair, and Post Irrelevant. 3.2.3 Qualitative Analysis. To understand the roles of the modera- tors, we qualitatively analyzed the posts and comments published by the moderators. We applied an approach developed by Altheide et al.[1], which is widely used in various fields. 4 RESULTS AND DISCUSSION Following the above explained methods, we analyze the political- factchecking dataset, present the results and discuss the findings in this section. 4.1 Role of Crowd in Finding Claims There are 499 posts in the dataset published by the crowd. Table 2 shows topic distribution of the claims. 70% of all the claims be- long to the top 4 topics– Economy, Election, Security, Health. For each topic, we identified the posts which have been fact-checked (Confirmed, Mostly True, Half True, Mostly False, False, Unverifiable, Partisan Bias) and which have not been fact-checked (Please Verify, No Flair). Table 2 shows number of fact-checked posts, number of not fact-checked posts, and the percentage of fact-checked posts for each topic. Among the topics having at least 10 posts, Health, Foreign Affairs, and Economy related posts have significantly higher probability of getting fact-checked than the Immigration and Equal- ity related posts. Among all the posts, 352 of them are request for fact-checking claims, 108 are about seeking opinion/information (e.g., where can I find ...), and 49 posts are giving judgment regarding an issue without requesting any fact-check. About half of the fact-checking requests are statistical in nature (e.g., check a number, check degree of a phenomenon) and other half are non-statistical (e.g., check if someone said something). There are 13 requests for checking authenticity of photos and videos. Figure 2: Distribution of source categories (a) and checked entities (b) We investigated the sources of the claims which crowd wants to see fact-checked. Figure 2a shows the distribution of source types. We observe that Alternative Media and UGC Platform are the major categories of sources the crowd wants to fact-check. These two source categories together cover 53% of all the claims. The most common UGC platform is imgur.com 4. We also study what entities the crowd wants to fact-check. Most (45%) of the posted claims are about Person (e.g., Barack Obama, Donald Trump) and Policy/Issue (e.g., Medicare, Tax rate). 4.2 Role of Crowd in Checking Claims Fact-checking is a complex task which consists of multiple sub- processes. It’s interesting to see how the crowd self-organize itself to accomplish this task by contributing to the subprocesses. We analyzed the comments and replies to study the different types of actions performed by the crowd. Table 3 shows the distribution of actions in comments. Note that a comment can be long and may contain multiple actions. In such cases, we labeled it with multiple Table 2: Distribution of topics Topic # Posts # Fact-checked # Not Fact-checked % Fact-checked Economy 130 62 68 47.69 Election 83 34 49 40.96 Security 68 24 44 35.29 Health 63 30 33 47.62 Other 39 12 27 30.77 Foreign Affairs 34 16 18 47.06 Media Bias 27 10 17 37.04 Equality 20 6 14 30.00 Immigration 14 3 11 21.43 Religion 6 4 2 66.67 Education 5 4 1 80.00 Environment 4 2 2 50.00 Figure 3: Source category distribution actions. That is why the percentages do not sum up to hundred. About (70%) of the comments are used to provide argument. The crowd also identifies if a posted claim is verifiable or not and seek for information if further clarification is required. Some users also post irrelevant (ads, jokes) comments. We observe that contacting source of a claim, a primary task in professional fact-checking, is not practiced by the crowd. Only one comment was found where the user mentioned contacting the source. We further studied the nature of the provided arguments. There are 1, 080 arguments which contain factual evidence. 701 of these contain additional information leading to a fact-supported infer- ence, either supporting the claim or refuting it. We observe that some commenters give judgment or opinion without factual justifi- cation. There are 860 such cases. We also analyzed the source of the Table 3: Distribution of actions performed by the crowd Action # Comments Percentage Provide Argument 1942 69.56 Post Irrelevant 402 14.40 Seek Clarification 355 12.71 Assign Flair 141 5.05 Check Verifiability 48 1.72 Contact Claim Source 1 0.04 Table 4: Top-12 cited domains Domain # Citations wikipedia.org 298 politifact.com 150 reddit.com 111 washingtonpost.com 91 nytimes.com 78 youtube.com 68 whitehouse.gov 51 factcheck.org 40 huffingtonpost.com 34 imgur.com 32 forbes.com 29 cnn.com 27 Table 5: Average number of factual evidence and opinion- ated judgments per flair Flair Avg. # Factual Evidence Avg. # Opinion/Judgment Mostly True 3.22 1.83 Half True 3.17 1.67 Confirmed 3.10 1.56 False 2.89 2.59 Mostly False 2.75 1.86 Partisan Bias 2.73 1.90 Please Verify 2.59 2.40 Unverifiable 2.38 3.33 None 1.74 3.10 factual evidence provided by the crowd. Figure 3 shows distribu- tions of the categories. The top category is News/Information Media. There are 305 cases (28%) where the commenter presented factual evidence but didn’t provide citation. Table 4 shows the top-12 cited domains. In many cases, the commenters cite evidence from popular fact-checking platforms such as politifact.com 5 and factcheck.org 6. Table 5 shows the average number of comments containing fac- tual evidence per post for each flair category. It is observed that, in general, Unverifiable, Please Verify flair categories receive less factual evidence and more opinionated comments compared to the conclusive (e.g., Confirmed, Mostly True) categories. We measured the number of days required for the crowd to fact-check a claim. The Reddit API doesn’t provide the exact flair as- signment timestamp. A simple heuristic gives us an approximation of the flair assignment timestamp. We observe that before assigning a flair to a post, the moderators publish a comment mentioning and justifying the flair to be assigned. We use the timestamp of the latest comment made by a moderator containing the assigned flair’s words in the text as an approximation of the flair assignment timestamp. Subtracting the post creation timestamp from this gives us an approximation of the required fact-checking time. The mean, median and standard deviation of the required fact-checking time is 7, 1, and 20 days, respectively. 4.3 Role of Moderators A qualitative analysis [ 1 ] of the moderator posts revealed several roles that moderators play to keep discussions on topic and fact- based. At the submission level, a moderator’s roles can be com- pared to those of a gatekeeper who defines rules of a group and decides which posts qualify for fact-checking. At the comment level, moderators acted as seminar leaders” in which they took part in verification, analysis, and evaluation of evidence posted by users. The most prominent role of moderators at the submission level, defined by the frequency of posts with similar themes, emerged as reminding users of the rules and regulations, and issuing warnings. For instance, one post starts as: I’ve seen posts in this thread increasingly devolving into polit- ical bickering, off-topic arguing, and.....Get your act together, keep things on-topic, and keep things fact-based. Otherwise, your posts or comments may be removed”. The second most prominent role of moderator is to encourage users by thanking them for participation and showcasing impact of their works. For instance, part of a post reads: so many of you make this a fantastic little sub with a lot to offer. Please keep it up.” Another post adds, ...Presidential debate drew over 16,000 page views here and more than 4,330 unique viewers.” The third prominent role is to announce new events and topics to be covered. Other roles include seeking suggestions on various topics (e.g., how to cover a live debate). Some of the moderator posts were short while others provided more details. The number of words in a post ranges between one and 673. The moderators posted 381 (13.44%) out of 2, 835 comments. In addition to assigning flairs based on available evidence, moderators took on several roles at the comment level. We find 165 instances where the moderators themselves presented source with factual evidence. Such sources include links to books, columns, datasets, news articles, press releases, research papers, and transcripts. The following comment represents a good example of how moderators help users: OK one more source. This from another biography on Michelle, this link is super long so i will post that here then the quote in a reply”. One noticeable role of moderators appeared to be changing flair as more evidence is presented. For instance, one moderator had labeled a fact as mostly true: Marking this as Mostly True. Yes, 4.5M jobs were created, since the lowest point of the recession. a net gain of 300K since the start of Obama’s administration.” As more evidence came in, another moderator changed the flair to half true: Marking this one as ‘Half True’ based on everyone’s research here. The 4.5 million jobs created seems factually verified. Whether it was better than the Bush recovery from the first recession seems unclear. But Cutter seems to have suggested she was in error on the Reagan recovery.”. Another role was to ask for clarification about vague posts. For example, a typical moderator comment representing this role reads: So can you help us with what part of this you’d like us to fact check? The US military quote from a NY Times article as accurate? Or other?” We find 69 similar comments. 4.4 Role of Computation After observing the way politicalfactchecking operates, we identified several places where computation techniques can work with the crowd hand in hand and produce a faster, scalable and sustainable fact-checking model. Argument Classification: One critical role of the moderators is to go through the arguments provided by the crowd and produce a con- clusive decision based on the evidence. We argue, machine learning techniques can facilitate this task by automatically separating fac- tual evidence from opinionated or evidence lacking judgment. Even though the dataset at hand is small in size, nonetheless, we built a binary classifier using Gradient Boosting algorithm and trained it over the manually coded comments (coding details in section 3.2.2 and 4.2). Word tokens were used as features after removing the stop words. The model was evaluated using 5-fold cross-validation. Its average precision of identifying comments containing factual evidence and evidence lacking judgment is 85% and 82%, respectively (i.e., the model is accurate 85% of the times when it says that a comment contains factual evidence). In a crowdsourced environ- ment, where the number of moderators is significantly smaller than the crowd size, such an automated argument classifier can greatly reduce the workload of the moderators by straining the evidence lacking judgments from large number of comments. Stance Detection: Another important task in fact-checking is to juxtapose supporting and opposing evidence. The moderators scrutinize the comments and identify the supporting/opposing evi- dences. We argue, like the argument classification task, identifying stance of an evidence can also be automated using computation tech- niques; particularly, natural language processing, computational reasoning, and machine learning. Given a claim and an evidence, the goal should be automatically detecting the stance of the evi- dence towards the claim. Our preliminary investigation over the reddit/politicalfactchecking dataset suggests that stance detection is a harder problem than argument classification. [ 17, 19] have studied the stance detection problem where the target is an issue rather than a claim. Recently, Fake News Challenge7 have released a large dataset of news headline-body pairs with manually labeled stance information. We plan to continue investigating the stance detection problem. In addition to the above mentioned tasks, there are other rooms for automation as well. For instance, given a claim, identify existing fact-checks or related evidence from the web, predicting the veracity of a claim based on provided evidence, soliciting arguments from users based on expertise, and so on. 5 CONCLUSION AND FUTURE WORK Despite of many limitations of crowdsourced fact-checking on red- dit/politicalfactchecking, this model outlived many fact-checking 7http://www.fakenewschallenge.org/1005 sites 8. It uses less resources compared to most professional fact- checking sites and utilizes the wisdom of the crowd to check hun- dreds of facts. Yet, it received negligible attention from scholars and professionals. Our findings show strong potential of this crowd- sourced fact-checking model. The results contradict traditional beliefs that crowd is unable to fact-check and they provide opin- ions, not facts 9. We found that more than 50% of the user comments providing an argument in support or against a claim had contained factual evidence. This contradiction may result from the contribu- tion of moderators who often intervene to keep discussion on topic and fact-based. This study has revealed that crowds do more than fact-checking. They, among other roles, help find facts, play a role in deciding which facts are worth checking as well as which facts are verifiable. What professional fact-checking sites can learn from this study is that a small number of trained fact-checkers can help check a large number of facts if they can properly lead crowds in the right direction. This study also adds to previous research on automated fact- checking tools which could strengthen the crowdsourced model and improved its efficiency in multitudes. In conclusion, a future fact- checking model must incorporate all three components–automated assisting tools, crowds and professionals–to survive and thrive in the current media ecosystem. Each of these components comple- ments one another. In future, we plan to investigate the comment replies and understand how discussion forms in a crowdsourced fact-checking model. We also intend to build robust stance detection techniques to facilitate the fact-checking process.","Hassan, N., Yousuf, M., Mahfuzul Haque, M., A. Suarez Rivas, J., & Khadimul Islam, M. (2019, May). Examining the roles of automation, crowds and professionals towards sustainable fact-checking. In Companion Proceedings of The 2019 World Wide Web Conference (pp. 1001-1006)."
ML_DB_098,Investigating the emotional appeal of fake news using artificial intelligence and human contributions,"The creation and dissemination of fake news can have severe consequences for a company’s brand. Researchers, policymakers and practitioners are eagerly searching for solutions to get us out of the ‘fake news crisis’. Here, one approach is to use automated tools, such as artificial intelligence (AI) algorithms, in conjunction with human inputs to identify fake news. The study in this article demonstrates how AI and machine learning, with its ability to analyze vast amounts of unstructured data, can help us tell apart fake and real news content. Specifically, this study examines if and how the emotional appeal, i.e., sentiment valence and strength of specific emotions, in fake news content differs from that in real news content. This is important to understand, as messages with a strong emotional appeal can influence how content is consumed, processed and shared by consumers.",Computer Science,Article,2019,N,Y,31,"When a headline asks a question, the answer should be no”. Although this may read like a bizarre or even humorous statement, this adage even has a name: Betteridge’s law (Betteridge, 2009). Formulated by the British journalist Ian Betteridge, it is based on the notion that journalists use this style of headline if they know the story is probably bullshit, and don’t actually have the sources and facts to back it up, but still want to run it” (Betteridge, 2009). This quote certainly rings true today. For example, a 2017 article from YourNewsWire claimed that according to an NPR study, more than 25 million Hillary Clinton votes were fraudulent, suggesting that Clinton had actually lost the popular vote by a huge margin. This claim was false and the study in question was never conducted by NPR; still, it was among the most viral stories shared on Facebook in 2017. Although not a new phenomenon, the generation and impact of fake news and alt- facts have reached new heights, driven mainly by the increasing digitization of information and the explosion of social media (Baccarella et al., 2018; Berthon and Pitt, 2018). Fake news, defined as news that intentionally present misinformation with the intent to deceive the audience (Bakir and McStay, 2018; Horne and Adali, 2017; Kumar and Shah, 2018), also known as disinformation (Hannah et al., 2015), can have severe consequences for brands, businesses and societies as a whole. A prime example is the 2016 US presidential election and the concern about how false stories on social media may have impacted the election outcome (Allcott and Gentzkow, 2017). This has led a number of commentators from reputable news outlets to suggest that Trump would not have been elected without the influence of fake news (Dewey, 2016; Olson, 2016; Parkinson, 2016; Read, 2016). As a well- informed public is key to any effective democracy, fake news can be especially dangerous in the context of public opinion and political information. In addition, brands can also be impacted by and impact fake news in a number of ways (Berthon and Pitt, 2018). First, brands can be the direct target of false stories with major consequences: Pepsi’s stock declined 4 per cent when a fake news story about its CEO telling Trump supporters to take their business elsewhere” went viral. Second, brands can affect fake news stories by consciously or unintentionally associating themselves with dubious content. When this happens, this may be 4 perceived as brands accepting or even endorsing the false stories, which, on the one hand, may render legitimacy to the false information and, on the other hand, contaminate a brand’s image (Berthon and Pitt, 2018). As an illustration, Kellogg’s received substantial backlash from consumers for advertising on the alt-fact site Breitbart and eventually removed its ads (Helmore, 2016). As the above examples illustrate, fake news can threaten the viability of a brand, which is often regarded as a firm’s most valuable intangible asset (Aaker, 1991; Keller, 1993). Marketing communication managers are well advised to be vigilant about if and how their brands are associated with fake news. With the rising use of social media, false content can originate from and be shared by any of a brand’s stakeholder group – employees, customers, users or third-party news outlets, among others. Hence, researchers, policymakers and practitioners are eagerly searching for solutions to get us out of the fake news crisis” (Rothman, 2018). Systemic solutions that involve rethinking the role of marketing communications and branding in creating reality are called for (Berthon and Pitt, 2018); however, these systemic changes will need time. In the short term, managers require tactical solutions that allow them to identify fake news when designing and executing brand communication strategies. These approaches will need to allow practitioners to flag dubious news at any one of the three core elements of the marketing communication process: the source, the receiver and the message itself. Here, one tactic currently explored is to verify the source of the information, for example, through publishing lists of most trusted” and least trusted” news sources (e.g., Strike Social). A problem with this approach is that these lists are static and may be outdated by the time they are published. Another approach has been to limit the sharing of false content on social media sites like Facebook or Twitter (Baly et al., 2018), which is challenging because fake news generally thrive on social media thanks to the mechanisms of sharing, which amplifies their dissemination. A third strategy is to analyze the news content itself and flag dubious content. For example, fact-checking organizations such as Snopes, FactCheck or PolitiFact manually verify that the claims made in articles are actually factual; however, this is a slow and tedious process that does little to identify false information proactively. On the other 5 hand, entirely automated fact- checking tools lag behind in terms of availability of authoritative data and are less trusted by humans (Baly et al., 2018). One approach that has gained more attention recently is to use automated tools, such as artificial intelligence (AI) algorithms, in conjunction with manual fake new identification approaches. At the time of writing this article, social media giant Facebook, for instance, had just announced to expand its team of content specialists, in addition to relying on algorithmic approaches to fake news screening (Hern, 2018; Wakefield, 2019). The premise is that automated tools, such as AI algorithms, can effectively assist humans in the important task of identifying fake news (Baly et al., 2018; Graves, 2018; Hao, 2018; Berthon and Pitt, 2018; Strickland, 2018). Building on this anticipation, in this article, I demonstrate how AI in conjunction with human inputs can help us tell apart fake and real news content. Although there is a large body of research examining the factors that contribute to the dissemination of fake news, there is relatively little work that helps us understand how the content of fake and that of real news differ (Baly et al., 2018; Horne and Adali, 2017). In other words, of the three strategies mentioned above (i.e., to verify the source, analyze the content and limit the sharing), I focus on the content. This study contributes to our understanding of news content by investigating if, and how, fake and real news vary with respect to their emotional appeal. More specifically, this study focuses on understanding whether the overall sentiment valence and strength of specific emotions in fake news articles differ from those in real news articles. This is important to understand, as messages with a strong emotional appeal can impact how content is consumed, processed and shared (Meenaghan, 1995; Taute et al., 2011). With these aims in mind, this article proceeds as follows: It first provides conceptual foundations of fake news before discussing the current knowledge of fake news identification as it relates to the key elements in the marketing communication process (i.e. the source, the content and the receiver). The next section (Section 4) then describes the empirical study underpinning this paper and presents the study results. Section 5 discusses these results in the context of extant brand communications literature, and the paper concludes with contributions for practice and academia as well as avenues for future research. 6 Conceptualizing fake news: factuality and intent to deceive Fake news has become a buzzword, especially in the aftermath of the 2016 US presidential election, so much so that the Collins Dictionary made it the word of the year” in 2017 (Habgood-Coote, 2018). Early studies on the topic of fake news have defined the phrase in terms of particular types of media content, such as political satire, news parodies or news propaganda (Tandoc et al., 2018). Contemporary discourse defines fake news as viral posts, often on social media, based on fictitious accounts constructed to look like news reports. BuzzFeed’s media editor Craig Silvermann is credited with helping to popularize the term in 2016, when he identified 140 Macedonian websites that fabricated stories to attract audience interest and profitable clicks. One key element of fake news, according to Silvermann’s definition, is an underlying profit motive of its creators (Habgood-Coote, 2018). A second key motivation implied in many definitions of fake news is ideological: Fake news providers create fake stories to promote particular ideas or people that they favor, often by discrediting others (Allcott and Gentzkow, 2017). A similar ideological motif is emphasized by the Ethical Journalism Network (2017), which defines fake news as information that has been deliberately fabricated and disseminated with the intention to deceive and mislead others into believing falsehoods or doubting verifiable facts” (Ethical Journalism Network, n.d.). These illustrations show that different people have a different understanding of what fake news means. However, for parsimony, it is important to clearly conceptualize fake news in this article. This is the purpose of the following section. Originally derived as a plural of new”, news have been defined in various ways, such as the account of recent and interesting events (Kershner, 2005), the presentation of new information (Stephens, 1988) or the account of events that have a significant impact on people (Richardson, 2007). News is often viewed as an output of journalism and is expected to provide independent, reliable, accurate and comprehensive information (Tandoc et al., 2018). Kovach and Rosenstiel (2014) suggest that the key purpose of journalism is to provide citizens with the information they need to be free and self- governing” (Kovach and Rosenstiel, 2014, p. 12); thus, journalism has 7 a responsibility to report accurate and true information in the output it produces. These definitions of fake news also prompt us to think about the connotation of fake”. The Oxford Dictionary defines fake as not genuine; imitation or counterfeit” (Oxford English Dictionary, 2018). The word fake” is often used interchangeably with not real”, false”, fraudulent” or inauthentic” (Andrea, 2016; Oxford English Dictionary, 2018). Thus, according to this notion, one could judge whether a news item is fake or real based on the degree to which it relies on facts. However, one problem with this unidimensional conceptualization is that it would consider certain news types, such as satire or parody, as fake news; however, these forms play an increasingly important part of the media ecosystem and a critical and informed public (Plangger et al., 2019; Rubin et al., 2016; Tandoc et al., 2018). Satire, for example, relies on actual events but uses humor, sarcasm and exaggeration, rather than objective facts, to portray these events. Satire news outlets promote themselves and their stories as delivering entertainment to their audiences, rather than independent, accurate and reliable information (Rubin et al., 2016; Tandoc et al., 2018). In a meta-analysis of scholarly articles published on fake news between 2003 and 2017, Tandoc et al. (2018) introduce a typology of fake news based on two dimensions, similar to Hannah et al.’s (2015) typology of leaked secrets. Each dimension in Tandoc et al.’s (2018) typology encompasses a spectrum, ranging from low to high. The first dimension is facticity, i.e. the degree to which fake news relies on facts. High- facticity news items provide accurate and reliable real accounts of events, whereas low-facticity stories often take a broad social context upon which they fabricate ficticous accounts. The second dimension, author intent to deceive, refers to the degree by which the creator of a piece of content intends to misinform people for ideological or profit motiviations. Satire news have a low intent to deceive – their main purpose is to entertain their audiences, and they assume an open disclaimer that their stories are not factual (Tandoc et al., 2018). On the other hand, creators of fabricated news content intent to mislead their readers, without any disclaimer. Recent scholarly work has emphasized the two dimensions proposed by Tandoc et al. (2018) to define fake news (Bakir and McStay, 2018; Horne and Adali, 2017; Kumar and Shah, 2018). Following these conceptualizations, the present study defines real news as news articles with a high degree of facticity and low (no) intent to deceive. Fabricated 8 (or fake) news have a low degree of facticity and a high intention to mislead their audience. Table 1 presents a typology of news definitions; highlighted in italics are the types that this paper focuses on. Table 1 Typology of news definitions (adapted from Tandoc et al. (2018)) Degree of facticity Author’s intention to deceive High Low High Propaganda news Real news Low Fake news Satire news Having delineated the key constructs, the following section reviews the existing literature on fake news identification and positions the current study within this literature. Fake news identification: source, receiver and content Fake news detection has been extensively examined in the field of communications and computer science. Working from the three key elements of (brand) communication mentioned earlier, i.e. the source, the receiver and the message (i.e. content), the fake news literature can be categorized into three different paradigms: source-based, context- based and content-based studies (Table 2; Potthast et al., 2017). Table 2 Elements of communication and corresponding paradigm for fake news detection literature alongside a selection of relevant work Key element of communication Source Receiver Message Paradigm of fake news detection Source-based Context-based Content-based Fact-checking Style-based Baly et al. (2018) Mukherjee and Weikum (2015) Popat et al. (2016) Kwon et al. (2013) Bessi et al. (2015) Del Vicario et al. (2016) Vosoughi et al. (2018) Etzioni et al. (2008) Magdy and Wanas (2010) Chen et al. (2015) Potthast et al. (2017) Horne and Adali (2017) Horne et al. (2018) 9 Source-based fake news detection studies attempt to identify the degree of faking by assessing the reliability of the source reporting the information. For example, Baly et al. (2018) predict the factuality of reporting and bias of various news sources using a machine learning tool. Mukherjee and Weikum (2015) present a model to predict the trustworthiness of news communities, such as Digg and Reddit, and how trustworthiness interacts with content credibility and user expertise. Popat et al. (2016) propose a machine learning tool that leverages the interaction between the language of articles and the reliability of the underlying Web sources to identify and predict false claims. Context-based fake news detection is mainly concerned with understanding the dissemination of fake news online, thus focusing on the receiver of a message in the communication model. For instance, Kwon et al. (2013) propose a model to characterize rumors by examining the temporal, structural and linguistic aspects of their diffusion on social media. Del Vicario et al. (2016) conduct extensive quantitative modeling to understand the mechanisms behind the spreading of misinformation on Facebook. In a recent Science publication, Vosoughi et al. (2018) investigate the diffusion of 126,000 verified true and verified false news stories distributed on Twitter from 2006 to 2017. They found that false news stories diffused farther and faster, were more novel and inspired replies with different emotions (such as surprise and disgust) compared to true news stories (such as sadness, trust and anticipation). Content-based fake news studies analyze the textual or visual content to identify dubious content, thus focusing on the third key element in the communications process, i.e. the message. Here, two subfields exist: knowledge-based (or fact checking”) studies use methods borrowed from information retrieval research to extract facts from a given narrative and compare them with known truths. For instance, Etzioni et al. (2008) use a tool to extract factual claims made in a given narrative, compare them with facts retrieved from the Web and flag discrepancies. In another study, Magdy and Wanas (2010) demonstrate a statistical model that checks factual claims from a given text and determines how frequently they are supported by facts retrieved from the Web. The underlying assumption of knowledge-based approaches is that Web resources or the frequency by which a fact is mentioned is an indicator of its truth; however, this is problematic and the reliability and credibility of almost any website can be questioned (Potthast et al., 2017). A number of fact- 10 checking organizations rely on the knowledge-based approach to fake news identification. The second subfield to content studies, style-based fake news detection, focuses on modeling the degree of faking and its manifestation in a given text assuming that deception has its own style. Here, extant studies have examined the textual features, such as semantics and syntax. For example, Potthast et al. (2017) use 1,627 articles that have been manually fact- checked by professional journalists from BuzzFeed and apply stylometric analysis using a machine learning algorithm to predict factuality of stories. In another study, Chen et al. (2015) examine methods for the automatic detection of clickbait content as a form of deception, by analyzing both textual (i.e., semantics and syntax) and non-textual (i.e., images) elements of content. They conclude that clickbaiting can be identified through certain linguistic patterns, such as the use of suspenseful language, unresolved pronouns, a reversal narrative style, forward referencing, image placement, reader’s behavior and other important cues” (Chen et al., 2015, p. 18). Featuring prominently among these other important cues” are emotional features derived from their text analysis, e.g., via sentiment analysis; however, this is not further explored in their study. Horne and Adali (2017) analyze a comprehensive set of features including stylistic, complexity and psychological characteristics in a data set of real, fake and satire news. The authors conclude that fake news use shorter, simpler and more repetitive content. They also found that fake news content displays stronger negative emotions compared to real news; however, the type of emotions in fake news is not further explored. Horne and Adali (2017) also found a strong dissimilarity between the title and text body of news and suggest that the title and the body should be analyzed separately. In a follow-up study, Horne et al. (2018) use a data set encompassing over 136,000 news articles from 92 sources, which they characterize based on 130 features from seven categories: structural, sentiment, engagement, topic- dependent, complexity, bias and morality. Their analysis suggests that highly engaged fake articles express more negative emotions, thus confirming previous findings by Bessi et al. (2015) and Horne and Adali (2017). Although a number of studies under the content-based paradigm exist, this field of research is still underdeveloped (Potthast et al., 2017). In particular, the above literature discussion suggests one 11 interesting finding common across a number of studies under this paradigm: That the content of real and fake news is different with respect to emotions portrayed in it. However, what is currently less understood is the valence of emotions and how different types of emotions differ across different types of narratives. The study presented in this article helps close this gap in our understanding. Specifically, the research uses AI, in addition to human analysis, to determine whether the valence and the strength of specific emotions differ in fabricated and real news articles. The next section will outline the methodology used in this study to achieve this research objective. A study about the emotions in fake news and real news The research conducted here was guided by the following research questions: RQ1. How do the overall sentiment valence and specific emotions displayed in the titles differ with respect to real and fabricated news articles? RQ2. How do the overall sentiment valence and specific emotions displayed in the article body differ with respect to real and fabricated news articles? To answer these research questions, I use a data set encompassing political news articles for each of the above two types of news: real and fabricated. This data set was used in a previous study by Horne and Adali (2017) and data collection followed a two-step process. The first step, identifying the appropriate source for each of the two news categories, relied primarily on manual input. Specifically, this step relied on published lists of news outlets that had been previously deemed trustworthy” or misleading” by human fact checkers. For instance, the sample source for real news articles encompassed Business Insider’s list of most trusted and least trusted” news outlets (Engel, 2014) and includes well-established news outlets that all have low (no) intent to 12 mislead their audience. Fabricated news articles were sourced from Zimdars’ (2016) list of misleading news sources which have had at least one news article identified as false on the fact-checking website snopes.com. Table 3 summarizes the sources for each of the three types of news articles in this study. Table 3 News article sources (adapted from Horne and Adali (2017)) Real news source Fabricated news sources ABC BBC CBS NBC NPR The Economist The Guardian USA Today Wall Street Journal Washington Post Abcnewsccom.co Before it’s news DC Gazette Ending the Fed Infowars Libertywritersnews Real News Right Now True Pundit The second step involved randomly selecting 75 political news articles from each source group – real news and fabricated news, for a total of 150 articles. Only hard” news articles were included in the data set, whereas editorials or opinion pieces were excluded. Horne and Adali (2017) made the above- described data set publicly available on the open-source platform github , so this platform was accessed to obtain the data set for the current study. Although Horne and Adali (2017) analyzed a comprehensive set of linguistic and psychometric features by which fake and real news content differed, their study did not consider if and how specific types of emotions differ in fake and real news content. This is the focus of my study, for which the results are outlined in the following section. Analyzing emotions and sentiment using AI The unit of analysis in this study encompasses the written text in the body and titles of news articles in the data set described above. Following the suggestion by Horne and Adali (2017), the present study will analyze the 13 text body and titles of real news and fake news separately. The first step in the data analysis included analyzing the data set using the AI application IBM Watson. AI systems, such as IBM Watson, use natural language understanding to assign meaning to text-written everyday human language and to detect language patterns, topicality, sentiment and other linguistic characteristics (Kietzmann et al., 2018; Rubin et al., 2016). In addition, machine learning algorithms enable an AI system to learn by itself, i.e., to improve its performance without the algorithm being changed by a trained coder. Among Watson’s suite of applications is a Natural Language Understanding Service”, which uses linguistic analytics to analyze target phrases in context of the surrounding text for focused sentiment and emotion results” (IBM, 2018). AI applications, such as IBM Watson, have been used in other academic research, for example, to extract personality profiles from written text (Majumder et al., 2017; Mostafa et al., 2016) or to understand the sentiment portrayed in a body of text (Cambria et al., 2013; Pitt et al., 2018). AI algorithms have also been extensively used to perform linguistic analyses in the context of fake news detection (Chen et al., 2015; Horne and Adali, 2017; Potthast et al., 2017; Rubin et al., 2016). As part of its Natural Language Understanding Service, IBM Watson calculates scores for emotions detected in a narrative based on Plutchik’s (2001) work on basic human emotions. The five emotions analyzed by Watson include sadness, joy, fear, disgust and anger, and for each emotion, the score ranges from 0 to 1: A score of 0 means the text does not convey the emotion, whereas a score of 1 indicates that the text expresses the emotion strongly (IBM, 2018). In addition, Watson returns an overall sentiment score indicating the valence (positive/negative/ neutral) and strength of sentiment displayed in the focal document. Sentiment scores range from −1 (strong negative sentiment) to 1 (strong positive sentiment); a score of 0 suggests a neutral sentiment. Following the Watson analysis, the second step in the data analysis included further analyzing the emotion and sentiment scores using the content analysis software DICTION (Pitt et al., 2017; Short et al., 2018) for descriptive statistics and the statistical software package SPSS 25. Table 4 displays the summary statistics for article body and titles. An observation to be made from Table 4 is that the body of real news articles appears to be much longer and uses longer words on average than fabricated news articles. However, the titles of real news appear to be shorter but use longer words on average than fabricated news. 14 Table 4 Descriptive statistics Text body Title Real news (n=75) Fabricated news (n=75) Real news (n=75) Fabricated news (n=75) Shortest text (words) 126 100 6 6 Longest text (words) 2316 1700 16 21 Mean total words 699.4 477.1 9.9 11.8 Total words 52458 35781 739 887 Mean word size (characters) 5.06 4.85 5.65 5.45 Sentiment valence by news type For each news type, the proportions of narratives with positive, negative and neutral sentiment were computed; this was done separately for the article body and the title. Inferential statistics comparing two population proportions (Z scores; Hair et al., 2011) were calculated to test whether the proportion of narratives displaying an overall positive (negative, neutral) sentiment differed across each news category. The level of significance was set at = 0.05. The results in Table 5 suggest that there is no significant difference in the sentiment valence of the article body of fabricated versus real news articles. However, the article titles for each news category differed with respect to sentiment valence (Table 5). Specifically, a larger proportion of fabricated news display a negative sentiment in their titles, and proportionally fewer fabricated articles reveal a neutral sentiment. Table 5 Sentiment valence in the text body and titles Text body Title Real news (n=75) Proportion Fabricated news (n=75) Proportion Z Real news (n=75) Proportion Fabricated news (n=75) Proportion Z Positive 0.080 0.160 −1.508 0.040 0.053 −0.387 Negative 0.920 0.840 1.003 0.240 0.600 4.467* Neutral 0.000 0.000 −1.508 0.720 0.347 4.583* Total 1.000 1.000 1.000 *p < 0.05 15 Specific emotions by news type An independent-samples t-test was used to test the hypothesis that the two types of news differ on specific emotion dimensions. The results (Table 6) suggest that emotions in the article body itself differ significantly for three out of the five emotion dimensions analyzed. Fake news articles were significantly less joyful than real news articles (t(144.434) = 2.073, p = 0.040). In contrast, fabricated news articles expressed significantly higher levels of disgust (t(147) = –3.720, p = 0.000) and higher levels of anger (t(136.408) = –3.433, p = 0.000) than real news articles. In other words, the text body of fake news articles was less joyful but angrier and more revulsive than the text body of real news articles. There were no significant differences in the article body of fabricated and real news with respect to the dimensions of sadness (t(147) = –1.408, p = 0.161) and fear (t(140.389) = –1.409, p = 0.161). With respect to the article titles, the results in Table 7 suggest that fabricated news articles displayed significantly higher levels of disgust (t(131.647) = –3.253, p = 0.001) and higher levels of anger (t(147) = –3.418, p = 0.000) than real news articles. No significant differences were found in the titles for the emotion dimensions of joy (t(130.789) = –0.913, p = 0.363), sadness (t(147) = 0.737, p = 0.462) and fear (t(147) = 0.653, p = 0.515). 16 Table 6 Emotion dimensions in the text body Independent-samples t-test Cohen’ s d n/s 0.345 n/s –0.614 –0.588 Table 7 Emotion dimensions in the article title Independent-samples t-test n/s n/s n/s –0.567 –0.563 aEqual variances not assumed as Levene's test is significant at the p<0.05 level; *p < 0.05; ***p < 0.0001 p (2-tailed) 0.16 0.040* 0.16 0.000*** 0.001*** 0.462 0.363 0.515 0.001* <0.001*** df 147 144.434 140.389 147 136.408 147 130.789 147 131.647 147 t –1.408 2.073 –1.409 –3.720 –3.433 0.737 –0.913 0.653 –3.253 –3.418 Fabricated news text body (n=74) SD 0.146 0.201 0.186 0.183 0.172 Fabricated news text body (n=74) 0.140 0.211 0.127 0.216 0.1391 Mean 0.451 0.365 0.205 0.411 0.281 0.238 0.186 0.131 0.315 0.255 Real news text body (n=75) SD 0.154 0.178 0.151 0.176 0.131 Real news text body (n=74) 0.155 0.144 0.170 0.149 0.135 Mean 0.417 0.43 0.166 0.302 0.195 0.256 0.159 0.147 0.216 0.178 Sadness Joya Feara Disgust Angera Sadness Joya Fear Disgusta Angera 17 Discussion If, as alluded to earlier, fake news is about stories with low degree of facticity and high intent to deceive for ideological or profitable motivations, then one would expect the content of fake and real news to substantially differ. Previous studies have confirmed that substantial differences exit and have suggested that the emotional appeal is one important feature by which to tell fake and real news apart (Bessi et al., 2015; Chen et al., 2015; Horne and Adali, 2017). However, what was lacking in the extant literature was an understanding of if and how specific emotions differ between fake and real news. This is the research objective of the present study. Specifically, this work was guided by the following research questions: How do overall sentiment valence and specific emotions displayed in titles differ with respect to real and fabricated news articles? And, how do the overall sentiment valence and specific emotions displayed in the article body differ with respect to real and fabricated news articles? The following section discusses the findings related to these research questions. Titles are a strong differentiator on emotions between fake and real news When looking at only the titles of fake and real news, the results suggest substantial dissimilarities between the two types of news. First, fake news titles are significantly more emotional” and less neutral in sentiment than titles of real new articles, thus confirming previous findings by Horne and Adali (2017) and Horne et al. (2018). Table 8 provides an illustrative example of a fabricated news title displaying a high (negative) emotional tone in comparison to a real news title with neutral sentiment. Table 8 Example of fabricated news title and strong sentiment valence Fabricated news title – Negative sentiment Real news title – Neutral sentiment BREAKING! Dems Caught Counting Hillary Votes 6 TIMES In THIS City, TIME FOR PRISON! Report: White House preparing response to Russian election interference 18 To better explain these findings, I turn to the existing literature on message framing in marketing and brand communication. Marketing communication strategies can be roughly divided into rational versus emotional appeals (Dens and Pelsmacker, 2010). A rational message framing aims to inform consumers using factual claims, whereas an emotional framing aims to evoke emotions in consumers (Johar and Sirgy, 1991). An emotional appeal in a message facilitates message processing in a number of ways, for example, through containing less information to processed by consumers, improved levels of attention (Heath, 2007) and higher levels of recall (Ambler and Burne, 1999; Friestad and Thorson, 1986). Given consumers have limited capacity to absorb all the information contained in a message (Lang, 2000), an emotional appeal may be more effective at persuading consumers than a rational message frame. Furthermore, studies suggest that a majority of links sha","Paschen, J. (2019). Investigating the emotional appeal of fake news using artificial intelligence and human contributions. Journal of Product & Brand Management."
ML_DB_099,"Combatting visual fake news with a professional fact-checking tool in education in France, Romania, Spain and Sweden","Educational and technical resources are regarded as central in combating disinformation and safeguarding democracy in an era of ‘fake news’. In this study, we investigated whether a professional fact-checking tool could be utilised in curricular activity to make pupils more skilled in determining the credibility of digital news and to inspire them to use digital tools to further their transliteracy and technocognition. In addition, we explored how pupils’ performance and attitudes regarding digital news and tools varied across four countries (France, Romania, Spain, and Sweden). Our findings showed that a two-hour intervention had a statistically significant impact on teenagers’ abilities to determine the credibility of fake images and videos. We also found that the intervention inspired pupils to use digital tools in information credibility assessments. Importantly, the intervention did not make pupils more sceptical of credible news. The impact of the intervention was greater in Romania and Spain than among pupils in Sweden and France. The greater impact in these two countries, we argue, is due to cultural context and the fact that pupils in Romania and Spain learned to focus less on ’gut feelings’, increased their use of digital tools, and had a more positive attitude toward the use of the fact-checking tool than pupils in Sweden and France.",Social Science,Article,2021,N,Y,4,"Faced with the challenges that are caused by information disorder and infodemics, there is a demand for educational interventions to support citizens and safeguard democ- racy [ 1 –3 ]. Education is considered to be key, since automated fact-checking has significant limitations, not least when it comes to debunking visual images and deep fakes [4 ,5 ]. In addition, platform companies and fact-checkers struggle to keep pace with the speed and spread of disinformation (e.g., [1, 6]), which makes it critical that citizens develop resilience to disinformation by learning to navigate digital news in more up-to-date and autonomous ways. Disinformation—defined as inaccurate, manipulative, or falsified information that is deliberately designed to mislead people—is intentionally difficult to detect. This poses a challenge not only for professional fact-checkers in mainstream media and digital platforms, but also for media literacy specialists, whose expertise does not extend much beyond imparting basic source verification strategies [3]. Yet, the journalistic profession has been able to benefit from a growing number of fact-checking initiatives that have generated digital tools and novel responses to infodemics. However, such tools have not broadly reached the general public, which has mostly been left to its own devices. This gap between professionals and the general public is further widened by the evolution of Information 2021, 12, 201 2 of 25 disinformation itself; fake news is now not only text-based, but also increasingly image- based, especially on the social media used by young people, and so debunking news requires more sophisticated approaches. Building resilience to fake news requires navigating online information in new ways and with the support of digital tools, similar to the methods used by professional fact- checkers [ 7– 9]. Because new technology makes it hard to see the difference between a fake and a real video [ 10 ] or to distinguish a misleading image in a tweet from a credible one [11], teenagers often struggle to determine the credibility of images and videos when these are presented in deceptive ways [12 – 14 ]. Citizens need a combination of digital knowledge, attitudes, and skills to navigate the complicated digital world of post-truth, as highlighted by theories of media and information literacy, such as transliteracy [15 ] and technocognition [8]. Young people growing up in an era of online misinformation have been found to struggle to separate fake news from real news [12, 14 , 16– 18 ]. Teenagers stating that they are quite skilled at fact-checking may not hold the skills they think they have [ 13 , 19]. The idea that young people are digital natives, knowing how to navigate digital media much better than other generations, does not have any support in the research. Instead, there is a call for educational efforts to promote the media and information literacy of teenagers with diverse backgrounds [14,20,21]. Research has highlighted the existence of a media and information literacy divide between people and pupils in different groups, highlighting a digital inequality between citizens [14 ,22 – 25]. Teenagers with poor socio-economic status may spend more time online on entertainment and simple tasks than peers with better support from home, and they may also find it difficult to separate fake news from real news [ 14, 21, 26 ,27 ]. Access to computers will not automatically bridge this divide since source-critical thinking has multiple interlinked dimensions and it is very complex and intellectually challenging to determine whom to trust online [ 28 ,29]. Pupils need more education designed to promote media and information literacy in general and visual transliteracy in particular in order to overcome this divide in different contexts. Research indicates that it is possible to support people’s abilities to evaluate on- line information by giving short instructions on how to identify misleading headlines on Facebook and WhatsApp [ 7], by the use of games that are designed to alert against manipulative tweets [30] and by educational interventions that support pupils’ civic online reasoning [20,21,31]. However, because the technological advances in visual media manip- ulation are leveraging the spread of false or misleading information, researchers are calling for ’more intensive digital literacy training models (such as the lateral reading” approach used by professional fact checkers)’ ([7], p. 7). In this study, we took on this challenge by evaluating a professional digital fact- checking tool in classroom settings in France, Romania, Spain, and Sweden. The aim of this design-based study was to make the professional plug-in InVID-WeVerify useful in curricular activity in order to improve pupils’ skills in evaluating misleading images and videos. We investigated the potential benefits and challenges of implementing the latest advances in image and video verification in collaboration with teachers across Europe. The tool, InVID-WeVerify, is a free verification plug-in that is available in multiple languages used today by professional journalists and fact-checkers to verify images and videos in newsrooms, such as France24, India Today, Canal 1, and Volkskrant [32 ]. The plug- in has been downloaded across the globe more than 40,000 times, and it is used on a daily basis by, among others, fact-checkers at Agence France-Presse (AFP) to investigate rumours and suspicious content regarding, for example, Covid-19 and politics. 1.1. Educational Interventions to Support Fact-Checking in a Post-Truth Era International organizations, like UNESCO and the European Union, underscore the im- portance of education to promote so-called media and information literacy as an important defence against propaganda and disinformation [1, 33 ]. Media and information literacy may Information 2021, 12, 201 3 of 25 be viewed as an umbrella term covering knowledge, skills, and attitudes described by re- searchers as information, news, media, and digital literacies [ 33 –35 ]. Information literacy— the ability to evaluate and use information wisely—has especially been noted as a ‘sur- vival skill’ [ 36 ]. In line with this, the theory of civic online reasoning underscores how the ability to effectively search for, evaluate, and verify social and political information online” is essential for all citizens ([ 18], p. 1). The multi-modal aspects of digital infor- mation involve new challenges when people search for, find, review, analyse, and create information [37–39]. Researchers also call for more research on civic online reasoning with new and more complex tasks and test-items paying attention to pupils’ knowledge, skills, and attitudes in different educational settings [20]. Today, the ability to read, write, and interact across a range of platforms, tools, and media, described as transliteracy, has become a key literacy in a world of digital multimodal information [ 40 ]. Transliteracy has been enlarged to embrace the double-meaning of digital convergence: ‘1. the ability to embrace the full layout of multimedia, which encompasses skills for reading, writing, and calculating with all the available tools (from paper to image, from book to wiki); 2. the capacity to navigate through multiple domains, which entails the ability to search, evaluate, test, validate and modify information according to its relevant contexts of use (as code, news, and document)’ ([ 41 ], pp. 15–16). Transliteracy echoes technocognition as an emerging interdisciplinary field that involves technological solutions incorporating psychological principles to solve disinformation issues [8]. In the present study, we focus on the latter aspect of transliteracy, more precisely, how tools can facilitate navigating a digital information landscape. Scholars point out that journalistic principles and technology may support citizens in nav- igating a digital world of deep fakes and misleading visual and text-based information [8,9]. The use of digital tools to support the verification of news has been discussed in terms of technocognition and civic online reasoning [42]. These prescriptive theories emphasise that citizens need to be better at scrutinising online information, and that this is a psychological, technological, and educational challenge. In a post-truth era, people need to consider that images and videos may be manipulated and also be able to use digital resources, such as text search and reverse image search, to corroborate information. Professional fact-checkers use technology to read laterally, which means that they verify information on a webpage by corroborating it with information on other credible webpages [9]. Researchers note that education and technology that support lateral reading may be key to safeguarding democracy in a post-truth era that is saturated by disinformation [7–9]. However, the use of profes- sional fact-checking tools in education to support pupils’ transliteracy, lateral reading, and technocognition has not been studied in previous research to date. What has been noted in media and information literacy research is that pupils of- ten struggle to separate credible from misleading digital multimodal information [12, 14 ]. Even individuals with proficient news media knowledge may struggle to evaluate evi- dence online [17 ,43 ]. The high expectations of news literacy programmes [ 3] should be understood in light of these challenges. Scholars also emphasise that technology and edu- cational interventions are not quick fixes for the complex challenge of misinformation [44]. More time in front of computers does not necessarily make pupils more skilled at navi- gating online information [18 ,45 ,46 ]. Without adequate media and information literacy, pupils may fail to separate credible information from misleading informationl, because they are not able to use effective and adaptive strategies when evaluating manipulated images and junk news [ 28]. In education, it is critical that the educational design includes a combination of challenging and stimulating tasks, and different types of hard and soft scaffolds to help pupils use online resources in constructive ways [47–52]. While noting the many challenges, we still find a few studies highlighting the ways in which it is possible to support pupils lateral reading skills in education. Educational designs for promoting civic online reasoning have made it possible for teenagers at the univer- sity and high school level to scrutinise digital news in a similar manner to professional fact-checkers [ 20, 21 ,31 , 53 ]. Previous research has also identified that it is possible for upper Information 2021, 12, 201 4 of 25 secondary school pupils to use digital tools that are designed for professional historians in critical and constructive ways if they are supported by an educational intervention comprising supporting materials and teaching [54,55]. 1.2. Design-Based Research Implementing innovative technology in schools is often linked to design-based re- search in education, also known as design experiments, design research, or design study [56]. Testing and developing digital tools that may hold new dimensions and practices is often at the core of design-based research [ 56 ], not least since this may provide new practical and theoretical insights. design-based research aims to test and build theories of teaching and learning, and produce instructional tools that survive the challenges of everyday practice” ([57], p. 25). The usefulness of design-based research comes from the methods where researchers and teachers collaborate to identify challenges and test new materials and methods in complex classroom settings with a purpose to promote pupils’ learning [ 58, 59 ]. In line with a previous call for research that features practitioner co-creation of knowledge as a vehicle for use and uptake” ([ 60], p. 98) and congruent with Akkerman et al. [61], we acknowledge that dialogue with teachers regarding technology is essential in educational design-based research. Design-based research advocate Ann Brown [62] stresses the importance of collect- ing data from messy classrooms in order to measure learning where it usually occurs. She underscores how measuring effects through pre- and post-tests designed to fit the research focus and this is central in our study ([62 ], Additionally, see Figure 1 and the section below on materials and methods). Our research is based on the assumption that the design of materials and methods is important for learning and we focus on developing new tools and theories for teaching in the complex reality of teaching and learning [ 63]. In line with the methodology of design-based research we see that the materials and methods that were developed through iterative studies in the classrooms should preferably survive the challenges of classroom practices and remain to be used in teaching long after the research project is completed [57 ,64 ]. Thus, design-based research professionals argue that educational science needs to develop ideas and products that work in thoughtful ways [65] and, in this article, we present some steps in this direction.Study Design Phase Intervention FOCUS GROUPS PREPARATION PILOTING Researchers created: - Lesson plan - Handouts - Slides 34 teachers Testing and discussing InVID-WeVerify Teacher tested intervention materials and provided feedback LESSON I - Introduction to concepts - Active exercises identifying misinformation - Sharing of results and experiences LESSON II - Fact-checking strategies - Testing plug-in in groups - Explainers - Sharing experiences - Tips from teachers PRE-TEST - Background variables - Attitudes towards news - Strategies of credibility - Test: 2 fake items and 1 credible item POST-TEST - Attitudes towards news and InVID- WeVerify - Strategies of credibility - Test: 2 fake items and 1 credible item different from pre-test FINALISING Researchers updated lesson plan, handouts and slides from teacher input Figure 1. Study design. Information 2021, 12, 201 5 of 25 1.3. The Present Study Noting the major challenge of fake news, the limited ability of pupils to navigate information in digital environments and the existence of digital fact-checking tools, we see an opportunity to answer the calls for media and information literacy interventions with a design-based research approach. In this study, we investigated whether a two-hour educational intervention in four European countries, using a computer-based tool that was designed for professional fact-checkers, could stimulate pupils’ visual literacy and make them better at determining the credibility of digital news. We explored the following hypotheses: Hypothesis 1. Pupils will become more skilled at assessing digital news after the intervention. Specifically, we expected the following: a The total post-test score will be significantly better than the total pre-test score. b The ability to debunk false and true items separately will also be significantly better in the post-test. Hypothesis 2. Better performance in assessing information will be facilitated by the use of digital tools. In addition, we investigated the following exploratory research questions: Q1 How do media attitudes and digital information attitudes vary across countries? Q2 How does performance on pre- and post-test vary across countries? 2. Materials and Methods 2.1. Participants A total of 373 upper secondary school pupils, aged 16–18, across the four countries participated in the lessons and responded to the questionnaire during the Autumn term of 2020. All of the pupils agreed to complete the pre-test with anonymised responses for research purposes (with informed consent in line with the ethical guidelines of all countries). Of 373 pupils, there were 238 who took both the pre- and post-test, and this was the number of participants that we used in the analyses. The number of complete responses in each country was: 59 in France, 22 in Romania, 47 in Spain, and 110 in Sweden. The gender distribution was: 144 girls, 83 boys, and 11 pupils, which indicated that they did not wish to specify their gender or identified as non-binary. The different sample sizes in each country were primarily due to lockdowns and challenges that are linked to schooling during the Covid-19 pandemic. 2.2. Material Media and information literacy theories, such as transliteracy, civic online reasoning, and technocognition, all emphasise the importance of using digital tools when evaluating online information. The plug-in InVID-WeVerify is such a tool and it offers multiple functionalities that provide support to users when verifying videos and images [ 66, 67 ]. InVID-WeVerify makes it possible to (a) conduct reverse image searches with multiple search engines (e.g., Google, Yandex, Bing); (b) analyse images with forensic filters to detect alterations in their structure, such as quantisation, frequencies, colours, and pixel coherence; (c) scrutinise the details of an image with a magnifying lens; (d) fragment videos into key-frames; and, (e) retrieve metadata about videos and images. Thus, the tool is designed to support advanced media verification strategies. However, introducing a professional tool for fact-checking in education may have lit- tle effect if the tool is not understood or is found to be unusable by teachers or pupils. General media literacy principles reinforce the need for sense-making uses of technology in terms of knowledge acquisition and societal values, recommending that the tool or operational device not be the main entryway to problem-solving [41 ]. This is consistent Information 2021, 12, 201 6 of 25 with prior research pointing to the fact that dialogue with teachers regarding technology is essential in educational design-based research [ 61]. Therefore, we initiated our endeav- our with a study design phase, in which 34 teachers from France, Romania, Spain, and Sweden participated in focus group discussions with the aim of testing the tool and pro- viding feedback on the usefulness of the tool in education (for a complete design overview, see Figure 1 ). Focus group discussions were organised to assess the perception of disin- formation by teachers in their local context and their perspectives on InVID-WeVerify functionalities, especially in relation to image reverse search, automated video key frames extraction, and image forensics. The findings from these focus group discussions high- lighted that implementing the tool in class may be very complicated and pointed to a need for scaffolds [68] . The results also highlighted some cultural differences; for instance, challenges may be greater in Romania than in Sweden due to the different media cultures and technical resources available in the two countries. Learning from teacher feedback, we designed educational materials to scaffold the use of the tool in classrooms. This was achieved through close collaboration between teachers and researchers. Researchers from the participating countries discussed and created materials and methods for stimulating transliteracy and technocognition. These materials were then introduced to a teacher who tested and piloted them in teaching and then provided feedback. In this phase, we also de- signed and piloted credible and fake news items for use in pre- and post-tests (see example, items in Appendix A). The final educational design, limited to a two hour intervention, was agreed upon by 16 social studies teachers and eight researchers that were situated in the four countries. Materials for the educational intervention included a lesson plan for teachers, hand- outs for use in the classroom, and presentation slides. The educational intervention was introduced with an initial 60 min lesson on the theme ’News in a world of fake news: Definitions, credibility, and identification of different types of misinformation’, and in- cluded a combination of lectures (presenting concepts that are linked to misinformation, examples of fake news, and summing up discussions) and active exercises for pupils where they were asked to (a) come up with news sources (see Figure 2) and (b) identify different types of misinformation. The lesson was concluded with a sharing of results and collective discussions about what the pupils learned from the lesson. Figure 2. Pupil active group task designed to stimulate conversation and link the educational content to pupils’ perceptions of news. The second lesson, which was also 60 min in duration, focused on ’Individual defence against misinformation and disinformation: Understanding image manipulation with InVID-WeVerify’. This lesson started with a short lecture on how fact-checkers use lateral reading and verify information by considering: (a) Who is behind this information? (b) What is the evidence? and (c) What do other sources say? [9]. The teacher acted as a fact-checker by conducting a reverse image search and forensic analysis of an image with InVID-WeVerify. Next, the teacher showed a fake video and verified this using the InVID- WeVerify key frames analysis. Thereafter, the pupils downloaded the plug-in and worked Information 2021, 12, 201 7 of 25 in groups of two to three to verify images and videos with InVID-WeVerify. The pupil task (Figure 3) was provided in PDF format, which makes it possible for them to click on hyperlinks and use InVID-WeVerify ’in the wild’ with authentic examples of misleading images and videos. Figure 3. Pupils’ task designed to stimulate and scaffold their use of InVID-WeVerify. The step-by-step task was designed to prompt pupils to use different techniques to debunk images and videos. After the group work, the teacher showed slides explaining how to debunk the fake images and video using InVID-WeVerify (see Figure 4) and dis- cussed, in class, what the pupils had learned. Summing up the two lessons, the teacher then presented some information regarding how to navigate online news in clever ways with tips, such as: (a) set filters on credible sources (i.e., reviewed news feeds from established news media); (b) be careful about frames and deceptive design (what looks great may be the very opposite); (c) rely on several search engines; (d) look for other independent sources, double check!; (e) think first then share—share with care!; and, (f) stay cool! 2.3. Procedure With an aim to develop new methods and materials that are useful in the complexity of everyday classroom practices, we made sure to collect a rich set of data, enabling us to investigate the possibilities and challenges of this educational design [ 56 ,64 ]. The inter- vention took place in October 2020 during the Covid-19 pandemic, presenting us with a special challenge in conducting the educational effort. It was initially planned for March 2020, but was postponed due to school lockdowns in three out of the four countries. The in- tervention started and ended with an online questionnaire, which included a test with two fake news test items (a misleading image and a misleading video) and one credible news test item. We made sure to include both credible and fake information, because scholars have noted that exposure to fake news may lead to diminished levels of trust in credible news [ 69]. We used different items in the pre- and post-tests, and counterbalanced these items between groups to ensure that the results would come from the intervention and not the test items. Test items—one true news item, one item with a manipulated image, Information 2021, 12, 201 8 of 25 and one fake video—were introduced with the following instruction: ’You will now be asked to rate the credibility of news on social media. When you do this, you are free to double check the information online’. The items were presented in a randomised order to minimise the order effects. All of the items included were social media posts intended to ’go viral’, which is, they were designed to attract attention, clicks, and shares. Figure 4. Explainers informing how to debunk fake images and video using InVID-WeVerify. The pre- and post-test also included questions regarding the pupils’ use of digital tools when they assessed the credibility of the information. We asked: ‘When you answered the questions in the survey, did you use any digital tools? For example, did you use Google or reverse image search to examine the credibility of the news? Yes/No’. If they checked ‘Yes’, we asked them: ‘How did you use digital tools to help you examine the credibility of the news? (check multiple boxes if you did multiple things) (a) I used text searches (for instance on Google), (b) I did reverse image searches, (c) I used multiple search engines, and/or (d) other (please specify)’. We also asked the pupils questions regarding their background, their attitudes towards news, and how they usually determine credibility (see Appendices B and C). These factors have been identified as important in previous research and in research highlighting the complexity of media and information literacy [13 ,14, 70]. In order to investigate the pupils’ self-perceived skills and attitudes, we asked them to rate their ability to find and evaluate information online and their attitude towards credible information sources in line with previous research [ 13 , 14]. Answers were given on a five-point scale (see Appendix B Question 3–6). The participants were then asked to rate statements on their strategies to determine the credibility of news on a scale from 1 (never) to 7 (very often), with questions being adapted from Frunzaru and Corbu [70] (see Appendix B Question 7). In the post-test, we asked the pupils to rate their user experience of InVID-WeVerify in order to assess their perception of the visual verification tool. All of the questions in the tests were asked in the native language of the pupils. We also interviewed teachers and asked them (a) what worked well in the teaching, (b) problems in the teaching, and (c) suggestions for improvements. Information 2021, 12, 201 9 of 25 2.4. Design The study design was a repeated measurement using pre- and post-tests around the educational intervention with the InVID-WeVerify-tool, where the order of the pre- and post-test items were counterbalanced to avoid the order effects. 2.5. Analysis We transformed false item scores by subtracting each false score from the maximum rating for each false item, so that a high score signified good performance. We also reversed the items in the news evaluation test, where higher ratings indicated less awareness of the need to fact-check information. We then summed all of the pre- and post-test items for the respective order conditions to yield two scores, a pre-test score and a post-test score, respectively. We made a two-way mixed ANOVA with time as the repeated measure, and the use of digital tools on post-test and language as the between-subjects variable, and total test score as the outcome variable, in order to analyse performance in relation to the hypothesised relationships. Because the sample sizes in each country were unequal, we followed Langsrud’s advice [ 71 ] and made ANOVAs with Type II squares for unbalanced design. Essentially, the Type II sum-of-squares allows for lower-order terms to explain as much variation as possible, adjusting for one another. Because of non-normally distributed data, we analysed the post-test scores on false and true items with the Wilcoxon rank-sum test, with the use of digital tools as the independent variable. Next, we investigated how the attitudes differed between the countries. Summary statistics of all attitudes are provided in Tables A1–A11 in Appendix C; only statistically significant differences are discussed here in the text. For the self-rated skills and attitudes in relation to digital proficiency, we ran one-way ANOVAs using the Type II sum-of-squares, with language as the independent variable. For the self-rated attitudes towards news and attitudes towards the digital tool, we made pairwise t-tests for each attitude, separately rating pre- and post-tests for each language. 3. Results 3.1. Performance Test The maximum total score on the performance test was 21, with each item providing a maximum score of 7. The mean total score on pre-test across all countries was 12.2 (SD = 3.2), and the mean total score on post-test was 13.2 (SD = 2.9), a statistically significant difference (t(467.63) = 3.58, p < 0.001). Table 1 presents the pre- and post-test performance for each language version of the tests. The median total score on false items on pre-test was 9 (MAD = 3.0) and 10 (MAD = 3.0) on post-test, a difference that was statistically significant (W = 34742, p < 0.001). For the true items, the median on the pre-test (3; MAD = 3.0) did not result in a statistically significant difference (W = 26,283, p = 0.29) from the median on the post-test (3; MAD = 3.0). Table 1. Mean total scores on pre- and post test for the separate languages with standard deviation in parentheses. Measure French Romanian Spanish Swedish Pre-test score 12.1 (3.2) 10.5 (4.5) 12.1 (3.1) 12.5 (2.8) Post-test score 13.1 (2.6) 13.7 (3.4) 13.9 (3.4) 12.7 (2.7) 3.2. Differences in Pre- and Post-Test Scores in Relation to Use of Digital Tools and Language Regarding the use of digital tools, 14% of the participants stated that they had used digital tools in the pre-test and 44% stated that they had used digital tools in the post-test when evaluating the news. Table 2 presents the digital tool use in pre- and post-test for each language version of the tests Information 2021, 12, 201 11 of 25 on true post-test items, there was also a statistically significant difference (W = 5383, p = 0.007 ), with an advantage for those using digital tools. We present medians and median absolute deviations in Table 3. Using digital tools is clearly advantageous; however, there is also greater spread (MAD) in post-test scores within the group using digital tools as compared with the group not using digital tools. Table 3. Medians and median absolute deviation (in parentheses) for scores on total false post- test (maximum 14) and true post-test (maximum 7) for participants using/not using digital tools on post-test. Type of Items No Digital Tools Digital Tools Total false post-test 10(3.0) 11(1.5) True post-test 3(1.5) 4(3.0) 3.4. Attitudes For the digital attitude scale, we report the means in Table 4 and, for the news eval- uation scale and InVID-WeVerify attitudes, we refer to Tables A1–A11 and Table A12, respectively, in the Appendix C. For the self-rated attitudes and skills, the participants were provided with a five-point rating scale. Table 4. Means and standard deviations (in parentheses) for fact-checking ability, search ability, internet info reliability, and credibility importance based on a five-point rating scale. Attitude Measure M(SD) Fact-checking ability 3.3(0.9) Search ability 3.6(0.9) Internet info reliability 2.8(0.6) Credibility importance 4.3(0.9) The mean scores were moderate for the media and information literacy attitudes, except for internet info reliability and credibility importance. The pupils were quite sceptical about news on the internet (info reliability) and valued credible news highly (credibility importance), which may be due to the fact that they were just about to go through a fact-checking intervention, but it may also reflect a more general scepticism of online information and a positive attitude towards credible news. In addition, pupils rate their ability to assess online information (fact-checking ability and search ability) quite highly. 3.5. Self-Rated Attitudes and Skills For self-rated fact-checking ability (see Figure 6A), there was a statistically significant difference between languages (F(3234) = 12.36, MSE = 0.72, η2 p = 0.14, p < 0.001). Tukey corrected pairwise comparisons showed a statistically significant difference between Spanish and Swedish (p < 0.001), as well as between French and Swedish (p < 0.001). For self-rated search ability (see Figure 6B), there was a statistically significant dif- ference between languages (F(3234) = 17.32, MSE = 0.67, η2 p = 0.18, p < 0.001). Tukey corrected pairwise comparisons showed a statistically significant difference between Span- ish and Romanian (p < 0.001), French and Romanian (p < 0.001), Swedish and French (p < 0.001), and Romanian and Swedish (p < 0.001). For the ratings of internet info reliability (see Figure 6C), there was a statistically signif- icant difference between languages (F(3234) = 12.36, MSE = 0.32, η2 p = 0.039, p = 0.024 ). Tukey corrected pairwise comparisons showed a statistically signi","Nygren, T., Guath, M., Axelsson, C. A. W., & Frau-Meigs, D. (2021). Combatting visual fake news with a professional fact-checking tool in education in France, Romania, Spain and Sweden. Information, 12(5), 201."
ML_DB_100,Invisible Friend or Foe? How Journalists Use and Perceive Algorithmic-Driven Tools in Their Research Process,"The use of algorithmic tools by journalists for information-gathering has received particular attention in recent years. While it might facilitate the research process, there are also concerns about their impact on journalism. Based on reconstruction inter views with 27 journalists, we first answer the primary question to what extent journalists actually use algorithmic-driven tools for research purposes. Then, we analyze which folk theories journalists create during their use of algorithm-driven tools. Results show that algorithmic tools specifically designed for the journalistic research process are rarely or not at all used. Yet, more crucially, search engines and social media, that are driven by algorithms, play a major role when it comes to the search, selection, and verification of sources and information. However, journalists are not aware of this hidden research assistant facilitating their research process. When explicitly asked, they profess specific notions regarding the use of algorithmic-driven tools in the form of folk theories, which are predominantly negative regarding the influence of AI on journalism. At the same time, there is a still a strong feeling of a professional authority among journalists who feel they are able to work autonomously of any kind of influence, including algorithms.",Journalism Studies,Article,2022,Y,Y,0,"Invisible Friend or Foe? How Journalists Use and Perceive Algorithmic-driven Tools in Their Research Process The number of online methods for finding, selecting, and verifying sources available to journalists has surged during the last two decades. Through websites, search engines, and social media, journalists are able to consult a wide range of both elite and non-elite sources (Lecheler and Kruikemeier 2016; Van Leuven et al. 2018). Among online methods, the use of algorithmic tools by journalists for information-gathering has received particular attention in recent years (Von Nordheim, Boczek, and Koppers 2018). Research shows that these tools can be used for multiple purposes, such as selecting sources (Peterson-Salahuddin and Diakopoulos 2020; Thurman et al. 2016), detecting trends or finding story ideas (Diakopoulos 2020; Fletcher, Schifferes, and Thurman 2020), analyzing big data sets (Stray 2019), and for fact-checking purposes (Hansen et al. 2017). As a consequence, algorithms can be considered the journalist’s ‘research assistants’. While algorithmic-driven tools might facilitate the research process, there are also concerns about their impact on journalism in democracies. Most fundamentally, many algorithmic-driven tools are not specifically designed and developed with journalistic values and norms in mind (Diakopoulos 2019). Rather, algorithmic-driven tools struc- ture what information is easiest to find, and emphasize or de-emphasize specific sour- ces of information following certain rules pre-determined for them (Hamilton et al. 2014). This has raised concerns about algorithmic bias in, for instance, social-media tools for journalism (Thurman et al. 2016). This might mean that journalism no longer follows its own rules and norms in finding, selecting and evaluating information, but must consider its performance in relation with outside elite actors, such as tech companies. This has obvious consequences for how we conceptualize independence in journalism, and the fourth estate” role of journalists in democracies (Helberger et al. 2019). When looking at journalists themselves, this problem becomes even more pertinent. To journalists, algorithms are often black boxed, meaning that the technology and its workings are complex and untransparent that they are difficult to (fully) understand (Diakopoulos, Zhang, and Salway 2013). Making use of algorithms might not only influence or steer the research process and pose a threat to the independence of jour- nalism in democracies, journalists might not even know or understand the process of influence in the first place. This will also have negative consequences for the way jour- nalists gather, select, analyze, and verify information (Jamil 2020). For instance, when journalists go after breaking news” on social media, the information that is shown in the ‘timelines’ of journalists depends on various automated decisions resulting from various algorithms, possibly trapping them in an information feedback-loop they may not know how to escape (Gillespie 2014). So far, we have very limited knowledge about the extent to which journalists are aware of the use and influence of algorith- mic-driven tools or software and how this might guide their work (Fletcher, Schifferes, and Thurman 2020; Thurman et al. 2016). Therefore, in this study, we seek to under- stand to what extent journalists are aware of the algorithmic forces possibly influenc- ing their research activities. Based on reconstruction interviews with 27 journalists, we first answer the funda- mental question of the extent to which journalists actually use algorithmic-driven tools for research purposes. Then, we apply Peterson-Salahuddin and Diakopoulos (2020) concept of ‘algorithmic folk theories’ and test which folk theories journalists create during their use of algorithm-driven platforms. Because it is hard to understand how algorithms actually work, we expect that journalists create their own beliefs on what they exactly are and how they work. Subsequently, we try to understand how these folk theories influence the journalistic research process and which mitigating strategies journalists use to counter a possible unwarranted influence of algorithms on their research process. Together, this study is one of the first to use journalists’ own 2 Y. DE HAAN ET AL. perceptions in understanding the use and awareness of algorithmic-driven tools. We also argue that the use of algorithmic-driven tools at the start of the information-pro- duction process has normative implications in mediated communication. Using Algorithmic-Based Tools in Journalistic Research Before proceeding it is important to create more clarity on the concepts of AI and algorithms. While the concept of AI refers to technologies that outperform tasks of human intelligence, in practice, or at least in the media sector, we still talk of narrow AI. This is a bundle of technologies and techniques collection of ideas, technologies, and techniques that relate to a computer system’s capacity to perform tasks normally requiring human intelligence”. AI is driven by algorithms, a series of steps that is undertaken in order to solve a particular problem or to accomplish a defined out- come” (Diakopoulos 2015: 400). Algorithms can be automated to make autonomous decisions. This is the case when using search engines or social media or specific tools to search and analyze large amounts of data. In this article, we therefore use the term algorithmic-driven tools to indicate alle types of tools and software that make use of algorithms, including search engines. While a considerable number of studies have delved into the use of AI in the field of journalism, much of this research has focused on the production phase (2020; Dierickx 2020; Diakopoulos 2019; Jones and Jones 2019; Kunert 2020) and the distribu- tion phase (Bod_x0013_o 2019; Nechushtai and Lewis 2019 ; Trielli and Diakopoulos 2019). The use of AI in the production phase focuses on how AI facilitates the editing, formatting and production of a journalistic story. This can also relate to translation software or to on robot journalism, where articles are automatically generated. Studies on the distri- bution phase of journalism relates to how AI can facilitate in the dissemination of the journalistic content to the public. While, in this digital era the research and production phase are sometimes intertwined as journalists can publish articles online while still doing research and adapting the article along the way (Diekerhof 2021), we still can distinguish the research phase in which information and sources are found, selected, analyzed, and verified. Studies about the use of algorithms in the research phase of journalism in democracies remains fragmented. Based on a current state-of-the-art, we identify four distinct ways how algorithms can assist in the research process. First, algorithmic-driven tools may play a role in the selection of sources and infor- mation (Peterson-Salahuddin and Diakopoulos 2020; Thurman et al. 2016). The use of search engines and social media for this purpose being a prevalent example. Previous work shows that the use of search engines is so ingrained in journalists’ day-to-day practices, that journalists might not consider this algorithmically guided information (Hornmoen et al. 2017). Tylor (2015) found that journalism students click on the first two search results as often as they do on all the subsequent results together, indicat- ing that the algorithms behind search engines may have an effect on the selection of sources. Thurman and colleagues (2017) argue that the academic world ‘has failed to recognize fully the changes to journalistic sourcing practices brought about by social media’ (Thurman, D€orr, and Kunert 2017: 838). Caplan and Boyd (2018) even state that algorithmic platforms, such as social media and search engines, have power over what DIGITAL JOURNALISM 3 is seen as newsworthy. This underscores the importance of finding out not only which tools journalists use, but also to what ends and how they do so. Second, moving to more specific tools, certain software can help suggest story ideas (Diakopoulos 2020; Fletcher, Schifferes, and Thurman 2020). Studies have focused on analyzing specific tools, such as SocialSensor, that helps detect news trends on social media (Thurman et al. 2016) and INJECT that suggested story angles for Norwegian newspaper journalists (Maiden et al. 2020). We do not know if specific software to sug- gest story ideas or angles is commonly used in daily practice. The third way algorithms are used in the research process revolves around the ana- lysis of datasets. Algorithms are found to be of possible value in preparation, by, for instance, collecting data from different documents (data extraction), suggesting pos- sible connections or language analysis (Gutierrez-Lopez et al. 2019). Particularly, for investigative journalists this is believed to be promising. Yet, research so far shows that algorithmic-driven tools are still not commonly used among investigative journal- ists as they face problems of data availability and do not readily provide unique sto- ries (Stray 2019). The fourth role that algorithms may play in the research phase, involves fact check- ing. Within journalism studies, most research of this application consists of case stud- ies, like those describing FactWatcher (Hassan et al. 2014) and Claimbuster (Hassan et al. 2017). Several authors mention difficulties that arise when attempting to adopt fact-checking tools, such as the lack of a database containing false information (Pathak and Srihari 2019). Often the existing fact-checking corpora are either too small in size, do not provide detailed annotations or are limited to a single domain” (Hanselowski et al. 2019, p. 1). Algorithmic-driven tools have the potential to facilitate journalists in their research purposes in multiple ways. At the same time, they also have the power to steer or influence the information flow of journalists, thereby threatening their independence. To understand this influence, we first need to know how journalists make use of algo- rithms in their daily working routine. Therefore, we pose the first research question: RQ1: How do journalists use algorithmic-driven tools in their research and selection process? Awareness of Algorithm-Driven Tools Using an algorithmic tool is one thing, but knowing that one makes use of it, is another. While many journalists use social media as a primary source for news (Lecheler and Kruikemeier 2016), an important question remains to what extent they are aware that these social media are driven by algorithms. There is much concern about the role of algorithms and filters in the use of news sources (Hamilton et al. 2014). As Diakopoulos states: Algorithms, driven by vast troves of data, are the new power brokers in society” (2014: 2). They present an outside influence to an independ- ent, self-confident and largely endogenous journalistic system honed in many democ- racies over decades (Schapals and Porlezza 2020). Therefore, it is of crucial importance to understand the extent to which journalists are aware of the use of algorithmic- driven tools as their virtual gatekeeping assistants. Do they know that their research process might be influenced by algorithms, and what do they do about it? 4 Y. DE HAAN ET AL. Studies among social media users in general show that users are often unaware that the selection of posts in news feeds or timelines is algorithmically generated (Eslami et al. 2015; Powers 2017). One might expect that journalists, who use these tools professionally and who are aware of the central role they play in public dis- course, have more knowledge about the way they work and their influence. The evi- dence here, however, is mixed. A survey of German journalists more than ten years ago, for example, found that many journalists were very critical of the content pre- sented by search engines, with little more than a quarter of them (26.8%) agreeing with the statement that the technology produced ‘neutral’ results (Machill and Beiler 2009). However, the same study found that they did not outperform the general pub- lic in search assignments, meaning that this critical attitude made little difference in practice. More recently, research has shown that journalists are quite critical when using social media as a source. However, this seems to be more due to the user gen- erated nature of the social media, than the algorithmic mechanisms behind these media (Brandtzaeg et al. 2016). Research among journalists who have more knowledge of computational journalism or algorithmic systems show for a need of more disclos- ure on algorithmic systems (Diakopoulos and Koliska 2017). Even if some journalists are unaware of the algorithmic nature of some of the tools they employ and the role AI might play in their work, this does not mean they do not have opinions about them. A poll conducted by Press Gazette, a British journalists’ trade magazine, found that 69% saw AI as a threat to independent journalism (Mayhew 2020). A global survey of ‘early adopter’ media professionals, who were inter- ested in the topic, found this group (perhaps unsurprisingly) to have a rosier outlook, but 24% still saw cultural resistance to AI in newsrooms as an obstacle to adoption (Beckett 2019). Research shows that these sentiments can have real consequences for newsroom choices. The perceived compatibility with journalistic ideals is an important factor driving the adaptation of technology in the newsroom (Milosavljevi_x0013_c and Vobi_x0014_c 2019). This means that to ascertain a full picture of the use of algorithmic-driven tools, one needs to also consider attitudes that exist towards the technology among journalists. Folk Theories on Algorithmic Driven Tools Our review of existing literature suggests a research gap concerning journalistic atti- tudes towards the use of algorithmic-driven tools. Studies of the general public (Thurman et al. 2019, Logg, Minson, and Moore 2019) used either questionnaires or experimental settings, giving test subjects a clear choice between human and algorith- mic input. In both approaches, respondents are explicitly made aware of the role algorithms and AI play. However, since journalists might not be aware of the algorithmic-driven nature of tools they employ, such an outspoken approach might be ill-suited to reveal implicit attitudes. To add to the problem, most software used in newsrooms is proprietary, meaning even expert users might be unable to deduce its inner workings. In the case of AI-systems this is further complicated by the fact that even its developers might be unsure of its actual functioning: the black-box problem (Castelvecchi 2016). DIGITAL JOURNALISM 5 This presents a dilemma. How can we study the use of untransparent technology by users who are (partially) unaware of their use? Other researchers have neatly circum- vented both these issues by looking only at the interaction between the user and the technology. Bucher (2017), for instance, explored users’ perception of Facebook’s news feed algorithm through interviews, proposing the ‘algorithmic imaginary’ as a guiding principle for interpreting the behavior of social-media users. In Bucher’s own words, the algorithmic imaginary is not to be understood as a false belief or fetish of sorts but, rather, as the way in which people imagine, perceive and experience algorithms” (p. 31). A concept that expands this idea further, looking at implications for behavior and learning, are ‘folk theories’. Gelman and Legare (2011) define these as implicit and imprecise but ... intuitive theories that have broad implications: they organize experi- ence, generate inferences, guide learning, and influence behavior and social inter- actions” (p. 379). The concept has been used successfully to study interaction between algorithmic systems and the general public. An open-answer survey carried out in Norway identified a number of folk theories of algorithms in general (Ytre-Arne and Moe 2020). Studies looking at the role folk theories played in user interaction with social-media algorithms found that they were key in driving user behavior. For instance, a study of user backlash against proposed algorithmic selection in Twitter timelines (DeVito, Gergle, and Birnholtz 2017). In depth-interviews of social media users suggest that users draw on diverse sources of information in developing folk theories and that they evolve over time (French and Hancock 2017). More recently, folk theories of social media have also been successfully identified in the newsroom (Peterson-Salahuddin and Diakopoulos 2020). Focusing on journalists’ thoughts about social-media platforms, researchers identified a common thread run- ning through all folk-theory understandings, finding their interviewees understood social media distribution algorithms as filters that did or did not allow audiences to be exposed to their content” (p. 30). Folk theories differed in the perceived effect pub- lisher attributes, engagement, and properties of specific social-media platforms had on this process. Interestingly, the editorial process involved a constant negotiation between folk- theory understanding of social-media algorithms and traditional journalistic values. (I’m not going to manufacture ... a story that isn’t really in line with [editorial practice] just because that’s what Facebook’s algorithm likes.” p. 34). This makes the folk-theory lens seem like a promising way to identify comparable strategies with regards to AI in a newsroom setting. Drawing on this concept, and recognizing the necessity to deter- mine awareness of tool use, we define our second research question as follows: RQ2: To what extent are journalists aware of their use of AI and what folk-theories exist surrounding it? Method The goal of this study is to understand if, and for which purposes, journalists make use of algorithm-driven tools in their research process and to detect a possible algo- rithmic influence in the information journalists use. We also aim to create insight into the way journalists perceive algorithmic tools, and what this means for how they use 6 Y. DE HAAN ET AL. them. In January 2020, prior to this study, we conducted ten interviews with journal- ists in the Netherlands and asked them which algorithmic tools they used. However, this approach turned out not to be successful, as many journalists could not spontan- eously mention any tools they used. When provided with a list of tools, many journal- ists were not acquainted with them. We realized that a lack of algorithmic awareness is the underlying problem here. Therefore, for this study we decided to conduct recon- struction interviews in which we discussed one or two specific journalistic stories by the interviewee. This way, we were able to distill their research steps in detail, without creating confusion about the terminology. This approach also enabled us to discuss steps that are taken largely subconsciously and helped create a complete picture of the tools the journalists use. Subsequently, semi-structured follow-up questions were asked to gain insight into the awareness of the possible influence of the tools used, and of the impact of AI and algorithms on the profession more generally. The reconstruction interviews took place with journalists in the Netherlands. As in many European countries, news organizations in the Netherlands are experimenting with AI applications, for research, production and distribution purposes (Mediaperspectives 2021a). A few newsrooms have been experimenting with auto- mated applications to generate stories, developing their own AI robots (Gieling, 2021). While the industry embraces innovative AI technologies, calls for responsible AI are growing. Several media companies, including Dutch public broadcaster NPO, commer- cial broadcasting companies RTL and Talpa and the two major publishers Mediahuis and DPG Media have signed a letter of intent to adhere to ethical guidelines for the use of AI (Mediaperspectives 2021b). In this study, we choose to look at ordinary journalists that spend much of their daily work finding, selecting and verifying information, and not technology experts or early adopters in the newsroom (Beckett 2019) or publishers or CEOs of media organi- zations (Newman 2021). We selected the interviewees based on their work-setting and frequency of publication. First, while large newsrooms might facilitate and encourage the use of algorithmic tools, freelancers or small newsrooms might be less aware or have less opportunities. Second, beat journalists might use other tools and for other purposes than investigative journalists (Stray 2019). In total, 27 journalists were inter- viewed, ranging from journalists working as freelancers, working in small newsrooms (up to 10 journalists) to large newsrooms (more than 100) and ranging from journalists who publish daily, or weekly, to journalists who work on large investigative produc- tions for a longer period of time (see Table 1). For reasons of diversity, we invited jour- nalists from a range of news organizations, including national and regional newspapers, broadcasters and online-only platforms. Prior to each interview, the interviewer selected one or two stories that the journal- ist had recently produced. By asking the journalist to draw a timeline showing every single step and discussing these, the interviewer tried to reconstruct the process in as much detail as possible. They specifically asked where information was found and selected, whether specific tools were used for selection and verification, and whether other tools were used to facilitate the research process, including those for data stor- age, translation, etc. If the interviewer detected the use of algorithm-driven tools, the DIGITAL JOURNALISM 7 journalist was asked about their knowledge of and viewpoints on such tools. The second part of the interview focused on the role of AI in journalism more generally, and the possibilities and challenges they might create for the journalistic research pro- cess. Each interview took approximately 1.5 hours and most were conducted using online teleconferencing software, due to the lockdown instituted in the wake of the COVID-19 pandemic. Two researchers conducted the interviews between January and April 2021. The first five interviews were done together in order to test the topic list. Afterwards, the interviews were conducted separately. All the interviews were transcribed by two research-assistants and analyzed by the two researchers using the qualitative soft- ware Atlas.ti. A grounded-theory approach was taken in which different rounds of coding was done (Strauss and Corbin 1990). The two researchers first coded, staying close to the material, after which in the following rounds of coding they rearranged the codes and formed thematic categories (Salda~na 2013). The final coding tree resulted from an iterative process of reflection between all the authors of this article. The coding tree consisted of five general codes including activities, explicit use of AI, job, tools, and knowledge of AI. These were then specified, resulting in 56 sub-codes, including different journalistic activities such as editing, image manipulation, polling, factchecking or generating story ideas. Knowledge of AI was sub-coded with the codes AI policy, mitigating strategies, sentiment towards AI and tool suggestion. In the last round, different reflection sessions were done to come to overarch- ing categories. Results To understand the possible influence of algorithms in the journalistic research process, we first need to know to what extent they are actually used. Our literature review shows that algorithmic tools can be used for multiple purposes, including selection of Table 1. 27 respondents to represent a cross-section of journalism in the Netherlands. Publication frequency High Medium Low Large newsrooms Online journalist Wire reporter Multimedia datajournalist Entertainment newspaper reporter Newspaper reporter Documentalist Newspaper reporter Investigative radio reporter Newspaper reporter Teamsize Medium newsrooms Online journalist Online journalist Online journalist TV- news producer Print magazine reporter TV- news producer Radio news producer Survey specialist Investigative TV journalist Investigative TV journalist Small newsrooms Radio news producer Investigative online reporter News magazine online editor Investigative online journalist Individuals (freelancers) Investigative online journalist Feature print reporter Foreign correspondent Feature print reporter Respondents were drawn from newsrooms of various sizes and selected according to publication frequency. This sample method ensured the full spectrum of Dutch journalism was represented on both measures. 8 Y. DE HAAN ET AL. sources and information, suggesting story ideas, analyzing datasets, fact checking, and generating stories. However, our study shows that the journalists we interviewed hardly make use of specific algorithmic tools. Nevertheless, every research process is algorithmic-driven: it heavily leans on search engines and social media tools, in par- ticular for the selection and verification of information. Search Engines and Social Media Tools Dominate in the Research Process Search engines, particular Google, are used in every step of the research process by every journalist we spoke to, except for one, who resorts to Duck Duck Go for privacy reasons. They are used for classic research purposes such as finding, selecting, and verifying sources and information. As a journalist of a current affairs magazine said: When I look for interviewees, I want people who are recognizable to the reader. So, basically, I ask around and Google a little. And one finding takes you to the next. Because you Google someone and you find a piece with another name, and then you Google that name again. And you continue like that. While newsbeat journalists are more likely to start with Google and switch to offline methods to contact sources for verification purposes, investigative journalists also fre- quently make use of niche search engines such as PubMed, Global Forest Watch and LexisNexis. They also are better acquainted with specific Google applications such as using Google Earth to compare images of a location over time. Moreover, investigative journalists make use of documentarians at the newsroom, specialized in thorough research of (historical) documents. Both investigative and newsbeat journalists frequently use social media, for instance to analyze how audiences discuss certain topics, to search for sources and get in contact with them, to discover what is happening at a certain place in real time and to find out what topics are discussed by other media and how. In addition, investigative journalists may also use social media to verify accounts, to check if a source’s statement is credible or to discover links between different social media accounts. As such, their searches on social media seem more thorough. An investigative journalist explained: Some information is online but might not typically be found through Google quickly. Then you really need to go to social media, through someone’s accounts and their activity online. And not necessarily only through official channels, but also just private ones. While verifying is done in the process of selecting information, none of our inter- viewees made use of automated fact-checking tools. Instead, fact checking among investigative journalists leans partially on cross-checking different search engines (e.g. niche search engines, Google Reverse Image Search, Google Earth), social media, self- collected data and official documents, but also strongly on (offline) contact with sour- ces and official institutions. Verification for newsbeat journalists often means checking what other media and official institutions report (often using search engines), as well as fair hearing of sources. The use of search engines and social media can hardly be ignored in the journal- istic research process. These algorithmic-driven tools are fully engrained in journal- ist’s research process. Besides for selection and verification purposes, specific tools such as feed readers (Inoreader, Feedly, Tweetdeck and CrowdTangle) are used to discover topics that are much debated on or often shared on social media, and, thereby, provide insight into the subjects and events that might be newsworthy. Such tools are used to help journalist find story ideas, particularly for beat journal- ists. Though investigative journalists could potentially use automated tools to help analyze datasets, the evidence for the use of such tools by our interviewees is merely anecdotal. In sum, while the debate on AI in journalism has increased exponentially the past few years, algorithmic tools specifically designed for the journalistic research process are rarely or not at all used by our interviewees. Yet, more crucially, search engines and social media, that are driven by algorithms, play a major role when it comes to the search, selection, and verification of sources and information. However, it remains unclear how journalists perceive these tools and to what extent they are aware of the algorithms driving them. Awareness and Knowledge Dominated by Folk-Theories While hardly any specific algorithmic tools are used by journalists, as we mentioned above, search engines and social media tools are embedded in the daily working rou- tine of journalists. In our interviews, we investigated whether journalists are aware of this, asking to what extent interviewees thought algorithms and AI had an effect on their work. In response, only a minority of journalists spontaneously named the algo- rithmic-driven tools they actually used. Those that did were mainly investigative jour- nalists or journalists that possessed a relatively sophisticated understanding of technology. For an investigative TV-reporter, working for an investigative news-pro- gram, the effect Google had on his work loomed large. Sometimes it can determine a lot. That’s what I had in this case. Some guy had written a critical piece and he came out on top. So I call him. Great source, great conversation. He’ll make it to the show ... .most likely the content is doing its work here ... Google thinks content is more important. That’s what earned him the top spot. Probably a lot of other people find this interesting, so this critical piece makes it to the top, since Google thinks that is important. The remaining responses mainly fell into two categories. The first did not know much about algorithms and AI. Particularly, they were not aware of how these tech- nologies are related to their own work at the newsroom. As one journalist said: I don’t know much about it. I’m really only involved with the journalistic process”. And similarly, another journalist said: I really don’t see the link with my work”. The second category of journalists did assume that algorithms and AI might have consequences for journalistic work, but just not their own. To them, the newsroom is involved with the core journalistic processes which does not involve the use of AI. Other depart- ments such as the data, sales or R&D development might make use of it. Others stated that at management level, there is much talk about AI, but not in the newsroom. A different picture emerged when the interviewees were prompted more generally about the effect the emerging presence of AI was having on journalism in general. As the journalists we interviewed were not aware or did not feel specifically involved with 10 Y. DE HAAN ET AL. algorithms in their daily work, we subsequently moved beyond the specific research process to understand how they perceive AI in general, trying to understand if they then might feel it can possibly influence their work. Here, the folk-theory concept proved a highly useful concept for categorizing notions held by journalists. As defined by Eslami et al. (2016), folk theories are a method for users to navigate the use of tech- nology that they cannot truly understand. Whether they are accurate or not is beside the point. They are a way in which users make sense of their own behavior. What interviewees’ conceptions of algorithms and in more general terms of AI had in common was that most defined it as a relatively autonomous force. The inter- viewees’ observations quickly fell apart into two main non-exclusive categories: seeing AI as dangerous or as a (potential) help. The former was far m","de Haan, Y., van den Berg, E., Goutier, N., Kruikemeier, S., & Lecheler, S. (2022). Invisible Friend or Foe? How Journalists Use and Perceive Algorithmic-Driven Tools in Their Research Process. Digital Journalism, 1-19."
ML_DB_109,Believe it or not: Designing a human-AI partnership for mixed-initiative fact-checking,"Fact-checking, the task of assessing the veracity of claims, is an important, timely, and challenging problem. While many automated fact-checking systems have been recently proposed, the human side of the partnership has been largely neglected: how might people understand, interact with, and establish trust with an AI fact-checking system? Does such a system actually help people better assess the factuality of claims? In this paper, we present the design and evaluation of a mixed-initiative approach to fact-checking, blending human knowledge and experience with the efficiency and scalability of automated information retrieval and ML. In a user study in which participants used our system to aid their own assessment of claims, our results suggest that individuals tend to trust the system: participant accuracy assessing claims improved when exposed to correct model predictions. However, this trust perhaps goes too far: when the model was wrong, exposure to its predictions often degraded human accuracy. Participants given the option to interact with these incorrect predictions were often able improve their own performance. This suggests that transparent models are key to facilitating effective human interaction with fallible AI models.",Computer Science,Proceeding,2018,N,Y,42,"In designating October 2009 as National Information Literacy1 Awareness Month, former U.S. President Barack Obama drew national attention to a key 21st century information challenge: Though we may know how to find the information we need, we must also know how to evaluate it. Over the past decade, we have seen a crisis of authenticity emerge. We now live in a world where anyone can publish an opinion or perspective, whether true or not, and have that opinion amplified within the information marketplace. Historically, we have relied upon information literacy edu- cation in our schools and libraries to teach our citizenry key 1https://en.wikipedia.org/wiki/Information_literacy Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. UIST ’18, October 14–17, 2018, Berlin, Germany © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-5948-1/18/10. . . $15.00 critical reading skills, the importance of consulting multiple, independent sources, and to investigate the potential for under- lying bias in whatever we read. For this reason, information literacy has been advocated as a distinct skill set and a nec- essary key to one’s social and economic well-being in an increasingly complex information society.” [26] However, the internet era presents new challenges to con- sumers of information. As information generation has accel- erated, making sense of it has become increasingly difficult. While breaking down traditional barriers to authorship has democratized information exchange and dissemination, the massive growth of information production by less-established sources has created significant new challenges for readers in accurately interpreting and assessing the veracity of this bar- rage of content [47, 10]. The deluge of new online articles and sources means that is practically difficult for individuals to consistently manually cross-check sources. The rise of mis- information – unwitting or deliberate – has made it harder still for readers to tell fact from fiction. In response, researchers in machine learning (ML) and natural language processing (NLP) have developed a variety of innova- tive new models that automatically fact-check claims [34, 31, 40]. However, these works have largely viewed fact-checking as a standard ML task in which the aim first and foremost is to achieve high predictive accuracy. While improving predictive accuracy is a laudable goal, we believe that consideration of the human element is equally important if such models are to be useful in practice. We argue that fact-checking models must provide three key properties for practical use: (i) model transparency, (ii) sup- port for integrating user knowledge; and (iii) quantification and communication of model uncertainty. Regarding (i), high predictive accuracy alone is insufficient; someone skeptical of online information is likely to be equally skeptical of any fact-checking tool. Indeed, many people distrust popular fact- checking services [4]. Consequently, a system must be trans- parent (and auditable) in how it arrived at its prediction so that a user can understand and trust the model. Concerning (ii), claim assessments will invariably rely at least partially on world-views (priors) pertaining to the perceived a priori cred- ibility of claims and sources; a fact-checking system should enable explicit, individual specification of these, in turn pro- viding a framework for users to easily inject their own views and knowledge into the system and realizing an integrated prediction that incorporates these. Finally, addressing (iii), system predictions ought to be presented as relative statements with respect to incomplete model specification and knowledge, rather than definitive judgments: an ML model should com- municate its confidence in its predictions while accounting for potential sources of errors, empowering users to conduct their own in-depth inspection and reasoning. In this work we present a mixed-initiative [19] approach that realizes the three properties above. We position automatic fact-checking as an assistive technology to augment human decision making. To intuitively complement and reinforce the information literacy skills that users bring to the partnership, we have designed the system to follow the same key steps ad- vocated by information literacy education in order to estimate claim veracity: 1) find relevant articles (textual evidence); 2) assess each article’s reliability and its relative support for the claim in question; and 3) assess claim validity based on this body of textual evidence. Moreover, by making the model’s reasoning process transparent to the user and interactive, our interface has further potential to help teach and structure the user’s own information literacy skills regarding the logical process to follow for assessing claim validity. Our mixed-initiative approach to fact-checking blends human knowledge and experience with the efficiency and scalability of automated information retrieval and AI. Given a claim in natural language, the system first automatically finds and re- trieves relevant articles from a variety of sources. It then infers the degree to which each article supports or refutes the claim, as well as the reputation of each source. Finally, the system aggregates this body of evidence to predict the veracity of the claim. Regarding user interaction, the (automatically inferred) source reputation and stance of each retrieved article can be changed via simple sliders to reflect user beliefs and/or to correct erroneousness model estimates. This, in turn, instantly updates the system’s overall veracity prediction. To evaluate our approach, we conduct three randomized ex- periments, designed to measure the participants’ performance in predicting the veracity of given claims. The first compares users who perform the task with and without seeing ML pre- dictions. Our results suggest that users tend to trust the model, even when it is wrong. The second compares a static interface to an interactive one in which users can fix or override model predictions. We found that users are generally able to do so, although this is less helpful when the model makes correct predictions. The last experiment compares a gamified task design to a non-gamified one, but we found no significant differences in performance and participation. Contributions. We provide: (1) a novel mix-initiative ap- proach to fact-checking, combining human and machine in- telligence; and (2) a user study of our approach, revealing the practical promise and challenges of this human-AI partnership. To foster future work by others, we share our anonymized data, source code for significance testing, and an interactive demo2. RELATED WORK Information Credibility Someone skeptical of online information is likely to be equally skeptical of any fact-checking website or software. For exam- ple, many people are reported to distrust popular fact-checking services [4]. Just as we consider information credibility fac- tors [16] in assessing a news article or website, therefore, we must also consider such factors in designing a website or web application to support fact-checking. Some established best practices include: websites should be clearly organized and navigable, professional looking, well written, updated, and functional [16, 18]. More specific to this domain, sites and tools should be explicit about any potential sources of bias, e.g., by providing an about” page to provide context, indicating any paid sponsors, discussing or posting an ethical code, and admitting when a mistake has been made. In relation to asking users to perform a potentially complicated task (e.g., consulting various uncertain evidence to fact-check a questionable claim) which heightens user uncertainty, a clean and usable website is even more important to further reduce cognitive load [43]. We have sought to adhere to the above best practices in designing our prototype web application. Human Fact-checking and Information Literacy Websites such as Snopes and PolitiFact have become increas- ingly important in providing expert fact-checking of popular claims. However, the reliance on human labor, particularly experts, does not scale to lets users check arbitrary claims. Crowdsourcing-based fact-checking sites, such as TruthSet- ter3, now also provide more scalable, peer-based assessment. Our design is inspired in part by recent research on crowd- sourced fact-checking [41, 42] which suggests that . . . the best remedy for propaganda and misinformation intended to manipulate public opinion is helping readers engage in critical thinking and evidence-based reason- ing. . . [which] can have benefits well beyond identifying specific instances of fake news” - it can teach users the critical thinking skills needed to detect and evaluate misinformation and fake news . . . In that work [41, 42], users can post claims, and other users can then share related sources, stances, and claims. We simi- larly structure the claim evaluation process through evidence collection and assessment. Whereas they develop a social, volunteer crowdsourced solution, we propose mixed-initiative approach between a single user and a machine learning sys- tem. Future work might usefully further integrate information literacy education with fact-checking of claims. Another intriguing approach to teaching information literacy turns the process on its head, engaging people in gamified generation of fake news stories. By learning how to write fake news, participants were subsequently found to be less likely to believe or be persuaded by actual fake news articles [37]. AI-based Fact-checking Many recent studies have explored the potential of AI for auto- mated fact-checking [50, 49, 13, 34, 32, 45, 24]. These studies have primarily focused on model variants and techniques that increase the predictive performance of models (e.g., accuracy in predicting veracity). Hybrid work combining ML with crowdsourcing [24, 32] has similarly focused on predictive accuracy, without considering information literacy educational objectives, or exposure consequences, for the crowd [41, 48]. While some models do generate explanations for their predic- tions [34, 32], it remains unclear how users might interpret and interact with these predictions. Our work in this area is distinguished by our human-centered approach: our mixed- initiative design emphasizes the human-AI partnership, and our evaluation measures how use of such predictive systems impacts human inference in assessing claim veracity. Recently, [22] present a visual analytic system for users to detect social media accounts that distribute misinformation. This is complementary to our goal of detecting false claims. Furthermore, although their system is inspired by a prediction model prediction [49], the users can not directly interact with that model to correct and override when its prediction is wrong. Designing Human-AI Interfaces As AI has been embedded within an increasing number of hu- man facing applications, there has been a concomitant growth in interest in designing interfaces for humans to interpret and interact with these systems and predictions [1, 8]. In response, researchers have proposed a variety of novel interfaces for interacting with machine learning models, but these tend to require significant expertise on behalf of users [2, 25]. Oth- ers have developed interaction techniques tailored to specific tasks, such as image segmentation [11], image search [15], text classification [28], topic models [44], code search [36], and others. The fact-checking task is especially challenging, as the the system needs to present convincing evidence for its predictions, assuming users are (understandably) skeptical. Beyond making ML more interpretable for people, there is also increasing appreciation for the greater potential capabili- ties that may be possible with hybrid AI-human collaboration. Jordan and Mitchel [21] recently opined about the kind of strengths each side of such a partnership, noting the potential to harness ML’s ability to extract subtle statistical patterns from large datasets in concert with human skills in pulling these into plausible narratives informed by diverse perspec- tives. Such partnerships are now materializing even in creative endeavors, such as AI-human co-design of fashion [23, 46], creative writing [6, 35], and music composition [14]. To build such effective partnerships, Horvitz’s suggestions for mixed-initiative design [19] remain relevant today, e.g.: We can enhance the value of automation by giving agents the ability to gracefully degrade the precision of service to match current uncertainty. . . We should design agents with the assumption that users may often wish to com- plete or refine an analysis provided by an agent. Our system openly conveys its fallibility by showing the confi- dence of each prediction to the user. By clearly communicat- ing rather than hiding this uncertainty, the system discourages users having over-confidence in the model or making poor decisions based on such a misunderstanding. Instead, the sys- tem presents the evidence it has in favor of its disposition, and leaves it to the user to consider this evidence in the context of their own prior knowledge and experience. We further provide an interaction mechanism by which users can inject their own beliefs into the system to refine its predictions. Without users having a mental model of how the system com- bines evidence to predict claim veracity (i.e., model trans- parency), such interaction would not be possible. The impor- tance of model transparency has been similarly reported in other studies, such as Kulesza et al. [27]’s report of a case study in which users responded positively to greater model intelligibility; as users learned more about the system through interaction, they became more satisfied with system output. As such, the study demonstrated that users valued going beyond black box” ML and were willing and able to learn more about a system in order to use it more effectively. Another area in which human-AI partnerships are being ex- plored is interactive machine learning (IML) [1]. For example, recent work has investigated design and evaluation of IML systems with non-expert users [44]. Allowing users to alter system inputs and observe how outputs change in response is one technique for realizing model explainability [1]. Visual analytics [39] models similarly facilitate human decision mak- ing via model interaction. Following this principle, our model also offers fast incremental updates to predictions, enabling lay users to easily alter inputs via sliders and see immediate model updates of estimated claim veracity. Unfortunately, adoption of best practices for usability in ML system design is not yet as widespread as one might hope [1]: . . . machine-learning systems also often inherently vio- late many existing interface design principles. For ex- ample, research has shown that traditional interfaces that support understandability . . . and actionability . . . are gen- erally more usable than interfaces that do not . . . Many machine-learning systems violate both principles: they are inherently difficult for users to understand fully and they largely limit the control given to the end user. This motivates greater collaboration between HCI and AI re- searchers, with both fields and their research products standing to benefit. HCI researchers have similarly endorsed the value for HCI to better understand and engage with AI [9, 17]. USER EXPERIENCE This section describes the user experience we seek to cultivate with our mixed-initiative design. The main goal for our user interface is to realize transparency, in that users can under- stand how the system makes its predictions and thereby know when (and when not) to trust them. Below we discuss how we present our model outputs to users, including its final disposi- tion regarding claim veracities and intermediate estimates of each article stance and source reliability. User interactions with our system proceed as follows. The user first enters a claim (or selects an example claim, e.g., Saudi Arabia has a new law that can force women to cover up their tempting eyes""). A list of articles relevant to this claim is then retrieved (along with the source of each article, i.e., the website where the article was published). Based on these articles, a prediction is made and presented to the user Figure 1: Top: the main results page, which includes the claim, its predicted correctness, and a table of relevant articles, their sources and inferred reputations and stances. Bottom left: sources link to pages that allow users to see the articles in our training set, which enables them to see why it has a particular predicted reputation. Bottom right: each headline links to the original article. regarding the correctness of the claim; for example the model may be 80% confident that the claim is true. Figure 1 shows a screenshot of the main results page for the example claim above. We display the claim and the system prediction regarding its correctness at the top. The interface emphasizes the textual evidence and reasoning underlying the overall claim estimate. In particular, we present a table of retrieved articles relevant to the claim, including the sources, headlines, and two predictions: the reputation of each source and the stance of each article headline. Each prediction is shown as a slider: reputation ranges between {low, unknown, high}, while stance ranges from {deny, neutral, support}. A key feature of our interface is that users can change (over- ride) the reputations and stances (inferred by the model) by moving the sliders. They can then observe how the prediction regarding overall claim veracity is affected. For example, for a headline associated with an article published by a reputable source, changing its stance from support to deny will tend to increase the chance that the claim is false. This interactive feature is beneficial in three ways. 1) It increases transparency: users can see how model predictions about each relevant ar- ticle contribute toward the overall veracity prediction for the claim. 2) Users can access a more personalized prediction, for example by lowering the reputations of sources they believe are not credible. 3) They can correct the system’s incorrect predictions, e.g., by changing predicted stances. To further aid transparency, each headline is linked to the orig- inal article, allowing users to easily browse each article to see the textual evidence and assess the model’s stance prediction. In addition, we generate a page for each source, and each occurrence of the source on a claim page is linked back to the source’s generated page. When a user sees, for a given claim, that a relevant article comes from a particular source with a given prediction reputation, the user can consult the generated source page to learn more about the source and its predicted reputation. The source page lists all articles from that source in our training data, the claim associated with each article, the journalist-annotated article stance, and the journal- ist annotation for the claim’s veracity. As before, users may click on each article to verify the stance label for themselves. By consulting the source page and seeing the various infor- mation about it, the user can thereby better understand the model’s predicted reputation for the source: the more false claims supported by a source, the lower its reputation. PREDICTION MODELS In this section, we describe the predictive models underlying the above user experience. Our system’s automated predictions are based on two machine learning classifiers: one for article stance, and one for claim veracity [34, 32]. Both classifiers are trained on the Emergent dataset [13], which contains 300 claims with 2595 relevant articles (on average there are 8.65 articles per claim). Each claim is labeled as false, true, or unknown. Article headlines are labeled by journalists as being either supporting, neutral, or refuting the claim. Number Claim Veracity 1 Tiger Woods is serving a suspension from the PGA Tour after failing a drug test False 2 There is a case of Ebola in Kansas City False 3 The police officer leading the Charlie Hebdo investigation committed suicide False 4 A 5-year-old boy was invoiced for missing a birthday party True 5 The Indian government fired an employee who hadn’t been to work in 24 years True Table 1: Five claims that we randomly selected for our user study from the test set of the Emergent dataset (excluding the attention check claim). Each claim is linked to the original Emergent’s webpage showing the relevant articles and their stances. The stance classifier accepts the claim and an article headline to predict the stance of the headline with respect to the claim. The veracity classifier operates over the outputs of the stance classifier for all of the relevant articles (and corresponding sources), yielding a prediction concerning the veracity of the claim. This veracity classifier explicitly models the reputation of each source. Sources that support true claims and deny false claims (in the training dataset) are given higher weights (i.e., more trusted). Both classifiers (stance and veracity) have an average accuracy of approximately 70%. For inference and learning, we first train the stance classifier, then use its outputs to train the veracity classifier. For the stance classifier, we use the same text features as in [13], in- cluding bag-of-words (common n-grams), dependency parse, and paraphrase alignment (word embeddings are not used due to the reported negative impact). Each logistic regression classifer is implemented using Scikit-learn [33] with L1 regu- larization, Liblinear solver [12] and default parameters. While prior work has used joint training of the two classifiers in a graphical model framework [34, 32], we favor simplicity and speed to facilitate real-time user interaction. The underlying model architecture of predicting article stances and using these predictions to estimate veracity is designed to improve transparency. While others have considered alter- native architectures, for example deep neural networks [50], these can achieve good predictive performance but are less transparent, and so less amenable to interaction and supporting decision making. In contrast, we prioritize transparency over raw predictive performance. In particular, we rely on linear models in which individual terms have well-defined seman- tics, and we adopt a Bayesian view so that users may express subjective beliefs as priors imposed over these variables. We operationalize this via a graphical user interface design. USER STUDY We conduct three experiments with participants from Amazon Mechanical Turk (MTurk). We required participants to have completed 1000 approved tasks with at least 95% approval rate. Participants were allowed to partake in only one of our experiments. The task takes less than 10 minutes on average and we paid $1.25, for roughly $7.50 per hour. While we did not collect participant demographics, because we post our task in small batches throughout the day, our participant demographics likely follow the general MTurk demographics reported in prior work [7, 38, 20]: mostly from the US or India, balanced gender, and younger than the working population. In all experiments, participants predict the correctness of five randomly selected claims (Table 1) from the Emergent [13] test set. Note that our machine learning models have not seen any of these claims or the associated relevant articles. An additional (sixth) claim served as an attention check (AC) [30]: a headline instructing participants to select a particular answer (e.g., If you read this headline please select neutral”). While this AC was designed to filter out participants who did not pay attention to the task, we observed that many partici- pants failing this check appeared to have honestly completed the task (e.g. many have accurate answers and helpful com- ments). We thus decided not to filter out any participants. Experiment 1 This experiment tests whether system predictions help humans predict claim veracity more accurately. Procedure: Participants are randomly assigned to one of two groups, Control and System. In both groups, users are first shown a screenshot of our results page (similar to Figure 1), but without the claim veracity prediction. In group Control, users are shown only the sources and headlines of relevant articles. In group System, they are also shown the source rep- utations and predicted article stances inferred by our system. The task for both groups is to evaluate the claim correctness, using a a Likert scale: Definitely false, Probably false, Neu- tral, Probably true, and Definitely true. After making this assessment, participants in both groups are shown the model’s prediction concerning claim correctness and given the option to change their assessment. After completing this exercise for all claims, participants complete a short survey. Results: We collected results for 113 participants (58 assigned to Control, 55 to System). We measure error by calculating the distance from the participants’ responses to the correct answers. For example, for a (definitely) false claim, an as- sessment of Probably false corresponds to an error of 1, and Definitely true corresponds to an error of 4. In Figure 2, we plot average errors over participants for each of the five claims, with the standard deviation displayed to characterize variance. On the left (sub-figure a), we show the error before the participants see the system’s prediction. Firstly, we observe mixed differences across the two groups and five claims. Participants in group System were seen to show higher average prediction error on claims 1 and 2 and lower error on claim 4, while claims 3 and 5 show only small differences between the two groups. (a) BEFORE seeing the system’s correctness prediction. (b) AFTER seeing the system’s correctness prediction. Figure 2: Experiment 1. Control group participants see a list of relevant articles with sources, while those in the System group see the same list along with predicted stances and reputations. Left: the errors of the stance classifier and of the participants before they see the overall veracity prediction. Right: the errors of the claim veracity classifier and the change in the participants’ errors after being shown these predictions (a positive change means that human error increases). Secondly, we observe variable accuracy of the automatic stance classifier. On claims 1 and 2, the stance classifier predicts the wrong answers for 80% and 90% of the relevant articles. Its error is 30% on claim 3 and 0% on claims 4 and 5. While average stance classifier accuracy is over 70%, this accuracy distributes unevenly across claims, with very low accuracy on two out of our five randomly selected claims. For example, in inspecting the stance classifier on claim 1, we found a probable reason for its weak performance on this claim. This is an incorrect statement about Tiger Woods, a claim that is denied by many of the relevant articles. However, the stance classifier incorrectly predicts these articles’ stances as being supportive. In the training set there are many articles about Tiger Woods that support a different (unrelated) claim. The classifier has incorrectly learned that the bi-gram ‘Tiger Woods’ indicates that the article is supportive. While a more sophisticated classifier might avoid this particular error, in general we should expect our AI systems to be imperfect. Thirdly, we note the pattern of human error appears to roughly follow the system’s stance classifier errors. It seems that when the stance classifier is wrong, participants are often misled by it, but when it is correct, it improves their predictions. This may suggest participants were overly trusting of our AI. While this is seemingly at odds with prior findings of users not trusting popular fact-checking services [4], it may stem from differences in participant demographics, participant incentives, or other factors of experimental design. For example, users in our study need to make predictions, instead of just saying whether they trust the fact-checking results. We return to this issue later in discussing study limitations. In Figure 2 (sub-figure b), we also plot the change in human error vs. claim after the participants saw our claim correct- ness prediction (a positive change means that human error increases). We observe that there are larger changes for errors made by participants in group Control: those who have not seen our stance predictions change their answers more than those who have. These response changes increase the error for some claims (e.g. claim 3), and decrease it for others (e.g. claim 4). This roughly corresponds to the errors by the verac- ity classifier, showing again that system predictions can both help users (when correct) or lead users to errors that reflect model fallibility or biases implicit in training data. To quantify our results, we fit two Generalized Linear Models (GLMs): one for the data before participants see our system’s correctness prediction, one after. We modeled human error as an ordinal response predicted by claim and participant as random effects, and the number of correct/wrong stance pre- dictions by our system as fixed effects. Specifically, we use the clmm function of the R package ordinal [5] with the formula: Human.Error ∼ 1+CSP+WSP+(1|Claim)+(1|Participant) where ‘ CSP’ is the number of correct stance predictions that the human participant sees. It is 0 for all group Control par- ticipants who did not see any stance predictions. For group System, it is equal to the number of correct stance predictions for the claim. For example in claim 1, where the stance clas- sifier is correct for 2 out of 10 articles, ‘ CSP’ is 2 and ‘WSP’ is 8 (WSP is similarly defined as the number of wrong stance predictions the participant sees). Also in the formula, the no- tation ‘(1|Claim)’ means that Claim is a random effect and an intercept is estimated for each claim. For the data before seeing the correctness predictions, the results for the fixed effects are: Coefficient Estimate SE p-value (two-tailed) CSP -0.053 0.029 0.064 WSP 0.076 0.031 0.014 these suggest that seeing correct stance predictions (CSP) de- creases human error while seeing wrong predictions (WSP) increases human error by a larger amount. Although the p- value for CSP is slightly larger than the 0.05 significant level, we consider that a solid evidence (the p-value is two-tail and includes the unlikely possibility that seeing correct stance pre- dictions increases human error). After seeing the correctness prediction, the results are: Coefficient Estimate SE p-value (two-tailed) CSP -0.016 0.029 0.523 WSP 0.063 0.031 0.040 We can observe that seeing correct stances is now not as help- ful because the participants can see the correctness prediction: in claim 4 and 5, many participants are able to lower their errors (Figure 2b). But seeing wrong stances is still harmful because these wrong stances cause the correctness classifier to make predictions with high errors (claim 1 and 2). Experiment 2 This experiment assesses whether participants are able to inter- act with the system to inject their own knowledge, fix model predictions, and improve their own predictions. Procedure: Whereas participants in Experiment 1 were shown only static screenshots, participants in Experiment 2 use our in- teractive interface. Participants were randomly assigned to two groups, Control and Slider. In the interface for group Control, all predictions (reputations, stances, and claim correctness) are fixed. However, those in the latter group (Slider) could change the (initially inferred) reputations and predicted stances, using the sliders, and observe how the prediction regarding overall claim correctness changes in response. To encourage more atte","Nguyen, A. T., Kharosekar, A., Krishnan, S., Krishnan, S., Tate, E., Wallace, B. C., & Lease, M. (2018, October). Believe it or not: Designing a human-ai partnership for mixed-initiative fact-checking. In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology (pp. 189-199)."
ML_DB_111,Combating fake news: a data management and mining perspective,"Fake news is a major threat to global democracy resulting in diminished trust in government, journalism and civil society. The public popularity of social media and social networks has caused a contagion of fake news where conspiracy theories, disinformation and extreme views flourish. Detection and mitigation of fake news is one of the fundamental problems of our times and has attracted widespread attention. While fact-checking websites such as snopes, politifact and major companies such as Google, Facebook, and Twitter have taken preliminary steps towards addressing fake news, much more remains to be done. As an interdisciplinary topic, various facets of fake news have been studied by communities as diverse as machine learning, databases, journalism, political science and many more. The objective of this tutorial is two-fold. First, we wish to familiarize the database community with the efforts by other communities on combating fake news. We provide a panoramic view of the state-of-the-art of research on various aspects including detection, propagation, mitigation, and intervention of fake news. Next, we provide a concise and intuitive summary of prior research by the database community and discuss how it could be used to counteract fake news. The tutorial covers research from areas such as data integration, truth discovery and fusion, probabilistic databases, knowledge graphs and crowdsourcing from the lens of fake news. Effective tools for addressing fake news could only be built by leveraging the synergistic relationship between database and other research communities. We hope that our tutorial provides an impetus towards such synthesis of ideas and the creation of new ones.",Social Computing,Proceeding,2019,Y,N,9,"Fake news is a major threat to global democracy resulting in diminished trust in government, journalism and civil so- ciety. The public popularity of social media and social net- works has caused a contagion of fake news where conspiracy theories, disinformation and extreme views flourish. Detec- tion and mitigation of fake news is one of the fundamental problems of our times and has attracted widespread atten- tion. While fact checking websites such as snopes, politifact and major companies such as Google, Facebook, and Twit- ter have taken preliminary steps towards addressing fake news, much more remains to be done. As an interdisci- plinary topic, various facets of fake news have been studied by communities as diverse as machine learning, databases, journalism, political science and many more. The objective of this tutorial is two-fold. First, we wish to familiarize the database community with the efforts by other communities on combating fake news. We provide a panoramic view of the state-of-the-art of research on var- ious aspects including detection, propagation, mitigation, and intervention of fake news. Next, we provide a concise and intuitive summary of prior research by the database community and discuss how it could be used to counteract fake news. The tutorial covers research from areas such as data integration, truth discovery and fusion, probabilistic databases, knowledge graphs and crowdsourcing from the lens of fake news. Effective tools for addressing fake news could only be built by leveraging the synergistic relationship between database and other research communities. We hope that our tutorial provides an impetus towards such synthesis of ideas and the creation of new ones. PVLDB Reference Format: Laks V.S. Lakshmanan, Michael Simpson, Saravanan Thirumu- ruganathan. Combating Fake News: A Data Management and Mining Perspective. PVLDB, 12(12): 1990-1993, 2019. DOI: https://doi.org/10.14778/3352063.3352117 This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 12, No. 12 ISSN 2150-8097. DOI: https://doi.org/10.14778/3352063.3352117 1. MOTIVATION Fake news is one of the major problems facing govern- ments, society, academia, and industry. The dissemination of false and misleading information has a corrosive effect on the trust of public toward various institutions [1]. While fake news has always existed in history, the advent of new technology has exacerbated its reach and potential for dam- age. Social media and online social networks have allowed weaponization of fake news at an unprecedented scale [15, 30]. Not surprisingly, the academic community has mobi- lized to counteract this phenomenon. Many research com- munities including databases, machine learning, data min- ing, journalism and political science have focused on differ- ent aspects of this problem. Our tutorial provides a unifying framework for catego- rizing prior research focusing on four facets of fake news: detection, propagation, mitigation and intervention. The work on detection seeks to identify which items are fake through diverse tools such as machine learning, content anal- ysis, propagation analysis, computational fact checking and so on. The study of propagation seeks to understand and model how fake news spreads in various media such as social media. Once the fake news has spread, there are many tech- niques for addressing it. The mitigation based approaches seek to minimize the severity by inoculating” users from falling for fake news, reduce the impact of filter bubbles, showing diverse viewpoints and so on. Finally, the interven- tion based approaches take an active role in reducing the spread of fake news. They could involve mild interventions such as amplifying real news alongside fake news to severe ones such as removing content or accounts. Goal of Tutorial. The goal of this tutorial is to provide an intuitive summary of research from other communities and discuss how prior work in the database community could be used to enrich them. Instead of delineating database and non-database research, we offer a synthesis based approach that interleaves non-database and database research that emphasizes the strong connection between them. We review various technical challenges, recent solutions and highlight a number of intriguing open problems at the intersection of various communities. We hope that this will empower database researchers to make impactful contributions in the fight against fake news. Scope of Tutorial. Given the huge amount of prior re- search on fake news and the limited amount of time, this tutorial provides a carefully selected subset of topics that we believe are most relevant to database researchers. For 1990 example, while we touch upon machine learning based ap- proaches for fake news detection, we focus extensively on how prior database research on data integration and fusion could also be used for detecting fake news items. Similarly, while we briefly introduce propagation models for fake news, we weave them with influence maximization works that have been carried out in the database community. Target Audience and Prerequisites. The target audi- ence consists of database researchers who wish to familiarize themselves with recent developments in fake news research and contribute to novel algorithms and systems for combat- ting it. This tutorial is also appropriate for practitioners who want an intuitive overview of the state of the art. The tutorial assumes familiarity with databases, data mining, and fundamentals of machine learning. Connection with Related tutorials. Due to the timeli- ness and relevance of fake news, there have been a number of tutorials in data mining and machine learning commu- nities on this topic. Related tutorials include: (a) Compu- tational Fact Checking: A Content Management Perspec- tive by Sylvie Cazalens, Julien Leblay, Philippe Lamarre, Ioana Manolescu and Xavier Tannier in VLDB 2018 and WWW 2018; (b) Fake News: Fundamental Theories, De- tection Strategies and Challenges by Xinyi Zhou, Reza Za- farani, Kai Shu and Huan Liu in WSDM 2019; (c) Computa- tional Solutions against Fake News: AI vs. DB Approaches by Naeemul Hassan and Dongwon Lee in AAAI 2018; (d) Mining Misinformation in Social Media: Understanding Its Rampant Spread, Harm, and Intervention by Liang Wu, Giovanni Luca Ciampaglia and Huan Liu in ICDM 2017; and (e) Fact Checking: Theory and Practice by Xin Luna Dong, Christos Faloutsos, Xian Li, Subhabrata Mukherjee and Prashant Shiralkar in KDD 2018. The proposed tutorial differs from existing ones in sev- eral aspects: (1) we provide a novel unifying framework for research on fake news that seamlessly combines prior work from database and other communities. (2) we highlight how various innovations made by the database community could be used to address fake news. (3) none of the previous tu- torials above offer the whole spectrum of detection, propa- gation, mitigation and intervention unlike us. 2. TUTORIAL ORGANIZATION In this section, we describe a high level overview of the major topics covered in the tutorial. 2.1 Primer on Fake News (15%) In this introductory part of the tutorial, we describe var- ious attempts at defining fake news [28]. Fake news is of- ten used as an umbrella term for various specific concepts such as rumors, conspiracy theories, hoaxes, clickbaits, me- dia hype, satire, misreporting, and sensationalism. While fake news has always existed, a number of recent phenom- ena such as the advent of social media, increasing polariza- tion and filter bubbles, perverse incentives and virality have exacerbated their reach. We briefly describe the ecosystem of fake news and why it has become a serious threat [18, 2]. Finally, we describe the impact of fake news that requires a sustained focus from researchers from various communities including our own. 2.2 Propagation of Fake News (15%) In the second part of the tutorial, we provide a concise summary of previous attempts to model the propagation of fake news such as [30]. Fake news often becomes more viral than real news and more often. A thorough understanding of the propagation of fake news could be used for: (1) detec- tion: fake news often propagates in a distinct way compared to real news that could be used for early detection [32]; (2) mitigation: by understanding the key contributors for prop- agation, we could mitigate it by attenuating the factors for virality [26]. We introduce how influence propagation mod- els studied by researchers in the database and other com- munities could be used for understanding the spread of fake news. 2.3 Detection of Fake News (40%) The objective of this part of the tutorial is two-fold. First, we wish to familiarize the database community with prior attempts by the ML and AI communities for detecting fake news. Second, we describe how various research techniques pioneered by the database community could be used for fake news detection. We hope that the synthesis of these ideas would result in a more sophisticated next generation mech- anism for combating fake news. 2.3.1 ML based Approaches Prior research [27, 33, 26] could be categorized along two dimensions. The first dimension controls the specific ML technique used: supervised or unsupervised. The second di- mension is based on the specific features used for the chosen ML technique. The unsupervised approaches often operate under the assumption that real and fake news have distinct features that could be learned automatically and used to distinguish them. If most of the news in a social network is real, then fake news could be considered as an anomaly al- lowing us to use prior work on outlier/anomaly detection. In contrast, supervised approaches begin with a labeled train- ing dataset that is used to train a machine learning or deep learning model. This model is then used to categorize news as real and fake. Of course, the model is periodically re- trained to prevent adversarial users from gaming it. Both these models work by extracting features for categorization. Popular features include content of the news, credibility of the creator, propagation trace of the article etc. 2.3.2 Database Approaches There have been some preliminary efforts by our commu- nity for fake news detection and computational fact check- ing [7, 12]. At its core, the problem boils down to the fol- lowing: given a claim and a database of facts, how can one determine if the claim is real or fake news? Most of the ap- proaches rely on closed world assumption whereby all true facts are assumed to be those stored in an appropriate repos- itory such as relational database or knowledge graph. By considering different ways by which claims and the database are instantiated, it is possible to use a wide variety of prior work from databases. Some promising avenues include: Data Integration is an important body of work [10] that allows one to combine a number of heterogeneous data sources (that could be structured or unstructured) into a single unified data repository. We first describe a scenario where all the data sources used for fact checking are reliable and discuss the challenges inherent in integrating them. We 1991","Lakshmanan, L. V., Simpson, M., & Thirumuruganathan, S. (2019). Combating fake news: a data management and mining perspective. Proceedings of the VLDB Endowment, 12(12), 1990-1993."
ML_DB_113,bigIR at CLEF 2019: Automatic Verification of Arabic Claims over the Web,"With the proliferation of fake news and its prevalent impact on democracy, journalism, and public opinions, manual fact-checkers become unscalable to the volume and speed of fake news propagation. Automatic fact-checkers are therefore needed to prevent the negative impact of fake news in a fast and effective way. In this paper, we present our participation in Task 2 of CLEF-2019 CheckThat! Lab, which addresses the problem of finding evidence over the Web for verifying Arabic claims. We participated in all of the four subtasks and adopted a machine learning approach in each with different set of features that are extracted from both the claim and the corresponding retrieved Web search result pages. Our models, trained solely over the provided training data, for the different subtasks exhibited relatively-good performance. Our official results, on the testing data, show that our best performing runs achieved the best overall performance in subtasks A and B among 7 and 8 participating runs respectively. As for subtasks C and D, our best performing runs achieved the median overall performance among 6 and 9 participating runs respectively.",Computer Science,Proceeding,2019,Y,N,8,"Between the time the claim is made and the time the claim is checked and pub- lished, as it is very time-consuming for journalists to find check-worthy claims and verify them. Another challenge is that fact-checking requires advanced writ- ing skills in order to convince the readers whether the claim is true or false [6]. In fact, it is estimated that check-worthiness of a claim and writing an article about it can take up to one day [7]. Moreover, manual fact-checkers are outdated [6]. Most of the fact-checking frameworks adopt the old content management systems specialized for traditional blogs and newspapers, but not built for the current modern journalism. A new approach is therefore needed for automated fake news detection and verification. The industry and academia have shown an overwhelming interest in fake news to address the challenges of its detection and verification. Many pioneering ideas were proposed to address many aspects of fact-checking systems with their focus varies between detecting check-worthy claims [8, 10, 7], checking claims factuality [11, 15, 16, 20], checking news media factuality [2], and proposing full automatic fact-checking systems [9, 14, 22]. There are also some shared tasks proposed and open to the research community interested in the problem such as FEVER-2018 task for fact extraction and verification [18] and CheckThat! 2018 lab on automatic identification and verification in political debates at CLEF [13]. This year, CLEF-2019 CheckThat! Lab [4] introduced two tasks to tackle two main problems of automated fact-checking systems. The main objective of the first is to detect check-worthy-claims to be prioritized for fact-checking [1], while the second focuses on evidence extraction to support fact-checking a claim [5]. In this paper, we present the approach adopted by our bigIR group at Qatar University to address the second task. Task 2 (Evidence and Factuality) addresses the problem of finding evidence over the Web for verifying Arabic claims. It assumes the system is given an Arabic claim (as a short sentence) and a corresponding ranked list of Web pages that were retrieved by a Web search engine for that claim. The system then needs to address four sub-problems, each is defined as a subtask as follows: 1. Subtask A: Rank the retrieved pages based on how useful they are for verifying the claim. 2. Subtask B: Classify the Web pages as very useful” for verification, use- ful”, not useful”, or not relevant”. 3. Subtask C: Within each useful page, identify which passages are useful for claim verification. 4. Subtask D: Determine the true factuality of the claim, i.e., whether it is ”True” or ”False”. We have participated in all of the four subtasks. Since it is the first year of the task (and thus our first attempt), we generally adopted a simple machine learning approach, where learning models were trained only on the given training data over hand-crafted features. We applied feature ablation to assess the impact of each feature on the performance of our models. For subtask A, to re-rank the pages based on their usefulness, we adopted a pairwise learning-to-rank approach with features extracted either from the page as a whole (such as source popularity, URL links, and number of quotes), from the relevant segments in the page (such as the similarity score of the most relevant sentence), or from the search results (such as the original rank of the page). Additionally, we extracted claim-dependent features such as the similarity between the claim and the title and the snippet of the page. For subtask B, we adopted a multi-class classification approach to classify the Web pages. We considered several features including word embeddings, named entities, similarity scores, number of relevant sentences in the page, and URL- based features (such as URL length, URL scheme, and URL domain). For subtask C, we adopted a binary classification approach to classify the passages within a useful page. Features included Bag-Of-Words (BOW), named entities, number of quotes, score of most relevant sentence from each passage, and the similarity score between the claim and the passage. For subtask D, we also adopted a binary classification approach to discover the claim’s factuality given the retrieved Web pages. To classify the claim, we first identify the most similar pages to the claim for feature extraction. For the selected pages, we consider their similarity scores, source popularity, and the sentiment of the page. Our contribution in this work is two-fold: 1. We participated in all of the four subtasks adopting a machine learning approach with relatively-different set of features in each. The features are extracted from both the claims and the retrieved Web pages. 2. Our best performing runs exhibited the best performance in both subtasks A and B among the submitted runs. The remainder of this paper is organized as follows. Section 2 describes how we processed and extracted features from the claims and retrieved pages. Sec- tions 3, 4, 5, and 6 outline our approach and discuss our experimental evaluation in detail for subtask A, B, C, and D respectively. Finally, Section 7 concludes and discusses possible future work. 2 Preprocessing  Feature Extraction In our work, we apply common main preprocessing for all subtasks to parse documents, identify relevant segments, and extract features. However, we include or exclude some features in each subtask. In this section, we describe in detail the preprocessing steps and introduce and motivate the features we extracted at all levels. For each page, we extract two types of features: features that depend on the claim/page relationship (claim-dependent) and features that depend solely on the page (page-dependent). In what follows, a text segment in a page is centered by one sentence, but also includes both the sentence that precedes and the sentence that follows it, as defined by Yasser et al. [21], to consider the context of the sentence. 2.1 HTML Parsing As the Web pages are in raw HTML format, we parse each page by extracting only the clean version of the textual body discarding images, videos, and scripts using newspaper2 and BeautifulSoup3 Python libraries. We removed stopwords using Python NLTK4 Arabic stopwords. We also discard the sentences containing less than 3 words, motivated by the empirical study done by Zhi et al. [22]. 2.2 Text Vector Representations In extracting our features, we consider two text vector representations: – Bag-of-Words (BOW): We consider BOW representation to represent full passages (mainly for subtask C). We considered only the terms that appeared at least 7 times in the training data, based on some preliminary experiments. – Distributed Representation (W2V): We consider word2vec embeddings [12] to represent the claim and the segments of a page; each is represented as the average vector of the embeddings of terms in the claim/segment. We used the pre-trained AraVec embeddings model proposed by Soliman et al. [17]. 2.3 Relevant Segments Identification To identify relevant segments in a page for a given claim, we represent the claim and each sentence in the page by their average of term W2V vectors. We then compute the cosine similarity between the vectors of the claim and each segment. Segments are considered relevant if the similarity score is higher than a threshold. 2.4 Page-Dependent Features We extracted two types of page-dependent features: credibility and content. Credibility Features To indicate the credibility of the page, we consider the following features: – Source Popularity (SrcPop): This feature may indicate trustworthiness, as it captures how popular a particular website is. We used Amazon Alexa rank5 motivated by Baly et al. [2] that used this feature to estimate the reliability of media sources. We consider this feature as a categorical feature by binning the ranking values into 10 categories, then we convert it to a one hot encoding vector of 10 binary features. 2 https://pypi.org/project/newspaper3k/ 3 https://pypi.org/project/bs4/ 4 https://pypi.org/project/nltk/ 5 https://www.alexa.com/ – URL Features: these features were used by Baly et al. [2] to detect the reliability of web sources. We used Python URL handling library urlib6 to parse the URL and extract the following orthographic features: • Length (URLLen) and Number of Sections (URLSecs): The length of the URL path and the number of sections separated by ‘/’ help indicate whether the website is legitimate, irregular, or a phishing website. • Scheme (URLScheme): The URL protocol (https or http) indicates the trustworthiness of the website. We extracted the URL scheme then we used scikit-learn label encoder7 to encode string values of schemes to integers. • Domain Suffix(URLSfx): The suffix of a URL domain determines the source and credibility of the website. For example, a website with domain suffix .gov is a federal government site and is more credible than a commercial website with a suffix of .com. We used label encoder to encode their string values into integers. Content Features From page body, we extract the following linguistic and similarity features: – Number of Quotes (NQts): For each page, we count the number of quotes in all relevant segments. This feature may be very useful to rank web pages and decide how useful they are for claim verification as it may indicate the credibility of the page by quoting sources. In our work, we considered only quotes with five words or more. – Number of URL links (NLinks): This feature represents the number of URL links in the retrieved page. It may indicate the credibility of the source by giving references. – Named Entities (NEs): Pages mentioning named entities may indicate the truthfulness of the page. We used Python polyglot NLP tool8 to recognize location, organizations, and persons entities in the most relevant segment of the page. We form a vector of 3 integer values representing the number of occurrences of every entity type in the segment. 2.5 Claim-Dependent Features We extracted the following features based on the claim-page interaction: – Original Rank (Rank): This feature is available from the search results and it represents how the page is potentially-relevant to the claim according to the search engine. – Similarity: This includes cosine similarity between claim and title (ClmTtlSim), claim and snippet(ClmSnptSim), and claim and a passage (ClmPsgSim). 6 https://pypi.org/project/urllib3/ 7 scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html 8 https://github.com/aboSamoor/polyglot – Number of Relevant Sentences (NRelSent): For every page, we com- pute the similarity between the claim and each sentence. We count the num- ber of relevant sentences in each page as it might indicate the relevance of the page. – Number of relevant webpages (NRelPages): For every claim, we count the number of webpages with a similarity score between claim and most relevant sentence higher than a certain threshold. – Score of the most Relevant Segment (MostRelSeg): This feature in- dicates how similar the most relevant segment is to the claim. – Sentiment (SntCnt): Sentiment analysis can help identify if the stance of the page is positive, negative, or neutral. This may help in identifying whether the page agrees with the claim or not. We use polyGlots Sentiment model9 to extract sentiments. From the most relevant segment, we get two values, the number of words with positive polarity and the number of words with negative polarity. 3 Subtask A: Reranking Retrieved Pages In this subtask [3], the goal is to rerank the retrieved pages based on their use- fulness for verifying a specific claim. In this section, we present our proposed approach, experimental setup and results, our selected runs for CLEF submis- sions, and finally we will present the CLEF results. 3.1 Approach Our approach is based on learning-to-rank (L2R). We propose a pairwise L2R model considering three different L2R classifiers, namely, SVM C-Support Vector Classification (SVC), which is implemented based on libsvm10, Gaussian Na ̈ıve Bayes (Gaussian NB), and the ensemble classifier Random Forest (RF), using Scikit-learn Python library.11 We consider the following features (discussed in Section 2): – Basic features: Rank, SrcPop, and MostRelSeg. – Similarity features: ClmTtlSim and ClmSnptSim. – NLinks. – NQts. 3.2 Experimental Setup Parameters We experimented with the three different classifiers mentioned in 3.1. We set the kernel for SVC to linear, and set the number of estimators for the RF models to 100 (based on preliminary experiments). For the NB models, we did not tune any hyper-parameters and used the default settings. 9 https://polyglot.readthedocs.io/en/latest/Sentiment.html 10 https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html 11 https://scikit-learn.org/stable/index.html Baselines We compare our models against a baseline that returns the pages ranked in their original ranks (i.e., based on relevance scores of the search engine, not on usefulness for fact-checking). 3.3 Evaluation on Training As we were constrained by the size of the training data, containing only 10 claims, we adopted leave-one-claim-out (LOO) cross validation to evaluate the trained models. We optimized our models using the graded relevance measure NDCG@20. We first experimented with different values of the cosine similarity threshold (0.4, 0.5, 0.6, and 0.7) when extracting relevant segments. In our unreported preliminary experiments, we observed that the best performing models were the ones trained with features extracted using a similarity threshold of 0.4 and 0.7, presented in Fig. 1 and Fig. 2 respectively. We also tried different combinations of features as shown in both figures. The results show that our models could not beat the baseline with only the basic features. However, NB models outperformed the baseline when other features were introduced. We also notice that introducing the ClmTtlSim and ClmSnptSim to the basic features improved the performance of our models, while excluding the SrcPop feature improved the performance. Moreover, our proposed NLinks and NQts features did not have a noticeable impact on the performance of the models. Fig. 1. Subtask A: Performance of L2R models on training data with combinations of features (cosine similarity threshold set to 0.4). Fig. 2. Subtask A: Performance of L2R models on training data with combinations of features (cosine similarity threshold set to 0.7). 3.4 CLEF Evaluation Runs As shown in Fig. 1, NB models outperform other L2R models over the training data, therefore we picked the 3 best NB models to submit to CLEF:. 1. NB trained with Basic, ClmTtlSim, and ClmSnptSim features, and excluding SrcPop. 2. NB trained with Basic, ClmTtlSim, ClmSnptSim, and NQts features, and excluding SrcPop. 3. NB trained with Basic, ClmTtlSim, ClmSnptSim, NQts, and NLinks fea- tures, and excluding SrcPop. Moreover, when the cosine similarity threshold was set to 0.7, RF outperformed other models, as shown in Fig. 2, so we also picked its best performing model: 4. RF trained with Basic, ClmTtlSim, and ClmSnptSim features, and excluding SrcPop. Results As shown in Table 1, the official CLEF evaluation shows that our best performing model on the test data was the NB model trained with basic, ClmT- tlSim, and ClmSnptSim features (excluding SrcPop) which achieved NDCG@20 value of 0.55. This was the maximum score achieved among 7 runs submitted for this subtask. We observed that the performance of our models on training data was better than on testing data; this can be attributed to the small size of the training dataset, containing only 395 pages from 10 claims, which could be insufficient and not a good representative to train the models. Table 1. Subtask A: Performance of CLEF submitted runs. Features Classifier NDCG@20 on train NDCG@20 on test {Basic+Sim} -SrcPop RF 0.704 0.47 {Basic+Sim+NQts} -SrcPop NB 0.693 0.52 {Basic+Sim} -SrcPop NB 0.692 0.55 {Basic+Sim +NQts+NLinks} -SrcPop NB 0.688 0.51 4 Subtask B: Classifying Retrieved Pages The main goal of this subtask [3] is to classify all retrieved Web pages based on how useful they are in detecting the claim’s veracity. A webpage is useful if it has enough evidence to verify the claim and if its source is trustworthy. In this section, we present our approach, experimental setup, training results, and CLEF results for our submitted runs. 4.1 Approach In our approach for this subtask, we use different machine learning algorithms to perform multi-class classification. We consider SVC as it shows to learn well from small datasets. We also include Gradient Boosting (GB) and RF as an ensemble model. As mentioned in 3.1, we use Scikit-learn Python library for our implementation. We consider the following features: – Basic features: Rank, SrcPop and MostRelSeg. – NEs in the relevant segment. – NQts. – URL features. – W2V representation of both the claim and the relevant segment. 4.2 Experimental Setup Parameters For SVC, we used an RBF kernel with regularization parameter C = 15 and L2 penalty, and we set γ to 0.01 to avoid over-fitting. For GB and RF models, we set the number of estimators to 100 and 150 respectively (based on preliminary experiments). Baselines As a baseline we adopted Wang et al. [19] method for feature extrac- tion and classification. Their dataset consists of short passages where passages are classified into five different categories. This baseline was selected because the feature extraction methods are implemented on short passages similar to the size of our extracted relevant segments. Moreover, they are working on fine-grain classification. Since our training data is highly imbalanced, we also used the Zero Rule al- gorithm as a baseline for this subtask. Zero Rule algorithm predicts the majority class in the dataset. In our training data, class -1 (non-relevant) is the majority class with 65% of the labels. 4.3 Evaluation on Training We conducted multiple experiments in attempt to find which features combina- tion will result in the best F1 score. We split our dataset into 70% for training and 30% for testing. From our experiments, we noticed that varying the simi- larity threshold when extracting relevant segments had a significant impact on the overall score. We concluded that our best performing models were the ones trained with features extracted with similarity thresholds of 0.4 and 0.7. Fig. 3 and Fig. 4 show the results obtained from our experiments using similarity thresholds 0.4 and 0.7 respectively. We observed that when training the classifiers with basic features and NEs the performance improved. On the other hand, incorporating some content fea- tures like URL features and W2V vectors had a negative impact on the per- formance of the classifiers. We also note that ensemble classifiers (GB and RF) outperformed the baselines and other classifiers all the time. Fig. 4. Subtask B: Performance of classifiers on training data with combinations of features (cosine similarity threshold set to 0.7). 4.4 CLEF Evaluation Runs As concluded in section 4.3, ensemble classifiers have outperformed SVC classifiers. So, for our runs we picked the GB and RF models. We selected the following models with cosine similarity threshold of 0.7: 1. GB Classifier trained with basic features. 2. GB Classifier trained with basic features and NEs. We also picked the following models when cosine similarity threshold is set to 0.4: 3. GB Classifier trained with basic features and NQts. 4. RF Classifier trained with basic features and NQts. Results Table 2 shows our training results compared to the official CLEF testing results. We notice that our best validation model with F1 score of 0.52 that combines basic features with NEs has achieved lower testing score. Meanwhile, our model that combines basic features with NQts has scored a testing F1 score of 0.31. The inconsistency between train and test F1 scores can be justified due to the small training dataset of only 395 webpages. Also, the imbalance in the classes of the dataset could have caused the models to overfit. Our best model that achieved F1 score value of 0.31 is the highest among all submitted runs for this subtask. 5 Subtask C: Classifying Passages In this subtask [3], the goal is to extract useful passages for claim verifica- tion within the useful retrieved pages. In this section, we present our proposed methodology, experimental evaluation, selected runs for this subtask, and CLEF results. 5.1 Approach Deciding whether a passage within a useful page is useful or not is a classifica- tion problem. Therefore, our methodology is based on using different machine learning classifiers namely SVC, NB, and RF. We consider the following features for this subtask: – BOW of the passage. – MostRelSeg in the passage. – ClmPsgSim. – NQts in the passage. – NEs in the passage. 5.2 Experimental Setup Parameters The three different classifiers mentioned in section. 5.1 were used in our experiments. We set the kernel for SVC to linear, and the number of estimators for the RF models to 100 in all the experiments. For the Gaussian NB, we did not tune any hyperparameters and we based our experiments on the default settings. Baselines We compare our models against the majority baseline. 5.3 Evaluation on Training Since we have only 6 claims in the dataset provided for subtask C, which contains only 167 passages from 31 different pages, we considered LOO cross validation in our experiments. We used F1 score as our evaluation metric. As shown in Fig. 5, SVC outperformed all other models with all groups of features. However, when the BOW features were excluded, the Gaussian NB achieved the best among all. We also observed that the two best performing models are the SVC model when the NEs features were excluded, and the SVC model when the NQts feature was excluded achieving an F1 score of 0.444 and 0.43 respectively. We also noticed that the performance of the SVC model trained with all features improved compared to when trained with BOW features only, achieving an F1 score of 0.427 as apposed to 0.387 Fig. 5. Subtask C: Performance of classifiers models on training data with combinations of features. 5.4 CLEF Evaluation Runs As shown in Fig. 5, SVC models outperformed other classifiers except when the BOW features were excluded, in which case the NB model achieved the best F1 score. Therefore, we picked the 3 best SVC models and the best NB model to submit: 1. SVC trained with all features. 2. SVC trained with all features excluding the NQts feature. 3. SVC trained with all features excluding NEs features. 4. NB trained with all features excluding BOW features. Results As shown in Table 3, in the official CLEF evaluation, our best per- forming model in the test phase was the SVC model trained with all features excluding the NQts features, which achieved F1 score value of 0.4. The low F1 of our models can be attributed to the big difference in training and testing data including passages from 6 claims and 59 claims respectively. Our highest scoring model is ranked 3rd out of the six runs submitted to the lab, and the maximum score achieved among all runs submitted for this subtask was 0.56. 6 Subtask D: Verifying Claims The goal of this subtask is to identify whether the claim is ”True” or ”False”. For a claim to be true, it should have supporting evidence that verifies its factuality. Table 3. SubTask C. Performance of CLEF submitted runs. Features Classifier F1 on train F1 on test All SVC 0.423 0.39 All-NQts SVC 0.43 0.4 All-NEs SVC 0.44 0.19 All-BOW NB 0.38 0.37 In this section, we present our approach, experimental setup, and training results for verifying the claims. Then, we discuss CLEF results for our submitted runs. 6.1 Approach Deciding the factuality of a claim is a binary classification problem. Therefore, we propose a supervised learning approach using different classifiers: GB, RF and Linear Discriminant Analysis (LDA). For this subtask, we select the most significant features from webpages to classify the claim. Unlike previous tasks, we consider SntCnt features to find the polarity of the webpage. In addition, we consider the usefulness of the article by using the most relevant segment extracted as explained in Section 2 to represent the webpage. In our experiments, we consider the following features for our binary classifiers: – Similarity Scores: out of all webpages associated with a claim, we only con- sider three different scores: maximum ClmTtlSim, ClmSnptSim, and MostRelSeg. – NRelPages. – For every claim, we select the webpage with maximum MostRelSeg value and extract the following features from it: SrcPop and SntCnt. 6.2 Experimental Setup Parameters For GB and RF classifiers, we found that the default parameters are the best (based on preliminary experiments). For LDA classifier, we found that using 5 components for linear discrimination is most effective in terms of accuracy. Baseline As a baseline for this subtask, we implemented Karadzhov et al. [11] method. They classify claims as ”True” or ”False” based on the top returned search results from several engines. They used an SVC classifier with RBF kernel in their experiments. The inputs to the classifier are word embeddings of the most relevant segment in the webpage, webpage snippet, and the claim. In addition to the word embeddings, the average and maximum similarity scores of the segments and snippets are included as features. We also adopt their method of segment extraction to compare with our approach. 6.3 Evaluation on Training We conducted multiple experiments to find which features combination will re- sult in the best factuality classification. Due to the limitation in the size of training dataset, we used 8-fold cross validation on all our models for this sub- task. We first experimented with different values of the cosine similarity threshold (0.4, 0.5, 0.6, and 0.7) when extracting relevant segments. In our unreported preliminary experiments, we observed that the best performing models were the ones trained with features extracted using a similarity threshold of 0.6 presented in Fig. 6. We noticed that the GB model trained with all features outperformed all other models. We also observed that our models outperformed the baseline score most of the time except when the NRelPages were excluded from the features. Furthermore, we conclude that NRelPages and SntCnt features are useful in classification of a claim. Fig. 6. Subtask D: Performance of classification models on training data with combi- nations of features (cosine similarity threshold set to 0.6). 6.4 CLEF Evaluation Runs Based on our training results presented in section 6.3, we decided to use the models trained on all features to classify the claims factuality on testing data. We selected the best ensemble classifiers with two different similarity thresholds. 1. GB classifier, with similarity threshold 0.7. 2. GB classifier, with similarity threshold 0.4. 3. RF classifier, with similarity threshold 0.4. 4. RF classifier, with similarity threshold 0.6. Results Table 4 shows our training results compared to the official CLEF testing results. Runs for subtask D were submitted over two cycles. In the first cycle, we classify the claims factuality using all webpages provided. In the second cycle, we classify the claims factuality using only useful webpages. We present the results for the second cycle in this section. As presented in Table 4, we notice that all models achieved very similar F1 test scores. However, our GB model trained with all features has the highest training and testing scores, achieving F1 score of 0.91 and 0.53 for training and testing respectively. Our highest scoring model is ranked 4th out of the nine runs submitted to the lab, and the maximum score achieved among all runs submitted for this subtask was 0.62. Results Table 4 shows our training results compared to the official CLEF testing results. Runs for subtask D were submitted over two cycles. In the first cycle, we classify the claims factuality using all webpages provided. In the second cycle, we classify the claims factuality using only useful webpages. We present the results for the second cycle in this section. As presented in Table 4, we notice that all models achieved very similar F1 test scores. However, our GB model trained with all features has the highest training and testing scores, achieving F1 score of 0.91 and 0.53 for training and testing respectively. Our highest scoring model is ranked 4th out of the nine runs submitted to the lab, and the maximum score achieved among all runs submitted for this subtask was 0.62. Table 4. Subtask D: Performance of CLEF submitted runs. Features Classifier F1 on Train F1 on Test All GB 0.91 0.53 All GB 0.83 0.51 All RF 0.80 0.53 All RF 0.66 0.51 7 Conclusion In this paper, we present our approach for task 2 of CLEF-2019 CheckThat! Lab. For subtask A, we proposed pairwise learning-to-rank approach using dif- ferent learning models to rank the retrieved pages based on their usefulness. Our best performing model trained using the basic and similarity features (ex- cluding source popularity) achieved an NDCG@20 of 0.55, which is the highest score among 7 runs submitted for this subtask. For subtask B, we proposed a classification model incorporating source popularity feature along with named entities. Our best performing model achieved an F1 score of 0.31, which is the highest score achieved among the 8 runs submitted for this subtask. For subtask C, we proposed a classification model considering BOW, named entities, and the number of quotes features extracted from passages. Our best performing model trained with all features (excluding the number of quotes) achieved an F1 score of 0.4 and got 3rd place. For subtask D, we proposed a classification model using sentiment features to find the polarity of the page, in addition to the number of potentially-relevant pages. Our best model trained with all features achieved an F1 score of 0.53 and got 4th place. That was our first attempt using a very small training data that was provided by the track organizers. With larger datasets, we plan to improve our classifi- cation models with more features including word embeddings, that are trained specifically for this task, and probably with deep learning models as well.","Haouari, F., Ali, Z. S., & Elsayed, T. (2019). bigIR at CLEF 2019: Automatic Verification of Arabic Claims over the Web. In CLEF (Working Notes)."
ML_DB_118,Challenges and opportunities for journalistic knowledge platforms,"Journalism is under pressure from loss of advertisement and revenues, while experiencing an increase in digital consumption and user demands for quality journalism and trusted sources. Journalistic Knowledge Platforms (JKPs) are an emerging generation of platforms which combine state-of-the-art artificial intelligence (AI) techniques such as knowledge graphs, linked open data (LOD), and natural-language processing (NLP) for transforming newsrooms and leveraging information technologies to increase the quality and lower the cost of news production. In order to drive research and design better JKPs that allow journalists to get most benefits out of them, we need to understand what challenges and opportunities JKPs are facing. This paper presents an overview of the main challenges and opportunities involved in JKPs which have been manually extracted from literature with the support of natural language processing and understanding techniques. These challenges and opportunities are organised in: stakeholders, information, functionalities, components, techniques and other aspect",Computer Science,Article,2020,Y,Y,5,"Journalism is under pressure from loss of advertise- ment and revenues, in combination with competing online distribution channels that stream free content, while experiencing an increase in digital consump- tion and readers who demand quality journalism and trusted sources [1]. Information is no longer consumed from a single newspaper. Instead, readers have access to and can contrast fresh and first-hand information sources available on the internet and social media at any time. News organisations are constantly adapting their business models to digital media innovations, to improve information quality, competitiveness and growth [2]. Journalistic Knowledge Platforms (JKPs) are an emerging type of platform that combines state-of-the-art artificial intelligence (AI) techniques such as knowledge graphs and natural-language pro- cessing (NLP); and exploit news and social media information over the net in real-time, using linked open data (LOD), encyclopaedic sources and news archives to construct knowledge graphs and provide fresh and unexpected information to journalists, help- ing them to dive deeply into information, events and story-lines. JKPs are increasingly driving innovation Proceedings of the CIKM 2020 Workshops, October 19-20, Galway, Ireland. email: Marc.Gallofre@uib.no (M. Gallofré Ocaña); Andreas.Opdahl@uib.no (A.L. Opdahl) orcid: 0000-0001-7637-3303 (M. Gallofré Ocaña); 0000-0002-3141-1385 (A.L. Opdahl) © 2020 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).CEUR Workshop Proceedings ISSN 1613-0073 CEUR Workshop Proceedings (CEUR-WS.org) and transforming newsrooms, leveraging information technologies to increase the quality and lower the cost of news production. In order to drive research and design JKPs that allow journalists to get most bene- fits out of them and support newsrooms with better solutions, we need to understand the challenges and opportunities that JKPs present for both users and developers. To do so, we have reviewed the research literature in light of our own experience with devel- oping News Hunter [3, 4, 5], a series of JKP prototypes in collaboration with a developer of newsroom tools for the international market. This paper presents a synthesis of the challenges and opportunities for journalistic knowledge plat- forms that we have found in the literature, hopefully describing the most central factors that are driv- ing development of JKPs today. These factors have been grouped into six categories: stakeholders, in- formation, functionalities, components, techniques and other aspects. We conclude that JKPs offer many opportunities for effective production of high- quality journalism, real-time information, enriched background information, and multilingual and cross- platform solutions for monitoring worldwide mul- timedia output, by offering solutions to problems such as language independence, complex newsrooms workflows, and disperse information. Central chal- lenges include leveraging pre-news information from social media and multimedia sources, precise seman- tic lifting and enrichment of texts, scaling semantic technologies to big data, and detecting and reasoning over events. This paper is organised as follows: Section 2 sum- marises the methodology used for screening the challenges and opportunities. Section 3 briefly re- views the research literature. Section 4 explains the coding process. Sections 5 to 10 synthesise the main challenges and opportunities for each factor respec- tively — stakeholders, information, functionalities, components, techniques and other aspects. 2. Method Our research method consists of four steps: Firstly, we selected the most relevant research papers that we have identified in our previous studies on JKPs archi- tectures and news angles [4, 6, 7, 8, 9, 10, 11, 12]. From these selected papers we manually extracted claims, i.e., sentences that express potential challenges or op- portunities. Secondly, a purposive sampling was conducted in- dependently by two expert coders (the authors). The coders generated multiple codes for each extracted claim and the codes were cleaned with the support of NLP and NLU techniques (i.e., Damerau-Levenshtein distance [13], word2vec [14], and Wordnet [15])1. From the resulting cleaned codes, we selected the most representative ones as preliminary codes and divided them into categories. Thirdly, based on the preliminary codes, claims were independently coded once again by both au- thors. This time, the coders were allowed to code each claim with multiple codes for each category. The cod- ing agreement was estimated using Gwet’s AC1 [19] inter-rater reliability coefficient with nominal ratings. Because coders were allowed to not to code, to com- pute the Gwet’s AC1, empty codes were not treated as missing values, instead, they were treated as if they where coded as undefined”. Hence, to compute the contingency tables for multiple codes we applied the following rule: the agreement between coders A and B only happens between correctly matching codes (A∩B) and the other codes (A△B) were matched with missing values and treated as disagreements. Finally, when both coders agreed on the final codes for each claim, challenges and opportunities were ex- tracted from each claim following the assigned codes. 1Implemented in python with support of Scikit-learn [16], NLTK [17], SpaCy [18] and other libraries. 3. Reviewed papers After a broad survey of the literature, we selected eleven papers describing describing five research projects related to JKPs as the starting point of our review: NEWS [20, 21], EventRegistry [22], News- Reader [23, 24, 25], SUMMA [26, 27, 28, 29] and ASRAEL [30]. NEWS is a project, in collaboration with the Spanish Agencia EFE and the Italian ANSA news agencies, that makes use of semantic technologies to improve news agencies’ workflows, productiveness and revenues by focusing on the annotation, intelligent information re- trieval and user interface aspects [21]. EventRegistry is focused on collecting news articles, identifying and extracting information about events, and summaris- ing and visualising them [22]. NewsReader extracts information about what, who, where, when from mul- tilingual news articles and represents events in time using RDF in a knowledge graph, allowing users to find networks of actors along time [25]. SUMMA col- laborates with BBC Monitoring and Deutsche Welle to develop a multilingual and multimedia platform us- ing state-of-the-art NLP techniques to monitor inter- nal and external media work and provide data jour- nalism services [27]. ASRAEL aggregates news arti- cles and leverages the Wikidata knowledge base to de- scribe and cluster news events and provides informa- tion retrieval tools to interact with the resulting news representations [30]. 4. Coding process In the purposive sampling step, we extracted 322 claims from the related literature and marked them up using 406 codes. After cleaning and tidying up the initial codes, we identified six top-level categories which we divided into 62 sub-categories to be used for preliminary coding. The following six top-level categories were used: • Stakeholder: the agent that the challenge or op- portunity is for. The agent can be either a tech- nical agent or social agent. • Information: the information needed to meet the challenge or exploit the opportunity. • Functionality: the service or functionality that the platform should offer to meet a challenge or exploit an opportunity. • Component: the part of a platform that must be created or improved to meet the challenge or ex- ploit the opportunity. • Technique: the IT solution used to meet the challenge or exploit the opportunity. • Other aspects: another type of concern that the challenge or opportunity involves, such as customer heterogeneity, performance or maintenance. We computed the inter-rater agreement for the preliminary coding with the AC1 coefficient for each category: 0.77 for Stakeholders, 0.65 for Components, 0.71 for Techniques, 0.71 for Aspects, 0.72 for In- formation types and 0.57 for Functionalities. The average AC1 is 0.69 with a standard deviation of 0.063, which according to Landis-Koch and Altman’s benchmark scales, express an acceptable agreement among coders [19]. Finally, the assigned codes were discussed between and agreed on by the two coders. 5. Stakeholders Stakeholders are agents that represent the forces and interests that drive the future of JKPs. The identified sub-categories of stakeholders are: general user, news professional, fact checker, archivist, ICT professional, audience, customer, researcher, news agency, public organisation and technical agent. General users interact with services provided by the JKPs or newsrooms. These can be divided between the internal users that belong to newsrooms and the ex- ternals ones. The internal users are news professionals like journalists who use JKPs for creating histo- ries [20]; fact checkers who conduct an essential task in combating with fake news and misinformation [28]; archivists who maintain up-to-date the ontology and news archives [20]; and ICT professionals and knowl- edge engineers who represent those users involved in the development and maintenance of JKPs [21]. Whereas, the external users are the audience [22]; the customers to whom new agencies offer services; and researchers who investigate JKPs or analyse data, as in the SUMMA project where [political scientists want] to perform data analyses based on large amounts of news reports” [27, p. 2]. The organisations influencing the JKPs are: the news agencies, including newsrooms; the public organ- isations which are those governmental agencies that interact with or consume services from newsrooms’ JKPs, as in the SUMMA project which provides media monitoring and analysis services to the BBC own newsrooms, the British government, and other subscribers” [27, p. 1]; and the organisations that are responsible for controlling news media standards, vocabulary and ontologies (e.g., IPTC organisation2), which are indirectly influencing JKPs because the work of many news agencies and JKPs depends on those standards, as in the NEWS project where most of the NewsCodes defined by IPTC do not have al- ternative versions in different languages, only in English” [20, p. 9]. Finally, the technical agent, which is a stakeholder that represents the JKPs and any system or techni- cal infrastructure in newsrooms that support or inter- act with JKPs. A particular subtype of technical agent are the external systems that communicate with news- room systems, like the information systems of poten- tial customers [20]. 6. Information JKPs cover the whole information pipeline from gath- ering information and news creation to knowledge ex- ploitation and distribution. Our study identified the following sub-categories of information to be consid- ered in JKPs: news content, textual data, multimedia data, data format, metadata, LOD, events and infor- mation needs. News agencies produce both textual and multi- media news content which have to be managed and distributed to their customers and audience [21, 20]. As textual data we consider the raw text in form of news articles, documents, markup files, PDF, web pages, biographies, history and geopolitical data of countries, reports, social media feeds and social blogs. Whereas, as multimedia we consider live broad- cast, spoken content, photographs, audio and video. Besides, news agencies produce contents in different formats like plain text, Information Interchange Model (IIM), News Industry Text Format (NITF), NewsML and RDF [20]. Metadata is used to annotate and manage the pro- duced content. Metadata can describe e.g., author, language, creation timestamp, location, keywords, category, provenance, priority, urgency, status, up- dates, rights, interest, description or media type. JKPs use Linked Open Data (LOD) to annotate and enrich content using semantic vocabularies and leveraging knowledge bases, as in the ASRAEL project where they leverage the Wikidata knowledge base to pro- duce semantic annotations of news articles” [30, p. 1]. News agencies create stories describing events and deliver them to their customers and audience [21], making the events the central information need. De- spite that, social stakeholders have other information needs: General users are interested in knowing who, what, with whom, where and when events took place, networks of timeline actors implications, find the events of a certain type or in a certain place, obtain facts and retrieve evidence [24]. News professionals need access to news agencies’ archives and knowledge bases for documentation purposes, find connections from past events, follow histories and identify emerg- ing topics [20, 23, 27]. While customers have different information needs mainly depending on their busi- ness or interests, e.g., the press cabinet of a company is usually interested in news items talking about the company or its rivals, whereas a sports TV channel is interested mostly in news items describing sports events” [20]. 7. Functionalities JKPs provide different functionalities to their users. We identified twelve main sub-categories of function- ality: news creation, verification, source selection, monitoring, knowledge discovery, trends, alert, sum- marisation, clustering, personalisation, business support and content management. News professionals use the JKPs for the news cre- ation process. JKPs guide journalists in writing up their stories, support them with contextual back- ground knowledge for those stories [21], provide means for comparing current events with other simi- lar events [30], and facilitate access to previous work for creating similar content for a different audience, region or language [27]. JKPs also support news professions with verification tasks like fact checking, provenance [24], rights and authorship manage- ment [20, 21], which are typically time-consuming tasks for news professions as explained in manual verification of claims is a tedious task, that consumes a lot of time and effort from journalists and professional fact-checkers” [28, p. 1]. Source selection and monitoring functionalities are two common functionalities across the studied JKPs, which harvest and store content from internal and external sources and monitor them in real-time. By doing this, JKPs relieve journalists from these time- consuming tasks, as it was happening in the BBC where each of its ca. 300 journalist monitors up to four live broadcasts in parallel, plus several other information sources such as social media feeds” [27, p. 1]. Knowledge discovery is one of the most attrac- tive functionalities of JKPs. Knowledge discovery allows users to obtain news insights, analysis and relevant information, like in NewsReader where it increases the user understanding of the domain, facilitates the reconstruction of news story lines, and enables users to perform exploratory investigation of news hidden facts” [24, p. 1]. Other interesting functionalities among JKPs are: trends used to dis- cover emerging topics, long-term developments and changes in events over time [22, 25]; alerts to keep users up-to-date with the last incoming items [26]; summarisation of news histories and events to provide additional insights [22]; clustering of story-lines and events [27]; and personalisation of both the JKPs and its functionalities according to users’ preferences and profiles [21]. JKPs provide functionalities to news agencies and newsrooms organisation and workflows. JKPs are used as business support systems to manage internal newsrooms output; monitor what is being broadcast, produced and covered [27]; overcome limitations in newsrooms’ workflows; and improve productivity and revenues [20]. Another functionality provided by the JKPs is the content management which allows news agencies to produce, store, organise, manage, maintain and distribute the content and metadata produced every day [20]. 8. Components JKPs rely on different components to fulfil its function- alities and support users. We split JKP components into five sub-categories: input, processing, storage, in- teraction and output. As input, we consider the different sources of con- tent and information used in JKPs that are relevant for stakeholders. The textual and multimedia sources are sources of interest. However, not all analysed projects treat the information in the same way or use the same information types, like ASRAEL which only uses the title and first paragraph to represent the events [30]; and not all contents receive the same interest by news professionals, as in SUMMA which considers enter- tainment programming such as movies and sitcoms, commercial breaks, and repetitions of content (e.g., on 24/7 news channels) [...] of limited interest to moni- toring operations” [27, p. 1]. The processing components cover tasks from har- vesting and annotating input sources to processing and lifting them, following an ETL process (i.e., Ex- tract, Transform, Load). Input sources are harvested using different components, each with a specific pur- pose: harvesting, translating, filtering and transcribing. A common characteristic of the analysed projects is that source selection and monitoring functionalities are conducted in real-time by harvesting informa- tion sources [22, 23, 27]. The harvested content is then translated [27] and filtered according with the different stakeholders’ interests and needs. Spoken content is transcribed [27] and images are textually described [21]. JKPs use specific components to automatically an- notate the harvested content with metadata to support functionalities like business support, content manage- ment and personalisation [20]. The annotated content is typically processed by different components which are organised in an NLP pipeline. The NLP pipeline processes the content through state-of-the-art NLP and NLU modules to perform linguistic tasks [25, 24]. These tasks are focused on capturing and extracting the different information types described in section 6. Both the results of the NLP pipeline and the annotated content are disambiguated and represented semanti- cally using lifting components. The lifting component links the semantic representation of news items to a knowledge base, for examples an RDF-based knowledge graph [25], and enriches the semantic interpretations with facts from external knowledge bases, for example from the LOD cloud [24, 30]. The JKP storage infrastructure is normally composed of an archive, a knowledge base and an ontology. The archive stores news articles, biographies, reports [25] and other textual and multimedia items; the knowl- edge base is where the lifted semantic representations of news items are stored and enriched with external information [24]; and the ontology is used to represent the structure of the news items, leveraged information. metadata and vocabulary [20]. JKP users interact with the previous components mainly using three types of interaction components: front-ends, tools and query engines. JKPs provide front-end components [21] to allow stakeholders to access the system functionalities; tools which offer features to journalists when creating news articles or to general users when interacting with the system, like money converters or dictionaries [20]; and query engines that allow users to query, analyse or visualise the database through APIs [27]. News agencies use two types of distribution com- ponents for delivering content to their audience and customers [20]: push and pull. Push components of- fer interfaces where information consumers can select and subscribe to streams of news [20], whereas with pull components, news agencies offer interfaces to ac- cess, browse and query their repositories [20]. 9. Techniques Techniques used in JKPs can be grouped in eight sub-categories: semantic technology, fact extraction, conceptual model, reasoning, network analysis, event analysis, NLP and training. Semantic technology is used to support functional- ities like knowledge discovery, news creation, verifi- cation, clustering, trends, and content management. Semantic technologies support knowledge discov- ery by providing means for lifting news items, and disambiguating, enriching and leveraging them with information from external knowledge bases [21, 25] – processes carried by the lifting, ontology and knowl- edge base components; news creation, by providing systems and vocabulary to automatically annotate news in annotation components [21]; and verifica- tion, by combining semantic technologies with the lifting and knowledge base components and linking factual claims to its sources and external knowledge bases [24, 27]. Semantic technologies and semantic representation techniques facilitate clustering news items and events [30], and detecting trends and story lines [24]. Moreover, semantic technologies provide shared semantic resources and formats which are used to support content management and facilitate conceptual interoperability [25]. Fact extraction techniques extract facts from news items and link them to facts in external knowledge bases (e.g., Wikidata, Wikipedia). These techniques are used to provide functionalities like verification and knowledge discovery [27] and are common features of lifting, knowledge base and query components. Conceptual models provide vocabularies and ontolo- gies which are used in conjunction with semantic tech- nologies to support and standardise functionalities like content management and personalisation. Ontologies can be used for defining user interests and preferences based on the provided vocabulary or as shared mod- els [20]. Conceptual models are applied in distribu- tion, lifting, annotation, ontology, query, knowledge base and source components. Both conceptual models and semantic technologies facilitate the usage of other techniques like reasoning, network analysis and event analysis. These techniques support functionalities like knowledge discovery, clus- tering and trends, and are applied in the lifting, knowl- edge base, ontology and annotation components. Rea- soning techniques abstract and infer new knowledge from news items, events and temporal aspects [24, 25]. Network analysis is used to find networks of actors and organisation implications through different events and time [24]. Event analysis is applied to detect, identify and annotate the events described in news [21, 20]. The above techniques are supported by NLP tasks like named entity detection, role detection, topic de- tection, temporal expression normalisation, temporal relation detection, factual claims extraction, natural language understanding [25, 29, 27]. These NLP tasks, among others, are also used in JKPs’ functionalities such as knowledge discovery, content management, summarising, verification, trends, clustering, query, lifting and annotation. In order to obtain optimal re- sults from the NLP tasks, different training techniques have to be used over extensive news corpus [30]. 10. Other aspects Stakeholders, information, functionalities, compo- nents and techniques are influenced or affected by additional concerns of various types. We organised these other aspects into the following sub-categories: standards, proprietary, human factors, customers het- erogeneity, big data, multilingual, timeliness, quality, software architecture, performance, maintenance, and legacy. Before moving into JKPs, news agencies used their terms, categories and vocabularies to describe their items. Yet, the interoperability between news agencies and customers was difficult. The usage of standards like like IPTC news codes and media topics, semantic vocabularies, NAF and RDF improved the interoperability between news agencies and other stakeholders [20]. JKPs keep track of proprietary news information like authorship, copyrights and sources [21, 20] as a part of the content management functionalities. Property information is used as metadata in annotation compo- nents and provides provenance and reliability infor- mation [24, p. 4]. There are different human factors influencing JKPs and stakeholders. Before JKPs, news professionals were performing many processes by hand like news tagging, verification tasks, fact searching, finding related articles, and source monitoring. Performing these tasks manually is time-consuming, error-prone, consumes a lot of efforts, and reduces the amount and precision of the added metadata [21, 20, 28, 22]. Therefore, customers have to manually filter irrele- vant content received from news agencies, creating an information overload problem which is contrary to the information relevance that customers expect from news agencies [21, 20]. Moreover, because the difficulty of manually monitoring and finding related articles from other news providers, the audience, customers and news professions can get biased or incomplete information [22]. Customers are heterogeneous, they have different in- formation needs and use different systems to interact with news agencies [20]. According to our study, JKPs deal with big data requirements like volume, velocity, variety: The AS- RAEL project estimated that the number of collected articles ranges between 100.000 and 200.000 articles per day” and collected news articles from around 75.000 news sources” [22, p. 1]. NewsReader used an archive that contains billions of articles, biographies, and reports” [25, p. 1]. The SUMMA platform [was] able to ingest 400 TV streams simultaneously” [27, p. 6]. Other information aspects that JKPs deal with are the multilingual and timeliness data aspects. Infor- mation and news production are created in multiple languages (e.g., Catalan, Norwegian, Spanish, En- glish, Italian, French, Portuguese and Chinese) and need to be translated, transcribed and delivered to customers and audiences in their languages of prefer- ence [20, 27, 25, 30]. The timeliness aspect refers to the temporal aspect of events, thus news professionals, audience and customers want to receive the informa- tion as soon as it is generated [21] and reconstruct story-lines or histories over time [24, 27]. Quality of the results and outputs of JKPs are summarised in news agencies are required to pro- vide fresh, relevant, high-quality information to their customers” [21, p. 1] and ignoring these quality requirements can imply economic losses for cus- tomers [20]. Aspects concerning technical agents and their components include the software architecture, perfor- mance, maintenance and relation of JKPs with other systems. The software architecture of JKPs should consider scalability to deal with big data require- ments [21, 24, 27], distribution to run its components and systems over multiple machines [20, 26], com- ponents independence so they can be used for other purposes [26], interoperability between components and systems [20, 25], and performance for reducing the processing and distributing time of information and live feeds [21, 24]. Manual maintenance is a time-consuming and error-prone task [20] which is automated with JKPs to keep the JKP and on- tology up-to-date [26]. As JKPs communicate with customers systems, legacy components and other newsroom systems, JKPs need to be designed to fa- cilitate the integration with other technologies and systems [20, 26]. 11. Conclusion JKPs are a new type of platforms which offer many opportunities for newsrooms and journalists by com- bining AI techniques such as knowledge graphs, LOD and NLP to improve and facilitate the production of high-quality journalism. We collected challenges and opportunities that JKPs present and organised them into six categories that we assume are important for the evolution of JKPs (stakeholders, information, functionalities, components, techniques and other aspects). JKPs offer new opportunities for consuming and interacting with news by providing enriched content from external sources like Wikipedia or Wikidata to stakeholders seeking relevant information, such as news professionals and general audiences. News texts are enriched with additional information about, e.g., involved actors, places and organisations, the connections with other news and related events. In- formation and data sources in JKPs are no longer split along dispersed and disconnected repositories as it happens in traditional solutions. Instead, the infor- mation pieces are connected by the knowledge graph. JKPs enhance functionalities like news creation and content management. News creation is improved with background information providing journalists with better information for their stories. Automatic meta- data annotation and the usage of standards like IPTC relieve archivists from manually annotating news and improve the content management capabilities of JKPs and newsroom workflows. Knowledge graphs in JKPs bring new forms of representing news-related content and exploiting it. Techniques like network analysis, event analysis and reasoning improve the background information and knowledge discovery in JKPs while opening new research questions for researchers. JKPs can use standards such as RDF, IPTC’s media topics and semantic vocabularies which simplify the interop- erability and understanding between news agencies and stakeholders. The most highlighted opportunities that have been identified in the literature include event detection and analysis over time, real-time and up-to-date trustworthy information, access to en- riched background information for supporting news creation, multilingual and multimedia cross-platform solutions, and tools for monitoring worldwide media output and internal newsrooms production. On the other hand, providing one-size-fits-all JKP solutions for all possible stakeholders is challenging, because of their diversity and differing information needs. Newsworthy information comes from diverse news sources like pre-news information from so- cial media or multimedia sources such as TV news programs. Leveraging these information sources is a complex task which requires new techniques to distinguish potentially newsworthy information from non-relevant content and extract information from multimedia items like images or videos. Summaris- ing and presenting news-related information in JKPs like background information, events in time or actor networks to users with different information needs and skills is not a trivial task. JKPs consist of different components which interact together and with exter- nal components that need to be integrated in JKPs systems. Extracting precise semantic representations of and reasoning over relations and time remain open research questions. JKPs deal with big data, but some semantic technologies, reasoning and AI techniques are not yet ready for it. Among the reviewed JKPs, the most common challenges are problems such as language independence, multiple news channels, complex newsrooms workflows, dispersed and diverse information, lack of facts, and integration with legacy and customer systems. After reviewing the literature, we have realised that there is not a clear definition and agreement about what constitutes an event. The event concept is used in different ways in the literature, from a handshake between two actors to bigger events like the Spanish Civil War or events in between such as a trial process. In this study, we have only reviewed five JKP-related research projects, although they are the five most cen- tral ones we have found. Hence, we may have omitted important issues that were not represented or brought up in these projects. We are therefore planning to ex- tend the number of considered projects through a sys- tematic literature review and contrast and expand our findings with published works on data and digital jour- nalism. A logical continuation of this expanded study is the formal identification and modelling of goals, re- quirements and use cases for JKPs, which we did not find yet in the literature. Furthermore, we plan to for- malise a reference framework for JKPs and continue the development of our JKP identified to validate and integrate our findings.","Gallofré Ocaña, M., & Opdahl, A. L. (2020). Challenges and opportunities for journalistic knowledge platforms."
ML_DB_122,Fact-checking statistical claims with Tables,"The surge of misinformation poses a serious problem for fact-checkers. Several initiatives for manual fact-checking have stepped up to combat this ordeal. However, computational methods are needed to make the verification faster and keep up with the increasing abundance of false information. machine learning (ML) approaches have been proposed as a tool to ease the work of manual fact-checking. Specifically, the act of checking textual claims by using relational datasets has recently gained a lot of traction. However, despite the abundance of proposed solutions, there has not been any formal definition of the problem, nor a comparison across the different assumptions and results. In this work, we make a first attempt at solving these ambiguities. First, we formalize the problem by providing a general definition that is applicable to all systems and that is agnostic to their assumptions. Second, we define general dimensions to characterize different prominent systems in terms of assumptions and features. Finally, we report experimental results over three scenarios with corpora of real-world textual claims.",Computer Science,Article,2021,N,Y,2,"Large scale spreading of incorrect information on the internet is a real threat that poses severe societal prob- lems [30]. As no barriers exist for publishing information, it is possible for anyone to diffuse false or biased claims and reach large audiences with ease [6]. This raises the important issue of how to tame the spread of false information, as this has affected public votes1 and has misinformed people about coronavirus remedies2 and spread3. Accordingly, there has been a great demand for fact-checkers to efficiently verify such news. Indeed, with the easy accessibility of large social networks and the advent of generating text using recent advances in Natural Language Processing (NLP) [12, 32], the surge of false news has overpowered the capabil- ities of manual fact-checking. Malicious users in social networks are still allowed to profit from misinformation and the affected networks have just started to take effective actions [1]. At the beginning of the COVID-19 pandemic, the spread of false coronavirus news has urged the World Health Organization to spotlight this issue, labeling it as an infodemic [2]. One approach to deal with this enormous volume of information is computational fact-checking [34], where parts of or the entire verification pipeline is automated, usually including some ML algorithms [25]. One influential system is ClaimBuster [14], which is an end-to-end fact-checking solution that Copyright 0000 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE. Bulletin of the IEEE Computer Society Technical Committee on Data Engineering Courtney Lee 39:08 9 14 3 22 Marc Gasol 35:23 6(2) 12(2) 6 18 Zach Randolph 29:26 4 9 0 10 Mike Conley 29:13 9 14 11(1) 24(4) Tony Allen 23:10 4 6 1 9 Quincy Pondexter 26:43 2 8 0 7 Beno Udrih 18:47 3 6 3 6 Jon Leuer 16:13 1 4 0 2 Kosta Koufos 12:37 0 2 1 0 Vince Carter 9:20 2 5 0 4 Table 1: Statistics of a basketball game. relies on NLP and supervised algorithms to identify and check factual and false information. Since then, there has been a stream of fact-checking systems. Some systems verify a certain input by utilizing structured data stored in knowledge graphs [16, 5] or re- lational tables (or just relations) [18, 19], while others rely on unstructured resources such as Wikipedia arti- cles [27, 36]. Other systems have the ability to verify multiple claims occurring within the same input text, such as an entire document [19, 15]. A recent approach also discards all evidence retrieval methods and relies on the implicit knowledge stored in large pre-trained language models (PLM) [20]. Nevertheless, the plethora of different methods calls for a thorough study of their differences with an exploration of the salient aspects related to the design of the systems and how they relate to the fact-checking process. Such study aims to (i) provide readers with a set of dimensions to model this kind of systems, (ii) inspect prominent systems by testing them on various datasets. We are interested in claims that can be verified by using existing relational tables. Storing data in tables is the go-to format for many applications as it offers declarative querying, scalability, and several other features. For example, reliable statistics for coronavirus are published on a daily basis as relations 4. The use of such tables supports the verification process and can help in relieving the work done by human fact-checkers. In- deed, manually verifying textual reports, which summarize the most important statistics, is time-consuming and requires automation [25, 3]. In this article, we start with formulating the problem of the computational verification of textual claims by using relational data (Section 2). We discuss four recent systems and highlight their main differences in terms of six generic dimensions (Section 3). We then experimentally compare the systems on several annotated claim corpora (Section 4). Finally, we conclude with some open challenges and future directions for research in this topic (Section 5). 2 Problem Statement We introduce our problem and related terminology. We assume a scenario where we have a natural language text containing one or more claims to be verified with some relational table(s). Such table(s) are either given as an input or predicted by a system. The following example contains (hypothetical) claims about a basketball game. Table 15 contains the information needed to verify such claims. Table values that are used to verify a claim are marked with the same superscript. 2 Example 1. Mike Conley had 11(1) assists. The field goal percentage for Marc Gasol is 50%(2). The team scored 100(3) points in total. Mike Conely scored the most(4) points.” The first claim can be verified with a simple look-up in the table over the Assists attribute. The second claim can be verified by computing the ratio of the Field Goals to the Field Goal Attempts of a certain player; thus, two cell values are needed for verification. The third (false) claim can be checked with an aggregation (summation) over the Points column. The fourth claim involves finding the player with the maximum number of Points. Other claims might need the involvement of two or more tables. Definition 1. Fact-checking is the process of checking that all facts in a piece of text are correct. Fact-checking is usually composed of three phases: (i) finding check-worthy claims, (ii) finding the best available evidence for the claim at hand, and (iii) validating or correcting the record by evaluating the claim in light of the evidence [25]. Claims can be provided in a structured form, such as the subject-predicate-object triples in a knowledge graph [5], or in plain text [31], such as the sentences in Example 1. In this article, we assume that the first step (i) has already been executed, and every sentence contains at least one claim. Definition 2. A textual claim is any subset of a natural language input that is to be verified against trustworthy reference sources. Data in such reference sources can be structured or non-structured. Non-structured data include textual documents while structured data include knowledge graphs, such as DBpedia [21], and relational tables. In this work, we are interested in relational tables as reference data. Specifically, we focus on tables that contain numerical data and on which numerical and Boolean operations can be computed. Definition 3. A statistical claim is any textual claim that can be verified over a trustworthy database by per- forming a mathematical expression on the database cell values. The claims in Example 1 are all statistical claims, while a claim such as Players who commit too many fouls are not allowed to play anymore.” is not. Definition 4. An explicit claim is a statistical claim mentioning a number that can be verified by comparing it against the result of a function that takes as parameters some cell values in the input relation. The first three claims in Example 1 are explicit claims. We assume that symbols LOOKUP, SUM, and DIVISION are defined. LOOKUP performs a look-up in a table given a primary key value and an attribute label. SUM performs a summation over the values of an attribute. DIVISION performs the division of two cell values. The first claim is a simple look-up over the table that could be modeled as LOOKUP(’Mike Conley’,’Assists’)==11. The second claim requires computing a ratio of Field Goals to Field Goal At- tempts for player Marc Gasol. This can be formulated as DIVISION(a,b)==0.5 where a=LOOKUP(’Marc Gasol’,’Field Goals’) and b=LOOKUP(’Marc Gasol’,’Field Goal Attempts’). The third claim could be modeled as SUM(’Points’)==100. Definition 5. An implicit claim is a statistical claim that does not mention a number and can be verified by a Boolean function that takes as parameters cell values in the input relation. The last claim in Example 1 is an implicit claim. Assuming MAX has also been defined, it could be mod- eled as MAX(Points)=LOOKUP(’Mike Conely’,’Points’). As we will discuss in the next section, implicit claims are harder to verify and usually require some form of supervised learning, such as the learning of neural representations [19] or the synthesis of a program from the input [8]. Definition 6. Given a text T containing a statistical claim c and a database D, the goal of Statistical-Claim Fact-Checking is to verify c with the information in D. Formally, the objective is to find a function f (T, c, D) that successfully maps to one of two labels (True or False). 3 This definition is generic enough to model existing fact-checking systems and other systems that can be adapted for this task6. It is independent of the different assumptions that apply for the different approaches. For example, multiple systems assume that the input text T contains a single claim [8], thus dropping the need for having the claim c as an input. Also, the database D is often simplified to one relational table given as input [15, 8], while other systems utilize multiple tables [19, 18]. Finally, the definitions above do not cover a notion of explainability of the provided result. Indeed, some systems do not provide any result explanation since the verification process relies on black-box methods, such as deep neural networks [8]. 3 Systems In this section, we study four systems that satisfy Definition 6. We analyze TABLE-BERT [8], TAPAS [15], AGGCHECKER [18], and SCRUTINIZER [19]. In Section 3.1, we describe how each system works and its assumptions. In Section 3.2, we introduce six dimensions to characterize such fact-checking systems. 3.1 Overview of Systems We introduce the systems, starting from the ones that rely on end-to-end NLP methods, and describe those that use query generation next. TABLE-BERT [8] models fact-checking of a statistical claim as a Natural Language Inference (NLI) prob- lem [22]. NLI is the task of determining whether a natural language hypothesis h can be inferred from a natural language premise p. In TABLE-BERT, a given table T is linearized and fed to the model alongside the natural language hypothesis p. The model consists of a pre-trained BERT model [12] that outputs a sequence-level representation of the input. This representation is then fed into a multi-layer perceptron, which predicts the entailment probability. If the output probability is greater than 0.5, then the hypothesis p is entailed by table T. This system assumes a table as input, i.e., that the reference data is available and already identified. TABLE- BERT comes with a corpus of tables and claims (annotated as true/false) that can be used for fine-tuning. This makes it usable on unseen tables, but our experimental results show that further fine-tuning for the domain at hand is needed to obtain good results. Moreover, TABLE-BERT can be fine-tuned with more examples that contain formulas unseen in the provided corpus. However, the original paper recognizes that the complexity of the formulas that the system can learn is limited and does not support composition of functions [8]. TAPAS [15] can be used to tackle the claim verification as a question-answering problem over an input table [4, 23, 33]. The model takes as input (1) a natural language question Q to be answered over (2) the input table T. Building on the success of pre-training models on textual data, TAPAS extends this procedure to structured data, by training a BERT [12] model on a large number of natural language questions and Wikipedia tables. This process enables the model to learn correlations between structured and unstructured data. After training, the encoder provides a representation of the input. The output is twofold: the model predicts (1) which cell values of the input table are used for answering the question and (2) what aggregation operation is performed on such values to produce an answer for the input question. As with TABLE-BERT [8], TAPAS assumes that the reference table is available, and is linearized in the input. Generating questions from the claim could be done using lexical-based methods as pioneered in Claim- Buster [13], or neural-based methods [35, 7]. While TAPAS has the benefit of being general, i.e., plug and play” on new domains, it has the limitation that extending it to new formulas or tables requires the full re-training from scratch. Moreover, it is not demonstrated that it could learn complex formulas. 6While the definition considers a binary label for the output, it can be extended to multiple labels such as partially true” or not enough evidence”. 4 AGGCHECKER [18] takes a relational dataset and a text document as input. It translates the natural language claims in the document into SQL queries that are executed to verify the claims. More specifically, each claim is mapped to a probability distribution over SQL queries. SQL queries are formed by combining query fragments using an iterative expectation-maximization procedure [11]. AGGCHECKER works out of the box on unseen relations and does not assume that training data is available for a new database. The system supports aggregation functions and could be extended to support more. Extending the system is not trivially done by just feeding more binary examples. It needs an update to the information retrieval engine to incorporate new query fragments, and an update to the probabilistic model to account for new SQL query candidates. A module to account for multi- variable formulas is also needed. Similar to TAPAS, the modification needed for this system to account for unseen functions goes beyond examples. The system benefits from the fact that claims in the same context are often semantically correlated by learn- ing a document-specific prior distribution over queries. As in practice accurate claims are more likely than inaccurate claims, the system increases the likelihood of the query which has a match between the query result and the claim. As multiple candidate queries are to be executed, an execution engine that merges execution of similar queries is used for efficiency. SCRUTINIZER defines fact-checking as a mixed-initiative verification problem [19]. The approach combines feedback from human workers with automated verification coming from ML classifiers. We neglect the human- in-the-loop part in this article and focus solely on the automatic verification. The system is based on four classifiers that take a statistical claim as input and predict (i) the relation(s) to be used, (ii) the primary key value(s), (iii) the attribute label(s), and (iv) the formula applied on the cells identified by the former three. In contract with other systems, the table is not given as input, but is predicted, and the cell selection is based on the predicted primary key values and attribute labels for such a table. This leaves out the need for inputting the table to the model, but limits the current system only to the table schemas seen during training. After cell selection is done, it can apply the predicted formula and verify the input claim. SCRUTINIZER can learn any query, including complex formulas, from the training data. However, the price to pay for this generality is that it trains the classifiers, therefore labels for these must be provided, and it does not suffice to have the true/false label for the claim as in TABLE-BERT. Aside from AGGCHECKER, all the systems use transformer-based language models [12] to encode lan- guage knowledge, but only TAPAS requires the expensive pre-training of such models. AGGCHECKER relies on a probabilistic model to map natural language claims to a probability distribution over queries. Others solu- tions rely on synthesizing a logical program [8], recurrent-based language models [26], reinforcement-learning approaches [37], and graph neural-networks [24]. Type Dimension AGGCHECKER TAPAS TABLE-BERT SCRUTINIZER Input Implicit Claims X X X Schema-Independence X X X – Multi-variable Formulas X – X – X Multi-tables X – X Output Interpretability X X – X Alternative Interpretations X X Table 2: Dimensions that characterize the systems (X – denotes partial support). 3.2 System Dimensions We believe that, given the increasing number of fact-checking systems, it is important to start characterizing them with clear dimensions to enable a more rigorous comparison. We first describe four main dimensions that 5 characterize the input across the different proposals. Then, we discuss two dimensions that characterize the output. A summary of the dimensions and how systems support them is reported in Table 2. 3.2.1 Input Dimensions Explicit claims are handled by all fact-checking systems, as they are much easier to deal with. However, the support for Implicit Claims requires a deeper understanding of the semantics behind the given sentence. One approach to dealing with this problem is feature-based entity linking where all entities are detected in the input statement and a set of pre-defined trigger-words are used to build programs representing the semantics of the statement [8]. However, such approaches are very sensitive to the error-prone entity linking process. Another approach is to learn such implicit claims in a supervised manner. SCRUTINIZER learns from the classifiers’ labels [19]. TAPAS also learns correlations between the text and the table during the pre-training process. Another dimension is Schema-Independence. AGGCHECKER, TABLE-BERT and TAPAS can consume potentially any table with any unseen schema, while SCRUTINIZER is limited to tables whose row index val- ues and attribute labels have been trained on. For SCRUTINIZER, adding new tables requires fine-tuning the classifiers. The operation is not expensive in terms of execution time, because its classifiers are based on a fine- tuning procedure, rather than having to pre-train again from scratch; however, it requires specific annotations that go beyond the true/false binary label. This dimension highlights that SCRUTINIZER is domain-specific and thus has to learn the related tables for the task at hand, while AGGCHECKER and TAPAS try to be agnostic of the table schema, and can handle any table as input. For TABLE-BERT, while it can be used on any unseen schema, our experiments show that it should be trained on the examples at hand in order to obtain good accuracy performance. In practice, computations involving values of a database go beyond simple look up and aggregation func- tions such as those reported in Example 1. The function for the verification of a claim can require complex Multi-variable Formulas. For example, the Compound Annual Growth Rate7 is a formula needed to verify a claim in our experiments. SCRUTINIZER handles complex formulas on the condition that they are observed in its classifier-specific training data, and resorts to a brute-force approach to assign predicted values to vari- ables. AGGCHECKER can be extended to handle complex aggregation functions. TAPAS handles aggregate queries with simple functions where the cell values have been selected by the model. It is not clear if and how TAPAS could support functions with more than one variable, and it would require training again the model from scratch such that new functions are learned. Finally, TABLE-BERT has no explicit notion of formulas, as it is a black-box model fine-tuned end-to-end on a binary classification task. According to the original paper and our experiments, TABLE-BERT struggles to learn how to handle formulas with multiple variables. TABLE-BERT and TAPAS assume that the right table to verify the input claim is also given as input. In practice, many tables can be available and the most likely one for the task at hand is identified by SCRUTINIZER and AGGCHECKER (Multi-Tables). Moreover, in some cases more than one table is needed to verify a claim and only SCRUTINIZER supports verification that requires the combination of values from multiple tables. This dimension highlights one of the limits of the methods that rely on the linearized data fed to the transformers, as it is hard to feed multiple tables without hitting the limit on the size of the input. 3.2.2 Output Dimensions Interpretability is a key dimension supported by methods that output the query used to verify the claim. How- ever, systems using a black-box model to verify claims, such as TABLE-BERT, lack interpretability as an expla- nation of the prediction is not provided. There do exist methods attempting to explain black-box models which 7It describes the net gain or loss of an investment over a certain period of time 6 include explanations by simplification [29]. However, there is no consistent method to define how faithful are the explanations to the model prediction [17]. TAPAS is not fully interpretable since it provides only cell values and, in some cases, the aggregation operation. AGGCHECKER and SCRUTINIZER expose the declarative query used to verify the associated claim. Systems that predict query fragments and combine them, rather than producing an answer in one shot, are easier to interpret [10]. Claims expressed in natural language can be incomplete or ambiguous in many ways. Some systems support Alternative Interpretations to clarify how the output changes depending on the details of the verification. Consider a simple claim Mike scored 30 points”, and a table with two players whose first name is Mike”. The claim is true for one player, but false for the other. AGGCHECKER resolves such ambiguities by evaluating multiple queries and soliciting feedback from users. SCRUTINIZER learns ambiguities conditioned that they are represented in the training data. TAPAS and TABLE-BERT do not include any clear means to resolve this kind of ambiguities, as they default to one interpretation in the current architectures. 4 Experimental Evaluation We evaluated the four systems above by using three datasets with real textual claims manually annotated with the correct checking label. We concatenate the claim to the sentence in case the sentence has multiple claims; otherwise, we only input the sentence. For every system, we report its coverage of the claims, the accuracy of the verification process, and the execution times. SCRUTINIZER Labels TABLE-BERT Label Sentence Claim Table Attribute Label Primary Key Value Formula Verdict There were 800 total deaths in China in May 2021. 800 total deaths total deaths May 2021 China a False Table 3: Labeled data for SCRUTINIZER and TABLE-BERT. 4.1 Datasets Our experiments are based on three use cases: Coronavirus scenario (C19), International Energy Agency (IEA) scenario, and Basketball Data scenario (BBL). Every example contains the textual claim, the relational table to verify it, and the outcome expressed as a binary label True/False. For SCRUTINIZER, the training examples contain also the labels for the four classifiers. For a fair comparison, we fed the associated relation as input to all systems. Given the limitations on the input size in TAPAS, we limit the input for this system to at most a sample of 11 tuples, including the one needed to verify the claim. Without this ad-hoc operation, the system fails with the entire relation as input. An example of our labeled data is shown in Table 3. For C19, we generated the training data from the relations (3M examples [19]) and used real claims for the testing. We denote the synthetic corpus as C19train. For testing the system with unseen claims, we analyzed the log of more than 30K claims tested by users on a website8. We found that more than 60% of the claims in such corpus are statistical and, among those, we have the datasets to verify 70%. From these claims, we randomly selected 55 claims and manually annotated them (C19test). For IEA, we obtained a document of 661 pages, containing 7901 sentences, and the corresponding corpus of manually checked claims, with check annotations for every claim from three domain experts. The annotations cover 2053 numerical claims, out of which we identified 1539 having a formula that occurs at least five times in the corpus. We denote the resulting dataset as IEAtrain. After processing the claims, we identify 1791 relations, 7 830 row indexes, 87 columns, and 413 formulas. Around 50% of the values for all properties appear at most 10 times in the corpus, with the top 5% most frequent formulas appearing at least 8 times. For the test data (IEAtest), we randomly selected 20 claims from the most common operations (look up and sum). For BBL, we use the data in a recent publication [28]9. We use the 1523 real annotated claims provided in the repository for the testing step and generate ourselves the training data from the tables as in the C19 scenario. We generate an initial dataset of 32.3K samples, where 90% is used for training classifiers and 10% for validation. We use 132 tables for this scenario. The dataset used for bootstrapping is denoted by BBLtrain and the test dataset as BBLtest. Our datasets (BBLtest and C19test) are publicly available10. Table 4: Ratio of supported training claims. AGGCHECKER TAPAS TABLE-BERT SCRUTINIZER C19train 21.49% 21.49% 37.82% 100% IEAtrain 33.06% 17.02% 27.28% 74.96% BBLtrain 53.00 % 56.17% 56.17% 56.17% As discussed in Section 2, the tested systems have limitations on the input data and on the space of supported formulas. These limitations are reflected in the percentage of training claims that every system can handle, as reported in Table 4. For example, complex formulas are present in our datasets, with more than 22% of the claims in IEAtrain with three or more variables. 4.2 Experimental Results For TABLE-BERT, we fine-tuned the binary classifier on top of the PLM with the training data after augmenting the data to ensure a balance between classes. For TAPAS, we tried to automatically translate the claims to questions as pioneered by ClaimBuster [13]. However, the precision was not satisfactory, e.g., we could not obtain any questions for 7 out of the 20 IEA test claims. To overcome this issue, we manually translated claims into questions for IEAtest and C19test, and relied on a pattern-based script to generate questions for BBLtest. For TAPAS and AGGCHECKER, we did not run any training. For SCRUTINIZER, we provided the examples with the labels for the 4 classifiers, and examples with binary labels for TABLE-BERT. For SCRUTINIZER, we do not rely on the user feedback in this experiment. Table 5: Verification accuracy on the test datasets. AGGCHECKER TAPAS TABLE-BERT SCRUTINIZER C19test 0.44 0.64 0.76 0.80 IEAtest 0.50 0.07 0.58 0.65 BBLtest 0.13 0.41 0.17 0.51 Table 5 reports the accuracy results of the experiments with all systems on the test claims. IEAtrain is heavily skewed as, for instance, there are formulas such as lookups and summations that are commonly used, unlike formulas comprising multiple variables which are scarce (formulas having ten or more variables form 4.32% of the training data). The formulas in IEAtest are different, as they contain functions supported by all systems 8 Table 6: Execution time of the test datasets (seconds). AGGCHECKER TAPAS TABLE-BERT SCRUTINIZER C19test 280.41 991.52 23.09 0.03 IEAtest 321.53 943.25 18.11 0.68 BBLtest 3472.44 12709.45 40.20 236.64 (lookups and summations in this case). For BBLtest, low results are explained by the fact that all systems have low coverage of the claims in the data and some claims require verification that spans across multiple tables. For C19test, the systems do slightly better as the test data comprises lookups and the attribute label and primary key value are usually explicitly stated in the sentence. We observe that the systems perform with mixed results, and none of them can get high accuracy in all cases. We can observe that in most cases the use of training data can lead to the best performance. This is evident for TABLE-BERT, which performs well in two datasets by making use of the true/false labels, and in SCRUTINIZER, which exploits the rich annotations for its classifiers and leads in all scenarios. However, for C19test, SCRUTINIZER fails for claims which require formulas that it has not seen in the training data. For running BBLtest with TAPAS and TABLE-BERT, we replaced abbreviations in the schemas of the table by their proper wording (for example, PTS” was replaced by Points”). This has improved the TAPAS accuracy from 0.19 to 0.41 and TABLE-BERT accuracy from 0.09 to 0.17. This is expected as such models, which have been trained on text and table together, correlate better a table schema containing Points” with the input text compared to the acronym PTS”. For the execution times, we distinguish the training and the testing. Classifier-training time is needed for SCRUTINIZER and TABLE-BERT; however, this is typically negligible (on the studied datasets) with the usage of GPUs. AGGCHECKER and TABLE-BERT, on the other hand, have zero setup time. We report the execution times for all test data in Table 6. TAPAS is the slowest as the model is jointly computing the relevant cells and performing an operation on them, compared to TABLE-BERT that requires a negligible amount of time to perform binary classification. SCRUTINIZER consumes negligible time in classifier predictions, but the brute- force query generation process could potentially take considerable amount of time when multiple combinations are available. AGGCHECKER, although having to perform evaluations of a large number of queries, successfully merges the execution of similar queries to increase efficiency. In summary, all systems are usable in reasonable time in our experience in an entry-level infrastructure with a low-end GPU. 5 Conclusions We focused our study on the problem of fact-checking a statistical claim with relational tables as reference data and considered four prominent systems. We make a first step towards categorizing fact-checking systems with generic dimensions. We have also experimentally evaluated the four systems on three use cases and gathered many observations on their coverage, their qualitative performance and their execution times. Our results and the proposed categorization can act as a blueprint when designing a system, as different applications have different requirements. For example, text coming from the basketball data scenario is unlikely ambiguous, so it is safe to neglect ambiguity resolution. However, text related to coronavirus is oftentimes ambiguous and resolution of the ambiguities is a must. Data-driven approaches excel with the provision that sufficient training data is accessible; however, this condition is not always easily met, as manual annotation is costly, especially in scenarios such as IEA where experienced labor is needed. Experiments highlight that training data generated from the tables is a valid solution, but it requires manual work. This aspect is especially important for SCRUTINIZER, which has the highest accuracy, but it is the system that requires most labeling efforts. We also remark that some systems worked only after pre-processing the input, by rewriting the claim as a question or by limiting and refining the tabular data. We can state that there is no one system that clearly fits for all scenarios. Choosing or designing a system has to be done keeping in mind the scenario(s) at hand. Finally, we discuss a promising research direction that we identified during the experimental campaign. Given that the systems are getting better at detecting a false claim, is there any hope that they learn how to repair a false claim with the correct information? Consider a basketball data scenario with claim Vince Carter scored 22 points in 39 minutes.” Having a look at Table 1, we see that the player scored 4 points in 9 minutes. A fact-checking system would mark the claims as false. However, we see in the table that another player (Courtney Lee) is the actual fit for the sentence. The sentence is still false, but if we aim at repairing it, the result will be very different. In one case we would change the values and in one case we would repair the claim with a different entity — which one is the correct fix? Looking at this example, someone may argue that the mistake is in the entity, following the principle that it is more likely to have one error rather than two in the same sentence. This is in line with several data cleaning systems for relational data, which repair tuples according to a minimality principle [9]. This aspect of fact- checking is not considered in t","Saeed, M., & Papotti, P. (2021). Fact-Checking Statistical Claims with Tables. IEEE Data Eng. Bull., 44, 27-38."
ML_DB_126,A value-driven approach to addressing misinformation in social media,"Misinformation in social media is an actual and contested policy problem given its outreach and the variety of stakeholders involved. In particular, increased social media use makes the spread of misinformation almost universal. Here we demonstrate a framework for evaluating tools for detecting misinformation using a preference elicitation approach, as well as an integrated decision analytic process for evaluating desirable features of systems for com batting misinformation. The framework was tested in three countries (Austria, Greece, and Sweden) with three groups of stakeholders (policymakers, journalists, and citizens). Multi criteria decision analysis was the methodological basis for the research. The results showed that participants prioritised information regarding the actors behind the distribution of mis information and tracing the life cycle of misinformative posts. Another important criterion was whether someone intended to delude others, which shows a preference for trust, accountability, and quality in, for instance, journalism. Also, how misinformation travels is important. However, all criteria that involved active contributions to dealing with mis information were ranked low in importance, which shows that participants may not have felt personally involved enough in the subject or situation. The results also show differences in preferences for tools that are influenced by cultural background and that might be considered in the further development of tools.",Social Science,Article,2021,Y,Y,3,"Misinformation in social media is currently attracting a lot of attention. Misinformation is not a new phe- nomenon and has probably existed since the dawn of humanity. Structural evidence of scientific research on mis- information can be found Allport and Postman’s (1946) basic law of rumour, which demonstrates that the strength of a rumour is dependent on the importance of the subject and individual con- cerns regarding it, as well as the time and ambiguity of the evi- dence on the topic. New technical capabilities, such as social media, have naturally made these effects more universal. The 2000s witnessed rapid developments in social media and its increased outreach to everybody with Internet access. This has facilitated the spread of information, including misinformation and rumours, in virtually everything from local neighbourhoods to global concerns (Del Vicario et al., 2016). Until recently, there has been limited scientific evidence on how to deal with misinformation, but research on the topic has increased rapidly over the past few years. For instance, researchers have suggested various ways of dealing with citizen awareness, such as nudging, as a way of vaccinating social media users against misinformation (Piccolo et al., 2019). Other topics studied include nudging for accuracy in sharing on social media (Pennycook et al., 2020) and the limits of human cognition in dealing with and spreading misinforma- tion. Finally, researchers have examined a variety of approa- ches for making fact-checking more efficient, such as automatic detection of misinformation and correction of data, while at the same time pointing out the importance of human fact-checkers, as fully automated fact-checking methods are not yet strong enough. 1 This systemic problem requires stakeholder involvement at different levels, as misinformation is so widespread and con- stantly changing. Extensive stakeholder involvement is necessary for designing policies, methods, and tools. However, existing approaches to developing online tools tend to follow the tradi- tional path of dissemination of knowledge from science to sta- keholders while viewing technology users as passive consumers of finished products rather than active co-creators. This is particu- larly alarming today when available anti-misinformation products and tools are still new to the mass market and hence malleable, which is rare in the life cycle of a product (Smith and Medin, 1981; Svahn and Lange, 2009). Value-based software engineering (Boehm, 2003) is an emerging approach that aims to develop software tools (e.g., the tool by Aurum and Wohlin, 2007) based on the values and objectives of various stakeholder groups (Biffl et al., 2006), providing an economic categorisation of the value concept based on the monetary exchange between a customer and a provider. In this study, we investigate two major research questions: ● What are preferences for, perceptions of, and views of the features of tools for dealing with misinformation? ● How do these preferences depend on the cultural back- grounds of stakeholder groups and participants? Our goal is to study the preferences of various stakeholder groups for features of tools, to study the impact of cultural background on these preferences, and to develop recommenda- tions for considering these preferences in the further development of tools for dealing with misinformation. The next section provides a background of misinformation and discusses why we need automatic tools in a general setting. Sec- tion ‘Methodology' describes the integrated methodology used, and Section ‘Results' presents the results and a discussion. Finally, Section ‘Conclusions' concludes the article. Background A variety of definitions exist for misinformation, disinformation, fake news, rumours, and similar terms, and a large number of them emphasise the distinctions between misinformation and disinformation, as well as between disinformation and fake news. A review of the 2016 Presidential election in the United States, for instance, identified six different types of misinformation: authentic material used in the wrong context, imposter news sites designed to look like known brands, fake news sites, fake infor- mation, manipulated content, and parody content (Wardle, 2016). Wardle and Derakhshan (2017) suggested that mis- information refers to misleading information created without the intent to harm, whereas disinformation refers to information deliberately fabricated with the intent to impact social groups or societies. Burgoon et al. (2003) discussed misinformation in terms of deceptive language and false context. Farrel et al. (2018) dis- tinguished between disinformation and misinformation, con- sidering both subsets of misinformation: Disinformation largely involves the intent to deceive, whereas misinformation does not need to involve intentional deception. Giglietto et al. (2016) proposed a taxonomy based on perceptions of the source, the story, and the context and decisions of the audience and the propagator. In their taxonomy, there is pure disinformation” when both the original author and the propagator are aware of the false nature of information but nevertheless decide to share it. There is misinformation propagated through disinformation” when information is originally produced as true and then shared by a propagator who thinks it is false. There is also disin- formation propagated through misinformation” when informa- tion is devised as false by a creator but is perceived as true by a propagator. Irrespective of such distinctions, both misinformation and disinformation impact the public debate on issues such as health and science (e.g., the anti-vaccine movement), foreign policy (e.g., the wars in Iraq and Ukraine), migration, elections and so on. Recognising this, researchers from a variety of disciplines, including social sciences such as journalism (Ekström et al., 2019) and psychology (Ecker, 2017), have examined misinformation and disinformation. The problems of misinformation and disin- formation are usually called wicked problems” by design scien- tists, as no single comprehensive solution is capable of fully resolving them and attempts to mitigate them often can make them worse. Some examples of this include the backfire effect (Nyhan and Reifler, 2010), false misinformation warnings (Freeze et al., 2020), and the naiveté of social engineering in technology (Tromble and McGregor, 2019). Misinformation and disin- formation are also studied with regard to social psychology (e.g., people’s values, beliefs, information literacy, and motivations), regulatory and technical perspectives (social media, detection tools), and the practice of fact-checking. Given the large volume of published work we rely here on Vanenzuala et al. (2019), who conducted a meta-analysis of 650 articles on this topic to identify regulatory, technical, and normative aspects of misinformation. Cognitive psychologists have investigated the effectiveness of corrections and warnings of misinformation for a long time. Ecker et al. (2010) studied whether the continued influence of misinformation can be reduced by explicit warnings at the outset that people may be misled. They found that a specific warning with detailed information was more efficient than a general warning reminding people that facts are not always properly checked. However, a specific warning can reduce reliance on an outdated source of information but not eliminate it. Pennycook et al. (2018) investigated how fluency via prior exposure con- tributes to the believability of fake news. They found that tagging fake stories as disputed is not an effective solution because it ARTICLE HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS 2 HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS | (2021)8:33 simply attracts even more attention to the problem. They also found that repeating headlines increases perceptions of their accuracy. Schwarz et al. (2016) found that the myth-versus-fact article format is not efficient to deal with fake news because such articles subtly reinforce the myths through repetition and further increase the spread and acceptance of misinformation. Unfortu- nately, such articles make misinformation even more easily accessible by repeating it and illustrating it with pictures. This increases the probability that misinformation that the commu- nicator wanted to debunk will continue to be delivered. They found that it is better to simply provide correct information rather than try to correct wrong information. They also identified five criteria that people use to assess the accuracy of information: acceptance by others, amount of supporting evidence, compat- ibility with one’s own beliefs, general coherence of the statement, and credibility of the information source. Lerman (2016) stated that the interplay between humans’ cognitive limits and the social media network structure influences the spread of information. Finally, Chan et al. (2017) found that debunking messages for the correction of misinformation only increases the effects of the misinformation. Misinformation and tools for mitigating it. Several tools have been developed to counter misinformation, such as Botometer, Foller.me, TinEye, Insigna, Rbutr, Fakespot, NewsGuard, Greek Hoaxes Detector, DejaVu and Social Sensor. ● Botometer detects social bots and classifies online social media user accounts as either bots or human beings. This classification is based on various features of the user account profile, online social network structures, historical patterns of activity, and language and sentiments (Yang et al., 2019; Botometer tool, 2019). ● Foller.me analyses the profiles and tweets of social network users and shows various user characteristics, for example, general information such as name, location, language, join date, and time zone; statistics about tweets (number of tweets, followers, following); and tweet analysis (tweet replies, retweets, tweets with links). The main idea is to understand the detailed profiles of social media users to verify social media content (Sloan and Quan-Haase, 2016; Foller.Me tool, 2019). ● TinEye analyses user-generated content, like photos and videos, as well as detects whether an image, audio content, or video content is fake (Middleton, 2017; Tineye Tool, 2019). Members of the global community, in particular journalists, use this tool and others, such as FotoForensics and Google Reverse Image, to examine user-generated content. ● Rbutr is a machine-learning algorithm applied to community feedback to capture webpages with disputed, rebutted, or contradicted parts elsewhere on the Internet. This tool also provides sector-wise (e.g., health, education, immigrant, climate change) repositories of news and community rebuttal (Mensio and Alani, 2019) and provides warning messages (e.g., This is potentially malicious”) for particular news webpages with a bad reputation. ● Fakespot is a browser plugin that assesses the validity of online reviews based on their URL (Mensio and Alani, 2019; Fakespot Analyzer Tool, 2019). ● NewsGuard is another browser plugin that integrates the opinions of a large pool of journalists and informs users about the reliability of news websites and organisations. It uses nine journalist credibility and transparency criteria that are combined into labels (NewsGuard Tool, 2019). ● Greek Hoaxes Detector is a browser plugin that analyses news articles and assigns labels such as scam,” hoax” or fake” (Ellinika Hoaxes Tool, 2019). ● DejaVu is a system for detecting visual misinformation in the form of image manipulation aimed for use by journalists (Matatov et al., 2018). ● Social Sensor is a software that gathers social media data and analyses trends and what influences them (Schifferes et al., 2014). The aforementioned tools were designed for particular purposes and are limited in several respects, such as the following: 1. Requirements for participation: Some tools were developed based on stakeholder feedback. However, the developers did not involve end users in the process of developing the tools. They also did not collect end users’ preferences regarding these tools. When stakeholders were involved, it was frequently only one group of stakeholders or a very narrow circle of professionals who deal with misinformation. This has resulted in a narrow focus on professional intent instead of on how consumers of information can reduce their uptake of misinformation. 2. Technical issues: Almost all of these browser plugins support only Google Chrome. 3. Lack of integration of the views of fact-checkers: Fact- checkers are part of a growing community that plays an essential role in media policies. However, several of these tools were developed without any consideration from this community, which has led to unnecessarily incomplete detection mechanisms. Consequently, the full potential of fact-checking services has not been fully realised, and the lack of transparency in development and input parameters makes them unclear. This has led to decreased user trust, which is why it seems reasonable to evaluate the functionality of existing fact-checking tools to identify possible gaps. This is best done in a collaborative environment with a high degree of involvement by relevant stakeholders (Horne et al., 2019). A few studies have focussed on assessing the perceived needs of journalists navigating misinformation. In Schifferes et al. (2014), 22 journalists participated in an interview regarding the functionalities most relevant for tools countering online mis- information. According to this study, journalists emphasise the need to predict breaking news and verify content on social media as true or false. Brandtzaeg et al. (2018) conducted a study with 32 journalists and social media users on perceptions of fact- checking tools, concluding that users must be able to understand the limitations of tools and that tools need to be transparent on all ends, including in terms of funding. To the best of our knowledge, policymakers have not yet been included in such studies, although it is clear that policies desire the delivery of tools for dealing with misinformation. Participatory governance and value-based software engineer- ing. Several scientific works have discussed the need to under- stand the typology and features of misinformation (Rossi and Lenzini, 2020; Koulolias et al., 2018). The design and evaluation process we argue for in this article involves two components: (a) co-creation by users and elicitation of user preferences and (b) adequate aggregation and evaluation mechanisms. By co-crea- tion,” we mean a process that is aligned with Peters and Heraud (2015) and Gummesson et al. (2014) as an adaptive and inclusive approach to participatory governance based on the engagement and involvement of various stakeholder groups. Participatory governance, which is embodied in processes that empower citi- zens to participate in public decision making, has been gaining acceptance as an effective means of tackling democratic deficits and improving public accountability. Participatory governance and co-production processes require an understanding of human factors such as individual patterns of decision-making processes, as well as cognitive and behavioural biases; institutional structures; perceptions of the risks, benefits, and costs of various policy interventions; as well as a need for compromise-oriented solutions to honour diverse views and a variety of voices. Participatory governance also requires the involvement of various stakeholders. Stakeholder involvement in decision- making processes and in the development of tools and decision support systems is essential for meeting stakeholder requirements (cf. Komendantova et al., 2014). Furthermore, authors such as Kujala and Väänänem-Vainio-Mattila (2009) have shown that it is essential to consider stakeholders’ values regarding the functionalities and features of a tool when designing new software and that tools so designed are more likely to be used by the groups in question. To achieve this, a number of techniques may be used. Khari and Kumar (2013) tested common approaches experimentally with stakeholders, concluding that value-oriented prioritisation (VOP) met the demands and the environment of the stakeholders better than other techniques. VOP, a so-called preference-based approach that relies on techniques and models from the field of decision analysis, aims to elicit users’ values by studying their preferences (see Vetschera, 2006, for an introduction in the context of software engineering). Basic VOP is a scoring-based additive weighting approach in which a stakeholder or prospec- tive user ranks features (or requirements) according to his or her value-in-use (see Azar et al., 2007). If there is more than one user or stakeholder, the VOP process turns into a group decision problem (i.e., gathering preferential data from several stake- holders or prospective users to identify a selection of features that provides maximum value to users while respecting the resources of the development team). However, VOP in itself is not flexible enough to handle ranking statements and aggregate preferences from several stakeholders in an elaborated way. For this purpose, there exist the novel methods from the field of decision analysis described in the following section. Methodology The empirical data in this study were collected during a co- creation process with stakeholder groups that used workshops and interviews to extract design components from stakeholder dialogues and findings. The aim was to provide insights into expected requirements for anti-disinformation tools. A specially adapted multi-criteria decision framework (Danielson et al., 2020) was then used to understand the desirability of various system features of a tool for mitigating misinformation. Workshop setup and participants. The co-creation workshops consisted of stakeholders from three groups (journalists/fact- checkers, policymakers and citizens) in three countries (Austria, Greece and Sweden). The purpose of the workshops was to dis- cuss misinformation and, over several sessions, collect percep- tions of misinformation, test and discuss tools that address misinformation, as well as various features of these tools. Fur- thermore, we explored how information about particular online tools can be transferred to stimulate critical thinking and trust, as the latter is an important parameter in software adoption (Wu et al., 2011). We used the following sampling and invitation process. After thorough desktop research, a list of organisations was created that identified the most important stakeholders on the topic. A final contact list of various organisations representing our three stakeholder groups (policymakers, citizens, and journalists) was prepared. Hosting pilot team members were assigned to contact the organisations and to update the list accordingly. Subsequently, formal letters of invitation were issued to the target participants. The letter included a brief description of the Co- Inform program and the workshop objectives. It also included the workshop agenda (Appendix II). The team followed up with phone calls to the identified stakeholders and personally explained to them the goals of the project, the workshop methodology, and the importance of their participation. Two days before the event, a reminder e-mail was sent to the list of confirmed participants that provided them with more informa- tion about the location of the event. The formats of the workshops, as well as the sampling and invitation processes, were identical for all three countries to exclude the possibility that the results were influenced by differences in sampling process or format. The policymaker group consisted of government organisations (Ministry of Finance, Ministry of Education, Ministry of Health), nongovernmental organisations (Solidarity Now, Danish Refugee Council, UNHCR and others), grassroots organisations (domain expert organisations like Velos Youth Center), and municipality services organisations (organisations that provided aid to refugees, like Greek Refugee Council, could help us recruit refugees). The citizen group consisted of people from local communities, people from civil societies, refugees, migrants, as well as academics. The journalist group consisted of people from news agencies, radio, and television. The first co-creation workshop took place in September 2018 in Tokyo, Japan, and was organised by the International Council for Information Technology in Government Administration and the Organisation for Economic Co-operation and Development (OECD). The 103 participants at the first multi-stakeholder workshop included 11 government chief information officers, 65 high-ranking public officials, 8 journalists, 8 executives of international organisations, 9 executives from the private sector, and 2 policymakers. The purpose of the workshop was to assess the effects of misinformation in society and suggest mitigation strategies for the public sector. The second co-creation workshop was portioned among the three countries and took place in February–March 2019. The purpose of this workshop was to assess the initial needs of participants around misinformation, their level of trust in news sources, and their perceptions of misinformation and to collect their recommendations on possible interventions and policies. In Vienna, the Co-Inform workshop was organised in cooperation with the Ministry of Economy and Digitalization and included 21 policymaker, journalist, and citizen stakeholders, including representatives of the Austrian Chamber of Labour, the Housing Service of the Municipality of Vienna, and the Austrian Association of Cities and Towns. In Sweden, it included 16 participants, of whom four were journalists, five policymakers (mainly from the Social Democratic Party), and seven citizens (including from Anti-Rumour Sweden). It was hosted by the Botkyrka Multicultural Centre. In Greece, the workshop took place in the community of Serafeio with 31 participants (9 journalists, 9 policymakers, and 13 citizens), including represen- tatives of the Ministries of Finance, Digital Policy, Health, Immigration and Education. The third co-creation workshop took place in these same countries in November 2019. The major theme of the third Co- Inform workshop was Which features make people engage with misinformation-combatting tools, and why?” The theme was addressed over a series of five sessions: introduction to the overall workshop process, categorisation theory exercise, assessment of features of the interface of a potential tool, Multi-Criteria Decision Analysis (MCDA) sessions, and repertoire grid-nudging focus group sessions. Altogether 15 participants attended the third Co-Inform workshop in Sweden: 3 journalists, 1 policy- maker, and 11 citizens. In Greece, 19 people participated: six citizens, seven journalists, and six policymakers. In Austria, 16 stakeholders attended the workshop: five citizens, six journal- ists, and five policymakers. The only difference among the aforementioned workshops was that the participants belonged to three different cultures: ● Workshop 2 (as per our article): We recruited participants from all stakeholder groups (citizens, journalists, policy- makers) who were related to organisations that worked with migrants. ● Workshop 3 (as per our article): We recruited participants from all stakeholder groups (citizens, journalists, policy- makers) without focussing on any specific domain. The format, agenda, and master plan of the workshops were as follows. All pilot countries (Greece, Austria and Sweden) followed a common format that included an agenda, templates, survey forms, exercises, and workshop sessions based on a common master plan that was prepared by the responsible Co-Inform project partners in consultation with Co-Inform project technical partners. Each workshop followed the same master plan. In addition, discussions were held in all three Co-Inform pilot countries on common topics as per the master plans of the workshops. A main objective of the third workshop was to collect input on perceptions of functionalities, user experience features, and system features of tools that deal with misinformation in social media. Four sessions were conducted during each workshop. During the first session, the participants were presented with features. This was followed by a detailed discussion of each feature and the collection of feedback on what should be included or added. The following features were subject to evaluation by the participants: ● Feature 1 (Awareness): I am aware of existing misinformation online. ● Feature 2 (Why and when): I want to know why a claim has been flagged as misinformative. And I want to know who flagged it and when. ● Feature 3a (How it spreads and by whom): I come across something that I find misinformative. I would like to know how this information has spread online and who has shared it. ● Feature 3b (Life cycle [timeline]): I want to know the life cycle (timeline) of a misinformative post/article (e.g., when it was first published, how many fact-checkers have debunked it, and when it was shared again). ● Feature 4a (Sharing over time): I want to be able to quickly understand how much misinformation people have shared over time through an overall misinformation score. ● Feature 4b (How misinformative an item is): I want to be able to quickly understand how much misinformation a news item or tweet may contain through the provision of an overall misinformation score. ● Feature 5a (Instant feedback on arrival): When I encounter a tweet from someone else that contains misinformative content, I want to be informed that it is misinformative. ● Feature 5b (Inform on consistent accounts): I want the Co- Inform system to inform me of which accounts (within my network) consistently generate/share/create misinformative content. ● Feature 5c (Self-notification): I want the Co-Inform tools to notify me whenever I repeatedly share misinformation. ● Feature 6 (Credibility indicators): I want to see credibility indicators that I will immediately understand, and I want the credibility indicators to look very familiar, like other indicators online. ● Feature 8 (Post support or refute): I want to be able to post links to reputable articles and data that support or refute the story or claim. ● Feature 9 (Tag veracity): I want to be able to tag the veracity of an element (tweet, story, image, or sentence/claim) in the current tab I am seeing. ● Feature 10 (Platform feedback): I want to be able to receive feedback on what the platform is doing and has done with the tags and evidence I have submitted. The participants then ranked the features under three criteria, creating three different rankings of the features, one for each criterion. The three criteria were as follows: ● Trust: for trust in this tool ● Critical thinking: for making me think twice before I trust and/or share ● Transparency: for transparency in how the tool makes judgements Thereafter, they ranked the three criteria with respect to their relative importance based on question on the form: The top- ranked features under Trust, do they provide more or less value for you compared to the top-ranked features under Critical thinking?” If the answer was yes,” then trust was ranked above critical thinking (i.e., it was deemed to be of more relative importance, because the participant perceived greater value if the top-ranked features for trust were available compared to the top- ranked features for critical thinking). Elicitation and evaluation. Danielson et al.’s (2020) decision analytic framework was used as the rank-based elicitation and evaluation method. The method was implemented in DecideIT 3.1, which was also used in the workshops. Briefly, DecideIT is capable of operating with incomplete or numerically imprecise input data, such as rankings and interval value statements, in a combined model. To represent the ranking statements, we used a cardinal ranking approach (P-CAR). P-CAR is a calibrated method of creating feasible input in the form of surrogate imprecise value statements, which are derived from rankings provided by stakeholders. The feasible information is represented in the form of linear inequalities (greater than) in combination with interval bounds and a focal point that represents the most feasible surrogate value for a given element given its position in the ranking. This enables conventional multi-attribute value aggregation (Dyer and Sarin, 1979) so the results can be evaluated across multiple stakeholders and criteria. See Danielson and Ekenberg (2019) for details on P-CAR. The evaluation method originated from earlier work on evaluating decision situations involving numerically imprecise input. To avoid problems with aggregation when handling set membership functions and similar features, higher order distributions for better discrimination between the possible outcomes are introduced. To alleviate the problem of overlapping results, the methodology also contains a new evaluation method based on the resulting belief mass over the output intervals, without introducing further complicating aspects into the decision model. During the process, consideration is given to the entire range of output values, as well as how plausible it is that a specific feature will outrank the remaining ones, thus providing a robustness measure. In this way, DecideIT can evaluate the actual proportion of aggregated values for which a feature is considered more favourable than another, that is, whether there is a significantly larger amount of the feasible information (i.e., in the set of rankings provided by the participants where one feature is deemed to provide more value compared to another feature). This can be seen more concretely in Fig. 1, which shows the proportion of feasible information (e.g., Feature 2 is deemed to be more valuable than the rest, Features 3a and 3b are basically equal and more valuable than Feature 4 and the remaining features). See Danielson et al. (2020) for a detailed description of the tool and its underlying theory and Larsson et al. (2018) for details on aggregation across multiple stakeholders/participants. Results In line with OECD’s Recommendation on Digital Government Strategies, the findings from the first co-creation workshop in Tokyo emphasised the need for open, inclusive, accountable, and transparent processes by national governments and highlighted the fact that digital transformation in the public sector, as well as increasing accessibility of the Internet, has exacerbated various problems related to misinformation. Given the importance of factual information for combatting misinformation in the public arena, governments need to collaborate with stakeholders and invest in innovative ways of dealing with misinformation. A number of specific actions were proposed to deal with this societal challenge. Empowerment of citizens, encouraged engagement, education, moderate legislative action, as well as investment in new technologies are invaluable means of tackling misinforma- tion. For fragmented technological and innovative solutions to succeed in tackling misinformation on a broad scale, they need to be integrated and embedded into a co-creational system of poli- cies. More collaborative and effective management of mis- information needs to be supplemented with informed behaviours among citizens. Creating a trusted environment for citizens with adequate education is necessary as we enter an era in which big technological advances have the potential to disrupt even more than they already have. The subsequent workshops took place in three different loca- tions on two separate occasions and provided cross-cultural data for comparing the needs of various stakeholder groups related to decision support models. Data were collected, and the needs of citizens, policymakers, and journalists were identified. Table 1 shows that the need for collaboration and facilitation of tools was identified in all three case countries and by all three stakeholder groups. The need for tools to address education and awareness raising was also identified in all three countries and across all three groups of stakeholders. These tools are also required for sharing reliable information. However, an automatic correction Fig. 1 Results for all groups of stakeholders. Preferences. Table 1 Major preferences of various stakeholders regarding how to address misinformation in the three case countries (Swede","Komendantova, N., Ekenberg, L., Svahn, M., Larsson, A., Shah, S. I. H., Glinos, M., ... & Danielson, M. (2021). A value-driven approach to addressing misinformation in social media. Humanities and Social Sciences Communications, 8(1), 1-12."
ML_DB_130,Supervised Learning for Fake News Detection,"A large body of recent works has focused on understanding and detecting fake news stories that are disseminated on social media. To accomplish this goal, these works explore several types of features extracted from news stories, including source and posts from social media. In addition to exploring the main features proposed in the literature for fake news detection, we present a new set of features and measure the prediction performance of current approaches and features for automatic detection of fake news. Our results reveal interesting findings on the usefulness and importance of features for detecting false news. Finally, we discuss how fake news detection approaches can be used in the practice, highlighting challenges and opportunities",Computer Science,Article,2019,N,Y,192,"SOCIAL MEDIA SYSTEMS have been dramatically changing the way news is produced, dissemi- nated, and consumed, opening unforeseen oppor- tunities, but also creating complex challenges. A key problem today is that social media has become a place for campaigns of misinformation that affect the credibility of the entire news ecosystem. A unique characteristic of news on social media is that anyone can register as a news pub- lisher without any upfront cost (e.g., anyone can create a Facebook page claiming to be a newspa- per or news media organization). Consequently, not only traditional news, corporations are increasingly migrating to social media (https:// www.comscore.com/Insights/Blog/Traditional- News-Publishers-Take-Non-Traditional-Path-to- Digital-Growth). Along with this transition, not surprisingly, there are growing concerns about fake news publishers posting fake” news sto- ries, and often disseminating them widely using fake” followers. 1 As the extensive spread of fake news can have a serious negative impact on indi- viduals and society, the lack of scalable fact checking strategies is especially worrisome. Not surprisingly, recent research efforts are devoted not only to better comprehend this phenomenon1 but also to automatize the detec- tion of fake news. 2,3,4 While a fully automated approach for the fake news problem can be quite Digital Object Identifier 10.1109/MIS.2019.2899143 Date of current version 3 May 2019. Affective Computing and Sentiment Analysis 76 1541-1672 ß 2019 IEEE Published by the IEEE Computer Society IEEE Intelligent Systems controversial and is still open for debate, a perti- nent research question is: What is the prediction performance of current approaches and features for automatic detection of fake news? Most of the existing efforts in this space are concurrent work, which identify recurrent pat- terns on fake news after they are already dissemi- nated, or propose new features for training classifiers, based on ideas that have not been tested in combination. Thus, it is difficult to gauge the potential that supervised models trained from features proposed in recent studies have for detecting fake news. This paper briefly surveys existing studies on this topic, identifying the main features proposed for this task. We implement these features and test the effectiveness of a vari- ety of supervised learning classifiers when distin- guishing fake from real stories on a large, recently released and fully labeled dataset. Finally, we discuss how supervised learning models can be used to assist fact-checkers in evaluating digital content and reaching warranted conclusions. FEATURES FOR FAKE NEWS DETECTION Most of the existing efforts to detect fake news propose features that leverage information pres- ent in a specific dataset. In contrast, we use a recently released dataset that allows us to imple- ment most of the proposed features explored in previous works.5 It consists of 2282 BuzzFeed news articles related to the 2016 U.S. election labeled by journalists and enriched with com- ments associated with the news stories as well as shares and reactions from Facebook users. In this paper, we discarded stories labeled as non factual content” (12%), and merged those labeled as mostly false” (4%) and mixture of true and false” (11%) into a single class, henceforth referred as fake news.” The remaining stories cor- respond to the true” portion (73%). The rationale is that stories that mix true and false facts may represent attempts to mislead readers. Thus, we focus our analysis on understanding how features can be used to discriminate true and fake news. On a coarse-grained level, features for fake news detection can be roughly categorized as fol- lows: 1) features extracted from news content (e.g., language processing techniques); 2) features extracted from news source (e.g., reliability and trustworthiness); and 3) features extracted from environment (e.g., social network structure). Next, we briefly survey previous efforts, describing existing features and how we implemented them. Textual Features consist of the information extracted from the news text, including the text body, the headline, and the text message used by the news source. For news articles embedded in images and videos, we applied image process- ing techniques for extracting the text shown on them. In total, we evaluated 141 textual features. Features were grouped in sets, which are described next. 1) Language Features (Syntax): Sentence-level features, including bag-of-words approaches, n-grams” and part-of-speech (POS tagging) were explored in previous efforts as features for fake news detection. 2,6 Here, we imple- mented 31 features from this set including number of words and syllables per sentence as well as tags of word categories (such as noun, verb, adjective). In addition, to evalu- ate writers’ style as potential indicators of text quality, we also implemented features based on text readability. 2) Lexical Features: Typical lexical features include character and word-level signals,7,6 such as amount of unique words and their fre- quency in the text. We implemented linguistic features, including number of words, first-per- son pronouns, demonstrative pronouns, verbs, hashtags, all punctuations counts, etc. 3) Psycholinguistic Features: Linguistic Inquiry and Word Count (LIWC) 8 is a dictionary- based text mining software whose output has been explored in many classification tasks, including fake news detection. 4 We use its latest version (2015) to extract 44 features that capture additional signals of persuasive and biased language. 4) Semantic Features: There are features that cap- ture the semantic aspects of a text2,3 are useful to infer patterns of meaning from data.9 As part of this set of features, we consider the toxicity score obtained from Google’s API. The API uses machine learning models to quantify the extent to which a text (or comment, for instance) can be perceived as toxic.” We did March/April 2019 77 not consider strategies for topic extraction since the dataset used in this paper was built based on news articles about the same topic or category (i.e., politics). 5) Subjectivity: Using TextBlob’s API, we compute subjectivity and sentiment scores of a text as explored in previous efforts. 4 News Source Features consist of information about the publisher of the news article. To extract these features, we first parsed all news URLs and extracted the domain information. When the URL was unavailable, we associated the official URL of news outlet with news article. Therefore, we extract eight (eight) indicators of political bias, credibility and source trustworthi- ness, and use them as detailed next. Moreover, in this category, we introduce a new set com- posed of five features, called domain localization (see below). 1) Bias: The correlation between political polar- ization and spread of misinformation was explored in previous studies. 10 In this paper, we use the political biases of news outlets from the BuzzFeed dataset as a feature. 2) Credibility and Trustworthiness: In this feature set, we introduce seven new features to cap- ture aspects of credibility (or popularity) and trustworthiness of domains. We collect, using Facebook’s API, user engagement metrics of Facebook pages that published news articles (i.e., page talking about” count and page fan” count). Then, we use the Alexa’s API to get the relative position of news domain on the Alexa Ranking. Furthermore, using this same API, we collect Alexa’s top 500 news- papers. Based on the intuition that some unre- liable domains may try to disguise themselves using domains similar to those of well-known newspapers, we define the dissimilarity between domains from the Alexa ranking and news domains in our dataset (measured by the minimum edit distance) as features. Finally, we use indicators of low credibility of domains compiled11 as features. 3) Domain Location: Ever since creating fake news became a profitable job, some cities have become famous because of residents who create and disseminate fake news In order to exploit the information that domain location could carry, a pipeline was built to take each news website URL and extract new features, such as IP, latitude, lon- gitude, city, and country. First, for each domain, the corresponding IP was extracted using the trace route tool. Then, the ipstack API was used to retrieve the location features. Although localization information (i.e., IP) has been previously used in works on bots or spam detection, to the best of our knowledge, there are no works that leverage these data in the context of fake news detection. Environment Features consist of statistics of user engagement and temporal patterns from social media (i.e., Facebook). These features have been extensively used in previous efforts, 12 especially to better understand the phenomenon of fake news. 13 Next, we detail the 21 features from this category. 1) Engagement: We consider number of likes, shares, and comments from Facebook users. Moreover, we compute the number of com- ments within intervals from publication time (900, 1800, 2700, 3600, 7200, 14400, 28 800, 57 600 and 86 400 s), summing up to 12 features. 2) Temporal Patterns: Finally, to capture tempo- ral patterns from user commenting activities, we compute the rate at which comments are posted for the same time windows defined before. CLASSIFICATION RESULTS We evaluate the discriminative power of the previous features using several classic and state- of-the-art classifiers, including k-Nearest Neigh- bors (KNN), Naive Bayes (NB), Random Forests (RF), Support Vector Machine with RBF kernel (SVM), and XGBoost (XGB). Given that we used hand-crafted features, there was no need to include a neural network model in the compari- son since it would only associate weights with the features, rather than find new ones. Affective Computing and Sentiment Analysis 78 IEEE Intelligent Systems We measure the effectiveness of each classi- fier w.r.t. the area under the ROC curve (AUC) and the Macro F1 score. In this case, the result- ing AUC is the probability that a model will rank a randomly chosen fake news higher (more false) than a randomly chosen news arti- cle. The AUC is especially relevant for fake news detection since the decision threshold can be used to control the tradeoff between true and false positive rates. The F1 score com- bines precision and recall per class in a single metric and the Macro F1 score provides the overall performance of the classifier. We compute 95% confidence intervals for the mean AUC and F1 by performing a fivefold split between training and test set, repeated ten times with different shuffled versions of the original dataset (a total of 50 runs). Table 1 shows the empirical results obtained from the fitted models using all features previously described. For each classifier, we learn a model from a set of previously labeled (i.e., preclassified) data, and then use it to classify new (unseen) news articles into fake” or not fake.” The best results were obtained by RF and XGB classifiers, statisti- cally tied with 0.85 (_x0001_ 0.007) and 0.86 (_x0001_ 0.006) for AUC, respectively. Moreover, inspecting the ROC curve for XGB (see Fig. 1), we observe that it is possible to choose a threshold so as to correctly classify almost all of fake news (true positive rate _x0019_ 1), while misclassifying 40% of the true news (false positive rate _x0019_ 0.4). This can be useful, espe- cially in assisting fact checkers to identify stories that are worth investigating. Finally, we assessed the relative power of the selected attributes in discriminating each class from the other by rank- ing features from each set based on X2 (Chi Squared). Fig. 2 shows the results. Although all feature sets have some discriminatory power, there are some of them (e.g., credibility and localization of news sources, and news engage- ment) that can be more useful to improve the performance of models for fake news detection. FAKE NEWS DETECTION IN PRACTICE Fact checking is a damage control strategy that is both essential and not scalable. It might be hard to take out the human component out of the picture any time soon, especially if these news regard sensitive subjects such as politics. In the case of social networks and search engines, predictions made by models for fake news detection could be used internally to limit Table 1. Results obtained for different classifiers w.r.t AUC and F1 score. Classifier AUC F1 KNN 0.80_x0001_0.009 0.75_x0001_0.008 NB 0.72_x0001_0.009 0.75_x0001_0.001 RF 0.85_x0001_0.007 0.81_x0001_0.008 SVM 0.79_x0001_0.030 0.76_x0001_0.019 XGB 0.86_x0001_0.006 0.81_x0001_0.011 RF and XGB performed best. Figure 1. ROC curve for the XGboost classifier. For BuzzFace, it is possible to correctly classify almost all of fake news with only 40% of false positive rate. Figure 2. Chi-Square feature importance. March/April 2019 79 the audience of news stories likely to be fake. This is why automatic labeling of news stories raises so many questions about fairness and algorithm transparency, suggesting that it is likely that the final call will still depend on an expert at the end point for a long time. On the bright side, automatic fake news detec- tion could be used by fact checkers as an auxiliary tool for identifying content that is more likely to be fake. Our results show that the prediction per- formance of proposed features combined with existing classifiers has a useful degree of discrimi- native power for detecting fake news. Our best classification results can correctly detect nearly all fake news in our data, while misclassifying about 40% of true news, which is already sufficient to help fact checkers. In this context, providing explanations that supported the algorithm’s out- put is crucial. For example, a certain story was considered false because it was posted by new newspaper hosted in the same IP address than a known blacklisted fake news source. Additionally, this kind of approach requires a continual pipeline where more stories get labeled each day and are, in turn, fed back to the models. Rather than verify- ing only the most suspicious stories, an active learning solution can be put in place, so that the model can also indicate which stories should be investigated in order to improve its prediction performance. More importantly, fake news is a rel- atively recent problem and the cost to label large datasets is still very high. In the future, larger vol- umes of labeled data will enable us to explore other techniques such as deep learning and push the boundaries of prediction performance","Reis, J. C., Correia, A., Murai, F., Veloso, A., & Benevenuto, F. (2019). Supervised learning for fake news detection. IEEE Intelligent Systems, 34(2), 76-81."
ML_DB_135,Computational Fact Checking: A Content Management Perspective,"The tremendous value of Big Data has been noticed of late also by the media, and the term data journalism” has been coined to refer to journalistic work inspired by digital data sources. A particularly popular and active area of data journalism is concerned with fact-checking. The term was born in the journalist community and referred the process of verifying and ensuring the accuracy of published media content; since 2012, however, it has increasingly focused on the analysis of politics, economy, science, and news content shared in any form, but first and foremost on the Web (social and otherwise). These trends have been noticed by computer scientists working in the industry and academia. Thus, a very lively area of digital content management research has taken up these problems and works to propose foundations (models), algorithms, and implement them through concrete tools. Our proposed tutorial: (i) Outlines the current state of affairs in the area of digital (or computational) fact-checking in newsrooms, by journalists, NGO workers, scientists and IT companies; (ii). Shows which areas of digital content management research, in particular those relying on the Web, can be leveraged to help fact-checking, and gives a comprehensive survey of efforts in this area; (iii) Highlights ongoing trends, unsolved problems, and areas where we envision future scientific and practical advances. We believe the audience is likely to get many ideas for applications and research. It is timely in that the amount of ideas and research works involved is currently quite significant, and non-computer scientists (journalists mostly) are eager to be involved in designing and using novel tools for their work. We believe a missing piece of the puzzle to make it happen, is a dissemination effort on the needs on one hand and the available and future scientific tools on the other hand. This tutorial is an attempt to provide such a dissemination effort.",Computer Science,Article,2018,Y,N,9,"The tremendous value of Big Data has been noticed of late also by the media, and the term data journalism” has been coined to refer to journalistic work inspired by dig- ital data sources. A particularly popular and active area of data journalism is concerned with fact-checking. The term was born in the journalist community and referred the process of verifying and ensuring the accuracy of published media content; since 2012, however, it has increasingly fo- cused on the analysis of politics, economy, science, and news content shared in any form, but first and foremost on the Web (social and otherwise). These trends have been no- ticed by computer scientists working in the industry and academia. Thus, a very lively area of digital content man- agement research has taken up these problems and works to propose foundations (models), algorithms, and implement them through concrete tools. Our proposed tutorial: (i) Outlines the current state of affairs in the area of digital (or computational) fact-checking in newsrooms, by journalists, NGO workers, scientists and IT companies; (ii). Shows which areas of digital content management research, in particular those relying on the Web, can be leveraged to help fact-checking, and gives a comprehensive survey of efforts in this area; (iii) Highlights ongoing trends, unsolved problems, and areas where we en- vision future scientific and practical advances. We believe the audience is likely to get many ideas for applications and research. It is timely in that the amount of ideas and research works involved is currently quite signif- icant, and non-computer scientists (journalists mostly) are eager to be involved in designing and using novel tools for their work. We believe a missing piece of the puzzle to make it happen, is a dissemination effort on the needs on one hand and the available and future scientific tools on the other hand. This tutorial is an attempt to provide such a dissemination effort. 2. AUDIENCE We believe the tutorial would be of interest to all confer- ence attendees, especially to researchers and students work- ing in one of the related areas. More broadly, from a societal perspective, there is a lot to gain by making data manage- ment scientists aware of the many concrete problems faced by fact checking journalists, activists and the general pub- lic, as they attempt to rely on data in order to separate the truth from misinformation. Many have already started do- ing so, e.g. [36, 37, 38]; we hope our tutorial may motivate more in the VLDB community to follow. 3. TUTORIAL STRUCTURE AND ORGA- NIZATION In Section 3.1, we provide a short history of journalis- tic fact-checking and presents its most recent and visible actors, from the media and/or NGO communities. Sec- tion 3.2 discusses the scientific content management areas which bring useful tools for computational fact-checking, then Section 3.3 presents the organization of the proposed tutorial. 3.1 Data journalism and fact-checking: an overview While data of some form is a natural ingredient of all reporting, the increasing volumes and complexity of digital data lead to a qualitative jump, where technical skills, and in particular data science skills, are stringently needed in journalistic work. A particularly popular and active area of data journalism is concerned with fact-checking. The term was born in the journalist community; it referred to the task of identifying and checking factual claims present in media content, which dedicated newsroom personnel would then check for factual accuracy. The goal of such checking was to avoid misinfor- mation, to protect the journal reputation and avoid legal actions. Starting around 2012, first in the United States (FactCheck.org1), then in Europe, and soon after in all ar- eas of the world, journalists have started to take advan- tage of modern technologies for processing content, such as text, video, structured and unstructured data, in order to automate, at least partially, the knowledge finding, reason- ing, and analysis tasks which had been previously performed","Cazalens, S., Leblay, J., Lamarre, P., Manolescu, I., & Tannier, X. (2018). Computational fact checking: a content management perspective. Proceedings of the VLDB Endowment (PVLDB), 11(12), 2110-2113."
ML_DB_138,Where is your Evidence: Improving Fact-checking by Justification Modeling,"Fact-checking is a journalistic practice that compares a claim made publicly against trusted sources of facts. Wang (2017) introduced a large dataset of validated claims from the POLITIFACT.com website (LIAR dataset), enabling the development of machine learning approaches for fact-checking. However, approaches based on this dataset have focused primarily on modeling the claim and speaker-related metadata, without considering the evidence used by humans in labeling the claims. We extend the LIAR dataset by auto matically extracting the justification from the fact-checking article used by humans to la bel a given claim. We show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) both in a binary classification task (true, false) and in a six-way classification task (pants on fire, false, mostly false, half true, mostly true, true).",Computer Science,Proceeding,2018,Y,N,58,"Fact-checking is the process of assessing the veracity of claims. It requires identifying evidence from trusted sources, understanding the context, and reasoning about what can be inferred from the evidence. Several organizations such as FACTCHECK.org, POLITIFACT.com and FULLFACT. org are devoted to such activities, and the final verdict can reflect varying degrees of truth (e.g., POLITIFACT labels claims as true, mostly true, half true, mostly false, false and pants on fire). Until recently, the bottleneck for developing automatic methods for fact-checking has been the lack of large datasets for building machine learning models. Thorne and Vlachos (2018) provide a survey of current datasets and models for factchecking (e.g., (Wang, 2017; Rashkin et al., 2017; Vlachos and Riedel, 2014; Thorne et al., 2018; Long et al., 2017; Potthast et al., 2018; Wang et al., 2018)). Wang (2017) has introduced a large dataset (LIAR) of claims from POLITIFACT, the associated metadata for each claim and the verdict (6 class labels). Most work on the LIAR dataset has focused on modeling the content of the claim (including hedging, sentiment and emotion analysis) and the speaker-related metadata (Wang, 2017; Rashkin et al., 2017; Long et al., 2017). However, these approaches do not use the evidence and the justification provided by humans to predict the label. Extracting evidence from (trusted) sources for fact-checking or for argument mining is a difficult task (Rinott et al., 2015; Thorne et al., 2018; Baly et al., 2018). For the purpose of our paper, we rely on the fact-checking article associated with the claim. We extend the original LIAR dataset by automatically extracting the justification given by humans for labeling the claim, from the fact-checking article (Section 2). We release the extended LIAR dataset (LIARPLUS) to the community1. The main contribution of this paper is to show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) both in a binary classification task (true, false) and in a six-way classification task (pants on fire, false, mostly false, half-true, mostly true, true) (Section 4). We provide a detailed error analysis and per-class results. Our work complements the recent work on providing datasets and models that enable the development of an end-to-end pipeline for fact- 1https://github.com/Tariq60/LIAR-PLUS 86 checking ((Thorne et al., 2018) for English and (Baly et al., 2018) for Arabic). We are primarily concerned on showing the impact of modeling the human-provided justification for predicting the veracity of a claim. In addition, our task aims to capture the varying degrees of truth that some claims might have and that are usually labeled as such by professionals (rather than binary true vs. false labels). 2 Dataset The LIAR dataset introduced by (Wang, 2017) consists of 12,836 short statements taken from POLITIFACT and labeled by humans for truthfulness, subject, context/venue, speaker, state, party, and prior history. For truthfulness, the LIAR dataset has six labels: pants-fire, false, mostlyfalse, half-true, mostly-true, and true. These six label sets are relatively balanced in size. The statements were collected from a variety of broadcasting mediums, like TV interviews, speeches, tweets, debates, and they cover a broad range of topics such as the economy, health care, taxes and election. We extend the LIAR dataset to the LIAR-PLUS dataset by automatically extracting for each claim the justification that humans have provided in the fact-checking article associated with the claim. Most of the articles end with a summary that has a headline our ruling” or summing up”. This summary usually has several justification sentences that are related to the statement. We extract all sentences in these summary sections, or the last five sentences in the fact-checking article when no summary exists. We filter out the sentence that has the verdict and related words. These extracted sentences can support or contradict the statement, which is expected to enhance the accuracy of the classification approaches. Excerpt from the LIARPLUS dataset is shown in Table 1. 3 Methods The main goal of our paper is to show that modeling the human-provided justification — which can be seen as a summary evidence — improves the assessment of a claim’s truth when compared to modeling the claim (and metadata) alone, regardless of the machine learning models (feature based vs. deep learning models). All our models Statement:Says Rick Scott cut education to pay for even more tax breaks for big, powerful, well-connected corporations.” Speaker: Florida Democratic Party Context: TV Ad Label: half-true Extracted Justification: A TV ad by the Florida Democratic Party says Scott ”cut education to pay for even more tax breaks for big, powerful, well-connected corporations.” However, the ad exaggerates when it focuses attention on tax breaks for ”big, powerful, well-connected corporations.” Some such companies benefited, but so did many other types of businesses. And the question of whether the tax cuts and the education cuts had any causal relationship is murkier than the ad lets on. Table 1: Excerpt from the LIAR-PLUS dataset use 4 different conditions: basic claim/statement2 representation using just word representations (S condition), enhanced claim/statement representation that captures additional information shown to be useful such as hedging, sentiment strength and emotion (Rashkin et al., 2017) as well as metadata information (S+M condition), basic claim/statement and the associated extracted justification (SJ condition) and finally enhanced claim/statement representation, metadata and justification (S+MJ condition). Feature-based Machine Learning. We experiment with both Logistic Regression (LR) and Support Vector Machines (SVM) with linear kernel. For the basic representation of the claim/statement (S condition) we experimented with unigram features, tf-idf weighted unigram features and Glove word embeddings (Pennington et al., 2014). The best representation proved to be unigrams. For the enhanced statement representation (S+) we modeled: sentiment strength using SentiStrength, which measures the negativity and positivity of a statement on a scale of 1-to-5 (Thelwall et al., 2010); emotion using the NRC Emotion Lexicon (EmoLex), which associates each word with eight basic emotions (Mohammad and Turney, 2010), and the Linguistic Inquiry and Word Count (LIWC) lexicon (Pennebaker et al., 2001). In addition, we include metadata information such as the number of claims each speaker makes for every truth-label (history) (Wang, 2017; Long et al., 2017). Finally for representing the justification in the SJ and S+MJ conditions, we just use unigram features. 2In the rest of the paper we will refer to the claim as statement. 87 Cond. Model Binary Six-way valid test valid test S LR 0.58 0.61 0.23 0.25 SVM 0.56 0.59 0.25 0.23 BiLSTM 0.59 0.60 0.26 0.23 SJ LR 0.68 0.67 0.37 0.37 SVM 0.65 0.66 0.34 0.34 BiLSTM 0.70 0.68 0.34 0.31 P-BiLSTM 0.69 0.67 0.36 0.35 S+M LR 0.61 0.61 0.26 0.25 SVM 0.57 0.60 0.26 0.25 BiLSTM 0.62 0.62 0.27 0.25 S+MJ LR 0.69 0.67 0.38 0.37 SVM 0.66 0.66 0.35 0.35 BiLSTM 0.71 0.68 0.34 0.32 P-BiLSTM 0.70 0.70 0.37 0.36 Table 2: Classification Results Deep Learning Models. We chose to use Bi- Directional Long Short-term Memory (BiLSTM) (Hochreiter and Schmidhuber, 1997) architectures that have been shown to be successful for various related NLP tasks such a textual entailment and argument mining. For the S condition we use just one BiLSTM to model the statement. We use Glove pre-trained word embeddings (Pennington et al., 2014), a 100 dimension embedding layer that is followed by a BiLSTM layer of size 32. The output of the BiLSTM layer is passed to a softmax layer. In the S+M condition, a normalized count vector of those features (described above) is concatenated with the output of the BiLSTM layer to form a merge layer before the softmax. We used a categorical cross entropy loss function and ADAM optimizer (Kingma and Ba, 2014) and trained the model for 10 epochs. For the SJ and S+MJ conditions we experiment with two architectures: in the first one we just concatenate the justification to the statement and pass it to a single BiLSTM, and in the second one we use a dual/parallel architecture where one BiLSTM reads the statement and another one reads the justification (architecture denoted as P-BiLSTM). The outputs of these BiLSTMs are concatenated and passed to a softmax layer. This latter architecture has been proven to be effective for tasks that model two inputs such as textual entailment (Conneau et al., 2017) or sarcasm detection based on conversation context (Ghosh et al., 2017; Ghosh and Veale, 2017). Class class size S SJ LR BiLSTM LR BiLSTM P-BiLSTM pants-fire 116 0.18 0.19 0.37 0.34 0.37 false 263 0.28 0.34 0.33 0.3 0.33 mostly-false 237 0.21 0.13 0.35 0.31 0.32 half-true 248 0.22 0.28 0.39 0.31 0.37 mostly-true 251 0.23 0.33 0.40 0.39 0.39 true 169 0.22 0.18 0.37 0.42 0.39 total/avg 1284 0.23 0.26 0.37 0.34 0.36 Table 3: F1 Score Per Class on Validation Set Class class size S SJ LR BiLSTM LR BiLSTM P-BiLSTM pants-fire 92 0.12 0.11 0.38 0.33 0.39 false 250 0.31 0.31 0.35 0.32 0.35 mostly-false 214 0.25 0.15 0.35 0.27 0.33 half-true 267 0.24 0.26 0.41 0.27 0.34 mostly-true 249 0.23 0.30 0.35 0.35 0.33 true 211 0.25 0.16 0.37 0.36 0.41 total/avg 1283 0.25 0.23 0.37 0.31 0.35 Table 4: F1 Score Per Class on Test Set 4 Results and Error Analysis Table 2 shows the results both for the binary and the six-way classification tasks under all 4 conditions (S, SJ, S+M and S+MJ) for our featurebased machine learning models (LR and SVM) and the deep learning models (BiLSTM and PBiLSTM). For the binary runs we grouped pants on fire, false and mostly false as FALSE and true, mostly true and half true as TRUE. As reference, Wang (2017 best models (text and metadata) obtained 0.277 F1 on validation set and 0.274 F1 on test set in the six-way classification, showing relatively similar results with our equivalent S+M condition. It is clear from the results shown in Table 2 that including the justification (SJ and S+MJ conditions) improves over the conditions that do not use the justification (S and S+M, respectively) for all models, both in the binary and the six-way classification tasks. For example, for the six-way classification, we see that the BiLSTM model for the SJ condition obtains 0.35 F1 compared to 0.23 F1 in the S condition. LR model has a similar behaviour with 0.37 F1 for the SJ condition compared to 0.25 F1 in S condition. For the S+MJ conditions the best model (LR) shows an F1 of 0.38 compared to 0.26 F1 in the S+M condition (similar results for the deep learning). The dual/parallel BiLSTM architecture provides a small improvement over the single BiLSTM only in the six-way classification. We also present the per-class results for the sixway classification for the S and SJ conditions. Table 3 shows the results on validation set, while Table 4 on the test set. In the S condition, we 88 ID Statement Justification label S S+M SJ S+MJ 1 We have the highest tax rate anywhere in the world. Trump, while lamenting the condition of the middle class, said the U.S. has ”the highest tax rate anywhere in the world.” All sets of data we examined for individual and family taxes prove him wrong. Statutory income tax rates in the U.S. fall around the end of the upper quarter of nations. More exhaustive measures - which compute overall tax burden per person and as a percentage of GDP - show the U.S. either is in the middle of the pack or on the lighter end of taxation compared with other advanced industrialized nations. false X 2 Says Rick Scott cut education to pay for even more tax breaks for big, powerful, well-connected corporations.” A TV ad by the Florida Democratic Party says Scott ”cut education to pay for even more tax breaks for big, powerful, well-connected corporations.” However, the ad exaggerates when it focuses attention on tax breaks for ”big, powerful, well-connected corporations.” Some such companies benefited, but so did many other types of businesses. And the question of whether the tax cuts and the education cuts had any causal relationship is murkier than the ad lets on. half-true X X 3 Says Donald Trump has given more money to Democratic candidates than Republican candidates. but public records show that the real estate tycoon has actually contributed around $350,000 more to Republicans at the state and federal level than Democrats. That, however, is a recent development. Fergusons statement contains an element of truth but ignores critical facts. mostly-false X X 4 Says out-of-state abortion clinics have marketed their services to minors in states with parental consent laws. As Cousins clinic in New York told Yellow Page users in Pennsylvania, ”No state consents.” This is information the clinics wanted patients or potential patients to have, and paid money to help them have it. Whether it was to help persuade them to come in or not, it provided pertinent facts that could help them in their decision-making. It fit the definition of marketing. true X X X 5 Obamacare provision will allow forced home inspections by government agents. But the program they pointed to provides grants for voluntary help to at-risk families from trained staff like nurses and social workers. What bloggers describe would be an egregious abuse of the law not whats allowed by it. pants-fire X X X 6 In the month of January, Canada created more new jobs than we did. In November 2010, the U.S. economy created 93,000 jobs, compared to 15,200 for Canada. And in December 2010, the U.S. created 121,000 jobs, compared to 22,000 for Canada. ”But on a per capita basis, in recent months U.S. job creation exceeded Canada’s only in October.” January happened to be a month when U.S. job creation was especially low and Canadian job creation was especially high, but it is the most recent month and it reflects the general pattern when you account for population. true X X X X 7 There has been $5 trillion in debt added over the last four years. number is either slightly high or a little low, depending on the type of measurement used, and thats actually for a period short of a full four years. His implication that Obama and the Democrats are to blame has some merit, but it ignores the role Republicans have had. mostly-true X X X X Table 5: Error analysis of Six-way Classification (Logistic Regression) see a larger degree of variation in performance among classes, with the worst being the pants-onfire for all models, and for the deep learning model also the mostly-false and true classes. In the SJ condition, we notice a more uniform performance on all classes for all the models. We notice the biggest improvement for the pants-on-fire class for all models, half-true for LR and mostly-false and true for the deep learning models. When comparing the P-BiLSTM and BiLSTM we noticed that the biggest improvement comes from the half-true class and the pants-on-fire class. Error Analysis In order to further understand the cause of the errors made by the models, we analyzed several examples by looking at the statement, justification and predictions by the logistic regression model when using the S, S+M, SJ, and S+MJ conditions (Table 5). Logistic regression was selected since it has the best numbers for the six-way classification task. The first example in Table 5 was wrongly classified in the S condition, but classified correctly in the S+M, SJ and S+MJ conditions. The justification text has a sentence saying Statutory income tax rates in the U.S. fall around the end of the upper quarter of nations.”, which contradicts the statement and thus is classified correctly when modeling the justification. The second and third examples in Table 5 were correctly predicted only when the justification was modeled (SJ and S+MJ conditions). For statement 2, the justification text has a sentence However, the ad exaggerates...” indicates that the statement has some false and some true information. Therefore, the model predicts the correct label halftrue” when modeling the justification text. Also, the justification for statement 3 was simple enough for the model to predict the gold label mostlyfalse”. It has a phrase like more to Republicans” while the statement had more to Democratic candidates” which indicates falsehood in the statement as well as discourse markers indicating concessive moves (but” and however”). Sometimes justification features alone were not enough to get the correct prediction without using the enhanced statement and metadata features. The justification for statement 4 in Table 5 is complex and no direct connection can be made to the statement. Therefore, the model fails when using SJ and S+M conditions and only succeed when using all features (i.e., S+MJ condition). In addition, consider the 5th statement in Table 5 about Obamacare, it seems that metadata features, which have the history of the speaker, might have helped 89 in predicting its factuality to be pants on fire”, while it was wrongly classified when modeling only the statement and the justification. For around half of the instances in validation set, all models had wrong predictions. This is not surprising since the best model had an average F1 score of less than 0.40. The last two example in Table 5 are instances where the model makes mistakes under all 4 conditions. The claim and justification refer to temporal information which is harder to model by the rather simple and shallow approaches we used. Incorporating temporal and numeric information when modeling the claim and justification would be essential for capturing the correct context of a given statement. Another source of error for justification-based conditions was the noise in the extraction of the justification particularly when the our ruling” and summing up” headers were not included and we resorted to extract the last 5 sentences from the fact-checking articles. Improving the extraction methods will be helpful to improving the justification-based classification results. 5 Conclusion and Future Work We presented a study that shows that modeling the human-provided justification form the factchecking article associated with a claim is important leading to significant improvements when compared to modeling just the claim/statement and metadata for all the machine learning models both in a binary and a six-way classification task. We released LIAR-PLUS, the extended LIAR dataset that contains the automatically extracted justification. We also provided an error analysis and discussion of per-class performance. Our simple method for extracting the justification from the fact-checking article can lead to slightly noisy text (for example it can contain a repetition of the claim or it can fail to capture the entire evidence). We plan to further refine the justification extraction method so that it contains just the summary evidence. In addition, we plan to develop methods for evidence extraction from the web (similar to the goals of the FEVER shared task (Thorne et al., 2018)) and compare the results of the automatically extracted evidence with the human-provided justifications for factchecking the claims. References Ramy Baly, Mitra Mohtarami, James Glass, Llu´ıs M`arquez, Alessandro Moschitti, and Preslav Nakov. 2018. Integrating stance detection and fact checking in a unified corpus. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 21–27. Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680. Association for Computational Linguistics. Aniruddha Ghosh and Tony Veale. 2017. Magnets for sarcasm: Making sarcasm detection timely, contextual and very personal. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 482–491. Debanjan Ghosh, R. Alexander Fabbri, and Smaranda Muresan. 2017. The role of conversation context for sarcasm detection in online interactions. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 186–196. Association for Computational Linguistics. Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780. Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR, abs/1412.6980. Yunfei Long, Qin Lu, Rong Xiang, Minglei Li, and Chu-Ren Huang. 2017. Fake news detection through multi-perspective speaker profiles. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Saif M Mohammad and Peter D Turney. 2010. Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon. In Proceedings of the NAACL HLT 2010 workshop on computational approaches to analysis and generation of emotion in text, pages 26–34. Association for Computational Linguistics. James W Pennebaker, Martha E Francis, and Roger J Booth. 2001. Linguistic inquiry and word count: Liwc 2001. Mahway: Lawrence Erlbaum Associates, 71(2001):2001. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543. 90 Martin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendorff, and Benno Stein. 2018. A stylometric inquiry into hyperpartisan and fake news. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 231–240. Association for Computational Linguistics. Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and Yejin Choi. 2017. Truth of varying shades: Analyzing language in fake news and political fact-checking. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2931–2937. Ruty Rinott, Lena Dankin, Carlos Alzate Perez, Mitesh M. Khapra, Ehud Aharoni, and Noam Slonim. 2015. Show me your evidence - an automatic method for context dependent evidence detection. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 440–450. Mike Thelwall, Kevan Buckley, Georgios Paltoglou, Di Cai, and Arvid Kappas. 2010. Sentiment strength detection in short informal text. Journal of the Association for Information Science and Technology, 61(12):2544–2558. J Thorne and A Vlachos. 2018. Automated fact checking: Task formulations, methods and future directions. In Proceedings of the 27th International Conference on Computational Linguistics (COLING 2018). James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and verification. In NAACL-HLT. Andreas Vlachos and Sebastian Riedel. 2014. Fact checking: Task definition and dataset construction. In Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 18–22. William Yang Wang. 2017. liar, liar pants on fire”: A new benchmark dataset for fake news detection. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017), Vancouver, BC, Canada. ACL. Xuezhi Wang, Cong Yu, Simon Baumgartner, and Flip Korn. 2018. Relevant document discovery for factchecking articles. In Companion of the The Web Conference 2018 on The Web Conference 2018, pages 525–533. InternationalWorldWideWeb Conferences Steering Committee.","Alhindi, T., Petridis, S., & Muresan, S. (2018, November). Where is your evidence: Improving fact-checking by justification modeling. In Proceedings of the first workshop on fact extraction and verification (FEVER) (pp. 85-90)."
ML_DB_141,Factoring Fact-Checks: Structured Information Extraction from Fact-Checking Articles,"Fact-checking, which investigates claims made in public to arrive at a verdict supported by evidence and logical reasoning, has long been a significant form of journalism to combat misinformation in the news ecosystem. Most of the fact-checks share common structured information (called factors) such as claim, claimant, and verdict. In recent years, the emergence of ClaimReview as the standard schema for annotating those factors within fact-checking articles has led to wide adoption of fact-checking features by online plat forms (e.g., Google, Bing). However, annotating fact-checks is a tedious process for fact-checkers and distracts them from their core job of investigating claims. As a result, less than half of the fact-checkers worldwide have adopted ClaimReview as of mid-2019. In this paper, we propose the task of factoring fact-checks for automatically extracting structured information from fact-checking articles. Exploring a public dataset of fact-checks, we empirically show that factoring fact-checks is a challenging task, especially for fact-checkers that are under-represented in the existing dataset. We then formulate the task as a sequence tagging problem and fine-tune the pre-trained BERT models with a modification made from our observations to approach the problem. Through extensive experiments, we demonstrate the performance of our models for well-known fact-checkers and promising initial results for under represented fact-checkers.",Information Science,Proceeding,2020,Y,Y,20,"As a means to combat misinformation [1 , 17], journalists conduct research with evidence and logical reasoning to determine the ve- racity and correctness of factual claims made in public, and publish fact-checking articles (or fact-checks) on their news outlets [57]. Fact-checks play a significant role in the news ecosystem with a shared journalistic purpose of rebuting false claims, therefore they tend to share certain common structured information (called factors) in their journalistic practices [ 57 , 60 ]: a typical fact-check usually introduces the claim to be checked and the claimant who made the claim, and finally arrives at a verdict describing the veracity of the claim. A fact-check could also describe the context where the claim is made, provide evidence to support or attack the claim, etc. An example of a fact-check with three reported factors (claim, claimant and verdict) is shown in Figure 1. This paragraph is excerpted from a fact-check published by PolitiFact in May 2019, titled D.A.R.E. still thinks marijuana is a dangerous drug for kids”, which reached a verdict False” to the claim D.A.R.E removed cannabis from its list of gateway drugs.” made by Viral image” on Facebook. These three factors (claim, claimant and verdict) can summarize the main message of the fact-check for readers on a too long; didn’t read” agenda, and, more importantly, are structured information that can be understood by algorithms for various applications [7 WWW ’20, April 20–24, 2020, Taipei, Taiwan Jiang, et. al. 60 , 64, 65 ], e.g., Google and Bing display these factors as structured snippets for search results that correspond to a fact-check [3, 30]. However, the availability of such factors as structured informa- tion is limited. Traditionally, journalists use creative language to embellish their content and attract readers; therefore hiding these factors within the text content of their fact-checks. Structured infor- mation has only been recently made available with the global effort on computational journalism [5, 8 ]. In the context of fact-checking, a schema named ClaimReview [ 50] was developed to help anno- tating these structured information on web pages: A fact-checker can embed the ClaimReview markup in their HTML content of fact-checks, or submit these factors manually through an online ClaimReview markup tool [ 14 ]. This is a tedious process for fact- checkers and distracts them from their core job of investigating claims. As a result, Duke Reporters’ Lab reported that less than half of their recorded fact-checkers have adopted this schema as of July 2019, and only for a part of their published fact-checks [39 ]. Therefore, how to extract more structured information from the remaining fact-checks becomes an emerging research concern. In this paper, we propose the task of factoring fact-checks for automatically extracting structured information from fact-checking articles. Leveraging a public dataset of 6K fact-checks available on DataCommons [ 9], we first conduct an exploratory analysis of the task. We find that reported claimants and verdicts can be mostly found exactly in the text content of fact-checks, while claims are mostly paraphrased from one or more sentences. In addition, we find that these factors are heavily distributed in head and tail sentences of fact-checks, albeit differently between well-known and under-represented fact-checkers. In order to automatically extract these factors, we formulate this task as a sequence tagging problem and conduct several experiments by fine-tuning the state-of-the- art pre-trained BERT models [10 ]. Our experiments focus on the following research questions: • How well can models extract claims, claimants and verdicts? • Can model performance be improved with modifications? • Can models trained on well-known fact-checkers generalize? Our experiments demonstrate the performance of BERT models on well-known fact-checkers, especially under the modification made from our empirical observations. Although it is challenging for models to generalize to under-represented fact-checkers whose fact-checks are unseen during the training process, we demonstrate promising initial results by conducting additional experiments. As this task directly faces the misinformation problem and therefore requires extremely high accuracy to be fully automated, we dis- cuss several potential applications with the performance as is, e.g., pre-populating ClaimReview markups in a human-in-the-loop pro- cess [14 ], or supporting other downstream tasks of computational fact-checking [7, 60, 64, 65]. To summarize, we make the following contributions: • Defined the task of factoring fact-checks to assist fact-checkers. • Explored existing fact-checks and their reported factors to un- derstand the challenges involved in the task. • Modified and fine-tuned BERT models and conducted extensive experiments to approach the task. The rest of the paper is organized as follows: § 2 introduces back- ground and positions our work around related areas, § 3 explores (a) Google snippet. A fact-check from PolitiFact is displayed. (b) Bing snippet. A fact-check from Africa Check is displayed. Figure 2: Structured fact-check snippets. When searching a made-up explanation of the word newspaper”, fact-checking fea- tures are displayed by both Google and Bing. data, § 4 formulates the task and introduces the model, § 5 reports the results of our experiments, § 6 discusses potential applications, limitations and future work, and finally concludes. 2 BACKGROUND Fact-checks have been around since early 2000s and came to a broader public consciousness in 2016 [ 16 ], directly in response to the misinformation epidemic [1, 17 ]. In this section, we briefly introduce the background of misinformation and fact-checking, and situate our task in the broader scope of computational linguistics. 2.1 Misinformation and Fact-Checking There have been significant efforts on understanding misinforma- tion from both researchers [34] and practitioners [62]. Early work on misinformation discussed its psychological foundations [ 42 , 49, 61 ], economic incentives [1 , 13 ] and social impact [ 27 , 59 ]. Mean- while, studies from the computational community were mostly focused on detecting misinformation on the web [51 ]: these studies modeled the problem as a classification task and utilized various features (e.g., stylistic differences [ 12 , 46 , 58 ], public responses [ 27 – 29 ]) to access the trustworthiness of information. Although solid results were reported, these models heavily rely on the inflamma- tory and sensational language used by misinformation to instigate its readers, instead of verifying the information per se [45]. Fact-checking, as a complementary approach (arguably) orthog- onal to stylistic features and public opinions, rebuts misinformation by checking the veracity of individual factual claims [16 , 57 , 60 ]. Fact-checking is a time-consuming process done by journalists col- lecting evidence and writing articles (i.e., fact-checks). Although recent studies explored the potential of adding automation to sev- eral stages of this process (e.g., discovering check-worthy claims from large corpus [20– 22], checking existing knowledge graphs for evidence [7, 64, 65]), high quality fact-checks are still scarce. As the availability of fact-checks is limited, the utilization of fact- checks becomes of vital importance. Platforms have developed a variety of applications to utilize fact-checks, e.g., downranking [6 ], moderation [25, 26 ]. An application promulgated by search engines Factoring Fact-Checks WWW ’20, April 20–24, 2020, Taipei, Taiwan (i.e., Google [30 ] and Bing [ 3]) is enriching result pages with fact- checking features to maximize the exposure of high quality fact- checks by displaying structured snippets of their factors when searching a relevant query [60 ]. Figure 2 shows examples of such structured snippets from Google (Figure 2a) and Bing (Figure 2b) when searching a made-up explanation of the word newspaper”. Such application relies on structured factors reported by fact- checkers. However, less than half of fact-checkers reported factors for their fact-checks as of July 2019, according to Duke Reporters’ Lab [39 ]. Our task provides an upstream support for this process, as we expect more structured factors can be obtained with the help of factoring fact-checks. Specific use cases and other downstream applications that can benefit from our task are discussed in § 6. 2.2 Extracting, Mining and Verifying Claims There are several existing tasks in the broader scope of computa- tional linguistics that are related to our task. Claim extraction (or detection, identification) is a task of finding factual claims in an article. The target claim could be either context dependent [ 36 ] or independent [ 38]. In the fact-checking context, ClaimBuster is a popular system that ranks claims by checkwor- thiness” in news articles or political speeches [21 , 22 ]. In § 5, we apply this tool as a baseline method for the claim factor, and show that the most checkworthy” claim in a fact-check is often not the fact-checked one. Argument mining is a more general task of labeling arguments in an article, including both claim extraction and relationship (e.g., supporting or attacking, being premise or evidence) prediction between arguments [4]. This task is often formulated as a sentence classification or boundary detection problem, and its research has been applied to many forms of literature, including persuasive essays [11, 52 ], scientific publications [ 54 ], Wikipedia articles [ 2, 48 ], debate scripts [19, 43], etc. Our task can be approximately viewed as a specific case of relationship prediction for fact-checks, as we aim to extract the target claim and the verdict that supports or attacks the claim, except with different task formulation and context. In addition, we focus on only two factors instead of labeling the argument structure of the entire fact-check. Claim verification is a recently proposed task, that takes a claim and the corresponding context where claim is made as inputs, and outputs a binary verdict on whether the context supports or rejects the claim [ 55 , 56]. In our task, the claim is unknown and the verdict is free text: we take a fact-check as input, and output the claim and its verdict simultaneously, along with its claimant. 3 DATA EXPLORATION DataCommons hosts a dataset of URLs for fact-checks [9]. We use an internal Google tool to extract main article text from these URLs, filter out non-English fact-checks and keep the remaining 6,216 ones for our data exploration. These fact-checks are usually long articles, with 1,038 words and 24.4 paragraphs on average. Each fact-check is labeled with its three factors: claim, claimant and verdict, reported by its fact-checker. Claims are usually one or two sentences, with 22.2 words on average. Claimants are mostly names of people or organizations, with 2.2 words on average. Ver- dicts are mostly adjective phrases and 2.5 words on average.0 1000 2000 3000 4000 # of fact-checks Others africacheck.org factly.in washingtonpost.com factcheck.org politifact.com Fact-checker 348 220 250 627 856 3915 Well-known Under-represented Figure 3: Who are the fact-checkers? the number of fact-checks follows a power law distribution over fact-checkers, where well- known” (top 5) fact-checkers publish 94% of fact-checks. In this section, we explore several questions of the fact-check dataset to understand our task. 3.1 Who Are the Fact-Checkers? We first answer the question who the fact-checkers are. As shown in Figure 3: the number of fact-checks follows a power law distri- bution over fact-checkers, where top 19% (5/27) of fact-checkers publish 94% (5,868/6,216) of fact-checks and 40% (11/27) of fact- checkers have reported only a single fact-check. Notably, PolitiFact dominates this dataset with 63% (3,915/6,216) reported fact-checks. In the rest of the paper, we refer to the top five fact-checkers as well-known” ones, as they are reputed journalistic organizations that are heavily focused on fact-checking and have specialized fact-check” columns on their websites. Among them, three fact- checkers (PolitiFact, FactCheck.org and the Washington Post) are US-based and have been contributing to fact-checks for more than 10 years. The other two fact-checkers are also well-known in their respective markets: Factly.in is from India and Africa Check is from Africa, both founded in 2012. The rest of the 22 fact-checkers are under-represented” (i.e., with few samples) in this dataset. These fact-checkers could be a) newly established fact-checking teams from reputed journalistic organizations (e.g., Australian Associated Press), b) well-known news agencies doing occasional fact-checking (e.g., CNN, the Verge), or c) new coming agencies for localized fact- checking (e.g., factcheckNI for Northern Ireland). In § 5, we discuss how this split affects our experimental design. Overall, this dataset records more than one third of all fact- checkers that verified signatories of the International Fact-Checking Network (IFCN) code of principles [24 ], and contains all the well- known fact-checkers except Snopes, therefore it is a reasonably representative sample of the current fact-checking ecosystem. 3.2 Can Factors Be Found in the Fact-Check? Next, we answer the question if factors can be found in the fact- check article. In the introductory example shown in Figure 1, the claim D.A.R.E. removed cannabis from its list of gateway drugs.” and verdict False” can be both matched in the text content of the fact-check, while the claimant Viral image” cannot. Instead, the fact-check uses Facebook post” as the claimant. To answer this question for all fact-checks in the dataset, we start with exact string matching between factors and fact-checks. We find that most verdicts (76%, 4,743/6,216) and claimants (80%, WWW ’20, April 20–24, 2020, Taipei, Taiwan Jiang, et. al.0 1 2 3 4 5 6 7+ # of matched claims 0 500 1000 1500 2000 # of fact-checks 1335 2089 1681 667 233 108 46 57 (a) Can claims be found? 79% claims can be matched in fact-checks and 45% are matched more than once.0 1 2 3 4 5 6 7+ # of matched claimants 0 1000 2000 3000 # of fact-checks 1213 3509 805 305 144 82 51 107 (b) Can claimants be found? 80% claimants can be matched in fact-checks and 24% are matched more than once.0 1 2 3 4 5 6 7+ # of matched verdicts 0 1000 2000 3000 4000 # of fact-checks 1216 3975 749 158 74 22 8 14 (c) Can verdicts be found? 80% verdicts can be matched in fact-checks and 16% are matched more than once. Figure 4: Can factors be found in the fact-check? Around 80% of factors can be roughly matched in fact-checks based on a fuzzy matching rule, and most of them are matched only once. 4,988/6,216) can be matched in the fact-check, while claims are more difficult and match only 32% (2,000/6,216). This number is counter-intuitive for claims, as we expect the fact-checked claim should appear more frequently in the text content of fact-checks. After reading through several examples of claims and fact-checks, we find that although most claims are not exactly repeated in the text content of fact-checks, they are mostly paraphrased from one or more sentences, e.g., in Figure 2, the reported claim from PolitiFact Says the word newspaper stands for ‘north, east, west, south, past and present event report.’” is paraphrased to ‘newspaper’ is an acronym for ‘North, East, West, South, Past and Present Report.’” in the fact-check. To find these paraphrased factors, we develop a fuzzy matching rule: we first traverse each paragraph and keep the ones that contain at least a certain percentage of the reported factors, and then find the minimum window substring1 [35 ] of the overlap from the paragraph as the approximate match for the factor. To choose a reasonable threshold for above-mentioned percent- age, manually check 20 fact-checks and matched factors under a spectrum of thresholds. We find that setting the overlap thresh- old around two thirds gives the best match without introducing false positives (i.e., incorrectly matched factors). We apply the same threshold for all factors: For claims, this allows us to ignore certain paraphrase and conjugation (e.g., says/said/saying”); For claimants and verdicts, this threshold represents an exact match for short factors less than or equal to two words, e.g., False”, Mostly false”, John Doe”;2 and allows some flexibility for more than two words, e.g., the claimant John M Doe” can be matched without the middle name M”. After matching, we again manually check 100 random samples and find no false positives. Figure 4 shows the histograms of matched factors. Note that factors can be matched zero times or more than once. Under fuzzy matching, more claims (79%, 4,881/6,216) can be matched in fact- checks and 45% (2,792/6,216) are matched more than once (Fig- ure 4a). The number for matched claimants is also slightly increased, with 80% (5,003/6,216) matched and 24% (1,494/6,216) matched more than once (Figure 4b). Matched verdicts are roughly the same, 80% (5,000/6,216) verdicts are matched and 16% (1,025/6,216) are matched more than once (Figure 4c). In general, this observation suggests 1Comparison at word-level and case-insensitive. 286% (5,338/6,216) of claimants and 79% (4,915/6,216) of verdicts are of less than or equal to two words. that around 80% of factors can be roughly matched in fact-checks, the remaining ones are framed to the extend that exceeds our al- lowed threshold. 3.3 Where Are the Factors in the Fact-Check? Our final question is where the factors are in the fact-check if they are matched. To answer this, we normalize the locations of matched factors by the number of words in the fact-check. This results in a relative position measure ranging from 0 to 1 for each factor in each fact-check, where 0 represents the head and 1 represents the tail of the fact-check. Then, we estimate the probability density functions (PDFs) for the relative position measure of each factor, and plot them in Figure 5. Recall that well-known fact-checkers publish 94% of fact-checks, the PDFs of relative factor positions from all fact-checks would mostly reflect the distribution of well-known ones. Therefore, we estimate separate PDFs for well-known and under-represented fact-checkers in addition to the overall PDFs to compare their difference. As shown in Figure 5, there are two high-level observations that apply in general for all factors: a) factors distribute heavily on head (<0.15) and tail (>0.85) sentences of fact-checks, an observation that we later utilize for model design in § 4, and b) overall distributions (thick grey lines) are similar to the distribution of well-known fact-checkers (colored dashed lines) as expected. For claims, Figure 5a shows that claims are distributed differ- ently for fact-checks from well-known fact-checkers and under- represented ones (Mann-Whitney U = 2.1 × 108∗∗∗):3 Claims are found on both head (<0.15, 41%) and tail (>0.85, 28%) sentences from well-known fact-checkers, but mostly head (<0.15, 53%) sentences from under-represented fact-checkers. This is because well-known fact-checkers usually start their fact-check with an introductory paragraph and end with a concluding one, both of which likely de- scribes the checked claim. Under-represented fact-checkers usually do not write the latter part, and most fact-checks only introduce the claim in the beginning. Claimants are distributed similarly between well-known and under-represented fact-checkers. The distribution for well-known ones is slightly shifted to the left (U = 1.1 × 107∗∗∗), as shown in Figure 5b. All claimants are heavily distributed on head sentences 3 ∗p < 0.05; ∗∗p < 0.01; ∗∗∗p < 0.001 Factoring Fact-Checks WWW ’20, April 20–24, 2020, Taipei, Taiwan0.0 0.2 0.4 0.6 0.8 1.0 Relative position of matched claims 0 1 2 3 Density All fact-checkers Well-known Under-represented (a) Where are the claims? Claims from well- known fact-checkers are distributed on both head (<0.15, 41%) and tail (>0.85, 28%) sentences of fact- checks, but mostly heads (<0.15, 53%) from under- represented fact-checkers. The difference is sig- nificant (U = 2.1 × 108∗∗∗).0.0 0.2 0.4 0.6 0.8 1.0 Relative position of matched claimants 0 2 4 Density All fact-checkers Well-known Under-represented (b) Where are the claimants? Claimants from well-known and under-represented fact-checkers are distributed similarly, heavily on head sen- tences (<0.15, 62% for well-known ones and 66% for under-represented ones). The difference is sig- nificant (U = 1.1 × 107∗∗∗).0.0 0.2 0.4 0.6 0.8 1.0 Relative position of matched verdicts 0 2 4 6 8 Density All fact-checkers Well-known Under-represented (c) Where are the verdicts? Verdicts from well- known fact-checkers are distributed on the very end of the fact-check (>0.85, 73%), but mostly head (<0.15, 59%) sentences from under-represented fact-checkers. The difference is significant (U = 5.6 × 105∗∗∗). Figure 5: Where are the factors in the fact-check? Factors distribute heavily on head (<0.15) and tail (>0.85) sentences of fact-checks, and overall distributions are similar to the distribution of well-known fact-checkers. of the fact-checks (<0.15, 62% for well-known fact-checkers and 66% for under-represented ones). For verdicts, Figure 5c shows different distributions of relative po- sitions between well-known and under-represented fact-checkers (U = 5.6 × 105∗∗∗). For well-known fact-checkers, the verdicts are matched mostly on the very end of the fact-check (>0.85, 73%), usu- ally in the last several words. This reflect the journalistic style of well-known fact-checkers, especially PolitiFact and The Washing- ton Post, whose verdicts are based on their own rating systems and are reached in the very end. These verdicts usually act as comple- ments to linking the verb to be, e.g., We rate it Pants on Fire!” by PolitiFact and It earns Four Pinocchios.” by the Washington Post. On the contrary, under-represented fact-checkers tend to introduce their verdicts in the beginning of the fact-check (<0.15, 59%), and their verdicts are usually adjectives used to modify nouns, e.g., A false rumor claims...”. These fact-checks also have no concluding paragraphs in the end, therefore verdicts can only be found in the head sentences. 4 TASK AND MODELS Data exploration shows that most factors can be fuzzily matched in fact-checks. In this section, we formulate the task of factoring fact-checks as a computational linguistic task and introduce our models for the task. 4.1 Task Formulation In general, certain linguistic patterns can be found when a factor appears in the fact-check. These linguistic patterns can be roughly categorized to two types: the languages used by the factor per se and its surrounding context. 4.1.1 Linguistic patterns. Claims usually appear following certain verbs as context, e.g., (someone) claimed...”, (someone) said...”, (an image/post) shows...”, etc., although the beginning of the claim might not be directly adjacent to these verbs (as additional infor- mation can be inserted, e.g., claimed, at somewhere, that...”). In addition, the languages used by claims are mostly factual sentences, and sometimes contains quotation marks, numbers or statistics, entities, etc., e.g., A 1988 exorcism took place in the only house still standing after Hurricane Ike.” is a claim checked by PolitiFact which contains the year 1988” and the entity Hurricane Ike”. Claimants, opposite to claims, are usually followed by above- mentioned verbs as context (i.e., someone”, ”an image/post” in previous examples), and they are usually named entities such as persons (e.g., politicians, celebrities) or organizations (e.g., news agencies, social media platforms). Verdicts are mostly adjective words or phrases that describe ve- racity, e.g., true”, mostly true”, false”, or phrases from specialized ratings systems, e.g., pants on fire” by PolitiFact and the Pinoc- chio” system by the Washington Post. Occasionally, verdicts can also be descriptive text, e.g., out of context because...”, outdated as of...”. In terms of context, verdicts are often explicitly pointed out by a concluding sentence, e.g., we rate this claim true”, but it could also be embedded in sentences modifying nouns, e.g., a false rumor claims...”. 4.1.2 Sequence tagging. Combining these linguistic patterns to- gether, it is implausible to build and expensive to maintain a rule- based system that matches patterns to extract these factors. Instead, we formulate the extraction task as a computational linguistic task, the sequence tagging problem, that can be approached by probabilis- tic models. In general, the goal of the sequence tagging problem is to prob- abilistically estimate the distribution of labels to each word in a sequence. This is a common problem that are shared by a number of existing tasks: Part-of-speech tagging task assigns a part-of-speech label (e.g., noun, verb) to each word in a sentence [40, 47 ]; Named entity recognition task assigns a entity label (e.g., person, orga- nization, country) to each word in given text [33 , 41 ]; Extractive summarization task assigns a binary label representing if a word should belong to the summary [18, 63], etc. The expected input of the sequence tagging problem is a se- quence of words, and the output is an equal-length sequence of labels (i.e., a label for each word). In our task, there are three posi- tive labels (i.e., claim, claimant and rating) representing if a word should belong to a factor, and a negative label representing a word not belonging to any factors. WWW ’20, April 20–24, 2020, Taipei, Taiwan Jiang, et. al.actually 100% flatisearththethatclaimfalseamadeDoeMJohn actually 100% flat .isearththethatclaimfalseamadeDoeMJohn Claim: The earth is flat . Claimant: John Doe Verdict: False Fluent tagger: Concise tagger: . Figure 6: Fluent and concise tagger. The fluent tagger selects indexes of the entire span of matched factors, i.e., John M Doe” as the claimant and all words starting from the” as the claim; The concise tagger indexes of only the overlapping words, i.e., skipping non-overlapping words M” for the claimant and actually 100%” for the claim. 4.1.3 Generating ground-truth labels. Recall that fact-checks and its factors are available in our data and we developed a fuzzy match- ing rule in § 3. This information can be used to generate equal- length sequences as labels for the sequence tagging problem. For a fact-check, we first initialize an equal-length sequence with all negative labels, and then traverse through all matched factors and replace negative labels with positive ones at selected indexes. We propose two methods of selecting indexes: A fluent tagger that selects indexes of the entire span of matched factors and a concise tagger that selects indexes of only the overlapping words. As shown in Figure 6, the fact-check is John M Doe made a false claim that the earth is actually 100% flat.” and its three factors are The earth is flat” as the claim, John Doe” as the claimant and False” as the verdict.4 The fluent tagger labels John M Doe” as the claimant and all words starting from the” as the claim, while the concise tagger skips non-overlapping words M” for the claimant and actually 100%” for the claim. Intuitively, the fluent tagger is focused more on readability as it generates continues phrases as ground-truth, but it also inevitably includes unessential details in the sequence. On the contrary, the concise tagger is focused more on brevity as it only selects essential words of factors, but the results could be less readable if the matched factors miss several words. 4.2 Models Sequence tagging is a traditional computational linguistic problem that has been studied for decades from early statistical methods (e.g., Hidden Markov models [31], conditional random fields [ 32]) to current neural architectures (e.g., recurrent neural network [ 15 ]). To date, the most applicable models usually leverage transfer learning following the pre-training/fine-tuning paradigm. In short, during pre-training process, models are trained on unlabeled data with certain objectives, and during fine-tuning, models are initialized with pre-trained parameters and then re-train on labeled data over specific tasks. 4.2.1 BERT models. BERT (Bidirectional Encoder Representations from Transformers) is a recently developed model [ 10]. Its pre- training objective is to predict missing (i.e., masked) words in a sentence conditional on both left and right context, as well as to 4This one sentence fact-check is hypothetical to demonstrate how indexes are selected. In general, it is unlikely that all factors can be found in a single sentence.... ... ... ... If ..., then ... [CLS] A post says that D.A.R.E. this false . A post says that D.A.R.E. this false . ... ... ... If ..., then ... [TAIL] Claim: D.A.R.E. ... Claimant: Post Verdict: False Claim: D.A.R.E. ... Claimant: Post Verdict: False Figure 7: The framework for factoring fact-checks. First, the framework feeds the fact-check and its factors to a tagging pipeline; Then, the framework passes sequences and labels to the BERT model and uses cross entropy loss to fine-tune the model; Finally, the predicted labels is fed to a recovery pipeline to obtain factors. We feed our inputs one paragraph at a time and chunk paragraphs exceeding maximum length. lead token [CLS] is replaced with paragraph positions [HEAD], [BODY] or [TAIL]. predict the relationship between sentences (i.e., if a sentence is a next sentence of another). During fine-tuning, it has shown the abil- ity to achieve state-of-the-art performance on many computational linguistic tasks by simply replacing input and output layers. As BERT provides an easy access to the state-of-the-art without any specific neural architecture design, we experiment with BERT to explore the feasibility of our task. The specific framework of model is shown in Figure 7. First, our framework feeds the fact- check and its factors to a rule-based tagging pipeline described in § 4.1.3 to generate sequences and labels for the BERT model; Then, our framework passes these sequences and labels to the BERT model, obtains the activations of its last layer, and feeds them to an output layer for predictions. During the process, cross entropy loss is used to propagate errors and fine-tune the model [ 44 ]; Finally, the predicted labels are fed to a rule-based recovery pipeline to concatenate words and predict factors. Note that BERT is designed for short sequences with a default maximum sequence length of 512 while fact-checks are in general longer sequences.5 A common strategy dealing with long sequences is to truncate after the maximum length, because the head of the sequence usually captures its essence for a number of tasks, e.g., summarization, classification. However, truncation is not fit for our task as factors can be matched anywhere in the text content of a fact-check, as shown in Figure 5. Therefore, we run our framework on paragraph level and feed our inputs one paragraph at a time. If 51,038 words on average, and the length would increase with the wordpiece tokenizer from BERT. Factoring Fact-Checks WWW ’20, April 20–24, 2020, Taipei, Taiwan Table 1: Test on well-known fact-checkers. Fact-checks from this test set are published by the same set of fact-checkers in the train set. BERT models significantly outperforms baseline methods, and replacing [CLS] with paragraph positions help to improve the overall performance of models. Lead token Tagger Claim ROUGE-1 Claimant ROUGE-1 Verdict ROUGE-1 F1 Precision Recall F1 Precision Recall F1 Precision Recall Baseline .183 (.183) .300 (.300) .141 (.141) .237 (.237) .181 (.181) .352 (.352) .660 (.660) .638 (.638) .702 (.704) [CLS] Fluent .636 (.853) .669 (.897) .633 (.850) .769 (.894) .803 (.934) .759 (.883) .931 (.975) .934 (.979) .930 (.974) Concise .592 (.864) .615 (.897) .596 (.870) .784 (.907) .789 (.913) .783 (.906) .938 (.971) .940 (.973) .938 (.970) Paragraph position Fluent .638 (.854) .674 (.902) .637 (.853) .794 (.889) .821 (.919) .789 (.884) .940 (.978) .942 (.980) .939 (.978) Concise .646 (.866) .664 (.889) .652 (.873) .839 (.928) .852 (.943) .834 (.923) .941 (.975) .944 (.979) .940 (.974) a paragraph alone is longer than the maximum length, we chunk the paragraph to sub-paragraphs and feed them to BERT in order. 4.2.2 Modification on lead token. In the original BERT, the lead token of the input sequence is a special token","Jiang, S., Baumgartner, S., Ittycheriah, A., & Yu, C. (2020, April). Factoring fact-checks: Structured information extraction from fact-checking articles. In Proceedings of The Web Conference 2020 (pp. 1592-1603)."
ML_DB_143,Combining Similarity Features and Deep Representation Learning for Stance Detection in the Context of Checking Fake News,"Fake news are nowadays an issue of pressing concern, given their recent rise as a potential threat to high-quality journalism and  well-informed public discourse. The Fake News Challenge (FNC-1) was organized in early 2017 to encourage the development  of machine learning-based classification systems for stance detection (i.e., for identifying whether a particular news article  agrees, disagrees, discusses, or is unrelated to a particular news headline), thus helping in the detection and analysis of  possible instances of fake news. This article presents a novel approach to tackle this stance detection problem, based on the  combination of string similarity features with a deep neural network architecture that leverages ideas previously advanced in  the context of learning efficient text representations, document classification, and natural language inference. Specifically, we  use bi-directional Recurrent neural networks (RNNs), together with max-pooling over the temporal/sequential dimension  and neural attention, for representing (i) the headline, (ii) the first two sentences of the news article, and (iii) the entire news  article. These representations are then combined/compared, complemented with similarity features inspired on other FNC-1  approaches, and passed to a final layer that predicts the stance of the article towards the headline. We also explore the use of  external sources of information, specifically large datasets of sentence pairs originally proposed for training and evaluating  natural language inference methods, in order to pre-train specific components of the neural network architecture (e.g., the  RNNs used for encoding sentences). The obtained results attest to the effectiveness of the proposed ideas and show that our  model, particularly when considering pre-training and the combination of neural representations together with similarity  features, slightly outperforms the previous state-of-the-art.",Information Science,Article,2019,Y,N,48,"Fake news (i.e., made up stories with the intention of deceiving, and that most of the times are used to achieve secondary gains) are undoubtedly one of the most serious challenges that journalists and the news industry are facing today. Given the ease in obtaining and spreading information through social networking platforms, it Authors’ addresses: Luís Borges, INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Rua Alves Redol, 9, Lisbon, 1000-029, Portugal, luis.borges@tecnico.ulisboa.pt; Bruno Martins, INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Rua Alves Redol, 9, Lisbon, 1000-029, Portugal, bruno.g.martins@tecnico.ulisboa.pt; Pável Calado, INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Rua Alves Redol, 9, Lisbon, 1000-029, Portugal, pavel.calado@tecnico.ulisboa.pt. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor, or affiliate of the United States government. As such, the United States government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for government purposes only. © 2019 Association for Computing Machinery. 1936-1955/2019/4-ART39 $15.00 ACM Journal of Data and Information Quality, Vol. 9, No. 4, Article 39. Publication date: April 201939:2 • Borges et al. is increasingly harder to know for sure what to trust, with the absorption of fake news by the masses having increasingly harmful consequences [ 48 ]. Automatically dealing with fake news has drawn considerable attention in several research communities [24, 26, 34, 36, 40 , 41, 45]. However, the task of evaluating the veracity of news articles is still very demanding and complex, even for trained specialists and much more for automated systems. A useful first step towards identifying fake news articles relates to understanding what other news agencies, in a given moment, are reporting about the same topic. This sub-task is often referred to as stance detection, and automating this process might be useful in developing automated assistants to help in fact checking. In particular, an automatic approach to stance detection would allow, for example, someone to insert an allegation or a news title, and recover related articles that either agree, disagree, or discuss that title. Then, the human checker would use her own judgment to assess the situation. Based on the aforementioned general ideas, a Fake News Challenge (FNC-1) was organized in early 2017 by a group of academics and contributors from the journalism industry to foster the development of systems applying artificial intelligence and machine learning for evaluating what a news source is saying about a particular issue. Specifically, FNC-1 involved developing models that, given the body of a news article and a news headline, estimate the stance of the article towards the headline (i.e., the article can either agree, disagree, discuss, or be unrelated to the headline). More information on the FNC-1 task, its rules, the dataset, and the evaluation metrics, can be found on the official website1. A total of 50 teams actively participated on the challenge, and the training/testing data splits have been released afterwards, in order to encourage further developments. The winning entries in FNC-1 relied on ensemble models combining similarity features computed between the headline and the body, with representations built from the words occurring in the texts (e.g., by processing word embeddings with convolutional neural networks). Despite the good results achieved by the participants, with scores of 82.02% and 81.98% for the two winning teams, in terms of the accuracy metric considered for the challenge, we believe there are several opportunities for further improvements. For instance, although deep neural networks are known to outperform other approaches (e.g., linear models based on careful feature engineering) in several natural language processing problems related to stance detection (e.g., in tasks related to measuring semantic similarity between sentences, and/or related to performing natural language inference), the results from FNC-1 showed that standard approaches based on convolutional and/or recurrent neural networks are not well-suited to this task, as they fail to model semantic relations with respect to large pieces of text (i.e., the bodies of news articles, composed of multiple sentences). A FNC-1 baseline that was introduced by the task organizers, which leverages a combination of feature engineering heuristics (i.e., word and n-gram overlap features, as well as indicators for refutation and polarity), achieves a competitive performance with the best systems, and it even outperforms several widely used deep learning architectures. In this article, we address the stance detection problem proposed in the context of FNC-1, using a novel approach based on the combination of similarity features with deep neural networks for generating effective representations. The main ideas and contributions of our work can be summarized as follows: • Taking inspiration on previous studies addressing natural language inference [ 4 , 49], we propose a deep neural network architecture for stance detection, as defined in the FNC-1 task. Our architecture leverages pre- trained word embeddings, uses shared parameters for some of the components (e.g., the sentence encoders), and attempts to match multiple representations learned from the inputs. Specifically, we combine/match representations inferred from three different inputs (i.e., the headline, the first two sentences of the news article, and the entire document) through a series of operations known to perform well on natural language inference [ 12], namely the vector concatenation, difference, and element-wise product. A final layer processes this result, and predicts the stance of the news article towards the headline. ACM Journal of Data and Information Quality, Vol. 9, No. 4, Article 39. Publication date: April 2019. Combining Similarity Features and Deep Representation Learning for Automatic Stance Detection • 39:3 • The proposed neural network architecture leverages a hierarchical approach for modeling the body of news articles, taking inspiration on previous studies addressing the classification of long documents [14, 52]. In this approach, a Recurrent Neural Network (RNN) is used for modeling the sequence of sentences, which in turn are individually modeled by a separate RNN encoding sequences of words. Specifically, we evaluated the use of bi-directional RNNs (i.e., either Gated Recurrent Units or Long Short-Term Memory units, optionally also in a stacked arrangement with shortcut connections), together with max-pooling and/or a neural attention mechanism that weights the individual word representations, for encoding sentences in our model (i.e., the headline, and the sentences in the news article being matched). Separate layers of bi-directional RNNs, also combined with max-pooling and/or neural attention, are used for encoding the sentences that constitute the document. This approach can directly incorporate knowledge of the document structure in the model architecture, at the same time also exploring the intuition that not all sentences/words will be equally relevant for predicting the stance; • We used the SNLI [4] and MultiNLI [49] datasets of sentence pairs, previously proposed for evaluating natural language inference models (i.e., models for checking if a given hypothesis sentence entails, contra- dicts, or is neutral towards a given premise sentence), to pre-train some of the components involved in our neural network architecture. Previous studies have already attested to the benefits of similar pre-training procedures in other natural language processing and text classification tasks [ 12 ]. We specifically used SNLI/MultiNLI sentence pairs to pre-train the components involved in encoding sentences, and also the part of the network that matches the headline with the first two sentences of the document (although this second component is not used in the neural architecture that achieved the best experimental results); • Learning effective representations for news article bodies can be quite challenging for neural methods, although this is essential for achieving good performance on the FNC-1 task [2, 30]. We therefore propose to combine the representations learned through neural network layers, with external similarity features. We specifically leverage features from previous FNC-1 methods (e.g., from the baseline method introduced by the organizers of the challenge), together with other methods proposed for similar text matching problems (e.g., BLEU and ROUGE scores [27, 33], the soft-cosine similarity metric [6], or the CIDEr score [47]). • We report on the results of an extensive set of experiments, evaluating the contribution of each of the aforementioned components. The results confirm that model pre-training can indeed improve the overall accuracy. Moreover, despite the use of the hierarchical attention method for encoding the news articles, which has been shown to perform well on tasks related to the classification of long documents [ 52], our results also show that hand-crafted similarity features are highly beneficial. The complete method establishes a new state-of-the-art result for the FNC-1 dataset, slightly outperforming the previous approach described by Bhatt et al. [ 2]. The source code supporting the experiments reported on this article has also been made available online through GitHub2. The remainder of this article is organized as follows: Section 2 presents fundamental concepts related to the use of deep neural networks in tasks such as natural language inference or stance detection, together with an overview of previous work related to FNC-1. Section 3 presents the neural network architecture proposed for handling the FNC-1 stance detection task, detailing (i) the use of bi-directional RNNs together with max-pooling and with an attention mechanism for encoding sentences, (ii) the hierarchical attention model for encoding larger pieces of text (i.e., the body of a news article), (iii) the method used for combining the representations generated for the headline and the body, (iv) the integration of similarity features inspired on other approaches for the FNC-1 task, and (v) the use of the SNLI and MultiNLI datasets for model pre-training. Section 4 describes the experimental evaluation of the proposed method, specifically detailing the evaluation methodology and 2http://github.com/LuisPB7/fnc-msc ACM Journal of Data and Information Quality, Vol. 9, No. 4, Article 39. Publication date: April 2019. 39:4 • Borges et al. presenting ablation tests that validate the contribution of the different model components. Finally, Section 5 presents the main conclusions and highlights possible directions for future work. 2 CONCEPTS AND RELATED WORK This section starts by reviewing neural network methods for modeling textual information, which are the fundamental building blocks of most modern approaches for tasks such as natural language inference or stance detection. It then overviews previous work developed in the context of the Fake News Challenge (FNC-1). 2.1 Deep Neural Networks for Natural Language Processing Supervised machine learning is extensively used for Natural Language Processing (NLP). In general, supervised learning concerns with inferring the parameters of models that take as input vector representations x and return as output another vector, where each dimension reflects the probability of the input belonging to a certain class. In NLP tasks such as document classification, x typically encodes features like words or characters occurring in the text. Bag-of-words approaches, and extensions considering n-grams, are arguably the most commonly used representations, treating words and/or phrases as unique discrete symbols, and weighting their contributions through heuristics such as the Term Frequency multiplied by the Inverse Document Frequency (TF-IDF). More recently, noting that bags-of-words often fail in capturing similarities between words, at the same time suffering from sparsity and high dimensionality, methods using neural networks to learn distributed vector representations of words (i.e., word embeddings) have gained popularity. These word embeddings can be (pre-)trained in an unsupervised manner over large corpora (e.g., by learning to predict target words using their neighboring words), through methods like word2vec [29] or GloVe [ 20]. One can then average the embedding vectors to generate representations of larger pieces of text, loosing word order as in bag-of-words approaches, or instead use sequences with the word embeddings themselves as the inputs to be processed by the learning models. Different machine learning methods have been used in NLP applications, and deep neural networks are nowadays a popular choice. In general, neural networks can be seen as as nested composite functions that transform vector representations, and whose parameters can be trained directly to minimize a given loss function computed over the outputs and the expected results. This training procedure involves an algorithm known as back-propagation, in combination with some variation of gradient descent optimization [ 15]. An optimization procedure that has been frequently used to train deep neural networks is the Adaptive Moment Estimation (Adam) algorithm [ 22]. Adam computes parameter updates leveraging an exponentially decaying average of past gradients, together with adaptive learning rates for each parameter. In practice, it performs larger updates for infrequent parameters, and smaller updates for frequent parameters. The Multi-Layer Perceptron (MLP) is a simple neural network architecture, which consists of a set of nodes forming the input layer, one or more hidden layers of computation nodes, and an output layer of nodes. The input signal propagates through the network layer-by-layer in a feed-forward manner, until it reaches the output node(s). Considering a single hidden layer, the corresponding computations can be written as shown in Equation 1. y = σ (σ ′(x · A + a) · B + b) (1) In the previous equation, x is a vector of inputs and y a vector of outputs. The matrix A represents the weights of the first layer and a is a bias vector for the first layer, while B and b are, respectively, the weight matrix and the bias vector of the second layer. The functions σ ′(.) and σ (.) both denote an element-wise non-linearity, i.e. the activation functions respectively associated to nodes in the hidden layer, and in the output layer of the network. The softmax function (i.e., a normalized exponential function that produces as output a probability distribution) is often used as the activation in the final layer of a MLP classifier, training the network to minimize a cross-entropy loss defined over the predictions and the ground-truth labels. ACM Journal of Data and Information Quality, Vol. 9, No. 4, Article 39. Publication date: April 2019. Combining Similarity Features and Deep Representation Learning for Automatic Stance Detection • 39:5 While MLPs have been extensively used in NLP applications, the problems in this area often involve capturing regularities over structured data of arbitrary sizes (e.g., sequences of word embeddings). In many cases, this means encoding the structure as a fixed width vector, which we can then be used for further processing. Other network architectures besides MLPs are thus commonly used in NLP, in order to transform a sequence of word embeddings x1, . . . , xT ∈ Rd into a vector (e.g., a sentence representation) s ∈ Rh . These include Convolutional Neural Networks (CNNs) and different forms of Recurrent Neural Networks (RNNs). CNNs involve the application of h filters, sliding them over the input sequence. Each filter performs a local convolution (i.e., an element-wise matrix multiplication followed by a summation) on the sub-sequences of the input, to obtain a set of feature maps. Then, a global average- or max-pooling over time is performed to obtain a scalar, and the scalars from the h filters are finally concatenated into the sequence representation vector s ∈ Rh . Assuming an input sequence x1, . . . , xT (e.g., a sequence of embeddings for a document with T words), a convolution layer of width k works by moving a sliding window of size k over the sequence, creating several instances of windows wi = [xi ; xi+1; . . . ; xi+k−1]. A filter, i.e. a linear transformation followed by an activation function, is then applied to each window, resulting in m vectors p1,..., pm where each is defined as follows: pi = σ (wi · A + a) (2) In the previous equation, σ (.) is an activation function that is applied element-wise, while A and a are parameters of the network. The m vectors are then passed through a max-pooling layer and a final representation vector r is obtained. Each element j of r is obtained as follows, where pi [j] denotes the j-th component of pi : r [j] = max 1<i ≤m pi [j]. (3) RNNs can, in turn, be seen as time-dependent neural networks, which at time step t (i.e., at position t for a given input sequence) compute a hidden state vector ht , which is obtained by a non-linear transformation with two inputs, i.e. the previous hidden state ht −1 and the current word input xt . The most elementary RNN is often called the Elman RNN, corresponding to the following equation: ht = σ (W 1 · ht −1 + W 2 · xt ) (4) However, previous research has noted that the Elman RNN has difficulties in modeling long sequences. Extensions have been proposed to handle this problem, and two well-known examples are Long Short-Term Memory (LSTM) units [17 ] and Gated Recurrent Units (GRUs) [11 ]. Both these approaches involve different components, i.e. gating mechanisms, which interact in a particular way in order to combine previous states with the current inputs. For instance, GRUs correspond to the following equations: zt = φg (Wz · xt + Uz · ht −1 + bz ) (5) rt = φg (Wr · xt + Ur · ht −1 + br ) (6)  ̃ht = φh (Wh · xt + Uh · (rt ⊙ ht −1) + bh ) (7) ht = zt ⊙ ht −1 + (1 − zt ) ⊙  ̃ht (8) In the previous equations, the operator ⊙ denotes the Hadamard product (i.e., the entry-wise product of two matrices), while xt denotes the input vector at time step t, and ht denotes the hidden state at time step t. The activations φg(.) and φh(.) are typically chosen to be the sigmoid and the hyperbolic tangent functions, respectively, while the multiple parameters W, U and b denote the different weight matrices and bias vectors, adjusted when training the neural network. Notice that a GRU involves two gates, namely a reset gate r, that determines how to combine the new input with the previous memory, and an update gate z, that defines how much of the previous memory to keep around. If we set the reset gate to all ones, and the update gate to all zeros, we again arrive at the Elman RNN model ACM Journal of Data and Information Quality, Vol. 9, No. 4, Article 39. Publication date: April 2019. 39:6 • Borges et al. that was discussed previously. The gating mechanism allows GRUs to better handle long-term dependencies. By learning the parameters for its gates, the network learns how its internal memory should behave, given that the gates define how much of the input and previous state vectors should be considered. LSTMs are an alternative type of RNN, with more parameters than GRUs (e.g., they have an extra gate) although also reported to outperform them when more training data is available, and in tasks requiring modeling longer-distance relations [ 53]. LSTMs correspond to the following equations, where ⊙, φg(.), φh(.) xt, ht , W, U and b share the same meaning as in the GRU equations. it = φg (Wi · xt + Ui · ht −1 + bi ) (9) ft = φg (Wf · xt + Uf · ht −1 + bf ) (10) ot = φg (Wo · xt + Uo · ht −1 + bo ) (11) gt = φh (Wд · xt + Uд · ht −1 + bд ) (12) ct = ft ⊙ ct −1 + gt ⊙ it (13) ht = φh(ct ) ⊙ ot (14) Notice that LSTMs apply different gating mechanisms, namely through the use of (i) a forget gate that controls how much of the previous memory will be kept, (ii) an input gate that controls how much of the proposed update gt should be kept, and (iii) an output gate that controls the output at time t. Both LSTMs and GRUs have been shown to achieve remarkable performance on many sequential learning problems [53], particularly when processing short texts. In addition, hierarchical arrangements of LSTMs or GRUs can be used for modeling long documents, accounting not only with word order but also with sentence structure [ 14, 52]. For instance Yang et al. [ 52] proposed a hierarchical attention network for document classifica- tion, considering documents to be composed of sentences, and modeling sentences as sequences of words. They used a bi-directional GRU (i.e., they concatenate the states produced by two GRUs to generate the result for each time step, with one GRU processing the input in the forward direction, and the other processing the input in reverse) to encode each word in a sentence, followed by an attention mechanism to weight relevant words in the aggregated representation of each sentence. The attention mechanism corresponds to the following equations, effectively weighting the contribution of each word in the sentence, when building the representation: uit = tanh(W h · hit + bw ) (15) αit = exp(u⊤ it uw ) Ít exp(u⊤ it uw ) (16) si = Õ t αit × hit (17) The vector hit corresponds to the word representation produced by the bi-GRU at time t, when processing sentence i, while W h and bw are parameters to be learned. The vector uit is a hidden representation of hit , uw is a word-level context vector to be learned, αit is an importance weight, and si is the sentence vector, calculated as the weighted sum of the word representations. To create a document vector, the same methodology can be applied. A bi-directional GRU encodes the sentence vectors, and an attention mechanism is used to determine the importance of each sentence, yielding a document vector. The equations for the document-level attention mechanism are analogous to those from Equations 15 to 17. To generate the final prediction, the document vector is processed through feed-forward layer(s) with a final a softmax activation. Besides document classification, another NLP task that is strongly related to the subject of this article, and where deep learning has been extensively employed, is Natural Language Inference (NLI). In brief, NLI concerns with determining if a given hypothesis sentence h can be inferred from a premise sentence p. Generalized versions ACM Journal of Data and Information Quality, Vol. 9, No. 4, Article 39. Publication date: April 2019. Combining Similarity Features and Deep Representation Learning for Automatic Stance Detection • 39:7 Fig. 1. Overview of a generic NLI approach, adapted from the description by Conneau et al. [12]. of the task have also considered multiple possible relations between the hypothesis and the premise (e.g., relations like entailment, contradiction, or neutrality), and large datasets such as the Stanford Natural Language Inference (SNLI) [ 4 ] or the Multi-Genre Natural Language Inference (MultiNLI) [ 49] corpora have enabled significant progress in terms of deep learning methods for building effective semantic representations of natural language information (e.g., models trained for NLI tasks can provide rich domain-general semantic representations). For instance, Conneau et al. [12 ] demonstrated that the supervised training of sentence embeddings, based on the aforementioned NLI datasets, can consistently outperform other state-of-the-art approaches for representing sentences in different NLP tasks (e.g., taking the average of word2vec [ 29] or GloVe [20] embeddings, using unsupervised methods such as skip-thought sentence embeddings [23], or using other supervised models such as the paragram-phrase approach described by Wieting et al. [21 ]). The model parameters obtained when solving the NLI problem can thus be used for initializing other NLP models. Conneau et al. have also advanced a generic architecture for addressing the NLI task, illustrated in Figure 1. In brief, the premise and the hypothesis can both be encoded by a CNN or an RNN, creating a vector representation for each sentence. These representations are then matched in some way (e.g., through a concatenation of the vectors, through the vector difference, and/or through an element-wise product), fed into a set of fully-connected layers, and finally processed through a feed-forward layer with a softmax activation, that generates a final prediction. As a sentence encoder (i.e., to generate sentence representations u and v, respectively from the premise and the hypothesis), Conneau et al. used a bi-directional LSTM, followed by a max-pooling operation over the sequence of output states. Then, the authors combined both these representations through the concatenation of u and v, the element-wise product, and the difference between the vectors (i.e., they used a combination of all three operations). Finally, the result was fed to multiple fully-connected layers, with a final softmax activation. Besides the aforementioned general model, several other NLI approaches have been proposed in the literature [ 7 , 16, 31 ], and evaluated over the SNLI and MultiNLI corpora (e.g., the authors of SNLI maintain a leader-board on the website3 describing the corpus). For instance, Nie and Bansal [31] developed a model that scores an accuracy of 86.1% on the SNLI test set, an acuracy of 74.6% on the MultiNLI matched testing set, and 73.6% on the MultiNLI mismatched testing set. Following the same general approach from Figure 1, these authors used a sentence encoder that takes as input pre-trained GloVe embeddings [ 20] for the words in the sentence, and processes/refines the embeddings through three layers of bi-directional LSTMs with shortcut connections (i.e., the input sequences for the i-th bi-LSTM layer are the concatenated outputs of all the previous layers, plus the original word embedding sequence). The final representations of the premise and hypothesis are generated through a max-pooling operation between all the hidden states of the last bi-directional LSTM. A similar approach is used in the present study to encode sentences in the stance detection problem from the Fake News Challenge, although also using inner-attention as in the study by Yang et al. [52], instead of just using max-pooling. 3http://nlp.stanford.edu/projects/snli/ ACM Journal of Data and Information Quality, Vol. 9, No. 4, Article 39. Publication date: April 2019. 39:8 • Borges et al. 2.2 Stance Detection in the Context of the Fake News Challenge The Fake News Challenge (FNC-1) consisted of a competition for evaluating NLP methods designed to solve a stance detection problem between the headline of a news article, and the main body of text for a news article. The leader-board of the competition points to several interesting methods, some of them reviewed in this section. The organizers of the competition also provided a baseline consisting of a gradient boosting classifier leveraging hand-crafted features, such as multiple similarity scores between the headline and the body (e.g., based on n-gram overlap). Using 10-fold cross validation, the baseline achieved a weighted accuracy of 75.20%, following the evaluation metric detailed in Section 4.1 (i.e., an accuracy metric that gives extra weight to some of the classes). The first place in the competition was obtained by the team SOLAT in the SWEN 4, which used an ensemble of two sub-models, each outputting predictions associated with a confidence score. The aggregated model made its decisions with basis on the weighted average of both sub-models, achieving a weighted accuracy of 82.02%. One of the sub-models was a gradient boosting classifier similar to that of the baseline, leveraging features like the number of overlapping words between the headline and the body, or similarity scores computed from word n-gram representations. The second sub-model was based on Convolutional Neural Networks (CNNs) for encoding the headline and the body, using pre-trained word2vec embeddings [29]. The outputs for the headline CNN and body CNN were concatenated and put through feed-forward layers. In second place came Team Athene5, with a weighted accuracy of 81.97% and using an ensemble of five identical sub-models with randomly initialized parameters. These sub-models were Multi-layer Perceptrons (MLPs) with seven hidden layers and a softmax activation at the end. Seven distinct feature types were considered, computed from either the headline, the body, or a combination of both. Examples of features include vectors of unigram occurrences, or the cosine distance between headline and body representations computed from a factorization of the unigram occurrence matrix (i.e., from representations produced through non-negative matrix factorization). The final prediction came from hard voting between the five sub-models in the ensemble. In third place, team UCL Machine Reading [37 ] achieved a weighted accuracy of 81.72%. This model was also fairly simple, consisting of a MLP with one hidden layer and a final layer with a softmax activation. The input feature vector resulted from a concatenation of Term-Frequency (TF) vectors built from the headline and the body, with the cosine similarity between TF-IDF vectors for the headline and the body. When considering the top three teams, it is important to notice that all of them leveraged hand-crafted features, together with other neural approaches. In a recent publication, Tosik et al. [45 ] reported on tests evaluating the individual contribution of hand-crafted features, using them to feed a single gradient boosting classifier (when directly considering the four class classification problem), or an ensemble of gradient boosting classifiers, where the instances are first classified as either unrelated or related, and then the related instances are assigned to one of the remaining three classes (i.e., agrees, disagrees, or discusses). Specifically, Tosik et al. leveraged text similarity features such as n-gram or word overlap, or the cosine similarity between TF-IDF vectors representing the headline and the body, as well as other miscellaneous features such as the presence of several refuting words in the headline/body, or the length of the headline/body. After achieving a weighted accuracy score of 78.63% on the FNC-1 testing dataset, the authors conducted ablation tests and concluded that the most helpful features were the overlap features between n-grams and words of the headline and body, the refuting features based on a lexicon, and distance measures such as the cosine similarity between TF-IDF vectors of the headline and the body, and the Word Mover’s Distance [ 25] between the headline and body text. On the other hand, sentiment features and other distance scores, such as the Hamming distance, did not contribute to the obtained result. 4http://blog.talosintelligence.com/2017/06/talos-fake-news-challenge.html 5http://medium.com/@andre134679/team-athene-on-the-fake-news-challenge-28a5cf5e017b ACM Journal of Data and Information Quality, Vol. 9, No. 4, Article 39. Publication date: April 2019. Combining Similarity Features and Deep Representation Learning for Automatic Stance Detection • 39:9 Pfohl and Legros [35] reported on other interesting submissions to FNC-1. These authors experimented with four different approaches, namely (i) a bag-of-words method, (ii) a basic LSTM approach, (iii) a method leveraging an LSTM together with an inner-attention mechanism, and (iv) a more sophisticated approach which the authors named conditional encoding LSTM with attention (CEA-LSTM). The bag-of-words model was built by averaging embeddings for words occurring in the headline, for words occurring in the body, and then concatenating these vectors. This result is processed by a feed-forward network with a softmax output layer. Both the basic LSTM model, and the model combining an LSTM with an inner- attention mechanism, processed a concatenation of the headline and the article body to classify the stance. The inner-attention was computed over a window with the first 15 tokens of the concatenated text. Finally, the CEA-LSTM processed the headline and the body with two separate LSTMs, using the final hidden state of the headline LSTM as the first hidden state of the body LSTM. An attention mechanism operated over","Borges, L., Martins, B., & Calado, P. (2019). Combining similarity features and deep representation learning for stance detection in the context of checking fake news. Journal of Data and Information Quality (JDIQ), 11(3), 1-26."
ML_DB_144,Real-time Claim Detection from News Articles and Retrieval of Semantically-Similar Factchecks,"Factchecking has always been a part of the journalistic process. However with newsroom budgets shrinking it is coming under increasing pressure just as the amount of false information circulating is on the rise [MAGM18]. We therefore propose a method to increase the efficiency of the factchecking process, using the latest developments in natural language Processing (NLP). This method allows us to compare incoming claims to an existing corpus and return similar, factchecked, claims in a live system—allowing factcheckers to work simultaneously without duplicating their work.",Computer Science,Article,2019,Y,Y,8,"In recent years, the spread of misinformation has be- come a growing concern for researchers and the pub- lic at large [MAGM18]. Researchers at MIT found that social media users are more likely to share false information than true information [VRA18]. Due to renewed focus on finding ways to foster healthy polit- ical conversation, the profile of factcheckers has been raised. Factcheckers positively influence public debate by publishing good quality information and asking politi- cians and journalists to retract misleading or false statements. By calling out lies and the blurring of Copyright c© 2019 for the individual papers by the papers’ au- thors. Copying permitted for private and academic purposes. This volume is published and copyrighted by its editors. In: A. Aker, D. Albakour, A. Barr ́on-Cede ̃no, S. Dori-Hacohen, M. Martinez, J. Stray, S. Tippmann (eds.): Proceedings of the NewsIR’19 Workshop at SIGIR, Paris, France, 25-July-2019, published at the truth, they make those in positions of power ac- countable. This is a result of labour intensive work that involves monitoring the news for spurious claims and carrying out rigorous research to judge credibility. So far, it has only been possible to scale their output upwards by hiring more personnel. This is problem- atic because newsrooms need significant resources to employ factcheckers. Publication budgets have been decreasing, resulting in a steady decline in the size of their workforce [Pew16]. Factchecking is not a directly profitable activity, which negatively affects the alloca- tion of resources towards it in for-profit organisations. It is often taken on by charities and philanthropists instead. To compensate for this shortfall, our strategy is to harness the latest developments in NLP to make factchecking more efficient and therefore less costly. To this end, the new field of automated factcheck- ing has captured the imagination of both non-profits and start-ups [Gra18, BM16, TV18]. It aims to speed up certain aspects of the factchecking process rather than create AI that can replace factchecking person- nel. This includes monitoring claims that are made in the news, aiding decisions about which statements are the most important to check and automatically re- trieving existing factchecks that are relevant to a new claim. The claim detection and claim clustering methods that we set out in this paper can be applied to each of these. We sought to devise a system that would auto- matically detect claims in articles and compare them to previously submitted claims. Storing the results to allow a factchecker’s work on one of these claims to be easily transferred to others in the same cluster. arXiv:1907.02030v1 [cs.CL] 3 Jul 2019 2 Claim Detection 2.1 Related Work It is important to decide what sentences are claims be- fore attempting to cluster them. The first such claim detection system to have been created is ClaimBuster [HNS+17], which scores sentences with an SVM to determine how likely they are to be politically per- tinent statements. Similarly, ClaimRank [JGBC+18] uses real claims checked by factchecking institutions as training data in order to surface sentences that are worthy of factchecking. These methods deal with the question of what is a politically interesting claim. In order to classify the objective qualities of what set apart different types of claims, the ClaimBuster team created PolitiTax [Car18], a taxonomy of claims, and factchecking organ- isation Full Fact [KPBZ18] developed their preferred annotation schema for statements in consultation with their own factcheckers. This research provides a more solid framework within which to construct claim de- tection classifiers. The above considers whether or not a sentence is a claim, but often claims are subsections of sen- tences and multiple claims might be found in one sentence. In order to accommodate this, [LGS+17] proposes extracting phrases called Context Dependent Claims (CDC) that are relevant to a certain ‘Topic’. Along these lines, [AJC+19] proposes new definitions for frames to be incorporated into FrameNet [BFL98] that are specific to facts, in particular those found in a political context. 2.2 Method It is much easier to build a dataset and reliably eval- uate a model if the starting definitions are clear and objective. Questions around what is an interesting or pertinent claim are inherently subjective. For exam- ple, it is obvious that a politician will judge their oppo- nents’ claims to be more important to factcheck than their own. Therefore, we built on the methodologies that dealt with the objective qualities of claims, which were the PolitiTax and Full Fact taxonomies. We annotated sentences from our own database of news articles based on a combination of these. We also used the Full Fact definition of a claim as a statement about the world that can be checked. Some examples of claims accord- ing to this definition are shown in Table 1. We decided the first statement was a claim since it declares the oc- currence of an event, while the second was considered not to be a claim as it is an expression of feeling. Full Fact’s approach centred around using sentence embeddings as a feature engineering step, followed by Table 1: Examples of claims taken from real articles. Sentence Claim? In its 2015 order, the NGT had banned Yes the plying of petrol vehicles older than 15 years and diesel vehicles older than 10 years in the National Capital Region (NCR). In my view, farmers should not just No rely on agriculture but also adopt dairy farming. a simple classifier such as logistic regression, which is what we used. They used Facebook’s sentence embed- dings, InferSent [CKS+17], which was a recent break- through at the time. Such is the speed of new devel- opment in the field that since then, several papers de- scribing textual embeddings have been published. Due to the fact that we had already evaluated embeddings for clustering, and therefore knew our system would rely on Google USE Large [CYK+18], we decided to use this instead. We compared this to TFIDF and Full Fact’s results as baselines. The results are displayed in Table 2. However, ClaimBuster and Full Fact focused on live factchecking of TV debates. Logically is a news ag- gregator and we analyse the bodies of published news stories. We found that in our corpus, the majority of sentences are claims and therefore our model needed to be as selective as possible. In practice, we choose to filter out sentences that are predictions since gener- ally the substance of the claim cannot be fully checked until after the event has occurred. Likewise, we try to remove claims based on personal experience or anec- dotal evidence as they are difficult to verify. Table 2: Claim Detection Results. Embedding Method P R F1 Google USE Large 0.90 0.89 0.89 [CYK+18] Full Fact (not on 0.88 0.80 0.83 the same data) [KPBZ18] TFIDF (Baseline) 0.84 0.84 0.84 [Jon72] 3 Claim Clustering 3.1 Related Work Traditional text clustering methods, using TFIDF and some clustering algorithm, are poorly suited to the problem of clustering and comparing short texts, as they can be semantically very similar but use dif- ferent words. This is a manifestation of the the data sparsity problem with Bag-of-Words (BoW) mod- els. [SR15]. Dimensionality reduction methods such as Latent Dirichlet Allocation (LDA) can help solve this problem by giving a dense approximation of this sparse representation [BNJ03]. More recently, efforts in this area have used text embedding-based systems in order to capture dense representation of the texts [WXX+15]. Much of this recent work has relied on the increase of focus in word and text embeddings. Text embeddings have been an increasingly popular tool in NLP since the introduction of Word2Vec [MCCD13], and since then the number of different embeddings has exploded. While many focus on giving a vector repre- sentation of a word, an increasing number now exist that will give a vector representation of a entire sen- tence or text. Following on from this work, we seek to devise a system that can run online, performing text clustering on the embeddings of texts one at a time 3.1.1 Text Embeddings Some considerations to bear in mind when deciding on an embedding scheme to use are: the size of the final vector, the complexity of the model itself and, if using a pretrained implementation, the data the model has been trained on and whether it is trained in a supervised or unsupervised manner. The size of the embedding can have numerous re- sults downstream. In our example we will be doing dis- tance calculations on the resultant vectors and there- fore any increase in length will increase the complex- ity of those distance calculations. We would therefore like as short a vector as possible, but we still wish to capture all salient information about the claim; longer vectors have more capacity to store information, both salient and non-salient. A similar effect is seen for the complexity of the model. A more complicated model, with more train- able parameters, may be able to capture finer details about the text, but it will require a larger corpus to achieve this, and will require more computational time to calculate the embeddings. We should therefore at- tempt to find the simplest embedding system that can accurately solve our problem. When attempting to use pretrained models to help in other areas, it is always important to ensure that the models you are using are trained on similar ma- terial, to increase the chance that their findings will generalise to the new problem. Many unsupervised text embeddings are trained on the CommonCrawl 1 dataset of approx. 840 billion tokens. This gives a huge amount of data across many domains, but re- quires a similarly huge amount of computing power to train on the entire dataset. Supervised datasets are 1CommonCrawl found at unlikely ever to approach such scale as they require human annotations which can be expensive to assem- ble. The SNLI entailment dataset is an example of a large open source dataset [BAPM15]. It features pairs of sentences along with labels specifying whether or not one entails the other. Google’s Universal Sen- tence Encoder (USE) [CYK+18] is a sentence embed- ding created with a hybrid supervised/unsupervised method, leveraging both the vast amounts of unsuper- vised training data and the extra detail that can be derived from a supervised method. The SNLI dataset and the related MultiNLI dataset are often used for this because textual entailment is seen as a good basis for general Natural Language Understanding (NLU) [WNB18]. 3.2 Choosing an embedding In order to choose an embedding, we sought a dataset to represent our problem. Although no perfect matches exist, we decided upon the Quora duplicate question dataset [SIC17] as the best match. To study the embeddings, we computed the euclidean distance between the two questions using various embeddings, to study the distance between semantically similar and dissimilar questions. Figure 1: Analysis of Different Embeddings on the Quora Question Answering Dataset Table 3: Comparing Sentence Embeddings for Clustering News Claims. Embedding Time Number Number Percentage of Percentage of method taken (s) of claims of clusters claims in claims in clusters clustered majority clusters of one story Elmo [PNI+18] 122.87 156 21 57.05% 3.84% Googe USE [CYK+18] 117.16 926 46 57.95% 4.21% Google USE Large [CYK+18] 95.06 726 63 60.74% 7.02% Infersent [CKS+17] 623.00 260 34 63.08% 10.0% TFIDF (Baseline) [Jon72] 25.97 533 58 62.85% 7.12% The graphs in figure 1 show the distances between duplicate and non-duplicate questions using different embedding systems. The X axis shows the euclidean distance between vectors and the Y axis frequency. A perfect result would be a blue peak to the left and an entirely disconnected orange spike to the right, show- ing that all non-duplicate questions have a greater eu- clidean distance than the least similar duplicate pair of questions. As can be clearly seen in the figure above, Elmo [PNI+18] and Infersent [CKS+17] show almost no separation and therefore cannot be considered good models for this problem. A much greater disparity is shown by the Google USE models [CYK+18], and even more for the Google USE Large model. In fact the Google USE Large achieved a F1 score of 0.71 for this task without any specific training, simply by choosing a threshold below which all sentence pairs are consid- ered duplicates. In order to test whether these results generalised to our domain, we devised a test that would make use of what little data we had to evaluate. We had no original data on whether sentences were semantically similar, but we did have a corpus of articles clustered into stories. Working on the assumption that similar claims would be more likely to be in the same story, we developed an equation to judge how well our corpus of sentences was clustered, rewarding clustering which matches the article clustering and the total number of claims clustered. The precise formula is given below, where Pos is the proportion of claims in clusters from one story cluster, Pcc is the proportion of claims in the correct claim cluster, where they are from the most common story cluster, and Nc is the number of claims placed in clusters. A,B and C are parameters to tune. ( A × Pos + B × Pcc ) × (C × Nc) Figure 2: Formula to assess the correctness of claim clusters based on article clusters This method is limited in how well it can represent the problem, but it can give indications as to a good or bad clustering method or embedding, and can act as a check that the findings we obtained from the Quora dataset will generalise to our domain. We ran code which vectorized 2,000 sentences and then used the DBScan clustering method [EKSX96] to cluster using a grid search to find the best _x000F_ value, maximizing this formula. We used DBScan as it mirrored the cluster- ing method used to derive the original article clusters. The results for this experiment can be found in Ta- ble 3. We included TFIDF in the experiment as a baseline to judge other results. It is not suitable for our eventual purposes, but it the basis of the origi- nal keyword-based model used to build the clusters 2. That being said, TFIDF performs very well, with only Google USE Large and Infersent coming close in terms of ‘accuracy’. In the case of Infersent, this comes with the penalty of a much smaller number of claims in- cluded in the clusters. Google USE Large, however, clusters a greater number and for this reason we chose to use Google’s USE Large. 3 Since Google USE Large was the best-performing embedding in both the tests we devised, this was our chosen embedding to use for clustering. However as can be seen from the results shown above, this is not a perfect solution and the inaccuracy here will introduce inaccuracy further down the clustering pipeline. 3.3 Clustering Method We decided to follow a methodology upon the DBScan method of clustering [EKSX96]. DBScan considers all distances between pairs of points. If they are under _x000F_ then those two are linked. Once the number of con- nected points exceeds a minimum size threshold, they are considered a cluster and all other points are consid- ered to be unclustered. This method is advantageous for our purposes because unlike other methods, such as K-Means, it does not require the number of clusters to be specified. To create a system that can build clus- ters dynamically, adding one point at a time, we set 2Described in the newslens paper [LH17] 3Google USE Large is the Transformer based model, found at, whereas Google USE uses a DAN architecture the minimum cluster size to one, meaning that every point is a member of a cluster. A potential disadvantage of this method is that be- cause points require only one connection to a cluster to join it, they may only be related to one point in the cluster, but be considered in the same cluster as all of them. In small examples this is not a problem as all points in the cluster should be very similar. How- ever as the number of points being considered grows, this behaviour raises the prospect of one or several borderline clustering decisions leading to massive clus- ters made from tenuous connections between genuine clusters. To mitigate this problem we used a method described in the Newslens paper [LH17] to solve a sim- ilar problem when clustering entire articles. We stored all of our claims in a graph with the connections be- tween them added when the distance between them was determined to be less than _x000F_. To determine the final clusters we run a Louvain Community Detection [BGLL08] over this graph to split it into defined com- munities. This improved the compactness of a cluster. When clustering claims one by one, this algorithm can be performed on the connected subgraph featuring the new claim, to reduce the computation required. As this method involves distance calculations be- tween the claim being added and every existing claim, the time taken to add one claim will increase roughly linearly with respect to the number of previous claims. Through much optimization we have brought the com- putational time down to approximately 300ms per claim, which stays fairly static with respect to the number of previous claims. 4 Next Steps The clustering described above is heavily dependent on the embedding used. The rate of advances in this field has been rapid in recent years, but an embedding will always be an imperfect representation of an claim and therefore always an area of improvement. A do- main specific-embedding will likely offer a more accu- rate representation but creates problems with cluster- ing claims from different domains. They also require a huge amount of data to give a good model and that is not possible in all domains.","Adler, B., & Boscaini-Gilroy, G. (2019). Real-time claim detection from news articles and retrieval of semantically-similar factchecks. arXiv preprint arXiv:1907.02030."
ML_DB_145,Toward Automated Factchecking: Developing an Annotation Schema and Benchmark for Consistent Automated Claim Detection,"In an effort to assist factcheckers in the process of factchecking, we tackle the claim detection task, one of the necessary stages prior to determining the veracity of a claim. It consists of identifying the set of sentences, out of a long text, deemed capable of being factchecked. This article is a collaborative work between Full Fact, an independent factchecking charity, and academic partners. Leveraging the expertise of professional factcheckers, we develop an annotation schema and a benchmark for automated claim detection that is more consistent across time, topics, and annotators than are previous approaches. Our annotation schema has been used to crowdsource the annotation of a dataset with sentences from UK political TV shows. We introduce an approach based on universal sentence representations to perform the classification, achieving an F1 score of 0.83, with over 5% relative improvement over the state-of-the-art methods ClaimBuster and ClaimRank. The system was deployed in production and received positive user feedback.",Information Science,Article,2021,Y,Y,70,"Misinformation has recently become more central in public discourse [6, 37, 46]. As a consequence, interest has increased in the scientific community to further natural language processing (NLP) approaches that can help alleviate the burdensome and time-consuming human activity of factchecking [38, 40]. Factchecking is known as the task of producing an informed assessment of the veracity of a claim [13, 14]. The main mission of factcheckers is to give citizens information to make political choices, improve the quality of public political discourse, and to hold politicians accountable [14]. Misinformation and misperceptions can undermine this goal [9, 11]. However, there is a very small number of factchecking organisations in the world, about 160, compared to the volume of media items produced daily. The speed at which information flows online means there is less time to verify the claims made and myths spread further before being factchecked, if they are factchecked at all. Automating any parts of the factchecking process could cut down the time it takes to respond to a claim. It could also protect human factchecker's time to work on the more complicated checks that need careful human judgement. When considered inside a factchecking organisation, the factchecking process consists of a series of tasks, which Reference [3] defined as a process consisting of four tasks: (1) monitor media, i.e., capturing content such as articles, videos, images, (2) detect claims, i.e., spotting when an item contains a checkable claim, (3) check claims, i.e., doing the research to check and verify that claim, and (4) publish, i.e., creating a piece of content that encapsulates the results of the check. The vast majority of the scientific research has focused on the determination of veracity [20, 39, 45] or checking claims; despite often having been given different names in the scientific literature, such as fake news detection [26, 41, 44] or arguably rumour detection [2, 24, 27, 43]. The component checking the veracity of a claim cannot function in isolation, it needs a list of claims to check as an input. The majority of previous work on factchecking has started from a readily available list of claims and has omitted the previous steps of monitoring and spotting in the factchecking pipeline [20, 40]. This act of spotting claims within a corpus is known as claim detection, i.e., monitoring news sources and identifying if a sentence constitutes a claim that could be factchecked [18, 21]. Monitoring and spotting within a factchecking organisation is a time consuming semi-manual task, which is inevitably limited by the resources available. Automating the process of claim detection could mean that factcheckers can monitor a greater set of media, extract the claims made, and hopefully make smarter choices about what the most valuable items to be checking that day are. If deployed in a live factchecking situation, then it could also help separate out claims made in real-time during a ministerial speech, for example. This could help factcheckers quickly skim transcripts when time is limited. Despite the importance of this component in the factchecking pipeline, automation is still in its infancy and there is a dearth of scientific literature. Previous works on claim detection [12, 18] rely on definitions of claims that incorporate the concept of check- worthiness and/or importance. Our objective is instead to avoid these subjective concepts to come up with an objective way of determining what constitutes a claim that is checkable; rather than being important or worthy of checking, which is bound to subjective interpretation. This article describes the iterative process we followed together with factcheckers to come with up an annotation schema that would effectively capture claims and non- claims. This annotation schema avoids factors that can be affected by personal biases, such as importance, in the manual annotation to produce an objective outcome. Following this annotation schema through a crowdsourcing methodology, we generated a dataset of 5,571 sentences labelled as claims or non-claims. Further, we set out to present the development of the first stage in the automated factchecking pipeline. It constitutes the first automated claim detection system developed by an independent factchecking charity, Full Fact, along with academic partners. The main contributions of our work are as follows: 2 RELATED WORK Most work around automatically factchecking claims has focused on the later stages of determining the veracity of claims, usually by building knowledge graphs out of knowledge bases, such as Wikidata [5, 7, 35, 36, 42]. Less work has been documented on the preceding stage of claim detection. One of the best-known approaches to claim detection is ClaimBuster [18]. They collected a large annotated corpus of televised debates in the USA. Their model combines Term Frequency-Inverse Document Frequency (TF-IDF), part-of-speech (POS) tags, and named entity recognition (NER) features on an Support Vector Machines (SVM) classifier and produces a score of how important a claim is to factcheck. This has the caveat of then having to choose a cut-off score to determine the claims that will be considered worthy or important enough for factchecking. Our approach is instead to define an annotation schema that is binary, determining checkable claims rather than check-worthy claims, and is built on several types of claims. It is a simpler fit for the use case in this factchecking pipeline, i.e., in a live stream of subtitles, we are unable to know in advance which sentence will make it to the top ranking until the end of the entire programme. In Reference [15] annotations were collected using ClaimBuster-inspired annotation guidance from volunteers together with their age, gender and education. With possible labels being verifiable check-worthy (VCW), verifiable not check-worthy (VNCW), and not verifiable (NV), they obtained 2,100 labelled sentences (reduced to 264 high-quality labelled sentences). They find that annotators with a natural sciences background agreed internally about what constitutes a check-worthy claim, whereas those with humanities, medicine, and ontology backgrounds saw more internal disagreement on check-worthiness. Furthermore, using 35 labelled control claims to test annotator skill, they find that the age group 40–49 obtains a higher average score, and label more claims, than that of 30–39, which correspondingly scores higher than age group 20–29. Another recent approach to claim detection is ClaimRank [12]. They compiled a dataset by taking the outcome of factchecking a political debate, published by nine organisations simultaneously. Models were created to predict if the claim would be highlighted by at least one or by a specific organisation. The modelling is done with a large variety of features from both the individual sentence and the wider context around it. A subsequent version of the dataset [21, 29] includes a larger set of sentences in two languages, English and Arabic. These datasets are similar to ours in order of magnitude, however, use a different definition of claim, as is the case with others tackling the determination of check-worthiness of claims [4, 17, 31, 47]. We further elaborate on this in the next subsection. A slightly different approach to claim detection is that of context-dependent claim detection (CDCD) [25]. This study proposes identifying claims given a specific context. Articles relevant to a topic are used to detect claims on that topic. Another piece of work worth mentioning is the development of the FEVER (Fact Extraction and VERification) dataset [39]. Whilst this is primarily aimed at work regarding claim veracity, the mere presence of a vast quantity of claims in the dataset allow it to be extended for claim detection in the future. 2.1 Previous Attempts at Defining Claims 1 2 • We introduce the first annotation schema for claim detection, iteratively developed by experts at Full Fact, comprising seven different labels. • We describe a crowdsourcing methodology that enabled us to collect a dataset with 5,571 sentences labelled according to this schema. • We develop a claim detection system that leverages universal sentence representations, as opposed to previous work that was limited to word-level representations. Our experiments show that our claim detection system outperforms the state-of-the-art claim detection systems, ClaimBuster and ClaimRank. • With the annotation schema, crowdsourcing methodology and task definition, we set forth a benchmark methodology for further development of claim detection systems. Toward Automated Factchecking: Developing an Annotation Schema and Benchmark f... 1 sur 7 19-05-22, 13:11 There is a body of work on claim detection that has not formalised the definition of a claim, e.g., [12, 21]. Instead it directly relies on what has been identified by external organisations. The lack of a formal definition prevents others from replicating or extending their work. These studies used claims identified by nine organisations in a political speech as a proxy. The annotations were sourced from publicly available online articles. This is different from our approach, where we crowdsourced annotations following our definition of the task. The authors of Reference [12] acknowledge this limitation, which led to high number of false positives in their experiments. For example, an article consisting of a debate transcript with editorial comments will not highlight repeated instances of claims. This creates inconsistent annotation—during a TV debate, popular claims are discussed on repeated occasions. This had to be re-annotated by researchers in Reference [29]. Another caveat is that only three of the nine annotator organisations contributing to those online articles sign up to be neutral and transparent in their selection of claims as verified signatories of the International Factchecking Network's Code of Principles. ClaimBuster [18] provides a definition of a claim that revolves around the question: “Will the general public be interested in knowing whether this sentence is true or false?” Claims are considered to be those sentences for which the answer to this question is yes. Their aim was for anyone to be able to feed in a source, e.g., a political speech, and for the system to produce a list of claims ranked by importance, which could directly feed into the editorial process. This definition of a claim includes the judgement of “importance,” which we avoid in our work. We believe it is an editorial judgement best left to factcheckers. ClaimBuster annotators were journalists, students and professors. Annotations that agreed with the authors of that study were selected to ensure good agreement and shared understanding of the assumptions. Researchers from the ClaimBuster team also defined an annotation schema called PolitiTax, a taxonomy of political claims that we considered. However, the categories were not useful for the downstream task of checking the veracity of the claim by routing it to the right dataset or team at Full Fact, in part, due to the level of granularity in the taxonomy and, in part, because the team at Full Fact is split across topics. There was also a taxonomy defined by factcheckers during the HeroX factchecking challenge [10], which is less granular than PolitiTax. It has four claim types—numerical, political stance, quotes, objects. During this work, we discovered that the latter three categories are rare and intersect with others, so we did not use them in our schema. 3 DATASET 3.1 Our Claim Definition and Process Writing the annotation guidance was a long process. Full Fact's formal definition of a claim during the 2015 UK election, was “an assertion about the world that can be checked.” Media monitoring volunteers were encouraged to ask a factchecker if they had doubts on whether something was assessable. We worked on codifying some of this thinking in conversations with the factcheckers. However, as we captured more and more claims, this definition proved insufficient. We wanted to understand whether a claim could be better defined by breaking it down into sub-categories for better consistency across time, topics and annotators. We opted for defining a typology that would capture the different types of claims, which would be more comprehensive than the previous short definition. Asking annotators to identify the category that a sentence belongs to would encourage more critical thinking. Likewise, choosing the right category for a sentence would significantly reduce the personal bias with respect to judging whether it is a claim, which is bound to personal judgement. Annotators would choose the category pertaining to a sentence, and we would then simplify the schema by mapping those types to binary labels, claim or not a claim. We chose to decouple the importance of the claim from the claim itself. We felt that importance was heavily subjective, reliant on context and best left to factcheckers. Importance is a subtle, and forever changing feature. Even though the most “important” issues in the view of the UK public are often about the economy, immigration and health, their relative positions change. In some cases new issues become important, e.g., in the UK, the importance of claims about the EU increased significantly after the 2016 EU referendum. We also chose to decouple the topic from the definition. By making our definition descriptive of the claim and not, by proxy, the topic, we would have a more consistent final dataset. In some cases the selection of topics is an inherently political choice, e.g., it varies across the population whether “drugs” relate to the topic of “crime” or “health.” This kind of classification was avoided. To come up with the schema that would capture what constitutes a claim, we followed an iterative process. In the first step, factcheckers identified sentences that were definitely not a claim. They iterated on potential rules and found examples that broke them. They also identified some constraints, for example, a claim needs to be checkable with more readily available evidence, which means that a personal claim like “I woke up at 7 a.m. today” is not a claim capable of being checked. We were most concretely able to exclude claims based on an individual's personal experience, as more often than not they were un-checkable. This is similar to ‘verifiable experiential’ statements [30]. We went through several versions of the guidance with different taxonomies. They were trialled within Full Fact, and then two versions with external volunteers. The first version applied the 2015 thinking and was a binary accept/reject classification task, accompanied by a guidance. It listed several types of qualities of claims and non- claims. Claims, for example, may be explicit, implicit, or trivial. Non-claims in this version were formed of personal experience and opinion. We decided against these categories in the end as they sometimes involve explicit judgements from our annotators – these choices can sometimes be highly political. For example, in the case of “The EU is made up of 27 [instead of 28] countries” or “The NHS is there for everyone” some annotators could classify them as trivial while others might consider them explicit legal claims. The implicit/explicit categories were also removed, because whether the claim is implicit or explicit is not important for the next downstream task in the factchecking process after claim detection. For the second version, we looked at Full Fact's factchecks. They mostly covered statistical claims. We also identified claims around current laws or rules of operation and correlation/causation claims, e.g., “there's no clear correlation between prisons’ performance ratings and whether they're publicly-run or contracted out to the private sector.” This became the basis of our claim categories. Merging these categories and removing personal experience was deemed to be a good proxy for claims. There were many other types of claims that we identified, such as definitions, voting records, and expressions of support. We limited our categories to seven to make the task realistic for annotators. We also wanted to minimise the overlap between categories to make the task single- choice. 3.2 Annotation Guidance Our annotation schema is the first to be created with a factchecking organisation. It comprises seven categories, only one of which can be assigned to each sentence. Annotators were given definitions and examples of the seven categories, including the more detailed breakdown shown in Table 1: Table 1. Breakdown of the 4,080 Sentences with Majority Agreement Category Subcategory Counts Example Not a claim 54.8% “Give it all to them, I really don't mind.” Other Other other 10.4%* “Molly gives so much of who she is away throughout the film.” Support/policy 5.5%* “He has advocated for a junk food tax.” Quote 4.7%* “The Brexit secretary said he would guarantee free movement of bankers.” Trivial claim 1.6%* “It was a close call.” Voting record 0.7%* “She just lost a parliamentary vote.” Public opinion 0.4%* “A poll showed that most people who voted Brexit were concerned with immigration.” Definition 0.0%* “The unemployed are only those actively looking for work.” Quantity Current value 9.9% “1 in 4 people wait longer than 6 weeks to see a doctor.” Changing quantity Comparison Ranking Prediction Hypothetical statements 4.4% “The IFS says that school funding will have fallen by 5% by 2019.”Claims about the future Personal experience Uncheckable 3.0% “I can't save for a deposit.” Correlation/causation Correlation 2.6% “Tetanus vaccine causes infertility.” Causation Absence of a link Laws/rules of operation Public institutional procedures 1.9% “The UK allows a single adult to care for fewer children than other European countries.” Rules/rule changes *The proportions for “Other” sub-categories are taken from a random sample of 160 claims labelled as “Other.” 3 4 5 6 • Personal experience. Claims that are not capable of being checked using publicly available information, e.g., “I can't save for a deposit.” • Quantity in the past or present. Current value of something, e.g., “One in four wait longer than 6 weeks to be seen by a doctor.” Changing quantity, e.g., “The Coalition Government has created 1,000 jobs for every day it's been in office.” Comparison, e.g., “Free schools are outperforming state schools.” Ranking, e.g., “The UK's the largest importer from the Eurozone.” Toward Automated Factchecking: Developing an Annotation Schema and Benchmark f... 2 sur 7 19-05-22, 13:11 These categories have proven to broadly cover sentences from political TV shows that Full Fact has encountered over several years. Categories have different levels of occurrence (see Table 1). As previously found [15, 18, 21], “Not a claim” is the most popular category, amounting to about 55% of the annotations. “Other” is the second largest category with 952 instances, 23% of the whole. It can be broken down into claims that are less well-defined, with formal sub-categories being: “Definitions,” “Voting records,” “Public opinion,” “Trivial claim,” “Support,” “Quote,” “Other other.” We amalgamate these, because they are likely to overlap, and we wanted our annotators to only select one option. For example, “She said she voted to keep free school meals.” is both a quote and a voting record. Furthermore, high granularity of categories allows one to perpetually think of rarer categories and a high number of categories slows down annotation unnecessarily. To verify if our level of granularity was correct, we split a sample of 160 sentences in “Other” into sub-categories. The vast majority are in the “Other other” category (see Table 1), supporting our chosen level of granularity. 3.3 Crowdsourced Annotation The annotations were done by 80 volunteers recruited through Full Fact's newsletter—this meant that volunteers were keen on factchecking. 28,100 annotations were collected for a set of 6,304 sentences extracted from subtitles of four UK political TV shows, 14 episodes in total. TV subtitles were chosen, because 69% of the UK population get their news from TV. The software used for collecting annotations was Prodigy, a self-hosted annotation platform. It was customised to support multiple annotators with a login and password screen where each user would enter their credentials. Sentences were shown in random order. The preceding two sentences were also shown on the screen to provide context and assist with potential co-references. Once a sentence was annotated five times by different annotators, it was not shown again. Annotators were encouraged to contact us for any clarifications needed, with thoughtful questions such as: “Where it appears that a claim is dressed up as rhetorical question, should we classify it as a claim? For example, ‘Why should unelected officials in Brussels make rules to stop bananas being sold in bunches of more than 2 or 3?’” To answer this, questions are classified as the claims that they implicitly contain. 3.4 Agreement At the level of all seven granular categories the inter-annotator agreement is moderate, with a Krippendorff's alpha [23] of 0.46. However, we attain higher values of alpha of 0.70 and 0.53 when we do the mapping of annotations into the binary claim/non-claim annotation task, following either of the two methods shown in Table 3. Most of the disagreement was between “Not a claim” and “Other claim.” This showed that it is hard to define the boundary and explicitly list all kinds of claims, as we saw in the process of creating the annotation guidance. The disagreements across all sentence types can be seen in Table 2. Table 2. Annotation Disagreements Qu 12 Corr 10 10 Law 2 19 11 Pred 3 42 27 25 Other 50 102 129 87 87 Not 114 58 90 69 103 668 Pers Qu Corr Law Pred Other The most prominent disagreement is between “Other claim” and “Not a claim.” The labels are shortened versions of those in Figure 1 due to space limitations. (Qu: quantity, Corr: correlation and causation, Pred: predictions, Pers: personal experience.) For mapping the seven categories in the schema into the binary classification task distinguishing claims and non- claims, two different mappings were initially proposed (see Table 3). These two methods were proposed by first assuming the two extremes, i.e., Quantity (2) should be deemed a claim, whereas Not a claim (7) belong to non- claim. Subsequently, these two reformulations of the taxonomy were proposed after meeting and brainstorming with factcheckers. While the method in the first row would be reasonable for achieving higher inter-annotator agreement, it would lead to a classification performance prioritising high precision at the expense of a lower recall. The method in the second row was ultimately selected for further experimentation, as recall was deemed important by factcheckers and it is in turn more realistic for not omitting any of the seven initial categories. Hence, we moved on to evaluate our claim detection system on the labels originating from the second row of Table 3. Here, “Other type of claim” is not in the positive class for two reasons. First, there is a lot of disagreement between it and the “Not a claim” class. Second, the kinds of claim in the “Other” section—voting records, quotes, statements about public opinion polls, are less frequently written about by Full Fact. Table 3. From Seven Categories to Binary Claim vs. “Not a Claim” Classification Claim Non-claim Omitted N 2 3, 4, 6, 7 1, 5 0.70 6,095 2, 3, 4, 5 1, 6, 7 – 0.53 4,777 N = number of sentences annotated by majority. (1) Personal experience, (2) Quantity in the past or present, (3) Correlation or causation, (4) Current laws or rules of operation, (5) Prediction, (6) Other type of claim, (7) Not a claim. The agreement of 60% is still low—that is a lot of sentences to throw away if we were only to consider agreement among all the annotators. So, instead, we choose a majority vote where at least three annotators marked the sentence and more than half of them agree. Of the initial 6,304 sentences, this filter selects 4,777 sentences, 3,973 not claims, and 804 claims. This is in line with previous studies where the proportion of claims is 10–30% in political TV [12, 18]. As extra training data, we add 794 claims from the Full Fact database. Out of them 766 are annotated by us as positive, because they fall into our claim categories, for example “The courts have said that the so-called ‘bedroom tax’ is illegal.” The remaining 28 are in the “Other type of claim” category, for example, “The British economy is not only getting better, it is healing.” If we keep the seven categories in the dataset, instead of mapping them into the two classes, then the same method based on majority votes leads to a slightly smaller dataset with 4,080 sentences, i.e., due to the slightly lower agreement on the broader set of seven categories. 4 METHODS To capture the diversity of sentences observed during political TV shows, we propose to leverage universal sentence representations. We use InferSent [8] as a method to achieve sentence embeddings. These embeddings are different from averaging word embeddings, because they take word order into account using a recurrent neural network. The method provided by InferSent involves words being converted to their common crawl GloVe implementations before being passed through a bidirectional long-short-term memory (BiLSTM) network [19]. The sentence embeddings were pre-trained on a large dataset of Natural Language Inference tasks. Additionally, we also tried concatenating POS and NER information to the embeddings. For each sentence, the POS/NER feature vector was the count of each POS/NER tag in the corpus. We input our sentence representations to a range of supervised classifiers implemented using scikit-learn [32], with the classifiers set to their default parameters. The four classifiers we tested include Logistic Regression, Linear SVM, Gaussian Naïve Bayes and Random Forests, all of which use the default parameters provided with scikit-learn. We use a number of other features as baselines: • Correlation or causation, Correlation, e.g., “GCSEs are a better predictor than AS if a student will get a good degree.” Causation, e.g., “Tetanus vaccine causes infertility.” Absence of a link, e.g., “Grammar schools don't aid social mobility.” • Current laws or rules of operation, Declarative sentences, which generally have the word ”must” or legal terms, e.g., “The UK allows a single adult to care for fewer children than other European countries.” Procedures of public institutions, e.g., “Local decisions about commissioning services are now taken by organisations that are led by clinicians.” Rules and changes, e.g., “EU residents cannot claim Jobseeker's Allowance if they have been in the country for 6 months and have not been able to find work.” • Prediction, Hypothetical claims about the future, e.g., “Indeed, the IFS says that school funding will have fallen by 5% in real terms by 2019 as a result of government policies.” • Other type of claim, Voting records, e.g., “You voted to leave, didn't you?” Public Opinion e.g “Public satisfaction with the NHS in Wales is lower than it is in England.” Support, e.g., “The party promised free childcare” Definitions, e.g., “Illegal killing of people is what's known as murder.” Any other sentence that you think is a claim. • Not a claim, These are sentences that do not fall into any categories and are not claims, e.g., “What do you think?” “Questions to the Prime Minister!” 7 8 9 , 10 α 11 (1) A number of variants of the state-of-the-art claim detection system by ClaimBuster, using different combinations of TF-IDF, POS and NER features, as in Reference [18]. (2) Averaging pre-trained word embedding vectors for all words in a sentence. We evaluate: • Word2vec [28] via the Gensim implementation [34], using the GoogleNews embedding. • GloVe [33] trained on Common Crawl, as well as combining them with dimensionality reduction using principal component analysis (PCA). (3) TF-IDF representations of sentences with logistic regression. Numbers have a significant role in claims—the “Cardinal Number” part-of-speech tag is the second most discriminating feature in Toward Automated Factchecking: Developing an Annotation Schema and Benchmark f.. 3 sur 7 19-05-22, 13:11 For our implementation of the ClaimBuster system, we use the Watson Natural Language Understanding API, the updated version of the Alchemy API used by the original authors. All other features were implemented as outlined in Reference [18]. The ClaimRank [12] model was harder to re-implement. To maintain impartiality Full Fact cannot use the data on sentence speakers when selecting which claims to check. This special care is also encouraged in the computer science research for systems that are integrated into the infrastructure of society by the ACM code of ethics [1]. Additionally, our dataset did not have data on applause, laughing, or speaker crossover. Even though we unfortunately could not use one third of ClaimRank's features, we trained the FNN, SVM and logistic regression classifiers on the remaining features in our dataset [12]. 4.1 Experiment Settings The dataset consists of 5,571 sentences (4,777 from annotations and 794 from the Full Fact database of claims), of which 1,570 are claims and 4,001 are not claims, which gives a 30/70 class imbalance. We use stratified fivefold cross-validation to train and test our models. We use precision, recall and F -score measures to assess classifier performance. We show the best-performing classifier for any given feature set. We also show 95% confidence interval for the precision and recall using binomial distributions. This demonstrates possible overlap in results between different models. The interval is wide for recall due to the small number of positive examples. The next section will present the results of applying these methods. 5 CLAIM DETECTION Here, we present results for the binary classification, using the class grouping shown in the second row of Table 3. 5.1 Analysis of Results Table 4 shows the results of our experiments. Note that in this task recall is especially important as factcheckers do not want to miss out important claims, and hence our priority is to maximise recall while also keeping a good balance of precision and recall as measured by the F1 score. Interestingly, the simple approach of TF-IDF achieves high precision but low recall. We call our new model “CNC,” which stands for “Claim/No Claim.” It achieves a better balance of precision and recall; logistic regression classifier gives the highest overall F1 score of 0.83, outperforming all other techniques. In the interest of space and clarity, we do not include the results of the other classifiers (i.e., Linear SVM, Gaussian Naïve Bayes, and Random Forests). The use of POS and NER features in our model has no effect on the performance. GloVe embeddings achieve performance close to our method with F1 scores 2% lower and substantially lower recall scores. Despite the overlap in precision scores between GloVe and our method, the overlap is minimal in terms of recall. Our CNC model also clearly outperforms the state-of-the-art method by ClaimBuster at 0.79 F1; hence our method yields F1 scores that improve ClaimBuster by over 5% in relative terms. ClaimBuster performs similarly to CNC in terms of precision, albeit with substantially lower recall scores. ClaimRank has the best precision scores across the board, but with the lower recall scores. CNC achieves a 6% relative improvement in F1-score over ClaimRank. Table 4. Results for the Claim/No Claim Experiments Features Classifier P R F P-interval R-interval TF-IDF LogReg 0.90 0.59 0.70 0.89–0.91 0.56–0.61 TF-IDF+num. preproc. LogReg 0.91 0.59 0.70 0.90–0.92 0.56–0.61 Word2Vec SVM 0.85 0.75 0.78 0.84–0.86 0.73–0.77 GloVe LogReg 0.89 0.76 0.81 0.88–0.90 0.74–0.78 GloVe+PCA LogReg 0.89 0.75 0.81 0.88–0.90 0.73–0.77 CB LogReg 0.90 0.59 0.70 0.89–0.91 0.56–0.61 CB +POS LogReg 0.88 0.68 0.76 0.86–0.89 0.66–0.71 CB +NER LogReg 0.88 0.60 0.71 0.87–0.89 0.58–0.63 CB +POS+NER LogReg 0.87 0.71 0.78 0.86–0.88 0.68–0.73 CB SVM 0.84 0.70 0.76 0.83–0.85 0.69–0.73 CB +POS SVM 0.86 0.74 0.79 0.85–0.87 0.72–0.76 CB +NER SVM 0.84 0.71 0.77 0.83–0.85 0.69–0.73 CB +POS+NER SVM 0.86 0.75 0.79 0.85–0.87 0.73–0.77 ClaimRank LogReg 0.93 0.65 0.77 0.92–0.94 0.63–0.67 ClaimRank SVM 0.93 0.53 0.67 0.92–0.94 0.51–0.55 ClaimRank FNN 0.89 0.61 0.72 0.87–0.91 0.58–0.","Konstantinovskiy, L., Price, O., Babakar, M., & Zubiaga, A. (2021). Toward automated factchecking: Developing an annotation schema and benchmark for consistent automated claim detection. Digital threats: research and practice, 2(2), 1-16."
SCOP_004,A Prototype Web Application to Support Human-Centered Audiovisual Content Authentication and Crowdsourcing,"Media authentication relies on the detection of inconsistencies that may indicate malicious editing in audio and video files. Traditionally, authentication processes are performed by forensics professionals using dedicated tools. There is rich research on the automation of this procedure, but the results do not yet guarantee the feasibility of providing automated tools. In the current approach, a computer-supported toolbox is presented, providing online functionality for assisting technically inexperienced users (journalists or the public) to investigate visually the consistency of audio streams. Several algorithms based on previous research have been incorporated on the backend of the proposed system, including a novel CNN model that performs a Signal-to-Reverberation-Ratio (SRR) estimation with a mean square error of 2.9%. The user can access the web application online through a web browser. After providing an audio/video file or a YouTube link, the application returns as output a set of interactive visualizations that can allow the user to investigate the authenticity of the file. The visualizations are generated based on the outcomes of Digital Signal Processing and machine learning models. The files are stored in a database, along with their analysis results and annotation. Following a crowdsourcing methodology, users are allowed to contribute by annotating files from the dataset concerning their authenticity. The evaluation version of the web application is publicly available online.",Computer Science,Article,2022,Y,Y,0,"News authentication is considered a vital task for reliable informational services. The COVID-19 pandemic situation that we currently experience showcased the importance of fact-checking in fighting disinformation to protect our societies and democracies. The role of audiovisual recording is considered crucial in documenting news articles, thus convincing audiences about the truth of the underlying events [1– 3]. With the advancement of Information and Communication Technologies and the availability of easy-to-use editing and processing tools, one unwanted side-effect is the falsification of multimedia assets (i.e., images, audio, video) to alter the presented stories, making them more appealing (or intentionally doctored). In this context, unimodal solutions have been implemented to inspect each of the individual media entities, while multimodal forensic services are also deployed through online collaborative environments, plug-ins, serious games, and gamification components [1,2,4,5]. While the detection of manipulated photos/images and the evaluation of the associ- ated forgery attacks remain critical [ 6], audio and video content have become even more popular nowadays. In this context, audio offers some unique features, such as less demand- ing processing needs and the inherent time continuity, making tampering inconsistencies easier to reveal [ 7 ,8]. Semantic processing and machine learning technologies empowerFuture Internet 2022, 14, 75 2 of 17 today’s digital forensics tools. However, these new capabilities can also be exploited for counter-/anti-forensic means, requiring constant and continuous effort. 1.1. Related Work Content verification has always been a very important part of journalistic workflows and a crucial factor of journalistic ethics and deontology. In the context of Journalism 3.0, where new business models of low or no pay journalism, combined with news aggregation and republishing and the reuse of amateur user-generated content (UGC) [9], disinforma- tion has become a major problem for journalistic practice. As a result, several fact-checking organizations have appeared in the past decade, intending to find and debunk false claims that are spread throughout the Web and social media services [ 10]. Recent research of academics and organizations has been directed towards highlighting the best practices for content verification, through international cooperation networks [11]. In the modern media ecosystem, data variety is a very important parameter of big data volumes [12 ]. This means that fact-checkers need to manage content in many different modalities (e.g., text, audio, image, video). Different approaches and methodologies have to be defined for each case [ 13 ]. In disinformation, media assets may be used in a misleading context to support a false claim, or may be manipulated themselves. In the first case, an image/audio/video file is followed by an untrue description or conclusion, while in the latter, the media file has been maliciously edited. Such manipulations may include actions, such as copying and moving parts of the file to a different place and splicing in segments of a different file, aiming at affecting the semantic meaning of the file [14 ]. Common cases can be found in all file types, whether image, audio, or video. In the case of image tampering detection, spatial techniques can be used to locate suspicious regions and discontinuities within an image file. Media Verification Assistant is a project that allows users to upload images and applies several algorithms to provide forensics analysis [14 ,15 ]. In contrast to static images, audio and video files introduce the dimension of time. In video files, besides the spatial analysis of single image frames, the detection of temporal discontinuities can be crucial for the spatiotemporal location of malicious tampering [ 16 ]. Such techniques are expected to be computationally heavy. Audio is a very important modality present in the majority of video files. In this sense, audio can be used autonomously for the authentication of both audio and video assets. Audio information retrieval techniques are much less computationally complex. Audio forensics tools are not, however, as well-explored as those applied to visual information. Two important toolboxes on the market are the ARGO-FAAS [ 17] and the EdiTracker plugin [ 1]. They are, however, paid services, and not publicly available. Audio forensics techniques address the processes of audio enhancement, restoration, and authentication of an audio asset so that it can be considered as evidence in court [ 18, 19 ]. Authentication techniques aim at detecting artifacts within an audio file that can indicate malicious editing. Traceable edits can be found in the file container information or in the audio content [ 20 ,21]. Techniques that inspect file container inconsistencies investigate the metadata, descriptors, or the encoding structure. When the audio content is investigated, the aim is to use dedicated software to detect certain artifacts that may be inaudible by human subjects. Several different approaches can be found in the literature. Electronic Network Frequency (ENF) techniques make use of the phenomenon of the unintentional recording of an ENF through interference. Electronic networks provide alternating current with a nominal frequency of 50 or 60 Hz, depending on the region. However, the real frequency of the current fluctuates around this value. The electronic equipment that is used for recordings captures this frequency fluctuation, which can act as a timestamp of the recording. It is possible to isolate and track the ENF in recordings to check whether there is phase inconsistency in the fluctuation, or even to find the exact time of the recording from the log files of the electronic networks [22–25]. Other approaches investigate the acoustic environment of the recording, such as the Signal-to-Reverberation ratio of a room [ 26 , 27]. The specifications of a recording device Future Internet 2022, 14, 75 3 of 17 have also been proven to be traceable in research, providing an indicator of whether parts of an audio file were recorded with a different device [ 20 , 28– 30]. Dynamic Acoustic Envi- ronment Identification (AEI) may rely on statistical techniques that rely on reverberation and background noise variance in a recording [ 31 ]. Machine learning techniques are proven to be very useful for acoustic environment identification. Machine learning models do not rely on the definition of a set of rules for decision-making, but require a dataset of pre-annotated samples to train a classification model that can identify different classes, in this case, acoustic environments [31–33]. Another methodology for audio tampering detection investigates file encoding and compression characteristics. A huge number of highly configurable audio encodings are available that differ in terms of compression ratio, bitrate, use of low-pass filters, and more. A file that comes from audio splicing is very likely to contain segments encoded with different configurations, which can be traceable [ 34– 36 ]. Most encoding schemes depend on psychoacoustic models that apply algorithms to discard redundant, inaudible frequencies. The Modified Discrete Cosine Transform (MDCT) coefficients can be investigated using statistical or machine learning methods to detect outliers in specific segments of the file [37 ]. Even when the file is reencoded in another format, there are often traces of the effect of previous compression algorithms [38–41]. Media authentication can be supported during content production using container and watermarking techniques, such as hash-code generation and encryption, and MAC times- tamp embedding. Recovery of the inserted hash code that was generated by algorithms, such as SHA-512, enables the detection of tampered points within an audio stream [42 ]. Similarly, embedding timestamp information in files can allow the identification of an audio excerpt with a different MAC timestamp that has been maliciously inserted [20]. Whether the aim is training machine learning models or evaluating proposed analysis methods, one crucial part of every audio authentication project is the formation of a dataset. This is a very complex procedure due to the task’s peculiarities, and it often acts as a bottleneck for the robustness of such techniques. Not many datasets are available for experimentation. In [43 ], a dataset was recorded featuring different speakers, acoustic rooms, and recording devices. In [44 ] a dataset with different encodings was created through an automated process. In [ 45 ], existing recordings were edited to create a dataset. In [7], an automated process was proposed for the creation of a multi-purpose dataset using an initial set of source files provided by the user. 1.2. Project Motivation and Research Objectives It has been made clear that machine learning solutions for audio tampering detection require a dataset for the training of models. Since datasets with real cases of tampered files are not available, most works require the formulation of artificial datasets for model evalua- tion. Such datasets are often difficult to handcraft, so they follow automated procedures for dataset creation, simulating real-world scenarios. As a result, the implemented models are case- and dataset-specific. There is no evidence for the generalization of the models in mul- tiple scenarios and tampering techniques. For this reason, it is not yet feasible to integrate automated audio authentication into professional workflows without supervision, as they cannot be considered reliable for production and real-world applications. Furthermore, models that are pre-trained in known datasets and conditions may be more vulnerable to adversarial attacks [46]. On the other hand, traditional audio forensics techniques require expertise and fluency with audio analysis tools. In such an approach, human intelligence and experience play a crucial role in the process of authentication. While this is the most reliable solution and the preferable option in courtrooms, it cannot provide a viable alternative with massive appeal. There is an urgent need for tools that can help in the fight against disinformation. Such tools should be accessible to a broad audience of journalists, content creators, and simple users, to improve the overall quality of news reporting. Average users do not have the expertise to apply audio analysis techniques in the same way as professionals of audio forensics. Future Internet 2022, 14, 75 4 of 17 The motivation for the current research emerges from the hypothesis that it is feasible to strengthen a user’s ability to recognize tampered multimedia content using a toolbox of supervisory tools provided online through an easy-to-use interface. State-of-the-art approaches for audio analysis and tampering detection were integrated into a web ap- plication. The application is available publicly through a web browser. The results of the algorithms do not provide an automated decision-making scheme, but rather a set of visualizations that can assist the user in a semi-automated approach. This means that the framework does not include a model that performs binary classification of files as tampered, or not-tampered, but the final decision is the responsibility of the user, taking advantage of their perception and experience as well as the context of the media asset. Through the use of the application, crowdsourcing is promoted for the creation of a dataset with real-world tampered files for future use. The remaining of the paper is structured as follows. In Section 2, the proposed web framework is presented, in terms of the functionality, aims, and technical specifications. The integrated algorithms and their operating principles are listed without emphasizing technical details. In Section 3, the evaluation results from the reverberation estimation models and the initial implementation of the prototype web application are presented. In Section 4, the research results are summarized and discussed, and the future research goals of the project are defined. In Section 5, some of the limitations of the presented research are analyzed. 2. Materials and Methods As stated in the problem definition section, the proposed approach consists of a frame- work for the assistance of professional journalists and the public in detecting tampered audiovisual content. The core of the framework is a web application with a graphic user interface provided to the public for the submission and analysis of content. The application incorporates an ensemble of algorithms that provide the user with supervisory tools for semi-automatic decision-making. The analysis strategy is audio-driven, as it makes use of the audio channel. The integrated algorithms do not classify files as tampered or not, but rather support the users in decision-making. The application offers the necessary crowdsourcing functionality for dataset creation and user cooperation. The framework was designed and implemented as a component of the Media Authentication Education (MAthE) project, which aims at providing educational and gamification tools to battle misinformation [4]. 2.1. A Web Application for Audio Tampering Detection and Crowdsourcing The main goal of the web application is to combine the effectiveness of state-of-the-art signal processing, machine learning advances and human perception for computer-assisted audio authentication. The application: 1. Implements state-of-the-art analysis options. An ensemble of algorithms is incorpo- rated, addressing multiple audio tampering strategies. Such strategies may include encoding detection, recording conditions, background noise clustering, and others. 2. Follows a modular approach. The algorithms that are provided in the initial imple- mentation are available as individual modules. This allows the existing algorithms to be upgraded in the future, as well as the extension of the initially provided toolbox. 3. Supports human-centered decision-making. As was explained, it is within the ra- tionale of the MaThe solutions to promote computer-assisted decision making. The algorithmic implementations provide intuitive visualizations aiming at assisting the user in content authentication, taking also into consideration the user’s personal experience and perception, as well as the context of the asset under investigation. 4. Is publicly available. As was explained, the web framework aims to address a wide public. An important prerequisite for this is that it is freely available for anyone to use and contribute. Future Internet 2022, 14, 75 5 of 17 5. Requires no audio or technical expertise. The design principles prioritize ease-of-use, following a typical workflow. A more experienced user with a technical and signal processing background, may get better insight and understanding of the produced visualizations. However, the detection of outliers or suspicious points in a file time- line is self-explanatory and does not require a deep understanding of the algorithms and mechanisms. 6. Promotes crowdsourcing. Users and teams can become involved and contribute to the project in several ways to further advance the field of audio tampering detection. They can submit files, annotated as tampered or not tampered, with a brief justifica- tion. Users can also randomly browse files from the dataset, analyze them, and mark them as tampered or not tampered. Finally, as this is an open-source project follow- ing a modular architecture, researchers and teams are encouraged to contribute with code and extensions. The main functionality of the MAthE AudioVisual Authentication framework is shown in Figure 1. Users can submit files for analysis and investigation, or contribute by annotating existing files concerning their authenticity. Once a file is submitted, the appli- cation returns analysis results, and the user can decide if they want to submit the file to the database along with an annotation (tampered or not tampered), submit the file to the database without annotation, or not submit anything to the database. Contributing users can access submitted files, annotated or not, examine the analysis results, and provide annotation (tampered or not tampered), following a crowdsourcing methodology. Figure 1. The MAthE AudioVisual Authentication framework and functionality. 2.2. The Computer-Supported Human-Centered Approach The main concept of our approach depends on the idea that actors with no expertise in signal processing, machine learning, and computational methods can benefit from the visualization output of such techniques with little or no training. Computational methods in media authentication usually try to detectanomalies within the file under investigation. Such anomalies can be visually depicted (e.g., with a change in color). A non-expert user can perceive such depictions and interpret them accordingly, even without understanding or knowledge of the technical details that led to this visualization. After locating the sus- picious points within the file, the user can base their reaction based on contextual infor- mation and their own critical thought. For example, an object within an image that looks Figure 1. The MAthE AudioVisual Authentication framework and functionality. 2.2. The Computer-Supported Human-Centered Approach The main concept of our approach depends on the idea that actors with no expertise in signal processing, machine learning, and computational methods can benefit from the visualization output of such techniques with little or no training. Computational methods in media authentication usually try to detect anomalies within the file under investigation. Such anomalies can be visually depicted (e.g., with a change in color). A non-expert user can perceive such depictions and interpret them accordingly, even without understanding or knowledge of the technical details that led to this visualization. After locating the suspicious points within the file, the user can base their reaction based on contextual information and their own critical thought. For example, an object within an image that looks like an anomaly in the visualizations and also dramatically alters the Future Internet 2022, 14, 75 6 of 17 image’s semantic meaning probably indicates tampering. This is the main idea of the ReVeal project [ 14 ], which deals with tampered images and was a major inspiration of the present work. There is evidence from experiments that users with no technical knowledge were able to detect tampering of images with the support of such visualizations [ 6]. In this approach, a gamification approach was also tested that allowed users to ask for the help of such a visualization toolbox [5] in order to detect fake news and proceed in the game [ 4]. While such techniques are not robust in the automated detection of media content tampering, they can push the limits of human intellect and support users to make better decisions on fake content recognition. In this direction, in the present work, several visualizations based on anomalies that are detected by audio processing are proposed. Since the audio channel is commonly part of video files, this toolbox aims at supporting users with no technical expertise to make decisions on the authenticity of audio and video files. 2.3. An Ensemble of Methods for Audio Tampering Detection In the related work section, several approaches for tampering detection are presented, which may fall into specific categories. Such categories include relevant audible or inaudible artifacts that are produced during the malicious editing of audiovisual files. As a result, depending on the type of forgery and the technical flaws of such an action, one technique may be more or less suitable. Hence, the motivation of the project derives from the hypothesis that it is irrelevant to try to evaluate different approaches to choose the most efficient, since this cannot be applied universally to every case [8]. The MAthE AudioVisual Authentication approach proposes a superposition of meth- ods in a modular architecture that includes a dynamic group of algorithmic elements. Such techniques are either outcomes of previous research work within the project [7 ,8 , 47] or were found in the literature. The modular architecture allows for the modification of existing functions in the future, as well as its extension with new modules that come from new research, literature review, or contribution within the academic society. Another hypothesis that has played an important role in the MAthE architecture design is that the lack of real-world datasets, as well as the diversity of the characteristics of tampered files, sets a bottleneck to the maturity of automated decision-making schemes. Most models are trained with artificially created datasets that address a specific type of tampering (recording device, room acoustics, encoding, etc.). Moreover, disinformation is only relevant at certain time points of a file, where the editing alters the semantic meaning of the recording. This is something that a human subject may easily understand. For this reason, the proposed design incorporates signal processing tools and machine learning models in a semi-automated approach [ 48 ]. It is not within the project’s expectations to provide automated massive authentication of archive files, but rather to assist humans in analyzing and authenticating a specific file under investigation (FUI). The outcomes of the system require a human-in-the-loop [ 49 ] strategy. This is considered an effective combination of machine processing capabilities and human intelligence. The initial toolbox of signal processing algorithms that was included in the prototype version of the MAthE AudioVisual Authentication application is presented below. It is noted that the technical presentation and validation of every approach is not within the scope of the current paper. Instead, a short description of the main functional principles of every category of techniques is given, along with references to publications with the technical details of different algorithms. The toolbox is dynamic, and it will be supported by incorporating state-of-the-art feature-based [ 50, 51] and deep [52] (machine) learning approaches for audiovisual semantic analysis. It can also be deployed as a mobile applica- tion [ 53 ]. It is expected to further grow and evolve through the use of the application and the continuous dissemination of the MAthE project. Future Internet 2022, 14, 75 7 of 17 2.3.1. Common Audio Representations This family of tools includes typical audio representations, such as waveform, energy values, and spectrograms. Such tools are available in most typical audio editing applica- tions. Sound waveforms are the depiction of the amplitude of sound pressure of every audio sample, expressing the variation in the audio signal in time. For an audio signal with a common sampling frequency of 44,100 samples per second, waveforms may include a huge number of samples to be depicted, which can be computationally heavy to represent in an interactive graph running on a web browser. For this reason, in the proposed toolbox, time integration is performed, showing the root mean square (RMS) value for successive time windows as a bar diagram. This is used as a tradeoff to avoid the excess information redundancy of the waveform. Mel scale spectrograms provide a spatiotemporal represen- tation of audio signals, depicting the evolution of the spectral characteristics through a time interval [ 54]. Spectral information is given for specific frequency bands that apply to the Mel scale, which is inspired by the psychoacoustic characteristics of human auditory perception. They are included in the toolbox because they can be useful, and they enhance the MAthE framework’s all-in-one solution so that users do not have to make use of more than one piece of software for analysis and decision-making. 2.3.2. Different Encoding Recognition This family of techniques investigates the existence of small audio segments in the FUI that have different compression levels or encoding characteristics. This indicates that they may be segments of another file that were inserted in the original file. One common naïve approach that can be very effective in some cases is the calculation of the bandwidth, because most compression algorithms apply low-pass filtering to eliminate the higher frequencies that are of minor importance to the human auditory perception. Feature vectors are descriptors of several attributes of a signal. Different encoding and compression levels, even if they are often proven to be inaudible in listening tests with human subjects, can affect the features that describe an audio signal. In the Double Compression technique for audio tampering detection that was proposed in [8], the FUI was heavily compressed. Features are extracted from the FUI and the compressed signal. For every time frame, the feature vector difference is calculated between the two signals. Parts of the FUI that have different encoding are expected to have different feature vector distances. Moreover, the gradient of differences is calculated. This measure is expected to reach peak values when there is an alteration in the compression levels, indicating suspicious points. The double compression algorithm is summarized as follows [8]: 1. Heavy compression to the audio file under investigation (FUI), thus creating a double- compressed file (DCF). 2. A feature vector is extracted the FUI and the DCF, creating the (T × F) matrices Fi (t), where i = 1, 2, T is the number of time frames and F is the length of the feature vector. 3. For every time frame, the Euclidean distance D(t) of the two matrices is calculated 4. D’(t) = D(t) − D(t − 1) is calculated to show the differentiation between successive time frames. 5. D’(t) is expected to present local extrema in time frames that include a transition be- tween audio segments of different compression, indicating possible tampering points. For the feature selection, an audio feature vector was evaluated in [ 7 ]. Using a dedi- cated dataset creation script, a set of audio files were created, containing audio segments of different compression formats and bitrates. Specifically, segments of mp3-compressed audio in different bitrates were inserted randomly within an uncompressed file contain- ing speech. Subjective evaluation experiments with three experts in the field of media production indicated that human listeners failed completely to detect the inserted seg- ments for mp3 bitrates above 96 kbps, while they recognized approximately 10% of the inserted segments for mp3s of 64 kbps [ 7]. The dataset that was created in [ 7 ], along with the script for customized dataset generation, are documented and provided pub- Future Internet 2022, 14, 75 8 of 17 licly at http://m3c.web.auth.gr/research/datasets/audio-tampering-dataset/ (accessed on 26 January 2022). The selected feature set includes several frequency domain attributes, namely spectral brightness, with predefined threshold frequencies 500 Hz, 1000 Hz, 1500 Hz, 2000 Hz, 3000 Hz, 4000 Hz, and 8000 Hz, as well as rolloff frequencies, which are the upper boundary frequencies that contain energy ratios of 0.3, 0.5, 0.7, or 0.9 to the total signal energy, and spectral statistics (Spectral Centroid, Spread, Skewness, Kurtosis, Spectral Flatness, 13 Mel Frequency Cepstral Coefficients, Zero Crossing Rate, and RMS energy). The technical details of the aforementioned feature vectors are outside the scope of the current paper, but the methodology and feature evaluation process are presented thoroughly in [7]. 2.3.3. Reverberation Level Estimation Another indicator that several segments of a FUI may have been inserted from a different file is the effect of acoustic conditions on the recording. Every space has different reverberation levels that affect the recording. Especially since most newsworthy events are not recorded in ideal conditions of professional recording studios, a regression model based on a Convolutional Neural Network architecture [ 47 ] was trained using a big dataset of sim- ulated reverberation to provide a numerical estimations of the Signal-to-Reverberation ratio for every audio segment. Segments with outlier values are possibly related to malicious audio splicing. Convolutional Neural Networks (CNNs) are a type of deep learning architecture that have gained popularity in audio recognition and event detection tasks [55 ,56]. One main reason for their recent widespread is is that there is no need for a handcrafted feature vector. Instead, a visual representation of the audio information is fed to the networks as an image, and the input layers extract hierarchical features in an unsupervised manner during training. Different kinds of input have been evaluated for deep learning techniques, with spectrograms being the dominant approach [54]. Signal-to-Reverberation-Ratio (SRR) can be a useful attribute that can indicate audio slicing. SRR expresses the ratio of the energy of the direct acoustic field to the reverberation acoustic field. It is determined by the acoustic characteristics of the space of the recording, the positioning of the sound source and the recording device. The distance where the levels of the direct and the reverberation sound are equal (SRR = 1) is called the critical distance. At distances closer than the critical distance, we can assume SRR > 1, and at distances that are farther than the critical distance, SRR < 1. The critical distance itself depends on the room acoustic attributes. For recordings that take place under different conditions, the SRR is expected to differ. When segments from different recordings are pieced together, it is possible to detect the inconsistency in their SRR, even if it is not audible by human listeners. Calculating the SRR for different time windows can provide another criterion for audio tampering detection. In the proposed approach, a deep learning regression model is used for a data-driven estimation of the SRR, based on simulation data. A 3600-second-long audio file containing pink noise was created, using the Adobe Audition generator. Using the same software, reverberation was added to the file with different SRRs. Ten different SRRs were chosen, resulting in 11 audio files (including the original), producing a 39600-second-long dataset. The same source audio file was used for all SRRs, so that the model is trained to recognize the reverberation and not information related to the content of different audio streams. The selected SRRs are shown in Table 1. Table 1. The different Signal-to-Reverberation Ratios that were used for the model training. Signal (%) 100 90 80 70 60 50 40 30 20 10 0 Reverberation (%) 0 10 20 30 40 50 60 70 80 90 100 Future Internet 2022, 14, 75 9 of 17 The dataset was used for the training of a CNN regression model. The output of the model is a continuous value from 0 (no reverberation) to 1 (only reverberation). The model architecture is provided in Table 2. Table 2. The architecture and hyper-parameters of the CNN model for reverberation level estimation. Layer Type Configuration 1 Convolutional 2D Layer 16 filters Kernel size = (3,3) Strides = (1,1) 2 Max Pooling 2D Layer Pool size = (2,2) 3 Dropout Rate = 0.25 4 Convolutional 2D Layer 32 filters Kernel size = (3,3) Strides = (1,1) 5 Max Pooling 2D Layer Pool size = (2,2) 6 Dropout Rate = 0.25 7 Convolutional 2D Layer 64 filters Kernel size = (3,3) Strides = (1,1) 8 Dropout Rate = 0.25 9 Convolutional 2D Layer 128 filters Kernel size = (3,3) Strides = (1,1) 10 Convolutional 2D Layer 256 filters Kernel size = (3,3) Strides = (1,1) 11 Flatten Layer 12 Dense Neural Network Output weights = 64 Activation = ReLU L2 regularizer 13 Dense Neural Network Output weights = 64 Activation = ReLU 14 Dropout Rate = 0.25 15 Dense Neural Network Output weights = 24 Activation = Linear 2.3.4. Silent Period Clustering Besides room acoustics, the background noise also characterizes a recording. The environmental noise that is recorded in speech recordings is often inaudible, since it is mixed with a speech signal of a much higher level. However, in the small periods of silence that occur between words and syllables, the background noise signal is dominant. In the case of combining two or more recordings to create a tampered audio file, different background noise patterns may be distinguishable. Initial investigation has shown that by exporting a feature vector from small segments of silence (~25 ms) and p","Vryzas, N., Katsaounidou, A., Vrysis, L., Kotsakis, R., & Dimoulas, C. (2022). A Prototype Web Application to Support Human-Centered Audiovisual Content Authentication and Crowdsourcing. Future Internet, 14(3), 75."
SCOP_049,Towards Understanding and Supporting Journalistic Practices Using Semi-Automated News Discovery Tools,"Journalists are routinely challenged with monitoring vast information environments in order to identify what is newsworthy and of interest to report to a wider audience. In a process referred to as computational news discovery, alerts and leads based on data-driven algorithmic analysis can orient journalists’ attention to events, documents, or anomalous patterns in data that are more likely to be newsworthy. In this paper we prototype one such news discovery tool, Algorithm Tips, which we designed to help journalists find newsworthy leads about algorithmic decision-making systems used across all levels of U.S. government. The tool incorporates algorithmic, crowdsourced, and expert evaluations into an integrated interface designed to support users in making editorial decisions about which news leads to pursue. We then present an evaluation of our prototype based on an extended deployment with eight professional journalists. Our findings offer insights into journalistic practices that are enabled and transformed by such news discovery tools, and suggest opportunities for improving computational news discovery tool designs to better support those practices.",Computer Science,Proceeding,2021,Y,Y,0,"A lot happens in the world every day—much of it is insignificant, but some of it warrants broader public attention. Journalism, and the news media more broadly, serves to help identify those things happening in the world that may be deserving of wider exposure, scrutiny, or debate in society [ 74]. This gatekeeping function—of influencing what gets published and publicized as part of the journalistic communication process—is a complex one driven by an array of forces both within and beyond journalism, and at various individual, organizational, social, and technical levels [ 78]. At the technical level, recent models of gatekeeping have begun to consider algorithmic influences in the context of broader sociotechnical gatekeeping practices [95 ], including at the information gathering stage of news production [ 25]. Conceived of as a way to potentially increase the efficiency and scale at which new news stories can be identified [ 42], such algorithmically informed approaches Authors’ addresses: Nicholas Diakopoulos, nad@northwestern.edu, School of Communication, Northwestern University, Evanston, Illinois; Daniel Trielli, dtrielli@u.northwestern.edu, School of Communication, Northwestern University, Evanston, Illinois; Grace Lee, gracelee@u.northwestern.edu, Medill School of Journalism, Northwestern University, Evanston, Illinois. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM. 2573-0142/2021/10-ART406 $15.00 Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021.406:2 Nicholas Diakopoulos, Daniel Trielli, and Grace Lee can help to orient journalists’ attention to events, documents, or anomalous patterns in data that are more likely to be newsworthy [ 15, 23 ], a process recently referred to as computational news discovery [25, 86]. This work focuses on a particular context of computational news discovery related to the identi- fication of algorithmic decision-making (ADM) systems in government. This is meant to support algorithmic accountability reporting, an emerging beat in journalism focused on investigating and scrutinizing algorithmic decision making across various domains of society [ 22]. For example, outlets such as ProPublica, Der Spiegel, and The Markup have spearheaded investigative journalism projects seeking to expose and hold accountable power wielded through ADMs.1 A few efforts have begun to catalogue a range of government use-cases for ADMs in the U.S., from services adminis- tration to regulatory enforcement [ 18], with one study finding that nearly half of the 142 surveyed agencies had used various AI and machine learning tools [31]. While some government initiatives have begun to develop responsible approaches to ADM deployment or even create registries of algorithms in use, such approaches are not widespread [35, 92]. The need for scrutiny of these systems is evident [32, 33] as is the need for additional research on the responsible design, develop- ment, use, and evaluation of ADMs deployed throughout the public sector [16, 36, 49, 61 , 73, 93]. In this work we focus on developing and evaluating a method and tool to help comprehensively monitor and track ADM usage across all levels of government to support journalism. An important underlying motivation for the current work is to find ways to reduce the effort needed to engage in the scrutiny of government algorithms. This research therefore undertakes the design and evaluation of a semi-automated computational news discovery tool called Algorithm Tips to enable such work. Algorithm Tips systematically and periodically monitors government websites (across U.S. government at all scales) for documents that may reveal new applications of ADMs. Through a series of automated evaluations, internal expert evaluations, and crowdsourced evaluations Algorithm Tips augments documents to produce news leads. These leads are presented in an online interface and sent to external professional journalists who ultimately decide whether to pursue the additional reporting needed to transform a lead into a publishable news story. We evaluated Algorithm Tips in an extended deployment with eight professional journalists who have experience reporting on algorithms in society for some of the largest most well-established news organizations in the U.S. Our findings articulate ways in which the tool meets the needs and practices of domain experts, and offers implications for the design and further research of computational news discovery tools more broadly. This research offers a couple of contributions, including (1) the design and development of a computational news discovery tool (Algorithm Tips) with design goals tailored to enable journalistic decision making around which leads to pursue in the domain of algorithmic decision making in government; and (2) an evaluation of that tool with eight professional journalists in an eight-week- long deployment which offers ecologically valid insights into how journalistic practices are enabled and transformed by such a news discovery tool. In particular we describe design goals related to supporting attention management, as well as verification and newsworthiness professional evaluations, and we tailor the information process and interface of Algorithm Tips to support those goals in the context of investigative and enterprise reporting. Our findings elaborate on how journalists made context-specific interestingness decisions using the evaluative information made available via automation, crowds, and experts; saw the tool as able to offer both a jumping-off 1See: Machine Bias: There’s Software Used Across the Country to Predict Future Criminals. And it’s Biased Against Blacks. ProPublica, May 2016. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal- sentencing; Blackbox Schufa. Der Spiegel. November, 2018. https://www.spiegel.de/wirtschaft/service/schufa-so- funktioniert-deutschlands-einflussreichste-auskunftei-a-1239214.html; and Swinging the Vote? The Markup. February, 2020. https://themarkup.org/google-the-giant/2020/02/26/wheres-my-email Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021. Towards Understanding and Supporting Journalistic Practices Using Semi-Automated News Discovery Tools 406:3 point for specific items and to provide background on the beat of government algorithms; and strove to manage their own effort and attention with respect to the tool. Overall our work provides insight into how to more effectively design and build computational tools to support the essential journalistic activities of news discovery and news gathering. 2 BACKGROUND In designing and developing Algorithm Tips we were informed by related work in (1) the domain of Computational Journalism as it relates to Human-Computer Interaction (HCI), including various tools that have addressed the topic of news discovery in different contexts; and (2) the topic of newsworthiness and the measurement of news values as it may support journalistic evaluations of potentially newsworthy information. 2.1 Computational Journalism Studies of journalism within the context of HCI [3 ] address a broad gamut of news-related activities, from information gathering [26, 29], production [56 , 64, 81], and sensemaking [14, 28], to distribution [ 80, 89] and audience consumption behavior [8, 34 , 63]. Oftentimes such research is oriented towards designing and developing new tools to augment journalists’ capabilities, or towards seeking to more broadly understand how journalists use computational tools in their sociotechnical practices [ 47, 88]. The current work contributes to this growing body of HCI literature on computational journalism, which itself is broadly oriented towards enabling information and knowledge production using algorithmic approaches that embrace journalistic values and advance journalistic practices [19, 23]. In particular we focus on the goal of supporting news discovery and gathering tasks [25, 70], which prior research has documented as challenging and time-consuming for journalists [47, 81]. In designing Algorithm Tips we were informed by prior work on computational news discov- ery tools. Early ideations about the role of computing in journalism suggested that algorithmic techniques might offer an information subsidy to the discovery and gathering phases of news production by allowing journalists to scan the environment for newsworthy events and happenings at increased scale, speed, and overall efficiency [ 42, 43]. Recently this has been framed as compu- tational news discovery (CND), defined as “the use of algorithms to orient editorial attention to potentially newsworthy events or information prior to publication” [25 ]. The premise for CND is supported by work in journalism studies showing that (non-computational) sources and infor- mation subsidies that are frequently used and routinely relied on do tend to save reporters’ time during news discovery [71]. Some of the first CND tools were developed in the context of social media monitoring to help detect newsworthy events, identify witnesses, or make sense of how people were responding to news events [ 26, 28, 76]. Given the scale and quality of social media data confronting journalists many of these tools support not only discovery tasks [ 30 ], but also curation and validation tasks [ 54, 62, 88, 100 ] which emphasize the importance of assessing and verifying sources and content [ 12]. CND tools have also been built to support a variety of use cases beyond social media listening, including the monitoring of numeric data streams or sets (e.g. crime, real estate, education) for trends, outliers, or anomalies [ 15, 55, 77], the monitoring of local court or business documents to surface newsworthy people, places, or companies [ 67], identification of local public meeting events that journalists may want to attend [97], and the ranking of claims in the media that may be worthy of fact checking [40, 46]. In this work we draw on this prior work to help elaborate design goals and inform key features for the design of the Algorithm Tips tool (See Section 3.1). We also distinguish the tool in its orientation towards monitoring and generating leads about the use of algorithmic decision making in government, a particular context that has not been addressed in prior work, but which is indicative Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021. 406:4 Nicholas Diakopoulos, Daniel Trielli, and Grace Lee of a larger class of investigative reporting efforts to monitor government documents that might be similarly supported technologically. Moreover, the evaluation of many of the tools developed in prior work has been based on case studies, or short-term scenario-based usage, rather than extended deployments (excepting [76]). In contrast, in this work, we report on a two-month-long deployment of Algorithm Tips with professional journalists in an effort to better study the use of such a tool as it relates to more ecologically valid journalistic workflows and practices [79]. 2.2 News Values and Newsworthiness Much of the research on tool development for journalists tends to emphasize the importance of aligning designs with journalistic values [ 23, 24, 47 , 51]. We address this observation in this section by presenting a closer examination of how CND tools might incorporate measurements of news values to support journalistic decisions of newsworthiness. Whether an event or occurrence in the world actually becomes a news story is the result of a sociotechnical gatekeeping process that is influenced by forces at various individual, organizational, social, normative, economic, and technical levels [ 78]. Yet research has shown that a number of factors, termed news values, have been repeatedly observed and are manifest in the types of stories that journalists tend to report and publish. For instance, journalists may consider news values such as timeliness, proximity, prominence, human interest, relevance, conflict / controversy, unexpectedness / surprise, reference to the power elite, audience fit, actuality, and consequence / impact, among others [ 5, 44, 45, 65, 75]. Whether a news lead is deemed newsworthy enough to become a news story arises out of a contextualized judgement process informed by these news values. In this paper we consider how to integrate measures of news values, or newsworthiness, into news discovery processes and tools in order to inform (but not supplant) professionals’ editorial judgements. In particular, we consider how different news values may be amenable to measurement using computational and crowd-based techniques (elaborated further in Sections 3.2.2 and 3.2.4), while acknowledging in our interface design that professionals should be enabled to make the final contextual judgment of whether and how a lead fits and is positioned at broader organizational and societal levels of interest [75]. All of the news discovery tools referenced above encode and reflect computational operational- izations of news values in some way, oftentimes in domain- and data-specific ways. For instance, several efforts have tried to capture the dimension of “unexpectedness / surprise” by detecting outliers or anomalies in numerical data streams or sets [27, 55, 77 ]. The Lead Locator system additionally measures “political relevance” in a domain-specific way, and includes a basic opera- tionalization of “magnitude” based on population sizes, which makes sense in the political reporting context [ 27]. The Vox Civitas system uses measures of “relevance” and “uniqueness” based on text analysis and cosine similarity to help filter social media posts in response to political speeches [ 29]. The Local News Engine captures the dimension of “reference to elites” by automatically extracting named entities of people, places, or companies from local documents [67]. The Reuters Tracer system explores several operationalizations of news values for evaluating detected events in social media, including “topical relevance”, “scale” (or “magnitude” of the event), “negative impact” including human, physical, and financial impacts, “location”, and “novelty” of the event [ 62]. In some cases news values are based on fairly straightforward statistical measures [27 , 55] while in others sophisticated machine learned models [ 62] or formal semantic models [ 60 ] are applied. There have also been a handful of prior efforts to measure news values in non-discovery contexts, such as for evaluating news headlines [68] or for predicting the sharing of news articles socially [91]. Given this prior work, and taking into account the specific context explored in the current work (i.e. focused on administrative documents related to the use of algorithms in government) we felt confident in computationally supporting four news values: timeliness, proximity, reference to the power elite, and topical relevance (See Table 1). Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021. Towards Understanding and Supporting Journalistic Practices Using Semi-Automated News Discovery Tools 406:5 In contrast to prior work on computational news discovery systems, this research also explores the use of crowdsourcing techniques to measure news values, and more importantly, evaluates these crowdsourced signals of newsworthiness with domain experts in a specific and extended deployment context. Crowdsourcing techniques have previously been used throughout journalistic processes for everything from checking documents and rating claims and content for factchecking or verification purposes, to identifying and verifying locations and context, co-developing topics for coverage, helping to moderate and route information in communities, gathering information, and even writing news articles [ 1 , 2 , 21, 50 , 52, 94, 101]. This work distinguishes itself from prior work by exploring the applicability of directly crowdsourced measures of news values in helping to augment professional judgements of newsworthiness. Crowdsourced rating approaches have been shown to be effective for the reliable measurement of a range of manifest and latent constructs in communications content [7 , 28, 53], which we extend here to apply to news values. We consider both quantitative evaluations of news values (i.e. numerical ratings on a 1-5 scale) and qualitative rationale for those ratings as an added dimension of context and information for end-users [57]. In particular we apply this approach to measuring news values that we think would benefit from the diverse social knowledge and independent evaluations that can be captured through crowdsourcing [84], including: bad news (which we frame more specifically as “negative societal impacts”), magnitude of impact, controversy/conflict, and unexpectedness/surprise (See Table 1). These measures are included in the Algorithm Tips user interface and are considered in our evaluation in terms of how journalists incorporate such crowdsourced measures of news values into their broader newsworthiness assessments. 3 ALGORITHM TIPS DESIGN Here we describe the design of Algorithm Tips both in terms of how information flows through the system and is augmented and evaluated, as well as in terms of how end-user journalists are able to view and interact with that information. The system was initially developed and piloted in early 2017 [ 90], and since then has gone through several design iterations to simplify the information architecture and design, reduce the amount of manual effort needed while taking advantage of automation where possible, and incorporate a novel crowdsourcing component. These iterations were spurred on by informal feedback from several professional journalists working at established news organizations who were shown early versions of the interactive prototype and methods either individually or in the context of sessions at practitioner conferences or workshops that were geared towards soliciting feedback. For instance, such feedback informed features to support the evaluation of newsworthiness including topical and date-based filtering and the highlighting of key entities in documents. We designed Algorithm Tips to support investigative or enterprise journalism [ 69] around the use of algorithms in government. Investigative or enterprise journalism reflects a typical orientation towards algorithmic accountability reporting [ 22, 23] and is characterized by extended time frames [ 42] with less of an emphasis on the speed or immediacy of the report with respect to events in the world [ 99 ]. Moreover, the specific reporting context of algorithms in society that we seek to support does not entail an exhaustive review of all leads. The goal is rather to signal when something is interesting and worthy of attention rather than demand the journalist exhaustively find everything of interest. We focus on supporting the initial news discovery phase (i.e. the initial contact or awareness a journalist receives about a potential lead) and on the transition into the news gathering process [ 70]. This design orientation has implications for how the tool is configured (e.g. to generate alerts on a weekly basis), what types of journalists might expect to find value from the tool (e.g. investigative or enterprise rather than daily or breaking), and the temporal affordances Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021. 406:6 Nicholas Diakopoulos, Daniel Trielli, and Grace Lee of the leads the tool might surface (e.g. oriented towards longer term issues and trends rather than events) [85]. After analyzing the extant computational news discovery systems and tools from the literature (See Section 2.1), and drawing heavily on [25 ], in the next section we articulate several more specific design goals for Algorithm Tips. The final flow of information through the system, from monitoring documents on the web, to suggesting news leads to professional journalists is depicted in Figure 1. The information process used to harvest and enrich documents is described in Section 3.2 and the interface used to present leads to end-user journalists is described in Section 3.3, both of which are described in terms of how they help support the overall design goals. 3.1 Design Goals • DG1-Attention: Be sensitive to available human effort and attention. Journalists are already overwhelmed with information from many different channels, sources, and information sub- sidies (e.g. press releases) [47, 81 ]. A lead discovery tool should be sensitive to the journalist’s attention economy by aligning suggested leads to their interests [ 25 ]. This can be supported through search and filtering that allows the information space to be interactively adapted, alerts that are configurable and schedulable to suit individual interests and timing needs, and support for marking of items so that users can come back to initially interesting leads when more attention is available. • DG2-Verification: Enable initial verification and quality assessment. In order for a lead to turn into a substantive piece of journalism it must be verified and confirmed as accurate [88]. This is an iterative process that unfolds as the lead is pursued and new information is collected, however, even at the initial phase a lead discovery tool should enable verification and information quality assessment activities [ 25]. This can be supported by including adequate context to evaluate or verify information and launch into follow-up research, such as by including a link to the original source document, provenance information on the search terms and search used to find the document, and date/time information to indicate the timeliness of the information [12]. • DG3-Newsworthiness: Enable newsworthiness evaluation. Journalists apply a range of context-specific criteria to a news lead in determining whether they think it is important enough to pursue and develop into a publishable news story [25]. These may include factors such as timeliness, proximity, significance, novelty, various dimensions of relevance, as well as whether it is likely to be of interest to their imagined audience. Journalistic decisions of newsworthiness can be supported by incorporating relevant metadata in the information design so that journalists can develop their own nuanced interpretations of these various factors. 3.2 From Documents to Leads In this section we describe the design of the first four stages of the Algorithm Tips information flow, beginning with the document monitoring apparatus. After documents are collected they are then evaluated, augmented, and filtered in several ways before they become leads that are delivered to journalists. In the following subsections we first describe the monitoring method, and then describe the details of different processes for automated, expert, and crowdsourced evaluation of the documents (See Table 1) which help support the design goals described in Section 3.1. 3.2.1 Document Monitoring. A method for targeted Google web searches and scraping was de- veloped in order to automatically monitor for potentially interesting instances of ADM systems Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021. Towards Understanding and Supporting Journalistic Practices Using Semi-Automated News Discovery Tools 406:7 Fig. 1. The Algorithm Tips information process and interface. used in government. In order to develop a list of search queries to target our concept of interest (i.e. ADMs) we considered how algorithms might be referred to across different government agencies (DG1-Attention). From our experience with public records requests for algorithms from state agen- cies we knew that the vocabulary used to describe algorithms internally could vary. For instance, an algorithm for predicting criminal recidivism might be referred to as a “risk assessment” by the relevant government agency. We created a list of algorithm-related terms such as “algorithm”, “predictive analytics”, “risk assessment” and so on. Initially, 61 terms were identified, based on brainstorming and the use of various thesauri. As described in [ 90] we then used term expansion and redundancy identification techniques to both widen the net and enhance the coverage or recall of the system while also striving for a non-redundant set of queries that would increase efficiency and be considerate of the demands the scraper put on external systems [9]. This process yielded a final set of 67 terms.2 Since our initial focus and area of interest is governmental algorithms we limit searches to U.S. government domains. This was done by appending the “site:.gov"" operator to the search. Initial results included a high degree of noise due to research papers hosted on government websites that described their methodologies using many of the search terms. A majority of those research papers were hosted by the National Institutes of Health (NIH). While informative, early assessments revealed that many of these references would not necessarily lead to algorithms being used by government agencies. As a pragmatic response, they were therefore filtered out of the searches by using another operator, ""-site:.nih.gov"".3 In the end, searches for data collection were constructed using the following format: “[search term]” site:.gov -site:.nih.gov. Searches are automatically executed by our system for all of the search terms tracked on a periodic basis (once per week in the current setup). Up to 100 results for each term are collected although a document (e.g. HTML, text, or PDF file) is excluded if its URL is a duplicate of a document URL found with a previously searched term. Also, if a document URL is found in subsequent rounds of data collection (e.g. in a future week) it is marked as a duplicate and only the first occurrence is retained. In an effort not to overload Google servers we only submit one search query every 1-2 minutes. 3.2.2 Automated Evaluation. Each document that is collected is automatically tagged with a few pieces of metadata to support the evaluation of several news values (DG3-Newsworthiness) (See Table 1). To support evaluations of the news value of “timeliness” we store a date-time stamp for every document, both for when it is initially found online, and for when it is finally published on Algorithm Tips. To augment documents with data about both the relevant jurisdiction (e.g. “City”, “State / Local Government”, or “Federal Agency — Executive”) and the government source that published information about the algorithm (i.e. an agency, organization, or municipality), we use the domain name of the URL where the document was found together with curated data provided by 2The final set of terms is available at REDACTED. 3Other government domains which might be expected to yield many research papers, such as nsf.gov, only accounted for a small minority of documents (∼1%) and were therefore not explicitly filtered out in order to maintain as wide a net as possible. Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021. 406:8 Nicholas Diakopoulos, Daniel Trielli, and Grace Lee the U.S. General Services Administration which links jurisdictions and sources to domain names.4 This information is meant to support the news value of “proximity” by conveying to end users the geographic scope of the lead. In order to support the news value of reference to “the power elite”, we automatically extract named entities for all people or organizations mentioned in each document.5 Based on frequency of occurrence in the document the top five people and the top five organizations are retained. In addition, we developed a custom model for assessing document relevance which scores each document on a scale from 0 (not relevant) to 1 (relevant) with respect to the topic of algorithmic decision making (DG3-Newsworthiness). This is a measure of topical relevance and is meant to align the editorial scope of Algorithm Tips’ leads with the presumed interest of external experts coming to Algorithm Tips for leads (DG1-Attention). To train the model we manually annotated a random subsample of documents collected via our scraping process. Data was collected across three separate dates in 12/2016, 3/2018, and 6/2019 with a goal of contributing to greater resilience of the model to temporal shifts in the data. Because text could not be extracted from all documents and due to link-rot between the collection and annotation, the final sample consisted of 223 relevant documents and 304 irrelevant ones. To ensure annotation reliability we developed guidelines in an annotator guidebook which includes clear definitions and criteria for relevance as well as positive and negative examples. We trained a second coder using the guidebook and on a random sample of 100 documents achieved an inter-rater reliability Cohen’s Kappa of 0.87 with one of the co-authors, indicating a reliable coding process. The second coder then annotated the remainder of the documents. The final set of 527 annotated documents was then used to train a model, with features including all term frequency inverse document frequency (TF-IDF) scores of unigrams, bigrams, and trigrams and excluding English stopwords. To establish the appropriate criteria for the model evaluation we first consider its intended use context. In particular, we assume that journalist end-users will have a limited amount of time available to review leads, and that they will be satisfied with finding some subset of reasonable leads without expecting to comprehensively find all leads (DG1-Attention). We similarly assume the internal expert evaluator has a fixed time budget but has the goal of increasing yield (i.e. finding as many relevant leads as possible in the time budget). A high precision will increase the yield. This use context therefore suggests that the classifier should be evaluated using the precision@k metric, which considers the precision of a ranked set of results of size k (where k controls the fixed time budget). We experimented with Bayes, Logistic Regression, Support Vector Machine, and Random Forest algorithms for learning the model and compared their performance with k = 10, 25, and 50. All models were trained using 5-fold cross validation by training on different subsets of 80% of the data to generate predicted scores for the 20% left out. We found that the Logistic Regression classifier outperformed the others, yielding a P@25=0.6 and P@50=0.6. So, for example, on a sample of 25 documents ranked with the model we would expect 15 of them (i.e. 60% ) to be relevant. In comparison, a random sample of 25 would be expected to yield about 10.58 relevant documents (i.e. 223 / 527 or ∼42% is the base rate of relevant documents in the sample). Given a fixed time budget that allows for the examination of, for instance, only 25 documents, the model is expected to boost efficiency by yielding about 42% more relevant documents in comparison to a random selection of documents (i.e. from ∼10.58 to 15). The model is therefore useful insofar as it should increase the yield of relevant documents identified by the internal expert given their fixed time budget. Alternatively, the model could be deployed to reduce 4https://github.com/GSA/data/tree/master/dotgov-domains 5We use the SpaCy large model for named entity recognition (https://spacy.io/usage/facts-figures) which has an accuracy very close to the state of the art at about 86% Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021. Towards Understanding and Supporting Journalistic Practices Using Semi-Automated News Discovery Tools 406:9 the time budget but maintain a fixed yield, however we opted not to use the model this way as we want to tune the system to find as many relevant documents as possible. Admittedly there is still like","Diakopoulos, N., Trielli, D., & Lee, G. (2021). Towards understanding and supporting journalistic practices using semi-automated news discovery tools. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW2), 1-30."
SCOP_070,Audiovisual Verification in the Evolution of Television Newsrooms: Al Jazeera and the Transition from Satellite to the Cloud,"With the spread of the digital sphere and the proliferation of images from indirect sources that can be accessed by systems and users, verification routines have become essential to ensure media corporations’ credibility. The advances in artificial intelligence which allow automated fact-checking (AFC) initiatives to be created help detect falsehoods, but they do not eliminate the need for human intervention. On the contrary, information professionals are necessary, and their functions increasingly include procedures such as mediating in videos and images. This study analyses the evolution of verification routines in audiovisual journalism and how new techniques have influenced the perception of trust worthiness and the reorganization of the television newsroom by allowing content from outside the media’s own newsroom. To do so, it combines a method that examines the main literature on verification processes and compares it with the procedure used by Al Jazeera. In this regard, an exploration was conducted out based on participant observation in this international TV channel via interaction with journalists and heads of the corporation. The results indicate that advances in verification procedures make it possible to introduce visual material from the social media into the corporation’s common news topics contributing to the transition from the traditional newsroom to the cloud structure and the inclusion of new audiences. These changes affect journalistic routines in a profession which has no longer been in-person for some time, in which correspondents coexist with journalists working in virtual mobility, seeking and receiving images in and from the social media.",Journalism Studies,Article,2021,Y,Y,0,"Audiovisual verification in the evolution of television newsrooms Anàlisi 64, 2021 87 Checking information has been basic practice in journalism since its incep- tion. In just a few years, however, the spread of the social media has changed the dynamics of how the large television corporations work. The growing presence of images from the digital sphere has led the entire newsroom to undertake verification tasks that require some degree of technological training. Every minute, 500 hours of video made by users from all corners of the globe using the cameras on their mobile phones are uploaded to the Internet (Mohsin, 2020). This shift poses a challenge for newsrooms, which have to ensure the veracity of this material in order to preserve their credibility. The digital sphere poses a scenario in which the reputation of journalism has been brought into play, since not only have the traditional media lost sole sovereignty of the discourse but the social media themselves have become a source with no stable interlocution protocols; instead, there are only fortu- itous exchanges which lack solid knowledge or trust. Hence, there is a growing interest in finding new technological verification resources, and the appear- ance of professional, specialized fact-checkers are at the root of this new reporting style (Graves, 2012: 1). Indeed, over the last decade, fact-checking platforms have been set up in over 50 countries, 90% of them since 2010 (Fernández-García, 2017). Artificial intelligence and advances in creating automated fact-checking ini- tiatives (AFC) have not yet rendered communication professionals expendable in this process, although if there is any suspicion they help streamline the detec- tion of lies, online rumors and other kinds of disinformation (Graves, 2019). 2. Theoretical context: Disinformation and editing on the Internet In this convergence of media industries, the digital sphere and telecommuni- cations, information flows at a speed that would have been unimaginable only a few years ago (Thussu, 2007: 43). The Internet has turned newsrooms into permanently connected networks with uninterrupted information flows that have to be updated with little processing time given the demand for news production. This evolution has changed journalistic work (Castells, Van der Haak and Parks, 2012). Verification neutralizes disinformation only if it has not yet reached its peak circulation, because once it spreads the rebut- tal may not reach the audience (Rodriguez-Fernández, 2019). TV audiences’ migration to the Internet has turned them into not only information consumers but also content broadcasters, thus generating what is called networking journalism in which professionals and amateurs work together to get a story (Jarvis, 2006) through collaboration, with the journal- ists acting increasingly as moderators (Duffy, 2011: 9). In this news structure determined by digital platforms, there are converg- ing possibilities for connectivity and innovation with the proliferation and saturation of news flowing at hyperspeed (Beckett, 2020). 88 Anàlisi 64, 2021 Lola Bañon Castellón Any technological advance has a potential distorting effect, and there is no information that cannot be turned into disinformation (Virilio, 1995). The Internet has facilitated an intentional change in meanings, which is why not only the volume of information but also the veracity of the content must be considered (Hernon, 1995). In this scenario, the definition of fake news comes from the lack of corre- spondence between statements and facts, but if it is also done intentionally, it becomes what common sense identifies as propagandistic lies. Disinforma- tion, on the other hand, is more difficult to detect (Del-Fresno-García, 2019). It is a phenomenon that is known for its intentionality, with false sources or no identification of the sources (Rodriguez Andrés, 2018), and an abuse of power to gain some benefit (Van Dijk, 2006). Hypertextual media are multiplying their possibilities for becoming active subjects of disinformation in a scenario in which an unprecedented volume of images and content is being put into circulation on the Internet. Machines are capable of generating products that are journalistic in appearance for the purpose of disinformation, a polysemic phenomenon that mutates quickly (Manfredi and Ufarte, 2020: 65). 3. Method, research questions and hypotheses The method used is based on a review of the scientific literature on the evolu- tion of fact-checking, the strategies used to deal with disinformation (Roozen- beek and Van der Linden, 2019) and the preservation of the media’s ethical principles in this communication context (McBride and Rosenstiel, 2013). Consideration of verification is fundamental in today’s digital journalism (Adair, 2019), and websites devoted to this purpose have been proven to be instruments that restore credibility (López Pan and Rodriguez Rodriguez, 2020) and work towards democratic construction in a period of changes (Amazeen, 2017), while acknowledging the difficulties posed by the discur- sive ambiguities (Lim, 2018). We conducted the research in this context with a procedure based on sci- entific production via a systematic review that enabled us to provide a snap- shot of the matter (Codina, 2017). We also applied participant observation using the Mixed Methods Review perspective, as well as interactions with stakeholders (Grant and Booth, 2009), namely the professionals in the Al Jazeera corporation who were consulted directly. This work was carried out on visits to Doha, which came about because I was on the international jury at Al Jazeera’s Documentary Festival from 2012 to 2017. I have also had interactions seeking updates in 2020 through my job as a journalist with Ahmad Ashour, the senior content editor in Al Jazeera’s digital unit, and Taysir Alouni, one of the corporation’s most veteran jour- nalists and a witness to how the channel has evolved since it was founded. The contacts were initially in-person, but in recent months the pandemic has meant that the work has been done via videoconferencing. I have also sought Audiovisual verification in the evolution of television newsrooms Anàlisi 64, 2021 89 the professional opinion of Esraa Rabaya, the head of strategy and audience development at Al Jazeera. In this case, the communication and information exchange was conducted by email and telephone. The other professionals that contributed their points of view have preferred to maintain their ano- nymity. My aim is not to analyze specific cases of verification but to examine the little-studied matter of how audiovisual journalistic verification routines have evolved, as well as the new techniques used. By allowing content pro- duced outside the medium’s newsroom to be used, this content has influ- enced the perception of trustworthiness and the reorganization of the televi- sion newsroom. In keeping with the literature exploration and especially my participant observation of the verification process, specifically in the case of Al Jazeera, the following research questions were posed: Q1. Does the inclusion of material from digital sources in the media’s agenda lead to changes in organization and hierarchies in a large audiovisual newsroom? Q2. Do verification processes broaden the topics on news programs’ agendas? Q3. Are new forms and aesthetics being created for reporting with the participation of materials from social media users? Q4. By permitting materials from the social media to be on the agenda, is the new verification a key resource in locating new audiences? From these perspectives, the intention is to analyze the influence of verifi- cation processes on the journalistic routines of Al Jazeera’s professionals and their impact on the different platforms that make up the audiovisual media conglomerate. Hence, the following hypotheses were put forward: H1. The new verification processes are essential in enabling the social media to be included into a global cloud newsroom model. H2. The process of confirming the veracity of sources gives rise to new structures in which there is direct, continual mingling of communication professionals with engineers and IT experts within the newsroom, giving rise to new hierarchies in the decisions on the topics on the agenda. H3. The increased accessibility to such diverse, plural materials and sources made possible by the social media increases the flow of images that may be broadcast, which makes it essential to use human supervision and algorithmic technology from artificial intelligence. 4. Interaction and evolution of Al Jazeera’s verification routines The television corporation Al Jazeera was founded in 1996 with the aim of providing a different view of the news for Arabic peoples (Miles, 2005: 234), and it quickly gained a wide audience in the region (El-Nawawy and Iskander, 2002). Five years later, its website in Arabic was set up (Satti, 2020). Its broadcasting and website in English were launched in 2008, chal- 90 Anàlisi 64, 2021 Lola Bañon Castellón lenging the West’s media hegemony in the region and becoming a point of reference for modernization in audiovisuals and communication in Arabic (Zayani, 2005:1). It began at a time when the land-based model of television distribution, which could easily be subject to government intervention, was evolving towards satellite broadcasting with the potential for expansion. Al Jazeera reached its international standing following the 9/11 attacks in 2001, when it became the first network to provide images of Osama Bin Laden. That material was also distributed to the major global television cor- porations through agencies. The matter of whether the images of the Al Qaeda leader were real and how they had been obtained even became a sub- ject of debate in high-level international politics. At that time, without today’s digital technology, verification depended on human resources, which for Al Jazeera meant having a correspondent in Afghanistan, Taysir Alony, who was keenly aware of the political climate in the region, as well as image and sound technicians who could certify that the videos and audios had not been manipulated. At times, the process ended in a decision to delay the broadcast, while other times the materials were broadcast but with subtitles under the images with the phrases “Attributed to Al Qaeda” or “We have been unable to verify the authenticity”. Today, technology has evolved to make it possible to use Internet-based video editing and management systems in the newsroom, which allows meta- data to be imported, registered, organized, and synchronized so different departments have access to the cloud where all the information is stored. This entails a radical shift in the newsroom’s professional relationship with the material that it handles, going from a binary relationship (in which the journalist sends something via satellite) to a networked distribution in which audiovisual materials and data move around the Internet and are placed not in a specific repository but in a shared-access cloud. 4.1. New editorial structure: The digital unit In 2006, Al Jazeera set up its digital unit after noticing the changes in tech- nology and politics as well as in the attitude of a prosumer audience that was shifting from being passive observers to active collaborators. The unit conducted an experiment by handing out cameras to activists in different regions in the Arab world, and in 2008, during Mubarak’s rule, it was considering the idea of a report with material from Egyptian bloggers. In December 2010, the Tunisian revolution erupted and the authorities of the Ben Ali regime closed down the Al Jazeera head offices. Even so, the corporation had no problems providing images from the main conflict sites because social media and mobile phones were quite widespread by then. The same happened days later in Egypt: thanks to the installation of a satellite, Al Jazeera broadcast the signal from Cairo’s Tahir Square to the world. Audiovisual verification in the evolution of television newsrooms Anàlisi 64, 2021 91 Figure 1. Recording from social media made in Tunisia. A man celebrating the fall of Ben Ali in January 2011 (Al Jazeera) 4.2. Distortion of user-generated content When the Arab revolutions erupted in 2011, thousands of images were sent to Al Jazeera’s headquarters, making it a vital source of information for the rest of the world. At decisive moments, the traffic on Al Jazeera’s website rose by 1,000% and liveblogging by 2,000%. The traditional blog was overwhelmed, and Al Jazeera had to seek a platform capable of securely storing the traffic it was receiving from the social media. It hired a content management system (CMS), a framework for managing images and content that hosts users’ forums and blogs (Bañon, 2016). User-generated content (UGC) is defined as images and content uploaded to the Internet by authors unrelated to the media (Van Dijk, 2009; Cheong and Morrison, 2013; Bahtar and Mazzini, 2016). At that time, Al Jazeera was still seeking a model for guidelines to validate the images it received from activists, especially Egyptians and Tunisians, who were particularly active. It was a new experience for the entire world media 92 Anàlisi 64, 2021 Lola Bañon Castellón and mistakes were inevitably made in the midst of this historical acceleration. Nevertheless, a culture began to coalesce in which verification played an essential role, with the war in Syria marking a turning point. A specific unit with newsroom professionals was created, tasked primarily with verifying UGC, which became a news source virtually as important as agencies and the traditional newsroom (Marai, 2017: 22). As the war in Syria dragged on, Al Jazeera became strongly committed to covering it, which brought about a new phase in the network’s verification procedures. Training was provided to a hundred journalists to improve their skills dealing with UGC. Even so, the network was unable to avoid mistakes. Recognizing and explaining them is part of its newsroom code of conduct. One example occurred in December 2015, when several channels, including Al Jazeera, broadcast a video going around YouTube in which some Canadian children were supposedly singing the song Tala al Badr Aleina to welcome refugees. The song comes from the Islamic tradition and tells of the times when the prophet Muhammad had to seek refuge in the city of Medina. At a time when there was a sense of solidarity towards Syrian refugees, the video received thousands of displays of solidarity around the world. However, after the broadcast it was proven that it had nothing to do with them. The net- work apologized and prepared a second piece explaining how the misinfor- mation had occurred (Marai, 2007: 26). Figure 2. Concert falsely attributed as a welcome for Syrian refugees in 2015 (Al Jazeera) Source: Access to the video broadcast with false information about the performance: ; access to the news item in which Al Jazeera apologizes and recognizes the mistake. Watch | Facebook Another example of how the atmosphere can create the conditions for dis- information occurred following the terrorist attacks in Paris on 13 November 2015, which were committed simultaneously by gunmen and suicide bombers Audiovisual verification in the evolution of television newsrooms Anàlisi 64, 2021 93 in different places and caused 130 deaths. The Associated Press had recordings of the attack and identified a suicidal woman as Hasna Ait Boulahcen, with- out providing photos of her. But they began to circulate in the social media and Al Jazeera trusted one that several media had published. It turned out that the person in that photo was really Nabila Bakkatha. They located her in Morocco and interviewed her, admitting the mistake (El Katatney, 2017: 62). Figure 3. Al Jazeera Plus broadcasts a video with statements by Nabila Bakkatha, a Moroc- can woman, rectifying the news in which she had been identified as a suicide terrorist (Al Jazeera) Source: Access to the news item in which Al Jazeera recognizes the mistake: 4.3. The effects of verification on the newsroom The introduction of streamed images from the social media also entails a psy- chological leap in the concept of the newsroom, and in Al Jazeera there was resistance from some journalists (Khalifeh, 2017: 29). In the midst of this digital context, the verification procedure always begins with checking routines similar to the journalistic procedures before the social media. Hence, before conducting technical checks, journalists con- duct a prior exploration in an attempt to discover the origin of the source and ascertain whether it is reliable and therefore sent by someone they know or shared by a close colleague. 94 Anàlisi 64, 2021 Lola Bañon Castellón The level of falsehood in materials from UGC can appear in different forms: — Totally authentic. — True, but not recorded on the date it is broadcast. — True, but recorded in a different place from the one stated. — Manipulated, with distorted or added elements. — False and created with the explicit intention to deceive (Ghazayel, 2017: 69). Using this classification, the basic verification routine follows these basic steps: — Checking Twitter lists, enabling the followers to be classified. — Creating our own list to add accounts that interest us. — Searching via Google for lists created by other organizations and peo - ple, creating networks. — Using platforms like Tweetdeck that enable interaction with the plat- form. — Joining a relevant thematic WhatsApp and Telegram list. — Language check. Al Jazeera has a target audience in which Arabic is the majority language, with 420 million speakers and 30 dialects (Unesco, 2020). That implies consideration of the language, especia- lly to confirm geographical location. — And there is one basic precaution in the newsroom: downloading the video before beginning the verification process, since there is a risk it may be taken down. 4.4. Credibility tools and IT Every day, digital journalism has more new resources to improve the verifica- tion procedure (Pellicer, 2019). Here are some of the accessible tools: — Reverse image search, enabling images to be located if they are found elsewhere in the Internet. Google Image Search or Tineye are used with the key frames. The origin and context of the photo can then be ascertained. — Comparing with other servers. — Invid or Rev Eyecon can also conduct reverse searches for videos. — ImgOps enables images with similar content to be compared and alte- rations to be detected. — YouTube Data Viewer enables access to the main metadata. — Fingerprinting Organizations with Collected Archives (FOCA) and Metashield Clean Up detect metadata and information that is not visible in documents that can be found on websites. — Foller.me is a tool to find information about Twitter profiles. Audiovisual verification in the evolution of television newsrooms Anàlisi 64, 2021 95 — Pipl enables people’s profiles to be found via what they post on the Internet. — Anylizer analyses videos frame-by-frame, discovering irregularities if changes have been made. — Google Earth, Wikimapia, Open Streets Map and Google Maps can be used to verify places via geolocation. The list for detecting bots or fake profiles does not end there, nor do the techniques. For example, one can also look for the video’s URL or its ID code in Twitter to trace the first person who started circulating it. It is also important to be able to demonstrate that the stated location is correct (Younes and Mackintosh, 2017: 45-46). 4.5. The purpose of the Al Jazeera+ online platform By having professionally checked material, this study has been able to broad- en the topics on the agenda by introducing material from the digital sphere while preserving credibility – an essential factor for any medium, and even more so in a region in serious political upheaval. Hence, Al Jazeera’s digital platform is an Internet space that works: the YouTube channel has racked up an average of three million views, and it has over a million followers on Twitter. In this vein, it has successfully managed a transition involving turning a simple news website into an experimental informative online platform (Bañon, 2016). As a result of this evolution, in 2014 the corporation launched Al Jazeera Plus (AJ+), offering videos directly for Facebook, YouTube, Instagram, Twit- ter and Medium through mobile apps. It thus uses the social media to attract followers by encouraging them to share their stories, with formats based on very recent images or very short videos with little text. These changes and the entry into the digital world rendered it necessary to rethink internal proce- dures (Roettgers, 2013). The experience of AJ+ increased the importance of verification in journal- istic routines: the professionals are not always on the ground and the eyewit- ness reports come from indirect sources whose authorship is unknown at first glance, nor is there always information on the circumstances in which the images were recorded or the reasons for spreading them. One example of a verification process taken to the extreme of instantaneousness was the case of the Address Hotel fire in Dubai on New Year’s Eve, 2015. Minutes before the traditional media reported the news, AJ+ used the Datamindr service to which it subscribed in order to verify the location of the tweets. They thus detected that three eyewitnesses were transmitting the information with Peri- scope. 1 The verification was completed by checking the location and com- ments from eyewitnesses in the social media and by securing permission to 1. A Twitter app that enables live videos to be transmitted 96 Anàlisi 64, 2021 Lola Bañon Castellón use images from one of them. Thus, AJ+ created a story in a matter of min- utes (El Katatney, 2017). Figure 4. The video of the Address Hotel fire was viewed three million times and shared 40,000 times in just one day, with an impact calculated at twenty million people Source: Al Jazeera. The observation that new formats have been incorporated into news spac- es has thus been confirmed. One feature of AJ+’s posts is raw videos uploaded directly from mobile phones to the social media which are far from the qual- ity standards of traditional broadcasts. They are products created for the dig- ital sphere bearing the shared behavior and consumption of different social media in mind. This has enabled AJ+ to achieve audience growth, especially at times of humanitarian crises such as in Palestine or the exodus of refugees from the war in Syria. Such newsworthy situations have revealed that the most solid criteria to become viral are relevance, immediacy, and emotiveness (Rahimi, 2015). AJ+ averages half a million interactions a day, reaching 30 to 40 million people (El Katatney, 2017: 66). 5. Results: Examples of verification and impact in the newsroom Al Jazeera’s verification unit contributes 60 news items a day on average, which are added to the topics in the agenda after passing the pertinent checks. Here are examples in which it successfully detected fake news: 2 2. Data provided by Ahmed Mansour, Senior Editor for Digital Content at Al Jazeera. Audiovisual verification in the evolution of television newsrooms Anàlisi 64, 2021 97 1. Displacement of Syrian refugees after floods in northern Syria (20-01-21): The verification unit discovered that the recording was from a video from 2019. 2. Photo of floods in Sudan (6-09-2020): The verification unit discovered the photograph was from Haiti, dated 2017. 3. Attack on Hassan Nasrallah’s vehicle in the Lebanon (18-01-21): The verification unit discovered that although the date was correct, the image showed an explosion in the Syrian city of Idlib. It is clear that using materials from the digital sphere entails a transition from the traditional, highly hierarchical newsroom to another model more in line with a horizontal rationale. This shift means that journalists have to work with a content management system, which enables them to make productions using accessible software and coexist with a system hosted in the cloud. The nine members of Al Jazeera’s verification unit are all under the age of 35 and have previously worked in the newsroom, preferably in posts in the international and sports sections. The trend is towards a newsroom in which there is a shift from being pro- ducers of visual and news products to mediators and custodians of quality standards, with verification the top priority. This evolution will intensify as the subscription systems for distributing signals using broadband or IPTV connections improve,3 thus enabling streaming media4 with audiovisual con- tent broadcast via the Internet. Guiding protocols are also increasingly being adopted to improve the security of the Internet as a mechanism for convey- ing the production processes (Hunter, 2021). In the case of Al Jazeera, this transformation itself has given rise to new sources and a differentiated agenda of topics, although journalists specializing 3. Internet Protocol Television. 4. Streaming audio-visual content broadcast via the Internet. 98 Anàlisi 64, 2021 Lola Bañon Castellón in the tasks typical of each medium are still required even in newsrooms that have undergone integration processes (Salaverría and Negredo, 2008: 169). In the traditional newsroom, organizing a live broadcast required hiring a satellite, finding a time zone when the content can be uploaded to it, and broadcasting it, not to mention a very significant economic investment. With the Make TV platform, Al Jazeera can download dozens of videos via Internet simultaneously. Thus, journalists from anywhere in the world can send news to a central clearinghouse, which decides whether the image should be stored or broadcast live, depending on the circumstances. One- way organization is thus giving way to work dynamics in the cloud, where the stream of images and data have to be managed. Satellites are still neces- sary for distribution, but the mobile networks have been playing an increas- ing role in channeling, exchanging, moving and organizing information. They generate and lend coherence to this movement and organization pre- cisely in the cloud system. At the core of this process are the verification routines, which provide rec- ognition to the materials from the digital sphere. This intensifies the need for mediation from journalists, IT professionals and cross-disciplinary profession- als. They are also highly specialized tasks that require a collaborative culture with the outside world, in which important entities become associates. For example, Al Jazeera is a member of the First Draft Partner, an entity encom- passing the largest social media platforms and verification projects which are seeking to tackle these challenges together (Fernández-García, 2017). Verification tasks guarantee credibility, but their use in a newsroom in the cloud where streams circulate poses four significant challenges: overcom- ing ethical tensions, putting proven facts into context, generating relevant information and observing public interest, and raising awareness about cer- tain social issues. (Srisaracam, 2019) 6. Discussion and conclusions This study has confirmed that including information and images originating from the digital sphere has made it necessary to broaden the agenda of topics and the number of sources (Q2). Since there is material coming from the circuit outside the newsroom, the verification process is crucial and has led to changes in the organization of the newsroom (Q1), with the creation of a group of journalists and technicians specializing in this task. The qualitative characteristics of these materials coming from the audience’s mobile phones are far from the usual quality standards in the professional sphere, but they provide advantages such as immediacy and accessibility, which is leading to a new aesthetics of reporting (Q3) and contributing to the culture of TV view- ers’ participation. In this sense, management and verification makes it possi- ble to use material from the social media with quality assurances. These social media have helped redirect audiences to the screen and locate new potential audiences (Q4) for television broadcasts or the channel’s website. Audiovisual verification in the evolution of television newsrooms Anàlisi 64, 2021 99 In this study, therefore, Hypothesis 1 was confirmed and we can con- clude that the verification processes and their improvement are key features in enabling traditional newsrooms to transition to a cloud structure, since they allow the veracity of visual material from the social media to be checked, thereby preserving a fundamental feature of quality journalism. This model is also shifting from hierarchically organized professional relationships towards more horizontal models, in which technological training is essential. Hence, Hypothesis 2 is also confirmed in the sense that the new news- room has news professionals coexisting with experts in artificial intelligence and IT. The new nature of the newsroom is no longer strictly in-person; cor- respondents coexist with journalists via virtual mobility, seeking and receiving images streamed from the social media. The cloud needs to organize the traf- fic, and managing it is leading to the establishment of new horizontal hierar- chies within the newsroom. This should be the subject of subsequent studies, because the harmonization of the list of functions of the different information professionals should include excellence as a goal not only to find the best image or video but also to create the context to provide audiences with the best explanation via new formats generated in the social media. Technologies enable all of the material to be available to the group in the cloud newsroom, although verification protocols are the first step in this availability. Neverthe- less, this availability is subject to usage protocols, for example prioritizing broadcasting in the network’s news programs over others for strategic reasons. Finally, Hypothesis 3 is also confirmed, as the use of algorithmic technol- ogy is essential in verifying the volume and diversity of materials generated on the Internet. However, at the same time, the need for verification does not exclude traditional techniques. A newsroom with a hybrid coexistence of professionals in no way diminishes the importance of journalism but instead makes it necessary. There is increasing evidence of the need to lend meaning and context to the information, which means that journalists keep and pre- serve their topic specialization. Moreover, insofar as verification preserves the audience’s trust, it allows topics and sources in the agenda that have traditionally been excluded, including new coverage. It is therefore a relevant and inevitable way to incor- porate new audiences, thereby helping to grow audiences and rendering it possible to maintain big audiovisual corporations. The additional challenge posed for journalism by the new cloud news- rooms is how to boost creativity and create protocols for internal cohesion to motivate the professionals to organize the creation of quality content that serves society.","Castellón, L. B. (2021). Audiovisual verification in the evolution of television newsrooms: Al Jazeera and the transition from satellite to the cloud. Anàlisi, (64), 85-102."
SCOP_103,Verification of Digital Sources in Swedish Newsrooms — A Technical Issue or a Question of Newsroom Culture?,"This article analyses and discusses attitudes and practices concerning verification among Swedish journalists. The research results are based on a survey of more than 800 Swedish journalists about their attitudes towards verification (Journalist 2018) and a design project where a prototype for verification in newsrooms – the Fact Check Assistant (FCA) – was developed and evaluated. The results of the survey show a lack of routines when it comes to verifying content from social media and blogs and considerable uncertainty among journalists about whether this kind of verification is possible. The development of the prototype initially created reactions of interest and curiosity from the newsroom staff. Gradually, however, the degree of scepticism about its usability increased. A lack of time and a lack of knowledge were two of the obstacles to introducing new verification routines. It is not enough to introduce new digital tools, according to the journalists. Management must also allocate time for training. The paper’s ultimate conclusion is that changing journalists’ and editors’ attitudes towards verification in this digital age appears to be guided by newsroom culture rather than technical solutions.",Journalism Studies,Article,2021,Y,Y,0,"The discussion about fake news and disinformation has made fact-checking and verifica- tion a focus of journalism research. Fact-checking has developed into a new genre in news journalism, both within established news organisations and in separate entities, such as Politifact in the US and Faktiskt.no in Norway (Graves 2016). However, there is an important difference between the notions of fact-checking” and verification” (Wardle 2018; Allern 2019). Fact-checking is the process of assessing the val- idity of claims that have reached the public domain through some form of media. For example, journalists could use political fact-checking to assess the validity of statements by politicians and public figures during election campaigns. These statements may be broadcast or published in traditional media or posted online. The results of a fact- check are often published with a judgement on a graded scale from completely false” © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor  Francis Group This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial-NoDerivatives License , which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited, and is not altered, transformed, or built upon in any way to completely true”. This form of fact-checking requires an ability to cross-validate argu- ments based on earlier established facts. When it comes to verification in the newsrooms, this practice has been part of the working process of journalists since modern forms of journalism were developed. The ver- ification tools used in newsrooms have evolved over time to account for digitalization and proliferation of unverified multimedia content on social media. Verification is a basic norm in journalism and is traditionally carried out before or during the process of publishing (e.g., live reporting). Kovach and Rosenstiel define verification as the crucial divide between journalism and other kinds of media content, like entertainment, propaganda, and fiction – the essence of journalism is a discipline of verification” (2001, 71). In order to be properly applicable in daily work, many countries have codified the value of verification into formal ethical principles. In Sweden, Accurate News” is the title of the first paragraph in the professional ethical rules of journalists.1 Verification of digital content is often associated with assessing the validity of content such as images, videos and texts that are circulated on social media. This verification is extensively technical in nature and requires a special set of skills in the domain of open-source intelligence (OSINT). There is evidence that political fact-checking and digital content verification can occasionally overlap, particularly when politicians support their claims using digital multimedia content circulating on social media. This leads to a demand for technical digital content verification experts and traditional political fact-checkers to work more collaboratively (International Fact-Checking Network 2020). Furthermore, this professional commitment to verification is challenged by both exter- nal and internal factors in media development (Ekström 2020). There is a large flow of information from professional sources with sophisticated methods to news management. There are also sources that circulate news” on different websites and social media plat- forms, and these sources are often hard to evaluate. Internally in the newsrooms, journalists experience increased pressure in their daily work. Structural changes on the market have led to fewer journalists producing more content for multiple platforms, and the space for critical investigation and verification has been reduced (Witschge and Nygren 2009). In addition, accuracy ideals are challenged by the development of 24-hour rolling news practices and so-called liquid news, where verification becomes part of the publishing process rather than something that precedes it (Karlsson 2012; Widholm 2016). The purpose of this article is to go deeper into the field of verification in today’s digital environment. The article describes a joint project between academics and leading Swedish media companies that aimed to develop and test a digital tool for verification of digital content and evaluating sources – the Fact Check Assistant (FCA). The article also presents a recent survey among Swedish journalists concerning attitudes towards verification. This leads us to four research questions: 1. What are the attitudes of Swedish journalists towards verification? 2. Are there established routines for verification in the newsrooms? 3. How can a digital verification tool improve these routines, and what functions should be included in the tool? 4. What kinds of problems arise from the introduction of new tools and routines for asses- sing digital sources and verifying digital content? 2 M. PICHA EDWARDSSON ET AL. Background Verification in the Journalistic Process There is common agreement on verification as a professional norm in journalism. However, it is complicated to study the journalistic process to see how verification is done in practice. Verification is a fluid and contested practice, and studies show there is a lack of consensus in how to verify (Hermida 2015). Only a small part of the journalistic work is devoted to cross-verification, the most usual form of verification. Cross-verification entails having at least two independent sources for a claim, the juxtaposition of two news sources / ... / against each other with the express intention of ascertaining the infor- mation’s reliability” (Godler and Reich 2017, 567). A German study shows that only 5.5 per cent of the working time is devoted to cross-verification (Machill and Beiler 2009). In an Israeli study, about half of the articles had any cross-verification (Godler and Reich 2017). In local journalism, cross-verification seems to be even more rare; in a Swedish study of coverage of local authorities and politics, only one-third of the articles had at least two independent sources (Nygren 2003). Qualitative interviews with Canadian journalists show that verification is part of the whole working process. The need for cross-verification depends on how sensi- tive the issue is, and journalists use their experience when deciding on what to check or not (Shapiro et al. 2013). A common solution is to use well-known and authoritative sources in cases where verification is not possible. The degree of cross-verification is much lower if the source has been frequently used before (Godler and Reich 2017). The conflict between speed and the need for verification is as old as journalism itself. Digitalization of the media system has brought this tension to a new level, and there are tendencies of softer attitudes towards verification among journalists. In a compara- tive study with journalists in Sweden, Poland, and Russia, about one-third of the partici- pants agree that verification is not needed before publishing online; it can be done during the process. Forty-two percent believe that the online audience has lower demands on verification than print and broadcast audiences (Nygren and Widholm 2018). Increasingly, transparency has become important as a professional norm – not to replace verification but to open the process for the audience. In live reporting, verification becomes a collaborative effort for journalists and the audience; facts become less fixed and more fluid (Hermida 2015). However, the journalist is still regarded as a truth- seeker, but often in collaboration with the audience. Another important strategy is to put the responsibility for the accuracy on the source by means of only referring to what is being said (Ekström 2020). In this digital flow of information, social media platforms are important sources for journalists seeking information. Interviews with online journalists in Western Europe show great ambivalence towards these sources in terms of how to evaluate and verify facts and pictures from social media (Brantzaeg et al. 2016). Journalists often stick to trusted official online sources, and they often use traditional methods such as calling sources by telephone to verify the accuracy of what they find on social media. The level of knowledge about digital verification tools is low, and only a few journalists use JOURNALISM PRACTICE 3 them. What online journalists are asking for are tools that will help them conduct basic evaluations of accounts, geolocation, the time stamp, and the history of accounts, for example. Attitudes towards verification are an important part of the different journalistic cultures that exist today. These cultures differ depending on the existing media systems and between different media types and media organisations (Zelizer 2005). There are two main dimensions concerning epistemology in the journalistic cultures according to Hanitzsch (2007): objectivism and empiricism. The first of these dimensions is a philoso- phical question – is it possible to reach an objective truth” or are the statements the result of subjective judgements? The second dimension is about how the journalist can justify his or her claims – by empirical means as facts or are they based on opinions, ana- lyses, and values? The importance of verification among journalists depends on these questions. When it comes to media systems, a clear difference has been observed earlier between the fact-based Anglo-Saxon tradition and the more subjective and analytical continental European journalism (Chalaby 1996). However, this has been changing over time. Over the last decade, fact-based journalism has been challenged by more subjective and values-based journalism, as, for example, in Fox News. As a reaction to this development, we observe the rise of fact-checking as a new genre within news journalism (cf. Graves 2016). Methods Action Research and a Survey In her keynote at the Future of Journalism conference in 2017, Claire Wardle asked for more cooperation between academics and practitioners to research the information dis- order (Wardle 2018). One example is her own organisation, First Draft, which is based at Harvard. By carrying out experimental projects that bring together media companies and journalism educators, the organisation tests new forms of fact-checking and verification. The Fact-Check Assistant (FCA) is a similar project based in Sweden. In 2017, Swedish public service television (SVT) and Swedish public radio (SR), along with two large quality newspapers (Dagens Nyheter and Svenska Dagbladet), received a grant from the Swedish authority for research and innovation, Vinnova. The purpose of the grant was to develop a tool for verification of digital sources in the daily newsroom work – the FCA. Södertörn University was the academic partner responsible for development and coordination. The purpose of the project was to design a prototype in collaboration with journalists and tailor the tool to their specific needs to adopt a systematic and transparent method of verifying claims. The development part of the Fact-Check Assistant can be described as action research as originally described by Lewin (1946). Action research methodology is research on the conditions and effects of various forms of social action, and research leading to action” (Lewin 1946, 35). Galtung (2002) describes the methodology as a way to not only under- stand a problem but also to solve it with the people involved. Kemmis, McTaggart, and Nixon (2014) argue that all action research shares some common key features, for example by rejecting conventional research approaches where an external expert 4 M. PICHA EDWARDSSON ET AL. enters a setting to record and represent what is happening” (Kemmis, McTaggart, and Nixon 2014). Two features are apparent, according to Kemmis et al.: . the recognition of the capacity of people living and working in particular settings to participate actively in all aspects of the research process; and . the research conducted by participants is oriented to making improvements in prac- tices and their settings by the participants themselves (Kemmis, McTaggart, and Nixon 2014). The case study presented in the article displays methods that are common in inter- action design, more specifically goal-directed design as described by Cooper et al. (2014), which has earlier been used in a large number of media studies when implement- ing and testing new technical tools. In one such study, as described by Thurman, Dörr, and Kunert (2017), an automated news writing robot was introduced to British journalists. In another study using interaction design, as described by Stray (2019), AI is introduced to investigative reporters in the United States and Britain. In a third study, a creativity support tool was introduced to journalists in Norway (Maiden et al. 2018). In line with the methodology of interaction design, these studies have in common that they intro- duce, develop, test and implement new innovative digital tools for journalists and evalu- ate the result of the implementation. Our developmental part of the project consisted of workshops where the purpose and idea of a fact-checking tool were discussed with journalists and researchers. After the workshops, a prototype fact-checking tool was developed based on the initial feedback received during the workshops. Journalists and researchers tested and evaluated the pro- totype and then it was further developed in an iterative process that started in late 2018 and ended in the spring of 2020. (The iterative development process is described in more detail further on in this article as this is part of the case study results.) The method of goal-directed design (Cooper et al. 2014) entails that the users, in this case journalists and researchers, are engaged in the technical development of the tool. The main idea behind goal-directed design is to use the needs of the users as the starting point for developing the tool and then aim to build the tool in direct relation to the user needs. This development process usually takes place in a number of iterations with the users. Between every test or iteration, the prototype is developed further, and after a number of tests, the prototype is ready to be launched as a new product. The other part of our study consists of empirical material from the survey Journalist 2018” gathered by JMG, University of Gothenburg, Sweden. This is a broad survey on the ideals and daily work of Swedish journalists, and part of it included questions on ver- ification, the answers to which we found particularly relevant to this study and are pre- sented here for the first time. The survey was sent to a random sample of members of the Swedish Union of Journalists, which organises about 85–90 per cent of all journalists in the country. The response rate was 52 per cent, and the maximum number of respon- dents was 1,151. We did not address the questions on verification to freelancers, so the number of respondents to these particular questions was 876. We include the results from the survey in this article to analyse the point of departure for the FCA project, the existing attitudes among journalists towards verification of digital sources, and the situ- ation in the newsrooms with regard to standards and routines. JOURNALISM PRACTICE 5 This study is limited to one country, Sweden, with a media system described as demo- cratic-corporative” by comparative journalism research (Hallin and Mancini 2004). The system is characterized by strong journalistic professionalism and a low degree of political influence in media. This is important to keep in mind when comparing the results with other countries. Results A Need for Verification Routines Very few of the Swedish journalists work in newsrooms with clear routines for verification of content from social media and blogs according to the survey. Only 17 per cent agree that there are clear routines, and 43 per cent disagree. There is also a large group with no opinion that give no answer, probably a sign of uncertainty in this area. There are some differences between media types; a slightly larger share of the journalists working in public service radio and television are aware of routines for verification of content from social media. This difference is also visible in other questions in the survey concerning ver- ification. For example, journalists in public service media put more emphasis on equal ver- ification in publishing on all platforms: 57 per cent of journalists in public service agree compared to 49 per cent in newspapers Table 1. Is it possible to completely verify content on social media? Journalists demonstrated considerable uncertainty about this question. Less than one-third agree that it is poss- ible, and an equal share disagree. Many respondents neither have an opinion nor answer this question. There are also large differences between age groups regarding this question. Young journalists have greater confidence in verification of digital content, while their older col- leagues are more sceptical. Thirty-seven per cent of the young journalists agree that this type of verification is possible, and only 20 per cent disagree. For journalists over the age of 50, the figures are the opposite: over 40 per cent feel that this kind of verification is not possible Table 2. A deeper analysis shows a strong correlation between the answers to these questions (Pearson .983, significant at the 0.01-level). The journalists who answer that there are clear routines for verifying digital sources also believe that it is possible to do this type of ver- ification. And the opposite: a lack of routines also results in lower confidence in the pro- spect of verifying digital sources. To summarise, the survey shows a low level of routines Table 1 . In my newsroom, there are clear routines in verifying content from social media and blogs (per cent on a scale of 1–5, where 1 is agree completely) Newspapers Magazines Public service Other Total 1 – Agree completely 7 5 11 13 8 2 9 6 12 12 9 3 15 18 19 14 17 4 20 14 18 18 18 5 – Disagree completely 32 25 20 11 25 No opinion 11 22 14 18 15 No answer 6 10 6 14 8 Total 100 100 100 100 100 N= 383 169 211 97 878 Source: Journalist 2018, a survey from JMG, University of Gothenburg. 6 M. PICHA EDWARDSSON ET AL. when it comes to verification of digital sources and a diverse picture when it comes to the possibilities to perform this kind of verification in daily work. The results from the survey were also confirmed in the newsroom study assembled in the first step of the FCA project. One of the university researchers visited three news- rooms: local, regional, and national. With a combination of interviews and observations, he found that journalists were interested in digital verification and there was a need for tools to promote this. There was a general consensus for the idea that a digital assist- ant could function as a support mechanism to systematise the process and share knowl- edge with colleagues in the newsroom. At the same time, journalists were clear about the lack of time in their daily work, and some said a digital tool could be too much when they instead can use traditional methods like picking up the phone (Larssen 2020). He also found considerable resistance against verification as a general concept – one journalist was provoked and concluded, it might give the impression that regular journalism does not do fact-checking. But that is exactly what we do, it’s what we are good at and what journalism is all about” (Larssen 2020, 209). A Three-pillar Model As the iterative development process of this case study is part of the study’s results, a common structure in goal-directed design studies (Cooper et al. 2014), we will describe the process in this section. The theoretical starting point for the developmental process is a model for assessing news credibility based on previous research by Metzger et al. (2003). This shows that a three-pillar model can be applied by integrating a comprehensive evaluation of (1) the medium, (2) the source, and (3) the message (Metzger et al. 2003). This model, shown in Figure 1, served as our theoretical foundation for building a sys- tematic verification process to assess news credibility in a digital environment. To assess each of these three pillars, the user must answer a set of questions in a methodological and consistent manner as described below. Assessing the Medium A medium can be an app, a website or any other service through which content can be posted or shared. To evaluate a medium, there are a number of questions that the journal- ist can answer, such as Table 2. It is possible to completely verify news and other information found in social media and websites? (per cent by age group on a scale of 1–5) –29 30–39 40–49 50–59 60– Total 1 - Agree completely 11 15 9 12 10 11 2 26 22 19 13 9 18 3 25 18 22 17 13 19 4 11 20 19 21 19 19 5 – Disagree completely 9 6 8 20 26 13 No opinion 11 12 12 13 9 12 No answer 7 7 11 4 14 8 Total 100 100 100 100 100 100 N= 98 230 226 226 96 878 Source: Journalist 2018, a survey from JMG, University of Gothenburg. JOURNALISM PRACTICE 7 . Does the medium have a good reputation in the past (or is it blacklisted)? . Are there any technical security concerns (e.g., no SSL access)? . Does the WhoIs database entry support details provided publicly? . Is the medium known for not having any particular bias? Poor ranking, popularity, and reach? . Does the medium have liability terms (legally accountable)? Does it allow readers to flag/report content? To answer these questions, it is possible to utilise a number of digital tools that are often used for investigating and assessing media platforms and applications. Two examples of such tools are WhoIs/reverse DNS checks to verify the authenticity of web- sites and Google Maps/Earth for verification of geobase information. It is important to acknowledge that tools to scrutinise a particular medium may vary depending on the type of the medium and the skill level of the journalist. Assessing the Source The source of particular content corresponds to the individual(s) or entity(-ies) that pub- lished or shared the content being fact-checked. Among the questions that help assess a source are . Does the source have verifiable credentials? . Did the source ever get caught publishing dis/misinformation in the past? . Does the source have a history of quality content published by reputable media? . Can you check the source’s network of influence? . Can you check for any difficulty to authenticate information about the source? Figure 1. The three-pillar model for assessing news credibility (Metzger et al. 2003). 8 M. PICHA EDWARDSSON ET AL. Similar to media, journalists can scrutinise sources using various tools that are available either freely online or for a fee. Examples of tools that can provide insights into the iden- tity and background of a source are LinkedIn, Facebook and Pipl. Assessing the Message (Content) Finally, journalists can dig deeper into the actual message to verify the content that has been posted or published. When evaluating the validity of specific content, it is important to consider its format, because the format dictates which verification tools are most suit- able. For example, when verifying the authenticity of an image-based claim, this may be done through reverse image search tools, while it is more appropriate to assess the val- idity of a text-based statement by consulting a reliable online source. It would be necess- ary to investigate the validity of the content by answering a set of predefined questions based on whether the content is an image, video, or text or a combination of all three. For example, if the claim is based on an image or a video, questions that could be asked to assess the content’s validity would be . Was the image used in a misleading context? Has it been manipulated/doctored? . Does the image contain mismatching metadata? . Is there any audio manipulation in any way? . Has the image been fact-checked before? . Are dates, names, numbers, etc., correct? . Has the image used references that are verifiable? There are many free and commercial tools that are often used by reporters to do advanced OSINT to verify the authenticity and validity of particular content (Hayden 2019). Examples of tools that are used to reverse-search multimedia content are Google Images and TinEye for images, the InVid browser plug-in and YouTube Data- Viewer. There are also tools to investigate whether an image was doctored or manipu- lated, such as Forensically, which is used to magnify and analyse images and extract metadata information such as the place and time the image was captured. Google and archive.org are examples of services that provide cached copies of content that existed on the web but was later removed. Issuing a Verdict on a Claim’s Validity To document and collect results from the verification process, a model developed in the new genre of fact-checking was taken into the FCA project. Based on the answers to the questions about the medium, the source and the message, the journalist can make an overall assessment of the claim depending on the degree of confidence the journalist has in the final judgement. The scale available consists of the following scores: false, mostly false, mixed, mostly true, true and no decision. The journalist should justify the score by a thorough and evidence-based rationale. It is noteworthy that the verdict is meant as an interpretation by the journalist and not an objective fact in itself since not all fake news can be considered false. According to Molina et al. (2019), false news is but one of seven types of online content that could be labelled as fake news”. The other types of fake news” are polarized content, satire, misreporting, commentary, per- suasive information, and citizen journalism (Molina et al. 2019). While it cannot guarantee a conclusive verdict, this three-pillar approach allows jour- nalists to consistently and transparently show how the various questions were answered and where there are missing gaps in the investigation. It is important to document the journey from start to finish so it can be integrated into a sustainable knowledge base for future reference, potential distribution and sharing. Figure 2 summarizes how the verification process of FCA is followed. All testers were guided through the process and also offered the possibility of watching extensive tutor- ials on the topic. Figure 3 shows an example of how an image-based claim is reviewed. The questions on the left have resources that can be used to answer them. These resources can be added and updated continuously. The above is an example of an image-based claim just added and available for review. Workshops and a Prototype Upon developing the theoretical framework of the three-pillar model in our project, it was important to put it to the test by not only training a set of journalists in how to use it and observing their use but also getting their feedback as part of a goal-directed design process (cf. Cooper et al. 2014). A total of fifteen journalists and editors working at six Stockholm-based Swedish media corporations and three journalists from other parts of the country participated in two full-day workshops. Most of the workshop participants expressed interest in implementing this model and found it helpful to speed up the process and make it consistent and systematic. They made suggestions to simplify and structure the process so they could break the work of verification down into separate tasks. For example, one task would be to review a particular medium and another task would be to assess the validity of a picture-based claim. Participants recommended setting up a system that would allow them to work on certain verification tasks and then temporarily leave their desks. When they returned later, they wanted to be able to pick up where they had left off without losing the work they had already done. In other words, they requested having a memory-based system that would allow them to organise their time and work around their own pace. This request was attributed to the fact that they were rather busy with daily journalistic routines. Another strong request that emerged from the workshops was to have a library or toolbox of all the available digital tools that would help them answer the questions in the three-pillar model. According to some of the attendees, each tool should be easy to learn, perhaps through a tutorial and documentation. Furthermore, they suggested allowing the toolbox to be customised with additional tools that could be added as needed. The project team also arranged a number of field visits to present the prototype and get initial reactions from editorial managers in the media companies. While the general reactions were of interest and curiosity, there was an apparent degree of scepticism about the tool’s usability within newsrooms as well as towards the notion of sharing work with journalists in other media. In one particular meeting, the discussion about 10 M. PICHA EDWARDSSON ET AL. the prototype was highly critical, with the editorial managers taking the position that the prototype was not actually what was needed. Instead, they suggested creating a tool to discover disinformation that is poised to go viral. Since this was beyond the scope of the project, we did not pursue it. After this initial step, the developers created a prototype online application. The pro- totype was based on the assumption that for every single piece of online content, there was at least one digital medium where that content was published or shared, and there was at least one source that had actively put that content – as the message – on the Figure 2. The process that testers follow to add and review entries in FCA. JOURNALISM PRACTICE 11 medium. Hence, the role of the prototype was to guide journalists through the verification process by evaluating the reliability of the medium, trustworthiness of the source, and val- idity of the content. Through the prototype, journalists would be able to add specific claims to fact-check. Each claim would have a reference to the medium, where it was pub- lished along with the source that published it. Additionally, the prototype was meant to harness Web 2.0-based interactivity by allow- ing multiple journalists to verify and cross-validate the same claims. This approach, which relies on the wisdom of the crowd, has proven to be powerful in improving the reliability of the outcome, particularly if those involved in the process are vetted in advance (Kittur and Kraut 2008; Ullrich et al. 2008). We chose the scalable Ruby on Rails development framework to develop the pro- totype given that it is a proven efficient open-source web development framework with a structure that adheres to object-oriented programming. Due to its multi- layered functional structure with the Model, View, and Controller components, it was easily scalable and could serve as a base for integrating new services with ease (Bächle and Kirchberg 2007). We also chose it for its speedy development Figure 3. An example of how an image-based claim is verified in FCA. 12 M. PICHA EDWARDSSON ET AL. process and high server performance in addition to its wide community and thorough documentation. Feedback and Testing in the Newsrooms In April 2019, the first version of the prototype was completed and ready for testing. Initially, the testing included students at the university’s journalism department, and in May 2019 a workshop was held to give journalists from the Stockholm-based media a hands-on trial. During this workshop, the participants made important com- ments on various functions, and the developers subsequently revised the prototype in order to be more extensively tested. During the development stage, three trainees who participated in the earlier workshops volunteered to be test pilots” for the proto- type and provide feedback. Later, another five journalists with different backgrounds than the nine Stockholm-based media companies also tested and evaluated the prototype. The test sessions and evaluations were recorded, and the test persons’ reactions and comments were collected and analysed. Semi-structured interviews were undertaken with journalists in the newsrooms as a follow-up of the test sessions and evaluations. The analyses of the workshop material aimed to discern and organise emerging patterns and themes in the re","Picha Edwardsson, M., Al-Saqaf, W., & Nygren, G. (2021). Verification of Digital Sources in Swedish Newsrooms—A Technical Issue or a Question of Newsroom Culture?. Journalism Practice, 1-18."
SCOP_149,Fighting deepfakes: Media and internet giants’ converging and diverging strategies against hi-tech misinformation,"Deepfakes, one of the most novel forms of misinformation, have become a real challenge in the communicative environment due to their spread through online news and social media spaces. Although fake news have existed for centuries, its circulation is now more harmful than ever before, thanks to the ease of its production and dissemination. At this juncture, technological development has led to the emergence of deepfakes, doctored videos, audios or photos that use artificial intelligence. Since its inception in 2017, the tools and algorithms that enable the modification of faces and sounds in audiovisual content have evolved to the point where there are mobile apps and web services that allow average users its manipulation. This research tries to show how three renowned media outlets—The Wall Street Journal, The Washington Post, and Reuters—and three of the biggest Internet-based companies—Google, Facebook, and Twitter—are dealing with the spread of this new form of fake news. Results show that identification of deepfakes is a common practice for both types of organizations. However, while the media is focused on training journalists for its detection, online platforms tended to fund research projects whose objective is to develop or improve media forensics tools.",Journalism Studies,Article,2021,Y,Y,4,"The implementation of artificial intelligence in techno- logically mediated communicative processes in the net- worked society poses new challenges for journalistic veri- fication. Simultaneously, it has enhanced different stages of news production systems. The effects of the technol- ogy that houses artificial intelligence are present both in the communicative flows and in a large part of the social- ization dynamics. Hence, the threat introduced by the emergence of deepfakes, doctored videos by using artifi- cial intelligence, arises as one of the most recent hazards for journalistic quality and news credibility. Although deepfakes are not only a concern for journalism, their existence has raised the uncertainty among users when trying to access news content. Likewise, the increasing sophistication of this form of fake news has put profes- sionals on alert (Vaccari  Chadwick, 2020). Misinformation has increased its relevance over the last few years, having now a major significance in the public agenda (Vargo, Guo,  Amazeen, 2018). In con- sequence, the number of projects and measures for counteracting this phenomenon has grown consider- ably. Example of this could be the Action Plan Against Disinformation developed by the European Commission (2018). Media and journalists are aware about how the Media and Communication, 2021, Volume 9, Issue 1, Pages 291–300 291 success of hoaxes undermines democracy and its relia- bility (Geham, 2017). Therefore, they try to react with actions that facilitate transparency and the fulfilment of their professional and ethical rules, like fact checking (Lowrey, 2017). This is an issue on which different lines of thinking have been opened. All of them try to counter- balance the result of misinformative political and social trends that became significative in 21st century societies (McNair, 2017) in a context where social media plays a central role as a space for the generation and dissemina- tion of fake news and the consequences that this entails (Nelson  Taneja, 2018). Techniques that guarantee the information verifica- tion’s efficiency—one of the core elements of journal- ism since its consolidation as a communicative technique in the modern age (Kovach  Rosenstiel, 2014)—are looking inside technological innovation for tools with the ability to support professionals in their daily tasks. It is true that the norms followed for producing accu- rate informative pieces are in some cases unclear and nuanced (Shapiro, Brin, Bédard-Brûlé,  Mychajlowycz, 2013). Nonetheless, journalism should not retain anti- quated verification techniques, but should rather update them to computational methods in order to evaluate dubious information (Ciampaglia et al., 2015). There are currently revamped verification systems with fact- checking techniques. Those may contribute to the elab- oration of news pieces that, after the application of a complex group of cultural, structural, and technological relations would show the legitimation of news in the dig- ital age (Carlson, 2017, p. 13). Although a high level of mistrust remains, some techniques used in these infor- mation verification services are able to build a bigger reliance by the users (Brandtzaeg  Følstad, 2017). Furthermore, this scenario has seen the emergence of new proposals for renewed professional practices and profiles (Palomo  Masip, 2020). This could be the case of constructive journalism, whose objective is regain- ing the lost trust of the media (McIntyre  Gyldensted, 2017). This is a journalistic movement that explores new paths. However, it will take time to see if these new approaches fit in the frame of emerging journalism in the Third Millennium, with a clear commitment to social ser- vice, transparency and accuracy. 2. Literature Review 2.1. Misinformation Through the Ages Falsehood, fantasy and fake news have walked along with the development of communication and journalism, initiating discussions about its practice and its role in soci- ety (McNair, 2017). Although there are evidence of misin- formation since the Roman Empire (Burkhardt, 2017), its major development took place with the invention of the print in the 15th century. The possibility of disseminat- ing written information in a faster and easier way made possible the circulation of falsehood too. Hence, the advent of new means of communication, increased the presence of deliberated false content, not always with harmful purposes. In this regard, one of the greatest examples of misinformation of our times was the radio broadcast of The War of the Worlds directed by Orson Welles on October 30th, 1938. That radio show was followed by thousands of listeners, and some of them believed that the Earth was under an alien attack, thanks to the narration of Welles’ cast (Gorbach, 2018). This radio show wanted to entertain the audience using an alteration of reality. However, manipulation of the truth has been used as a weapon in military conflicts over the centuries in order to ascribe malicious acts or characteristics to the enemy (Bloch, 1999, p. 182). A good example of this use of misinformation was the sequence of news published after the explosion in the boilers of the United States Navy ship USS Maine on February 15th, 1898. In the middle of the fight for being the most read against Joseph Pulitzer’s The New York World, William Randolph Hearst, editor of the New York Journal, sent a journalist to Cuba with the objective of telling the read- ers the details of a Spanish attack to this ship. Thus, when the correspondent arrived at the island reported that alleged attack did not exist. Nonetheless, the newspaper published a series of stories detailing the attack—even when they knew they were not accurate—causing a cli- mate of hate against Spain and acceptance of the coming war. Finally, The United States declared the war against Spain (Amorós, 2018, p. 34). After this conflict, misinfor- mation continued to be used against the enemy in war times. Thus, it is possible to identify strategies of its use in recent conflicts like the World War I and World War II, the Vietnam War or the Gulf War (Peters, 2018). 2.2. Fake News as a Threat to Journalism Falsity has cast a shadow over the discipline of commu- nication throughout history. One of the newest forms of misinformation is fake news, pieces that imitate the appearance of journalistic information, but deliberately altered (Rochlin, 2017). This form of deception has coex- isted with true news. However, the current communica- tive scenario, marked by the utilisation of high speed and low contrast means of communication—and among all social media—provides a fertile soil for the dissemina- tion of any form of misinformation (Lazer et al., 2018). Platforms like Facebook or Twitter are now among the primary news sources for Internet users (Bergström  Jervelycke-Belfrage, 2018). Fake news producers are aware of this fact. As a result, they have made the web the main channel for false content distribution, taking advantage of the possibility of communicating anony- mously provided by certain spaces (Vosoughi, Roy,  Aral, 2018). Furthermore, fake news producers have the chance of reaching as large audiences as consol- idated journalistic brands (Fletcher, Cornia, Graves,  Nielsen, 2018), which makes the verification of this fal- sities more difficult. Media and Communication, 2021, Volume 9, Issue 1, Pages 291–300 292 During the last few years, there have been differ- ent proposals for classifying fake news. Among them, the one developed by Tandoc, Lim, and Ling (2018) is perhaps the most exhaustive: news satire, a very common form of fake news with a large presence in magazines, websites and radio or TV shows; news par- ody, which shares some of the characteristics of news satire, but it is not based on topical issues. These pieces are fictional elements specifically produced for certain purposes; news fabrication, unfounded stories that try to imitate the structure of news published by legacy media. The promoters of these pieces try to deceive by blending them among the truthful ones; photo manipulation—alteration of images—and more recently videos—for building a different reality; adver- tising and public relations—dissemination of advertis- ing by masking it to look as journalistic reporting; and propaganda—stories from political organizations with the objective of influencing citizens’ opinion on them. Like some of the previous ones, they imitate the formal structure of news pieces. Regarding its formal structure, fake news try to imi- tate news items’ formal appearance. Thus, visual codes and elements like headlines, images, videos hypertext and texts conceived like journalistic pieces are common features of this misinformation strategies (Amorós, 2018, p. 65). Nonetheless, its major particularity is that fake news tries to attack the readers’ previous opinion, espe- cially on controversial issues related to racism, xeno- phobia, homophobia and other forms of hate (Bennett  Livingston, 2018; Waisbord, 2018). This connection makes possible the rapid replication of such content thanks to the ease of sharing through spaces like social media platforms. Thus, episodes like electoral processes (Lowrey, 2017), or more recently the Covid-19 pandemic (Salaverría et al., 2020; Shimizu, 2020), resulted in a deep growth of fake news circulation, at times using sim- ple methods but at times taking advance of the most advanced technology. 2.3. Deepfake: A Novel Form of Fake News Deepfakes, a combination of ‘deep learning’ and ‘fake’ (Westerlund, 2019), are “highly realistic and difficult-to- detect digital manipulations of audio or video” (Chesney  Citron, 2019). It can be defined as “a technique used to manipulate videos using computer code” (Fernandes et al., 2019, p. 1721), generally replacing the voice or the face of a person with the face of the voice of another per- son. Although the photo and video manipulation have existed for a long time, the use of artificial intelligence methods for these purposes has augmented the number of fakes and its quality. Some of these videos are humor- ous, but the majority of them are damaging (Maras  Alexandrou, 2019). Hence, this is a recent movement whose beginnings date back to 2017, starting then a rapid popularisation until now (Deeptrace Labs, 2018, pp. 2–4). This technique is the result of using Generative Adversarial Networks, algorithms designed to replace human faces or voices in thousands of images and videos in order to make them as realistic as possible (Li, Chang,  Lyu, 2018). The main advantage of these algorithms is that these systems are learning how to improve them- selves by creating deepfakes. Therefore, future creations will be improved thanks to past experiences. This fea- ture makes this misinformation procedure more danger- ous, especially due to the emergence of mobile apps and computer programmes that allow users without computer programming training to produce deepfakes (Nirkin, Keller,  Hassner, 2019; Schwartz, 2018). Farid et al. (2019, pp. 4–6) tried to label the different forms adopted by deepfakes in four categories: 1) face replacement or face swapping—this method involves changing one person’s face, the source, for another one, the target; 2) face re-enactment—manipulation of the features of the features of one person’s face like the movement of the mouth or the eyes, among others; 3) face generation—creation of a completely new face using all the potential provided by Generative Adversarial Networks; and 4) speech synthesis—alteration of some- one’s discourse in terms of cadence and intonation, or generation of a completely new one. As with other technologies, the same algorithms used for creating deepfakes could have a beneficial appli- cation in the field of psychology, building digital syn- thetic identities for voiceless users; or in robot sketches through advanced facial recognition for law enforce- ment, for example (Akhtar  Dasgupta, 2019; Zhu, Fang, Sui,  Li, 2020). Notwithstanding, its use seems to be more harmful than beneficial nowadays with examples of the use of these technologies in acts of fraud and crime (Stupp, 2019). Hence, one of the biggest challenges of deepfakes is to find out how to counteract them knowing that the debunking methods’ development is always late regard- ing the production of misinformation (Galston, 2020). However, a great deal of effort has been made—and is still made—to develop technology-based tools for detecting and correcting it, both from public and private organizations (Deeptrace Labs, 2018, p. 2). These tools will be helpful in almost all areas of communication, espe- cially for journalism. 2.4. Fact-Checking: Journalism’s Response to the Misinformation Wave In light of the above, verified information seems to be a necessity in our communicative context (Ekström, Lewis,  Westlund, 2020), especially because disruptive episodes like the coronavirus outbreak resulted in a clear increase of citizens’ informative consumption (Masip et al., 2020). Furthermore, political communication has shifted to a model in which political leaders share their messages online instead of doing it through traditional media (López-García  Pavía, 2019). Media and Communication, 2021, Volume 9, Issue 1, Pages 291–300 293 At this juncture, the media has increased the impor- tance of verification processes for correcting both inter- nal and external errors (Geham, 2017). Consequently, a new professional profile—the fact-checker—has emerged with the mission of debunking misinformation and prevent audiences of its consumption. These profes- sionals try to go to the origin of an information or a claim for gathering all the available data and contrasting it (Graves, 2016, p. 110). Fortunately, journalists have also benefited from the development of new technological tools designed for verifying images, videos or websites in an efficient manner (Brandtzaeg, Lüders, Spangenberg, Rath-Wiggins,  Følstad, 2016). Although verification has always been part of any journalistic process, the rapid growth of the fake news phenomenon over the past few years made this activity more important than ever. Thus, the census created by the University of Duke Reporters’ Lab counts now almost 300 fact-checkers in more than 60 countries by the by middle of 2020, a hundred more than on the same date in 2019 (Stencel  Luther, 2020). Regarding this, it is pos- sible to talk about fact-checking as a transnational move- ment (Graves, 2018) were both legacy and independent media organizations try to restore the trust lost by the media (Bennett  Livingston, 2018). 3. Method The starting point of this research will be the appli- cation of the Systematic Literature Review method (Kitchenham, 2004) as a method to set an approach on how deepfakes are being addressed and studied. Due to the novelty of this reality, this method will let us understand in an exhaustive way (Codina, 2017) what are researchers doing to assess this phenomenon and what efforts are being done to stop its spread. Hence, our method consisted in the following phases: 1) topic identification—‘deepfake’ and ‘deep fake’— and the period of analysis—all the available literature; 2) source selection—Web of Science’s SCI-Expanded, SSCI, CPCI-S, CPI-SS, CPCI-SSH, and Scopus; 3) search in databases—the selection of Web of Science and Scopus is justified by the importance of these two databases, which contain the most relevant contributions for the Social Sciences field in general and deepfakes specifi- cally; and 4) identification of the studied variables for each item—descriptive data (article title, date, jour- nal or conference, number of authors, and keywords), type of study, research techniques (observation, survey, interview, content analysis, case study, experimental or non-specified), principal contribution, DOI or URL, and institution and country. This search resulted in 54 different research items: 28 presented at international conferences and 26 pub- lished in academic journals—all of that after deleting duplicities and texts that did not fit the criteria, such as editorial articles, call for papers, or interviews, among others. These 54 examples comprise our sample that will be addressed in the next section in order to understand the path followed by researchers on this subject. Concerning the second stage of our study, it will analyse the approach taken by three renowned media outlets and news agencies—The Washington Post, The Wall Street Journal, and Reuters—and three of the most important Internet platforms—Google, Facebook and Twitter—in neutralizing the spread of deepfakes. Thus, case study of these six organizations will be applied in order to understand how they are managing to identify, label and notify deepfakes through differ- ent approaches—protocols, use of technology, collabora- tion with institutions, and funding of innovative projects. This will be done through the analysis of the available reports and statements of these six organizations. Consequently, the main goal of our study will be to identify the coincidences and disparities in the strat- egy of three major media outlets and three of the most important online platforms when trying to stop the diffu- sion of deepfakes. This will be relevant in order to under- stand if six of the main representatives from these two communicative fields are joining efforts and strategies in limiting or not its spread, and how these procedures could be improved. 4. Findings 4.1. Results of the Systematic Literature Review We will start by depicting the state of the research on deepfakes, especially the contributions indexed in the two main databases—Web of Science and Scopus. As shown in Table 1, research about this issue started in 2018 with four conference papers. However, it was quintupled in 2019, and during the first half of 2020 almost a half more of works on deepfakes than the previous year were published. Furthermore, the most salient element of this table is that this form of fake news used to have presence at conferences, but in 2020 they become a topic addressed in academic jour- nals too. Nonetheless, it is necessary to note that the situation resulting from the Covid-19 pandemic Table 1. Evolution of the studies on deepfake indexed in WoS and Scopus. Year Conference paper Journal article Total 2018 4 0 4 2019 16 5 21 2020 (1st half) 8 21 29 Media and Communication, 2021, Volume 9, Issue 1, Pages 291–300 294 has provoked the cancellation or postponement of many conferences. Regarding the authorship of this research, the most common approach is the participation of three authors. Thus, the arithmetic mean—3,13 authors—and the mode—14 articles have three authors—serve to confirm this. Also concerning the authorship, researchers from 24 countries were identified, most of them located in the United States and Asia—China, Japan, South Korea, India, or Taiwan. Finally, this review shows a large degree of unifor- mity concerning the type of studies published on deep- fakes. Almost all the reviewed articles and conference papers take a descriptive approach. This is because 32 of the items are the result of experimenting with new tools and algorithms to counteract it. Another important group of research is review articles on deepfake detec- tion and prevention or even about legal framework and legal concerns of this form of misinformation, something that was found 21 times. In sum, the novelty of deepfake implies a certain degree of youth for its research. At present, it is possi- ble to see two trends: Studies that present new forms to stop its spread, or studies that try to create context on its emergence and development. 4.2. Counteracting Deepfakes at The Wall Street Journal, The Washington Post and Reuters Recent advances in artificial intelligence and their democratisation have allowed average users to create deepfakes. This represents a major challenge for our soci- ety due to the potential harmful impact of these cre- ations, especially before electoral processes. Looking to the United States 2020 general election, The Wall Street Journal has created a division of 21 journalists whose unique objective is detecting, labelling and debunking misinformation, particularly deepfakes (Southern, 2019). This team is a joint effort of Standards  Ethics and RD departments, and this work is very linked to the use of technology with presence of journalist with video, photo, visuals, research and news experience that have been trained for deepfake detection (Marconi  Daldrup, 2018). Furthermore, The Wall Street Journal provides specialized training in fake news and deepfake identi- fication in partnership with different researchers. This has led to the development of a protocol to find exam- ples of this kind of misinformation with three stages: source examination (contact with the source, author- ship identification, and metadata check, among others), search for older versions of the footage available online, and footage examination with video and photo edit- ing programs. Meanwhile, The Washington Post has applied to deepfake detection very similar criteria to other fake news detection. Thus, the The Washington Post has added video experts to the tasks developed by the team led by Glenn Kessler—also known as ‘The Fact Checker’ (Kessler, 2019). The most important contribu- tion of this publication regarding this problem is the elab- oration of a taxonomy to classify and label deepfakes. The Washington Post was also pioneering in the use of scales to highlight the degree of truth and lie of any content. Regarding doctored videos, the newspaper sets out three categories of manipulation (Ajaka, Samuels,  Kessler, 2019): missing context (presentation of the video without context or with a context intentionally altered), deceptive editing (rearrangement and edition of the video in certain parts or details), and malicious transfor- mation (complete manipulation and transformation of the footage resulting in a completely new fabrication). A third approach to this reality could be the one adopted by the news agency Reuters. The news services provider reports its awareness and concern on the deep- fake spread (Crosse, n.d.). Hence, it has started a collab- oration with Facebook for detecting as much doctored user-generated content as possible among all the videos and photos that run on the platform (Patadia, 2020). In this regard, Reuters has started a blog whose objec- tive is verifying doctored materials in English and Spanish. All of that with the objective of debunking as much infor- mation as possible ahead of the 2020 United States elec- tion. This is a clear example of the emerging collabo- ration among technological platforms and the media, a joint effort in trying to stop the rapid growth of fake news and deepfakes in such significant moments like a presi- dential run-up. 4.3. Internet Giants’ Strategies Against Deepfakes The spread of falsehood through social media plat- forms and other Internet spaces is now a challenge for providers like Google, Facebook or Twitter. As a result, over the last few months they have started different initiatives whose unique objective is finding efficient ways to detect and stop the misinformation and, more recently, deepfakes. Regarding this, these three companies show differ- ent approaches against this matter. Google, for instance, has made available to the research community a large set of manipulated and non-manipulated videos (Dufour  Gully, 2019). With this initiative, they want to help in the development of identification techniques by taking advantage of the great amount of information saved in their files. In addition, they collaborate with the Defense Advanced Research Projects Agency to fund different researchers that are developing media forensic tools. On the other hand, Facebook is financing differ- ent research projects within its ‘Deepfake Detection Challenge.’ This initiative, boosted by companies like Facebook, Microsoft and Amazon Web Services and research units from various universities across the United States, tries to assist researchers that are work- ing on the development of artificial intelligence-based deepfake detection tools. Thus, a corpus of more than 100,000 videos was available to these researchers that Media and Communication, 2021, Volume 9, Issue 1, Pages 291–300 295 fight for presenting useful mechanism in order to win dif- ferent awards. Furthermore, Mark Zuckerberg’s social network tries to counteract this form of misinformation by deleting doctored videos or photos, or labelling it as fake news with the help of fact-checking media outlets (Bickert, 2020). This is particularly important for those related to the 2020 United States run-up due to the influence that fake news could have in this process. Finally, Twitter shows a simpler approach towards this problem. They summarize their strategy in the fol- lowing four rules (Harvey, 2019): Identification through a notice of Tweets with manipulated content, warning of its manipulated condition before sharing it, inclusion of a link to news articles or other verified sources in which users can find out why and how the content has been doc- tored, and elimination of all that manipulated content potentially harmful or threatening to anyone’s safety. These diverging strategies on behalf of the major online platforms are in part the product of self-regulated methods for fighting deepfakes, as there is still incipient intervention on behalf of the states in regulating con- tent on social media and other outlets. The question to be asked here is whether it is the online platforms’ sole responsibility to tackle misinformation or if there are any social interests in this situation for which other public entities should allocate resources to. The European Commission already pointed out in 2018 the need for governments to invest in research and detection of misinformation, while also prompt- ing these to hold social media companies account- able (European Commission, 2018). So far, in the last two years the EU has launched a series of initiatives to tackle the issue: a code of practice against dis- information, the creation of the Social Observatory for Disinformation and Social Media Analysis and the set-up of the Rapid Alert System, among other RD projects such as PROVENANCE, SocialTruth, EUNOMIA or WeVerify (European Commission, 2020). Despite a lot of efforts being made to avoid the spread of mis- information in the EU, deepfakes are still not as much on the agenda as other academics are asking for, while also describing their worry for seemingly understaffed programs (Bressnan, 2019). Measures taken by coun- tries to prompt social media companies in acting against fake news contain different levels of intervention and are mainly dedicated to counteracting disinformation in political advertisement. France and Germany, for exam- ple, require online platforms to establish an easily acces- sible and visible way for users to flag false information, while Australia requires all paid electoral advertising, including advertisements on social media, to be autho- rized and to contain an authorization statement (Levush, 2019). In the United States, some states have already taken specific measures to counter deepfakes, although these are still merely reactive and not preventive, such as Texas passing a law that criminalizes publishing and distributing deepfake videos with the intention to harm a candidate during the electoral process; or California, where a law was passed last October making it illegal for anyone to intentionally distribute deepfakes for deceiv- ing voters or perjure a candidate (Castro, 2020). The implications of these incipient interactions between governments and social media companies might have relevant governance questions in the forth- coming years, all the while these companies are also starting to take new approaches to their governance structures, such is the case of the Facebook, who set up the Independent Oversight Board, which “aims to make Facebook more accountable and improve the decision- making process,” in the words of Nick Clegg, currently Facebook’s VP of Global Affairs and Communications and former Deputy Prime Minister of the United Kingdom (Moltzau, 2020). 5. Discussion As shown in the previous section, the media and Internet platforms have initiated different strategies to fight mis- information and, more particularly, the spread of deep- fakes. In this regard, there are some similarities and dif- ferences among the strategies of these two communica- tive sectors. First of all, it seems clear that the collaboration among platforms and media outlets increases over time. Example of this could be the agreements among Reuters and Facebook whose objective is to detect fake news and share its correction. Furthermore, other fact-checking organizations collaborate with this social network in labelling false content and warn users about this. Another coincidence is the use of technology as a weapon in the battle against deepfakes. Both news media and digital platforms have understood that high technology and the use of algorithms as powerful as those used for creating fakes is the only chance to coun- teract them. Thus, media outlets are increasingly train- ing journalists and interdisciplinary teams in the use of these mechanisms that allow them to identify this form of misinformation. The third match could be the growing synergies between the academic and communicative sides. Thus, media outlets and platforms try to collaborate with researchers and institutions specialized in fake news detection, both in training and to apply their methods. Regarding the divergences when dealing deepfakes, online platforms are able to fund research projects whose objective is developing artificial intelligence-tools for identifying this form of fake news. The media, how- ever, does not have such possibilities due to the expen- diture of these activities. Another difference in dealing with this issue could be that the media use to correct misinformation instead deleting it. As shown before, some of the social media platforms have the elimination of doctored content among their strategies. This presents a clear challenge. Although deleting manipulated videos or photos ends Media and Communication, 2021, Volume 9, Issue 1, Pages 291–300 296 with the problem for future or potential users, does not for those users that have seen them. In contrast, labelling these materials as false or manipulated—the approach followed by verification media outlets—could be helpful for future users. 6. Conclusions Deepfakes have become a reality in our communicative system. Media outlets and Internet services providers try to counteract it with different outlooks. However, the development of the techniques for producing misinfor- mation seems to advance faster than those for debunk- ing it. Regarding this, the available research is mainly focused on two aspects: On one hand detection tools, and on the other hand, the implications of this form of fake news for democracy and national security. The fact that so far only big technological giants are capable of introducing hi-tech expensive solutions for fighting deep- fakes motivates that the available mass of research on the subject is fundamentally dedicated to address the questions raised by these corporations which are mainly technological. On the other hand, journalism focused media, which are not able to invest large amounts of money on deepfake detection are therefore unable to push their concerns into the research agenda. For this reason, producing research on the implications of deep- fakes for journalism and under journalistic premises presents itself as elemental, as well as further investiga- tion on how the media trains its professionals for detect- ing advanced misinformation. The novelty of this deceiving technique provokes its understudied situation, but the constant growth of works on this matter show that it will be an important field for researchers on misinformation and media foren- sics in the following years. However, the study is able to show to some extent that the media and digital plat- forms’ have notable similarities and differences when it comes to their strategies. This could be due to the dif- ferent nature of their business models, but neverthe- less sometimes it seems to be a matter of investment. Digital platforms have joined efforts with technological, academic or entrepreneurial partners, spending large amounts of money in this field, which is something that many medi","Vizoso, Á., Vaz-Álvarez, M., & López-García, X. (2021). Fighting deepfakes: Media and internet giants’ converging and diverging strategies against Hi-Tech misinformation. Media and Communication, 9(1), 291-300."
SCOP_163,News authentication and tampered images: evaluating the photo-truth impact through image verification algorithms,"Photos have been used as evident material in news reporting almost since the beginning of Journalism. In this context, manipulated or tampered pictures are very common as part of informing articles, in today's misinformation crisis. The current paper investigates the ability of people to distinguish real from fake images. The presented data derive from two studies. Firstly, an online cross-sectional survey (N ¼ 120) was conducted to analyze ordinary human skills in recognizing forgery attacks. The target was to evaluate individuals' perception in identifying manipulated visual content, therefore, to investigate the feasibility of crowdsourced validation”. This last term refers to the process of gathering fact-checking feedback from multiple users, thus collaborating towards assembling pieces of evidence on an event. Secondly, given that contemporary veracity solutions are coupled with both journalistic principles and technology developments, an experiment in two phases was employed: a) A repeated measures experiment was conducted to quantify the associated abilities of Media and Image Experts (N ¼ 5 þ 5) in detecting tampering artifacts. In this latter case, image verification algorithms were put into the core of the analysis procedure to examine their impact on the authenticity assessment task. b) Apart from conducting interview sessions with the selected experts and their proper guidance in using the tools, a second experiment was also deployed on a larger scale through an online survey (N ¼ 301), aiming at validating some of the initial findings. The primary intent of the deployed analysis and their combined interpretation was to evaluate image forensic services, offered as real-world tools, regarding their comprehension and utilization by ordinary people, involved in the everyday battle against misinformation. The outcomes confirmed the suspicion that only a few subjects had prior knowledge of the implicated algorithmic solutions. Although these assistive tools often lead to controversial or even contradictory conclusions, their experimental treatment with the systematic training in their proper use boosted the participants' performance. Overall, the research findings indicate that the scores of successful detections, relying exclusively on human observations, cannot be disregarded. Hence, the ultimate challenge for the verification industry” should be to balance between forensic automations and the human experience, aiming at defending the audience from inaccurate information propagation.",Computer Science,Proceeding,2020,Y,N,1,"Audiovisual records have been always considered as a more trusted representation of reality than written stories, resulting in the doc- umenting role of images, utilized as evidence to convince readers about news authenticity. However, the rapid evolution of Information and Communication Technologies (ICTs), along with the massive content production and distribution facilities, offered new multimedia editing and processing capabilities, which could be easily applied by ordinary individuals. In light of the above events, controversial and unverified User Generated Content (UGC) has overwhelmed Internet and Social Networking Sites (SNSs) (Jahnke and Kroll, 2018). Moreover, while the audience has always been seeking direct access to visual scenes of the events (Huxford 2001; Mitchell, 1994; Pantti and Sirén, 2015), their ability to distinguish between real and fake/doctored pictures has not been developed, thus resulting in frequent misbelief errors and, overall, disinformation vulnerabilities (Griffin et al., 2018). To deal with the unwanted misinformation phenomenon, the domain of Digital Forensics (DF) has emerged, forming the so-called Verification Industry”. The latter is associated with a multilevel and highly multi- disciplinary effort, involving debunking sites, experts among the impli- cated fields, specialized tools, academic individuals, institutes and consortiums that attempt to propose feasible solutions, through their collaboration in every-day practice and/or within featured research projects (Katsaounidou et al., 2018; Katsaounidou fi Dimoulas, 2018a, 2018b). Clearly, the detection of tampering trails, indicating possible content manipulation, is crucial in today's cross-validation practices, which are essential to news corporations. In this direction, various ap- proaches can be deployed, taking advantage of both human expertise and the offered algorithmic solutions, thus forming dedicated techniques for evaluating the different types of news-reporting media assets (i.e. text, images, audio, and video). Given that journalism is heading towards the era of heightened automation, machine-driven verification assistance could accelerate the involved authentication tasks, reducing the spread of manipulated articles (Katsaounidou and Dimoulas, 2018b). As already implied, the current work focuses on the case of image documents, analyzing the ways relevant alterations can be detected. While subjective observations and related experience are very helpful in this process, the implementation of fully- and semi-automatic inspection tools is considered equally or even more important (Krawetz and Hacker Factor Solutions, 2007; Silverman, 2013; Katsaounidou and Dimoulas, 2018b). The emphasis on the visual part is justified on the extensive use of photographic news-evidence material throughout the years of jour- nalistic cross-validation, as well as on the progress of the Digital Image Forensics (DIF) sector, which poses increased maturity over the other sub-domains, like the ones dealing with audio and video modalities (Katsaounidou et at., 2018; Vryzas et al., 2018, 2019). More specifically, automated solutions try to algorithmically indicate abnormal informa- tion” that human observation would/should have detected, e.g. lack or presence of (un)fitting shadows and other image reflections (Farid, 2009; Johnson and Farid, 2007; Krawetz and Hacker Factor Solutions, 2007). In semantic level, the retrieval of near-duplicate photo material, used in a different time, location and/or thematic context, could reveal possible misinformation attempts. Pixel-wise, the so-called content-based” methods examine the structural attributes of the picture data (and met- adata), aiming at detecting encoding inconsistencies that could be asso- ciated with forgery attacks (Farid, 2009; Ho and Li, 2015; Thakur, 2014; Wang, 2009; Katsaounidou et at., 2018; Katsaounidou and Dimoulas, 2018b; 2018a). Nowadays, with the advent and the vast progression of deep learning algorithms, a new category of computer-generated fake photos has emerged, taking advantage of recent Generative Adversarial Networks (GANs) and Convolutional Neural Network (CNN) architectures (Hsu et al., 2020; Hsu et a1., 2018; Hulzebosch et al., 2020; Marra et al., 2018; Yu et al., 2019). The so-called deep fake images and videos have started worrying about the scientific community, in a similar way that tradi- Hemgon 6 (2020) e0S808  which are treated as a whole new approach in our view as well. The same applies for the latest GAN image generation techniques, that are not treated as standard DIF cases in related publications (Gok hole ct al., 2020; Katsaounidou ct al., 2018; Katsaounidou, Vrysis, Kotsakis, Dimoulas  Veglis, 204 9a; Km tsaouni doc, Vryzas, Kotsakis  Dimoulas, 2019b; Zhcng ct a1., 2019). Furthermore, fake and deep fake facial im- ages are dealt as critical in various domains and multidisciplinary research activities (Hsu ct al., 2018; Hulzcbosch ct al., 2020; Tariq ct at., 2018). However, relevant works do not underrate traditional image manipulation techniques (and their tampering detection counterparts) that can be applied by broader users’ categories, i.e., non-experts in machine/deep learning (Hulzcbosch ct a1., 2020; Katsaounidou, 2020; Tariq et a1., 2018; Thakur and Rohilla, 2020). Overall, without holding the maturity and stability of traditional validation practices, these latest trends cannot be treated jointly, equally, or under the same assumptions with the preceding ones, so that they are not considered in the primary focus of the current research. No doubt, significant research progress has been observed in the DF and DIF fields during the last decades. On the contrary, a limited number of relevant tools has been developed to be used in every -day practice of media professionals, or even to assist end-users in their own informing veracity needs. For instance, the Crowd Crafting and Verily websites are aligned with the human factor approach to infer conclusions (Sil- verman, 2013). Other tools have been designed to reveal users' identity by detecting the authenticity of a profile on social networks (e.g., the WebMii⁴ and Pipl^ platforms). Findexifi and Jeffrey's Exif Viewer’ are online environments, facilitating the inspection of inconsistencies in the Exif picture metadata (from the acronym Exchangeable image file format, i.e. information related to features or settings of camera sensors, location-/time-tags, etc.). Content-based image analysis algorithms have also been implemented and are offered as online services to test various falsification scenarios (I€atsaounidou et a1., 2018, 2019a, 2019b; Hat- saounidou  Dimoulas, 2018a, 2018b; Katsaounidou, 2020; Middleton et a1., 2018; Zampoglou et at., 2016). Among others, the current research focuses on the evaluation of the offered capabilities and the anticipated helpfulness of such verification assistants, therefore further technical and functional insights are given next. As already implied, the progress in the availability of such tools is limited, compared to the associated research findings, which is further deteriorated when it comes to contemporary machine and deep learning automations. The latter approaches are practically available only to technologists and researchers involved in these fields. Based on conducted literature review, related papers are constantly produced and published, studying the different image tampering and manipulation detection techniques, both traditional and contemporary (Gol‹hale et al., 2020; Qureshi and El-Alfy, 2019; Tariq et a1., 2018; Thakur and Rohilla, 2020; Zhen g et a1., 2019). However, little effort is given to the adoption and evaluation of the associated tools and services from users' perspectives. In many views, this hysteresis of the practice over research deteriorates the true potentials of the algo-  tional DIF methods were the subject of laborious research efforts within rithmic approaches (Gloe et a1., 2007; Katsaounñidou Dimoulas, 2018a,  the last two decades (Hsu et a1., 2020; Katsaounidou et a1., 2018; Kat- saouni doc, 2020). In this context, a new era of DIF has just begun, which, according to many researchers, is significantly differentiated from traditional approaches. Hence, photos are digitally synthesized as fakes in the first place within this modern paradigm of visual forgery, which, theoretically speaking, does not actually fall into the typical image tampering chain. For similar reasons, most classical DIF tools cannot sufficiently contribute to the detection of such manipulation trails, because they exploit different inconsistency inspection mechanisms. Fortunately, the deployment of these practices requires technological knowhow that ordinary users do not pose, thus confining the production and dissemination of such misinformation streams to the experts (Gokhale et al., 2020; Hsu et al., 2018, 2020; Katsaounidou, 2020; Thakur and Rohilla, 2020). Based on the above, it is not coincidental that recent review papers, studying real-world photo tampering scenarios (and their detection), usually omit deep fake cases (Zheng et al., 2019), 2018b Katsaounidou, 2020; Williams et al., 2018). Hence, further in- sights concerning the perception and utilization of real-world DIF veri- fication assistants are pursued within this work.  7. I. Digital Image Forensics software  In general, DIF approaches fall into two major categories (Gloe et al., 2007). In the first one, the purpose is to read or extract the semantics tags  h ttps://crowclcrafting.org/. https://vert.ly/. http:    com/. https://pip1.com/. http://www.findexif.con/. h tt p://exif.regex.info/exif.cgi.  2  A.N. Katsaounidou ct aL  of a digital photo, i.e. the capturing time, date and location. For instance, Exif viewers are used for retrieving the relevant picture metadata, if they are available in the implicated encoding format. In the second category, the goal is to automatically detect altering trails, which could be linked to potential tampering attacks. Various strategies are deployed, usually relying on image processing and multi-modal decision making (i.e., through multiple comparison rules, applied to various visual features), as well as with the help of more sophisticated machine learning algorithms. In the latter example, the aim is to classify the different patterns of data coherence or inconsistency, implying authenticity or manipulation, through the process of learning by example” (Korus, 2017). Overall, the mortality degree of automated image verification tools is quite high; hence, applications that were available for use in the previous year are no longer in operation. Table 1 lists the consistently available services of the last four (4) years, which were considered candidates of our investiga- tion. This selection was made, bearing in mind that the ultimate chal- lenge for the verification industry” should be to balance between forensic automations and the human experience, parallelly cultivating media literacy in fact-checking practices. On the contrary, fully automated deep-learning approaches (GAN, CNN, etc.) are generally blind” to the users, usually treated as black boxes, without truly extending visual inspection skills. Though research efforts to physically interpret the contribution of these layered archi- tectures are currently very active, clearly such extensions are not suited in the working scenario. Accordingly, it was decided to include only first- generation DIF tools with an investigative character, concerning the interpretation of the exposed inconsistencies. The right order of assess- ment (i.e., you cannot proceed to evaluate the latest/unstable solutions without studying their preceding/mature technologies first) and the availability of these specific ready-to-use services, directed this real- world scenario explicitly. Therefore, except for the available web appli- cations, software modules and code on online repositories (e.g., GitHub) were excluded because of the absence of related interfaces, which occupy a central place in the present study. Finally, the idea of jointly evaluating multiple verification assistance means (both classical and contemporary) was discarded from the early beginning of the projects. Such a configu- ration would have created difficulties in the participants' guidance and training capacities, weakening the significance of the observations (which was somewhat experimentally supported afterward, based on the received comments). From the listed utilities of Table 1, the Image Verification Assistant (IVA) toolset (Fi pure 1) and the clone detection tool” of the Forensically platform were chosen for serving the needs of the current experimental procedure, as discussed above. The selection of these specific environ- ments was decided due to their online nature (they are publicly acces- sible to everyone through the Web) and the user-friendly character of the associated Graphical User Interfaces (GUIs). Both platforms feature multitudes of image integrity detection techniques, including the most popular evaluation principles, i.e. noise pattern analysis, inspection of compression/quantization block inconsistencies, reverse image search (to detect near-duplicate pictures), metadata retrieval (GPS/Geolocation, Exif, etc.) and others (Katsaounidou, 2016; Katsaounidou ancl Dimoulas, Hemgon 6 (2020) e0'S808  20J 8b). Furthermore, it is justified the interactive way of extracting tampering heatmaps, indicating the regions with potential forgery trails, while also incorporating various reporting options to summarize the monitoring results. An overview of the available IVA services and their involvement in the different doctoring scenarios is provided in Figure 1. Based on this diagram, some methods are best suited for detecting object-wise” operations and others for image-wise” manipulations (Katsaounidou ct al., 2018). Figure 2 presents an example of the Double JPEG Quantization (DQ)” algorithm in a typical copy-move scenario (object cloning) (Kat- saounidotet at., 2018, p. 121). The DQ technique evaluates the statistical inconsistencies of the underlying encoding to decide whether there are parts of the image with different compression levels, revealing possible regions of pixels from a different photo. Thus, if the original document is compressed at a level A, the duplication and insertion of an object might result in different visual dynamics (i.e., the blocks of the processed area might have lower or higher quality B, depending on the applied compression thresholds). Forensics algorithms like the DQ approach can identify the processing trails, illustrating these spatial variations with the help of comparison colormaps, like the one presented in Figure 2 (Kat- saounidotet al., 2018; Katsaounidou and Dimoulas, 2018b). In the current example of Figure 2, the forensic analysis outcome offers a somewhat clear indication of the region that processing has occurred. However, in most of the cases, the interpretation of the extracted heatmaps is not such easy or self-evident, even for domain specialists that are aware of the underlying algorithmic principles (hat- saounidou, 2016; Katsaounidou and Dimoulas, 2018b). The ability of ordinary users to comprehend photo forensics and especially to read behind the DIF representations is nowadays considered very important for the successful identification of doctored images, therefore for the detection of potential forgery attacks (Katsaounidou and Dimoulas, 2018b; Nightingale et at. 2017; Schetinger, Oliveira, da Silva and Car- valho, 2017; Gloe ct al., 2007; Williams et al., 2018). The systematic review of the human role in the new assisted-verification landscape will indicate the dedicated strategies for cultivating digital media literacy in the battle against misinformation. The need to equip journalists and broadly the audience with such knowledge and skillsets is close to the core of the research questions the current paper tries to answer. To the best of our knowledge, related multidisciplinary efforts are limited in the field. The lack of such featured studies might be rationalized in the marginal role of technologically-enhanced collaborative authentication practices in the existing body of media and communication literature. However, it can be foreseen that the number of such works will be increased within the next few years to fill the interdisciplinary gaps in this demanding and highly versatile domain.  1.2. Background and reloted work: literature review rind theoreticol justi cotton  1.2.1. The human factor Extending the above remarks, it is vital for the forensics communities to comprehend the level at which ordinary users can identify digital   Table 1. Automated Digital Image Forensics    services.  Image Forensics Toolsets FotoForensics Amped Authenticate JPEG Snoop Ghiro Forensic Image Analyzer Forensically PhotoDetective Image Verification Assistant    httta: Z/f‹at‹›f‹›rensics.c‹am/ h ttps://ampedsoftware.com/authenticate httt s://st›tirceft›rge. net/pr‹ijects/JPEGsnr›t›p/pr›stcl‹iwnl‹i<id h ttp://www.getghiro.org/ httt : ZZwww.tt›rcnsic-txitl4w iys.com Zf‹ rcnsic-image-anuI yscr/ https://29a.ch/photo forensics/ http: /Znietain mentions. com/photodetective.html http://reveal mklab. iti.gr/reveaI  3   Launch year 2012 2013 2014 2014 2015 2015 2015 2016  A.N. Kauaountdou et at Hemgon 6 (2020) e0'S808  Image Forgery Classification    Image Forgery Detection Algorithms  Object-wise  Object removal   Double JPEG     JPEG G hOsl (GHOST)  Error Level   High frequency  Median Filtering Analysis     noise    Noise Residuals   (external object)       Image-wise Media Verification Assistant: Image Forgery   ^’””^ Detection  Interpretation Resampling  S  Figure 1. ‘Image Verification Assistant” algorithms and their use in the most common image manipulation operations (Itatsaouni don et al., 2018 p. 121 p.                Figure 2. A step by step scenario of image forgery and the indications of the saounidou ct aI.,   p.   p. 121).  image forgeries (Katsaounidou and Dimoulas, 2018b). A related study has been conducted by Schetinger et at. (2017) and found that 470Zoof the participants were able to spot visual modifications. Given that the spe- cific test was entirely relying on simple computer-generated graphical interventions, someone can anticipate even poorer results for the detec- tion of more complex geometrical inconsistencies within a natural scene. Indeed, Nightingale et al. (2017) principally confirmed the above ex- pectations with their research on real-world photos, resulting in an approximate fake recognition score of 45%, but with noteworthy varia- tions among the different manipulation types. Pantti and Sirén (2015) published a related survey of semi-structured face-to-face interviews with 19 Finnish journalists from leading newspaper publishers and television broadcasters, asking them to appraise the value of amateur images during breaking-news coverage. The research outcomes provided support to the central role of journalists' commitment to accuracy and truthfulness, but with many of the participants to distant themselves from having re- sponsibility in the verification of the available UGC captures. The rest of the involved news professionals claimed that they attempt to perform or find some suitable form of authentication, indicating the need for some guidance or related practices. Previous studies assessed the human ability to detect and locate picture manipulations, relying explicitly on subjective observation, without any algorithmic assistance. Gloe, Kirchner, Winkler  Bohme evaluated the efficiency of DIF tools, wondering whether their outcomes can be unconditionally trusted. Particularly, they attempted a critical view on the reliability of the forensic techniques for the authentication of visual documents, examining two specific cases: the resampling detection approach and the digital camera identification strategy. At the same time, the necessity of explanation interfaces” for Detection Media Verification Assistant’ , while using Double JPEG quanti zation (DQ)”   comprehending the DIF-outputs was highlighted, focusing on fitting User eXperience (UX) design practices that would enhance the experience (and the trust) between humans and machines. Williams, Sherman, Smarr, Posadas  Gilbert (2018), for instance, analyzed the different ways that individuals perceive information coming from other people or machines (algorithms), thus aiming at identifying implicit biasing re- actions. More particularly, a dataset of authentic and tampered images was evaluated by experts (Expert Graphic Artist) and machine (Image Processing Algorithm). Participants were then asked to answer to what extent they agree with the evaluations mentioned above on a 5-point Likert scale. The researchers contacted a paired t-test to identify that no significant differences were found regarding the time needed for the subjective responses in each evaluation case. The second level of the analysis revealed that humans did not show specific trusting preferences in each of the judge types (perceptual vs. algorithmic). In specific, par- ticipants seemed to evenly agree with the analysis of the Algorithm and the Experts, when the images were untouched. In the case of altered pictures, the participants were more likely to agree with the Image Processing Algorithm than the Expert Graphic Artist. Overall, the con- clusions of the reviewed research converge in the unique role of the human factor, which is considered vital in the new assisted-verification era. Appreciating technology is very important for users to accept that digital services will complete veracity tasks for them, as the present work also tries to stress, thus pointing in the direction of developing the necessary media literacy.  1.2.2. Crowdsourced tmnge vert cntion Algorithms may classify a tremendous amount of content when it comes to breaking news events, but only human beings can sift through  4  A.N. Kotsoounidou ct aL  and make sense of material efficiently (Thorne and Vlachos, 2018; Graves, 2018). Indeed, the wise” crowd is an essential component for the verification practices, though there are continuously arising auto- mated solutions in the related research field (Katsaounidou ct at., 2018a). The idea of validating events using people's justification advantages (rational judgment, criticism, physical presence, etc.) is not new. The audience always had a strong impact on the published stories regarding their formulation and perception (Katsaounidou and Dimoulas, 2018a). It is no coincidence that the term collective intelligence” has been used for many years (Lévy, 1997), referring to the exploitation of all the in- dividual (cognitive) skills within a group to successfully deliver a task. Indeed, examples have shown that the best procedure for information authentication is a network of trusted sources, focusing on a specific topic area or a physical location (Silverman, 2013). Broadly speaking, the crowd” has always been a crucial part of how news-reports are shaped and perceived. Thus, the concept of crowdsourced fact-checking on in- cidents and emergencies concerning the daily agenda was always there. Todays' social media technologies, like Twitter, Facebook, YouTube, and others, enable users to engage in this kind of shared decision-making on a much larger, broader, and faster scale. While many flaws can be detected in this process, still, obvious benefits are suggesting that it is better now, within this networking pipeline, rather than before, without considering the targeted audience's opinions and preferences. In this context, Krawetz and Hacker Factor Solutions (2007) argues that the most straightforward image forensic evaluations rely on subjective observations, which can identify forgeries or misclassification errors, even without the use of sophisticated analysis tools. Yet, the incorporation of easy to operate (and interpret) automated solutions could further extend the potentials of machine-assisted verification, which the current work experimentally stresses.  2. Material and methods  2. 1. Hypotheses rind research questions: /ormuIntion of the studies  As already stated, the current work investigates visual tampering detection through the setup and execution of two complementary studies. The focus of the first level of research is to evaluate the degree at which ordinary individuals are able to identify parts of a picture that have been doctored. An online cross-sectional questionnaire was formed to validate the following Hypotheses (H): Ht: Ordinary users can identify some parts of images that have been manipulated, based on their subjective/visual inspection. Hz: The analysis of the collected/crowdsourced answers would be useful to verify the originality of pictures, used as evidence documents in news-reporting. The second study relies on the incorporation of selected image veri- fication assistance tools, that could help in recognizing specific forgery attacks. A repeated measures experiment was chosen this time, con- ducted by means of personal interviews into two different small-sized groups of experts in the field (i.e., journalists and digital photography specialists). This testing procedure was carefully set-up to answer the following Research Questions (RQ): RQ;: Are any differences observed in the subjective evaluation of image authenticity with and without the help of DIF tools (i.e., before and after the experimental treatment)? RQ2: Do DIF algorithms help in the choice of the correct answer (concerning visual tampering detection)? RQ3: Are there correlations between decision updates/modifications and specialty? This second study was further supplemented with an online experi- ment (N = 301), combining and extending the two above mentioned studies. In this case, both the ability of individuals' in identifying manipulated visual content and the usefulness of image verification al- gorithms were put into the core of the analysis procedure to examine their impact on the authenticity assessment task. Hemgon 6 (2020) e0S808  Overall, the two studies were designed to investigate whether pro- fessional and ordinary individuals are aware of the available image verification assisting services and their proper use. Also, the evaluation of the practicality of the tested tools (in their present form) would provide useful insights towards necessary updates, both in terms of the offered utilities and their usability, including their support and broader media literacy strategies.  2.2. Doto orgnnizntion. creation oJ a dataset with genuine and tampered imoges  With the advent of deep neural systems, the creation of publicly available repositories has gained much attention, including the case of doctored pictures. Although the DIF community provides a number of datasets for evaluating visual tampering detection, they differ signifi- cantly in quality and diversity of the implicated forgeries (Korus, 2017; Thakur and Rohilla, 2020; Zheng et at., 2019). Their main purpose is to train smart systems (machines) through Machine/Deep learning tech- niques (ML/DL), but without adapting to the needs of human visual ex- amination. Specifically, images from datasets, recommended for the associated forensic tools, were initially regarded, such as the-- wild-web-tampered-image-dataset”” (Zampoglou et al., 2016), the CASIA (Dong et at., 2013) and CAsIA +2.0¹' datasets (Pham et at., 2019), the Image Manipulation Dataset ¹' (Claristlein et al., 2012) and the Deutsche Welle Image Forensics Dataset ’² (Zampoglou et at., 2016). However, in most of these repositories, numerous examples seem to be irrelevant as news-reporting documents (i.e., they cannot be used to document specific news-stories, which was the main focus of the current work). Moreover, the levels of difficulty in verifying those images (both authentic and tampered) are entirely random and not rated, which would probably cause difficulties during the interpretation of the results (also given the vast amount of the involved samples). Finally, several photos of the Deutsche Welle Image Forensics Dataset” and the-wild-web-t- ampered-image-dataset” were utilized while demonstrating the IVA platform tools. Hence, they had to be excluded to eliminate the possi- bility of previous knowledge for some of the candidate participants. This choice also helps to avoid similar future inconveniences, since we would like to encourage targeted audience visiting such sites and the offered help information, to train themselves in the interpretation of the DIF maps. For this reason, it was decided the selection of representative samples containing both real and fake instances, with the latter being grouped into scalable levels of treatment, from relatively easy-to-reveal to quite demanding. The formation of this human-centric database allows us to record and weight the individual scores, occurred in each example, and also to facilitate the understanding of the underlying processing/altering operations and the associated algorithmic DIF visualizations, from the perspective of the average user. Since the task of observing images can quickly become tedious and underwhelming after a few sessions, a small- sized dataset was decided, initially consisting of 23 records in total (17 tampered and 6 authentic). The purpose of this accommodation was to help participants in fully completing the surveys, i.e. not quitting the activities/questionnaires without answering to all challenges (Katsaou- nidou, 2016). The formation of the specific set of samples was made after a careful and elongated validation procedure, in which, as much as three times the number of the final image samples were involved. Hence, while classical image manipulation processes were employed, the selection of the photos and the associated forgeries were meticulous, aiming at incorporating the under-examination news-documentation character.  https://mklab.iti.gr/results/the-wild-web—tampered-image—dataset /. https://www.haggle.com/sophatvathana/casia-dataset. ” https://githtib. com/namtpham/casia2groundtrrith. !! https://www5.cs.fart.de/research/data/image nlanigulatioiJ/. !° https:// evealproject.eu/the-cle\1tscIae welle image-forensics-clalaset/.  5  A.N. Kotsoounidou ct at  For instance, most of the authentic images are not very common in terms of  probability  of  occurrence,  rising  difficulties  in  related story-verification tasks. Likewise, doctoring attacks are also linked to visual evidence that increases the viral aspect of the underlying stories, posing difficulties in the implicated veracity needs. Overall, the finally assembled testing samples were carefully picked and validated as rep- resentatives of common/real-word image forgery attacks, requiring careful inspection. In the end, the ultimately formed set is the result of a conscious and laborious process, which is considered an advantage compared to larger-size online datasets and their unrealistic instances (as long as human inspection and evaluation are concerned). Specifically, 11 out of 23 photographs were derived from the Internet (5 authentic and 6 doctored, Figure 3), while 5 additional original pic- tures were downloaded and subjected to typical tampering operations, thus forced to the falsifications presented in Figure 4. The dataset was completed with the capture of 7 more photos using two digital cameras, which were again processed to generate pairs of real and manipulated instances (Figure 5). As far as manipulation kinds are concerned, splicing (i.e., synthesize a new object by multiple other photos) and cloning (i.e., object removal/insertion) processes were deployed, along with object duplication (within the same image), resampling and inpainting (Fig- ures 3, 4, and 5). Thus, Copy-Paste operations and particularly the employment of the cloning tool” were employed in the Adobe Photo- shop CS6 environment, while resizing/spatial transforming and blurring filtering were also partially applied. The choice of the mild level of visual processing is a deliberate decision that represents an essential element of the adopted procedure. The utmost goal is to facilitate the assessment of the traceability of typical operations, as they are produced by plenary users within broadly available photo-editing software utilities. The concept behind this practice was to simulate the way common visual alterations are created, primarily as the result of splicing multiple different sources. This context also fits with the selection of the proposed DIF tools, as justifie","Katsaounidou, A. N., Gardikiotis, A., Tsipas, N., & Dimoulas, C. A. (2020). News authentication and tampered images: evaluating the photo-truth impact through image verification algorithms. Heliyon, 6(12), e05808."
SCOP_181,Computational News Discovery: Towards Design Considerations for Editorial Orientation Algorithms in Journalism,"Computational news discovery (CND) is a particular application area within computational journalism related to the use of algorithms to orient editorial attention to potentially newsworthy events or information prior to publication. Previous work in this area has been concentrated on prototyping CND tools, which can, for instance, send alerts and leads to journalists about social media events, documents of interest, or salient patterns in streams of data. This article describes a qualitative interview study of journalists as they incorporate CND tools into their practices. Findings provide insights into how CND tools interact with the internal attention economy and sociotechnical gatekeeping processes of the newsroom and how future CND tools might better align with necessary journalistic evaluations of newsworthiness and quality, while ensuring configurability, human agency, and flexible applicability to a wide range of use cases. These findings begin to outline a conceptual framework that can help guide the effective design of future CND tools.",Journalism Studies,Article,2020,Y,Y,9,"There is perhaps no aspect of the news production pipeline that isn’t increasingly impacted by the use of algorithms. Computational approaches are now broadly applied in journalistic work including in information gathering (Thurman et al. 2016), providing signals to assess the veracity of content or sources (Fletcher, Schifferes, Thurman 2017), automatically generating written articles (Graefe 2016), creating new interactive bot experiences (Lokot and Diakopoulos 2016; Ford and Hutchinson 2019; Jones and Jones 2019), and optimizing or otherwise influencing the distribution of content on homepages, apps, or platforms (Bucher 2016). Computational journalism considers how computing—defined as the systematic study of algorithmic processes that describe and transform information” (Denning 2005)—is applied to support journalistic tasks and embrace journalistic values (Diakopoulos 2019a). Here computational journalism” is emphasized rather than related terms such as algorithmic journalism”, automated journalism”, or data journalism” (Thurman 2019; CONTACT Nicholas Diakopoulos nad@northwestern.edu This article has been corrected with minor changes. These changes do not impact the academic content of the article. ß 2020 Informa UK Limited, trading as Taylor  Francis Grou Zamith 2019; Coddington 2015) as it broadly captures the idea of using algorithms to transform information and data for journalistic purposes. At the same time, computa- tional approaches often entail the close collaboration of algorithms and people in hybrid systems that take advantage of the capabilities of algorithms for scale and speed, but also leverage the complementary capabilities of people (Brynjolfsson and McAfee 2014; Diakopoulos, 2019a). In particular this research examines this hybridiza- tion of algorithmic and human effort in the context of a specific application of compu- tational journalism in news production: computational news discovery. News discovery is described in some of the earliest ideation documents about com- putational journalism. Hamilton and Turner (2009) posit that, a reporter could be alerted when a trend appears, an anomaly arises, or when a specific individual or entity or location is referred to in the data stream”. The premise is that with growing volumes of information, computing can offer a subsidy to public interest journalism, lowering costs and increasing the efficiency and scale at which new news stories can be identified (Hamilton 2016). The intervening years have brought forth a variety of system prototypes that have reified the use of computing for discovering news, which has recently been referred to as computational news discovery” in a review of research on computational journalism (Thurman 2019). Monitoring systems can send alerts and craft leads that orient journalists’ attention to social media events, docu- ments of interest, or anomalous patterns in streams of data that may reveal important news stories. For instance, in computational fact spotting scenarios algorithms sift through claims to identify those that can be checked (Graves 2018). In one particular implementation, a machine learned classifier is used to identify fact checkable state- ments in CNN transcripts, which are then sent to newsrooms as daily tipsheets that fact checkers may refer to in making coverage decisions (Hassan et al 2017; Adair et al 2019). With this prior work in mind, here I define computational news discovery (CND) as: the use of algorithms to orient editorial attention to potentially newsworthy events or information prior to publication. Given the recent technical developments and demonstrations of CND systems, some research has begun to investigate how they are integrated into journalism practice, are applicable (or not) to different journalistic use-cases, and could be designed to enable more effective journalistic use (Diakopoulos 2019; Stray 2019). In furthering the technological lens to the study of computational journalism with a focus on the hybrid nature of newsroom technologies (Anderson 2013), here I adopt a human-centered and sociotechnical frame which considers how CND systems inter- act with and influence the experiences of journalists in undertaking their newswork. The focus is on individual perspectives on usage, including utility as well as social and normative expectations, rather than any wider organizational factors that may, of course, also play a role in adoption. In particular this work sets out to ask: What are the human-centered needs of journalists with respect to the effective use of computational news discovery systems in their sociotechnical gatekeeping practices of selecting and developing news items for publication? This research addresses this question by undertaking interviews with 18 stakeholders who have created, used, or both created and used computational news discovery systems in the context of journalism. 946 N. DIAKOPOULOS The results of this study offer insight into the experiences of practitioners with respect to computational news discovery systems. Based on these results this article contributes an initial elaboration of a conceptual design framework that can help inform and guide the future development of computational news discovery systems that are consistent with user needs and which begins to unpack the use of algorithms as part of sociotechnical pre-publication gatekeeping processes. In particular, the find- ings highlight the essential role of human effort and attention in developing leads into news items of publishable quality, elaborate factors related to the newsroom attention economy and how these tools fit within it, and suggest design opportunities for CND systems to better align with the various newsworthiness and quality evalua- tions journalists need to make across a wide range of journalistic scenarios. Related Work Here I consider two areas of related work which inform the conceptual approach taken in this research: (1) computational news discovery as an application area of computa- tional journalism which speaks to the use of algorithms in sociotechnical gatekeeping processes, and (2) design-oriented and human-centered approaches to studying jour- nalistic work enabled by computational tools that have the goal of identifying design implications. Gatekeeping and Computational News Discovery The concept of gatekeeping captures the idea that information can be variously impeded or passed onward in the process of communication. Not all news information is published and made widely available. There is a matrix of forces at play which impact gatekeeping decisions, including individual cognitive differences or biases, work routines for news production, organizational characteristics, external social insti- tutional actors such as advertisers or governments, and social systems such as culture or ideology (Shoemaker and Vos 2009). While some of the earliest work on gatekeep- ing focused heavily on the role of the individual (White 1950), contemporary models of gatekeeping consider not only human actors such as journalists, strategic professio- nals, and individual amateurs, but also technical actants such as algorithms and their role and interplay with other actors as part of broader sociotechnical gatekeeping practices (Wallace 2017; Thorson and Wells 2016; Lewis and Westlund 2015). Oftentimes when algorithms are considered in gatekeeping processes the focus is on their role in distributing news to the public via feeds (DeVito 2017), aggregators (Nechushtai and Lewis 2019) and apps (Bandy and Diakopoulos 2020), including by examining how editorial values are embedded into the code of such curators (Weber and Kosterich 2018). A less closely studied aspect of algorithmic gatekeeping relates to the role that algorithms can play not only in distribution but also in news produc- tion (Heinderyckx and Vos 2016). In other words, algorithms used in the input and throughput stages of gatekeeping (Wallace 2017) to inform a sociotechnical process prior to wider publication. While some recent work has considered how algorithms used at these stages can introduce biases into the information suggested to journalists DIGITAL JOURNALISM 947 in a sociotechnical gatekeeping process (Thurman et al 2016; Diakopoulos 2019), the current work aims to contribute more broadly to understanding how internally used algorithmic curators (i.e. CND systems) interact with human gatekeepers (i.e. journal- ists) in a sociotechnical news selection process. CND systems can contribute to gatekeeping by allowing users to monitor the vast and overwhelming scale of content produced and published on social media plat- forms. They can help to detect newsworthy events, aggregate responses, and identify, track, and suggest useful sources and witnesses during breaking news or other types of scheduled events like speeches (Diakopoulos, DeChoudhury, and Naaman 2012). For instance, The City Beat tool was developed to detect and alert journalists to local events in New York City, and was deployed to several newsrooms on a trial basis (Schwartz et al 2015). More recently, the Tracer system was developed to monitor mil- lions of daily tweets, cluster posts in order to detect events, and present those events to journalists in a sortable and searchable interface that has proven itself able to accel- erate Reuters’ news alerts in many cases (Liu et al. 2017; Nourbakhsh et al. 2017). CND systems can also help monitor data sources such as numeric data streams or textual documents and identify items of interest to be brought to the attention of journalists. For instance, the BBC’s Data Stringer prototype was developed to monitor data streams and trigger alerts when rules relating to trends or outliers were matched (Shearer, Simon, and Geiger 2014). The Marple system used statistical methods in order to send alerts to local journalists about anomalies, outliers, or trends in munici- pal data sets (Magnusson, Finnas, and Wallentin 2016). The Local News Engine scans data from courts, housing developments, and business licenses to detect the names of newsworthy people, places, or companies, which are sent to local media (Perrin 2017). The Tadam system ingests a variety of different documents from the Web, press releases, or document dumps in order to send alerts to reporters when a document shows up that matches their preset filters (Plattner, Orel, and Steiner 2016). The Washington Post’s Lead Locator system mines a national voter file dataset in order to help national politics reporters identify interesting locations for their reporting based on demographic patterns and political relevance (Diakopoulos et al 2020). Interactive data-driven expert systems have been deployed to help journalists identify what might be meaningful and newsworthy patterns warranting story development (Broussard 2015). News discovery approaches have also been applied in fact checking workflows to help spot fact checkable claims mentioned in the media (Graves 2018; Hassan et al 2017; Adair et al 2019). Journalism practice is also utilizing approaches from machine learning and data mining to expand the scope of individual investigations by filtering for known patterns that orient attention to entities or documents that suggest new lines of inquiry (Diakopoulos 2019a; Stray 2019). The current research considers both these bespoke approaches as well as more formalized systems (i.e. CND approaches and systems), with an emphasis on discovery from non-social media sources. These various approaches and tools represent a new journalistic source, offering access to informa- tion at the input stage of the gatekeeping process (Wallace 2017) by utilizing algo- rithms to orient editorial attention to items that journalists might not be aware of otherwise. In particular, this research seeks to understand the sociotechnical 948 N. DIAKOPOULOS gatekeeping processes around CND approaches and systems in order to inform the design of future CND tools. Design-oriented Studies of Newswork Human-centered design can be used to help develop new journalistic products, serv- ices, and experiences that de-center the role of technological affordances and instead focus on how to harness technology to meet user needs (Chaplin 2016). More broadly, design methods can facilitate a deeper understanding of a sociotechnical context and offer insights that can guide the creation of new technologies that support and align with journalistic goals. Recently, there have been calls for journalism studies to engage more deeply with human-computer interaction (HCI) research in order to better under- stand how journalists interact with algorithms and automation in news production and how journalistic values can come to be embedded into technologies (Aitamurto et al. 2019; Diakopoulos 2019). These in turn build on earlier observations of the need for scholarly attention towards the imagined values and engineering design ... of journalistic artifacts” (Anderson 2013), and the importance of how technological actants are inscribed and instructed by humans” (Lewis and Westlund 2015) so that such inscription is deliberate with respect to the goals of stakeholders and the intended contexts of use. This work addresses this conceptual space by examining the uses and practices surrounding current CND tools with an eye towards how the next generation of such tools could be designed to more effectively support journalistic contexts, work, and values. In particular, our findings are oriented towards identifying design implications that articulate various constraints, affordances, and social or nor- mative expectations of the possible design space for future CND tools (Sas et al. 2014). A design orientation aligns with the conceptualization of digital journalism studies conveyed by Eldridge et al (2019) by enabling empirically informed opportuni- ties for deliberate sociotechnical reconfiguration of journalistic practices. A growing corpus of research looks at software or tools to support journalistic activ- ity with an eye towards gaining insights that inform future designs and elaborate jour- nalistic concepts. Recent studies in this vein have examined the uses and limitations of automated writing software (Thurman, D€orr, and Kunert 2017), and the utility of social surveillance tools (Thurman 2017) which suggest opportunities for how future computational tools could better support journalistic work. Another line of research has studied visual analytic tools for investigative journalists, revealing different news discovery use-cases depending on whether users already had a hypothesis to verify (Brehmer, Ingram, and Stray 2014) or were more interested in hypothesis generation (Felix et al. 2015). Systems have also been built to help sort social media information such as Tweets or online comments, and user evaluations have underscored the desire of journalists to interactively configure information filters according to domain or scen- ario specific criteria, to receive adequate context to evaluate or verify information, and to use such tools as a starting point to identify angles of interest for subsequent inquiry (Park et al 2016; Diakopoulos, DeChoudhury, and Naaman 2012). Specific design requirements for software to support the use of user-generated content in newswork have also been developed, including requirements for flagging or marking DIGITAL JOURNALISM 949 leads for later, reflecting update frequencies, and supporting the ongoing nature of verification work (Tolmie et al 2017). The INJECT system was designed to support cre- ativity amongst journalists by providing cues about people, background, or conse- quences of a news story that could trigger ideas for novel stories (Maiden et al 2018). Taken together, these human-centered and design-oriented studies begin to suggest a rich and deeply contextual design space for CND systems, which the current research seeks to empirically elaborate and refine. Study Methods In order to better understand the use of CND systems and approaches from a user- centered perspective a qualitative study consisting of 18 semi-structured interviews was undertaken. The goal of the interviews was to elicit the perspectives and experi- ences of practitioners, with a particular eye towards how CND systems and approaches might be designed to more effectively integrate into journalism practice. Participants Participants for this study were selected using a purposive sampling strategy in order to deliberately reflect a range of systems and perspectives on those systems. Various editorial orientation systems and projects were identified by reviewing the literature as well as online articles and blogs. Systems were then selected to straddle both data- driven tools and products as well as one-off projects that utilized algorithms for directing editorial attention for the purposes of story finding, alerting, or document investigation. Specific systems studied include RADAR, 1 Newsworthy, 2 Klaxon,3 several computational fact spotting tools including those of the Tech  Check Cooperative (Adair et al 2019), FullFact, 4 and Chequeado,5 as well as a range of internal projects from outlets ranging from regional and national newspapers, to online news publica- tions, and interest-specific digital outlets. Individuals associated with the identified sys- tems were contacted via email for interview. Snowball sampling was then used to expand the sample by asking initial interviewees for referrals to other relevant poten- tial participants. These recruiting processes yielded 18 individuals that were interviewed. Most of the participants were initially interviewed for the purposes of the author’s journalistic endeavor to report on the media industry’s use of automation and algo- rithms in news production. Post hoc IRB approval was obtained to re-analyze the data collected in these interviews for the purposes of this research. Light disguise is used in the reporting of results in order to protect the privacy and confidentiality of inter- viewees (Bruckman 2006). This means that, while precautions have been taken to pre- serve anonymity and to not identify participants directly, active members of the communities where such projects are discussed may be able to guess identities. Any sensitive details are therefore omitted, as are the identities of the organizations where internal projects were selected for study since participants might be more easily re- identified if those outlets were named. Participants include individuals with a diverse range of perspectives on computational news discovery systems such as creators 950 N. DIAKOPOULOS (N ¼ 4, labeled C1 ... C4), users (N ¼ 7; U1 ... U7), or both creators and users (N ¼ 7; CU1 ... CU7). Interview Materials and Procedure An interview guide was developed iteratively as interviews were undertaken and ana- lyzed, allowing for theoretical sampling of concepts as they emerged from the data (Glaser and Strauss 2009). In its final form the guide included 20 questions including follow-up prompts, addressing topics typically covered in the following ordering: how the system’s leads are used in newswork, the overall utility of the system’s leads, the information interface of the system including how leads are presented, the news- worthiness of the leads provided, the volume of leads received and developed into news reports including time spent on leads, how the system fits into workflow, whether the leads were trustworthy, and any ethical reflections or other thoughts on the wider use of such tools in journalism (See Appendix A for more details). The semi- structured interview procedure allowed some latitude to focus each interview and additionally tailor questions and prompts as well as their ordering according to an interviewee’s particular expertise, experience, and perspective. Interviews were conducted over a two-year span, from early 2017 to late 2018. The median interview lasted 50.5 min (min ¼ 27; max ¼ 64) and was conducted via audio connection (e.g. phone or Skype) in English. All interviews were audio recorded with consent and were later transcribed. No monetary incentive was provided to participants. Analysis Interviews were fully transcribed and then analyzed using an iterative qualitative method involving open coding of key excerpts, constant comparison, typologizing, and memoing (Glaser and Strauss 2009; Lofland and Lofland 1994). This process was ongoing as interviews were undertaken, which helped inform follow-up questions in latter interviews based on the analytic results from earlier interviews. Analysis of inter- view materials was further augmented and grounded using document analysis of related materials from the various systems studied, including any extant blog posts, product descriptions, and video presentations where a system’s functionality or design were discussed. Findings From the iterative analysis of the transcribed interviews several factors emerged with respect to the use of CND systems and approaches. Chief amongst these factors is the role that humans must still play in developing and evaluating the leads produced by these systems. Several sociotechnical factors that moderate the attention environment with respect to CND systems are elaborated, including the willingness of reporters to pursue leads in different contexts, external factors related to news cycles, the scope of monitoring offered by tools, and the user interface used to convey leads. The findings DIGITAL JOURNALISM 951 further expand on how systems can serve to modulate important editorial evaluations related to newsworthiness and quality assessment that journalists undertake in their lead development work. The Human Role: Evaluation Participants consistently reinforced the idea that people should be involved in evaluat- ing the leads produced by CND systems. This held for the simplest of leads, such as alerts signaling a change to a web page, and for more complex leads where an algo- rithm might itself embed evaluative criteria to rank or draw attention to a subset of information more likely to pan out. While an algorithm permits a scale of monitoring that would otherwise be impossible, having people evaluate leads imbues the overall system with a degree of flexibility to suit different use-cases. Despite the varying degrees of evaluation an algorithm might itself encode, human evaluation of leads was seen as essential in at least three areas: (1) lead development, (2) newsworthiness assessment, and (3) quality assurance. These are briefly describe next and further elabo- rated in subsequent subsections. Additional editorial effort was often seen as needed in order to develop leads towards publication. In some cases, such as for the leads produced by RADAR, the leads could be published as-is, largely because they had already undergone substantial human editorial development before being distributed as leads. Publishers were able to directly excerpt and use snippets of the text from some of the Newsworthy leads as well. In such cases the editorial effort is mainly that of curation. But for most CND tools studied there was more substantive editorial attention needed, typically involv- ing additional reporting to gather and assess related information as the lead became publication-worthy. These contrasts highlight the spectrum of human effort and atten- tion that might be invested in leads. Newsworthiness assessment was another important role that people were seen to play. Participants articulated the entire gamut of news values in their evaluation of leads, but the role of people was seen as particularly essential in evaluating news- worthiness dimensions such as audience fit and actuality (i.e. relevance to the current moment). For instance, FullFact intentionally built their claim spotting system to separ- ate checkability” (which they thought an algorithm was suited to recognize) from checkworthiness” (which they thought should be left for a person to evaluate). We thought that importance was actually something that is an editorial decision and will change over time. Claims, for example, about the EU ... two years ago wouldn’t have been as important as they are now” (C3). By deferring checkworthiness judgements to people the goal was to keep the sociotechnical system more responsive and flexible to a dynamic world where the importance of statements might change over time. The third area of human evaluation relates to the desire to assure the journalistic quality of leads that are developed and eventually published. Several journalists acknowledged that CND leads could only be a starting point: I only ever viewed it as a preliminary screen that needed a lot more reporting ... I would be very very ner- vous about reporting just from algorithmic output” (CU7). Another participant con- curred, Reporters are ultimately going to want to vet everything themselves by hand 952 N. DIAKOPOULOS to ensure that it’s correct, to ensure that they understand it” (CU5). Having people check the leads supplied by an algorithm was seen as the most reliable way to ensure the highest quality standards were met. Lead Development Effort and Attention The editorial effort and attention needed for a lead to mature into a publishable story varies a great deal across use cases. In some cases a lead might entail a few hours of reporter effort to make calls or find illustrative local examples: Typically the people quoted in the RADAR stories are national people of limited relevance to our readers. So for us as a local paper it’s better to get reaction from the people involved locally” (U5). But in other cases it could be a whole day’s work to do an interview, get a photo, and work up the details of the lead. RADAR leads are already highly refined, but for less fully formed leads there might be even more work to do. One participant estimated it could take a day or two of effort to prepare a fact check for publication. Another participant remarked that to develop a Newsworthy lead at their radio sta- tion, We have to put like one or two people to work with it for one week” (U1). Statistical aberrations and trends may demand substantial work to assess how interest- ing they are and begin to explain them: If we see in some areas that the prices of housing are going up very fast ... Why is that? Then we can ask questions in that region: What’s happening here and try to find cases and do journalism from that” (U1). While it was accepted that CND leads would demand effort and attention to develop into meaningful journalistic contributions, some participants recognized there may be opportunities to utilize lower-skill labor to initially assess leads before passing them along to more seasoned journalists for further investigation. To determine if there’s a real story being suggested, one participant remarked, I’m hoping that basic- ally we can make it so that the leads are self-explanatory enough that people can basically sit their interns down and sit slightly less experienced reporters down” (CU2). Lower skill workers would thus act as an initial screen of leads produced by the sys- tem. On the other hand, some leads may not get taken up if there’s not someone with enough skill to interpret and evaluate them. As one tool creator explained, Some newsrooms that are maybe a little more data savvy, they tend to use these stories more whereas others that are not they don’t use them as much” (C1). So while there is a spectrum of human effort needed, there is also a spectrum of skill that is needed to pursue certain leads. The availability of human attention is a key factor in determining whether any given news lead develops into a full story. One participant described a situation where automatically generated leads were sent to collaborators but took months before they were looked at and yielded a story. Also, sending too many leads might overwhelm the available attention of a newsroom and users may simply tune them out: We couldn’t take care of everything ... there was too much information for us to handle because we are like 20 people working here in our newsroom” (U1). A lead may look interesting but there may not be enough human capacity to further develop it to watch, with the goal of not triggering alerts on irrelevant site changes that might end up overwhelming or distracting the user. A user of Newsworthy leads explained that he didn’t want every lead the system produced, but rather only the ones he was topically interested in. The Chequeado system is deliberately set up to monitor media that include at least one outlet from each administrative unit of the country. The scope of monitoring is thus an editorial decision that dictates how wide a net the sys- tem fishes with. This in turn impacts the volume, precision, and relevance of leads pro- duced, and presumably increases the likelihood that leads match with interests and receive attention. The scope of monitoring can also be algorithmically widened based on an initial query, augmenting human ability by obviating the need to know exactly what to monitor. If the monitoring scope of a system is set too wide, either through user configuration or algorithmic expansion, control can be maintained by supporting the filtering of leads according to various relevance criteria. This came up in the con- text of a fact spotting system in which several users indicated they were not inter- ested in checking the claims of pundits or other journalists and so wanted to filter those out to reduce distraction, while also recognizing that other outlets might still be interested in those leads. This underscores the variability in interests between different journalists and outlets and how monitoring scope and post-filtering can enable editor- ial control and support diverse uses. User Interface Design A final factor that impacts the flow of attention towards leads is the nature of the user interface (UI) and how it frames information for users. Some systems, such as Newsworthy or Tech  Check, send discrete chunks of information via email. By expli- citly marking individual items or sets of items (e.g. a set of claims for fact checking) for attention this may, however, create expectations around the relevance or import- ance of those items. Other systems, such as RADAR or Chequeado, provide an inter- face that presents a ranked list of items that can be browsed. A ranking UI has the advantage of communicating some degree of relevance that corresponds to the order- ing of leads, without definitively marking some subset as worthy of attention. A mix- ture of the two approaches involves sending discrete leads via email with a link to an interactive UI containing more details and a full ranking. Date-time order (i.e. most recent at top) is a common default for rankings, though leads can be sorted according to other criteria. Some interfaces provide keyword search functionality allowing users to find leads based on specific interests. Finding the right presentation of the information from the lead was seen as a chal- lenge: I’m fairly optimistic at this moment about us being able to generate useful leads. But I’m fairly pessimistic about us being able to communicate that well enough to the reporters who are supposed to then make sense of them” (CU2). At Chequeado they’ve carefully crafted the information shown in the UI. Each lead shows the claim, the media outlet where it was published, a link to the claim (e.g. article where it was detected), and then shows the paragraph where the claim was found as additional context. Initially users see the last 3 days of checkable claims, but can continue scroll- ing down until they eventually start seeing claims that are less fact checkable. The rationale for including some of the not fact checkable claims in the interface was to DIGITAL JOURNALISM 955 be able to collect feedback to further train the machine learning (i.e. by including negative cases). Newsworthy leads, on the other hand, consist of a chart that visualizes the trend or statistical anomaly behind the lead, a few sentences of generated text describing what it’s about, and a link to the original spreadsheet with the data back- ing the lead. Linking to the data was found to be important: Most or almost all reporters that get these leads and do something with them actually look at the data themselves” (C1). This approach was also taken with the Tech  Check leads: a link from each lead provides quick access to the source transcript which allows reporters to assess the context of a statement before further pursuing. Importantly, lead presen- tations included vit","Diakopoulos, N. (2020). Computational news discovery: Towards design considerations for editorial orientation algorithms in journalism. Digital Journalism, 8(7), 945-967."
SCOP_293,Arming the public with artificial intelligence to counter social bots,"The increased relevance of social media in our daily life has been accompanied by efforts to manipulate online conversations and opinions. Deceptive social bots—automated or semi- automated accounts designed to impersonate humans—have been successfully exploited for these kinds of abuse. Researchers have responded by developing artificial intelligence (AI) tools to arm the public in the fight against social bots. Here we review the literature on different types of bots, their impact, and detection methods. We use the case study of Botometer, a popular bot detection tool developed at Indiana University, to illustrate how people interact with AI countermeasures. A user experience survey suggests that bot detection has become an integral part of the social media experience for many users. However, barriers in interpreting the output of AI tools can lead to fundamental misunderstandings. The arms race between machine learning methods to develop sophisticated bots and effective countermeasures makes it necessary to update the training data and features of detection tools. We again use the Botometer case to illustrate both algorithmic and interpretability improvements of bot scores, designed to meet user expectations. We conclude by discussing how future AI developments may affect the fight between malicious bots and the public.",Computer Science,Article,2019,N,Y,143,"During the past two decades, we have progressively turned to the Internet and social media to find news, share opinions, and entertain conversations (Morris  Ogan, 1996; Smith  Brenner, 2012). What we create and consume on the Internet impacts all aspects of our daily lives, including our political, health, financial, and entertainment deci- sions. This increased influence of social media has been accompanied by an increase in attempts to alter the organic nature of our online discussions and exchanges of ideas. In particular, over the past 10 years we have witnessed an explosion of social bots (Boshmaf, Muslukhov, Beznosov,  Ripeanu, 2013; Lee, Eoff,  Caverlee, 2011), a presence that does not show signs of decline. Social bots are social media accounts controlled completely or in part by computer algorithms. They can generate content automatically and interact with human users, often posing as, or imitating, humans (Ferrara, Varol, Davis, Menczer,  Flammini, 2016). Automated accounts can be harmless and even helpful in scenarios where they save manual labor without polluting human conversations. In this paper, however, we focus on those actors in online social networks that surreptitiously aim to manipulate public discourse and influence human opinions and behavior in an opaque fashion. While more tradi- tional nefarious entities, like malware, attack vulnerabilities of hard- ware and software, social bots exploit human vulnerabilities, such as our tendencies to pay attention to what appears to be popular and to trust social contacts (Jun, Meng,  Johar, 2017). Unlike other social engineering attacks, such as spear phishing (Jagatic, Johnson, Jakobs- son,  Menczer, 2007), bots can achieve scalability through automa- tion. For example, multiple accounts controlled by a single entity can quickly generate posts and make specific content trend or amplify misinformation. They can trick humans and engagement-based rank- ing algorithms alike, creating the appearance that some person or Received: 22 December 2018 Accepted: 22 December 2018 DOI: 10.1002/hbe2.115 48 © 2019 Wiley Periodicals, Inc. wileyonlinelibrary.com/journal/hbe2 Hum Behav  Emerg Tech. 2019;1:48–61 opinion is popular. Therefore, defending from social bots raises seri- ous research challenges (Boshmaf, Muslukhov, Beznosov,  Ripeanu, 2012). Manipulation of public opinion is not new; it has been a common practice since the dawn of humanity. The technological tools of all eras—printed media, radio, television, and the Internet—have been abused to disseminate misinformation and propaganda. The deceptive strategies employed on all these types of channels share striking simi- larities (Varol  Uluturk, 2018). Nevertheless, social media are particu- larly vulnerable because they facilitate automatic interactions via software. As a result, social media platforms have to combat a deluge of attacks. Facebook recently announced that 1.5 billion fake accounts were removed over 6 months in 2018.1 Even a very low miss rate could leave millions of accounts available to be used as bots. In this light, it is not surprising that as many as 9–15% of active Twitter accounts were estimated to be bots in 2017 (Varol, Ferrara, Davis, Menczer,  Flammini, 2017), and that social bots are responsible for generating two thirds of links to popular websites (Wojcik, Messing, Smith, Rainie,  Hitlin, 2018). Public interest in social bots has also dramatically increased during the past few years. A recent Pew survey shows that 66% of Americans are aware of the existence of social bots and a consistent fraction of those believes that they are mali- cious (Stocking  Sumida, 2018). Despite high awareness, the survey also reveals that many people are not confident in their ability to iden- tify social bots. Part of the difficulty in learning to recognize bots is that they continuously evolve in response to efforts by platforms to eliminate them. Social bots that go undetected by platforms have become increasingly sophisticated and human-like (Varol, Ferrara, Davis, et al., 2017), introducing an arms race between bot creators and the research community. Indeed, industry and academic research about social bots has flourished recently. A first concern of this paper is to briefly review recent studies about social bots and the development of artificial intelligence (AI) countermeasures. A second goal is to present a study on how peo- ple interact with bot detection tools and what insight they gain. This is an important perspective that is often neglected by developers of AI tools. After all, winning this fight requires broad participation by social media users. 2 | L I T E R A T U R E R E V I E W There are many types of social bots. In this section, we review recent studies on the diverse characteristics of social bots, their activities, and impact. We then review the literature on bot detection algorithms. 2.1 | Characterization of social bots Some simple bots only do one thing: They post content automatically. The Twitter account @big_ben_clock is a quintessential representative of this class—it tweets every hour mimicking the real Big Ben. Similar accounts can automatically post or retweet/share to publicize infor- mation such as news and academic papers (Haustein et al., 2016; Lokot  Diakopoulos, 2016). These bots are naive and easy to identify: They share only one type of content and they do not try to misrepresent themselves or their motivations. In 2011, Lee et al. (2011) identified thousands of social bots. They created bots that posted meaningless messages through automated scripts. The under- lying expectation was that only other bots and no human would fol- low these honeypot accounts. The honeypot accounts did end up having many followers. Subsequent analysis confirmed that these fol- lowers were indeed bots and revealed the common strategy of ran- domly following accounts to grow the number of social connections. More sophisticated bots adopt various strategies to impersonate human users. Social bot developers can populate bot profiles by searching and collecting material from other platforms. A more extreme example of these kinds of bots are identity thieves: they copy usernames, profile information, and pictures of other accounts and use them as their own, making only small changes (Cresci, Di Pietro, Petrocchi, Spognardi,  Tesconi, 2015). State-of-the-art machine learning technologies can be employed as part of the algorithms that automatically generate bot content (Freitas, Benevenuto, Ghosh,  Veloso, 2015). Sophisticated bots can emulate temporal patterns of content posting and consumption by humans. They can even interact with other users by engaging in conversations, commenting on posts, and answering questions (Hwang, Pearce,  Nanis, 2012). Bots employ a variety of strategies to form an audience. There are bots designed to gather followers and expand social circles, with the goal of exerting some form of influence. Some bots, for example, search the social networks for popular accounts, follow them, and ask to be followed back (Aiello, Deplano, Schifanella,  Ruffo, 2012). This type of infiltration has been proven to be more effective when the target community is topically centered. Bots can identify and generate appropriate content around specific topics to gain trust and attention from people interested in those topics. Such attention often translates in following relationships (Freitas et al., 2015). This finding is even more worrisome when one considers that the targets were program- mers, a population expected to be more aware of social bots com- pared to other users. Another class of bots, whose activity is less evident, is that of so- called fake followers (Cresci et al., 2015; De Micheli  Stroppa, 2013). These accounts are managed by entities that get paid in exchange for following customers who want to increase their perceived popularity. The fake followers often follow each other, forming a network that lends credibility to each member and allows them to elude being flagged for lack of followers. A few dollars can buy thousands of fake followers. A recent report2 revealed that reality television stars, pro- fessional athletes, comedians, TED speakers, pastors, and fashion models all purchased fake followers on Twitter at least once. The companies that sell fake followers also sell fake engagement (likes, comments, retweets) and even custom-made fake personas. A group of social bots, sometimes referred to as a botnet, may act in coordination, posting the same content again and again to generate false popularity. This type of behavior is generally harder to detect as the accounts involved, when inspected individually, may appear as genuine. Their behavior becomes conspicuous only when one is able to detect a large number of such accounts acting in a strongly coordi- nated manner (Chavoshi, Hamooni,  Mueen, 2016). Not surprisingly, hybrid approaches that mix human and automated activity have been YANG ET AL . 49 devised to bypass bot and bot-coordination detection systems (Grimme, Assenmacher,  Adam, 2018). A recent study by Cresci, Di Pietro, Petrocchi, Spognardi, and Tesconi (2017) shows that neither humans nor supervised machine learning algorithms can identify this kind of bots successfully. 2.2 | Activity and impact of social bots Bot activity has been reported in several domains, with the potential to affect behaviors, opinions, and choices. Health is one domain of particular concern (Allem  Ferrara, 2018), where we have observed social bots influencing debates about vaccination policies (Broniatowski et al., 2018; Ferrara, Varol, Davis, et al., 2016) and smoking (Allem  Ferrara, 2016; Allem, Ferrara, Uppu, Cruz,  Unger, 2017). Politics is another key domain. During the 2010 U.S. midterm elections, primitive social bots were found to support some candidates and attack their opponents (Metaxas  Mustafaraj, 2012), injecting thousands of tweets pointing to websites with fake news (Ratkiewicz et al., 2011). During the 2016 U.S. presidential election, social bots were found to generate a large amount of content, possibly distorting online con- versations. Our analysis (Bessi  Ferrara, 2016) suggests that retweets were a vulnerable mode of information spreading: There was no sig- nificant difference in the amount of retweets that humans generated by resharing content produced by other humans or by bots. In fact, humans and bots retweeted each other substantially at the same rate. This suggests that bots were very effective at getting messages reshared in the human communication channels. We further explored how bots and humans talked about the two presidential candidates. We noted that bots tweeting about Donald Trump generated the most positive tweets (Bessi  Ferrara, 2016). The fact that bots pro- duce systematically more positive content in support of a candidate can bias the perception of the individuals exposed to this content, suggesting that there exists an organic, grassroots support for a given candidate, while in reality it is all artificially generated. Similar cases of political manipulation were reported in other countries (Stella, Ferrara,  De Domenico, 2018). Our analysis highlighted the presence of Twitter bots during the 2017 French Pres- idential Election (Ferrara, 2017). We explored tweets related to candi- dates Marine Le Pen and Emmanuel Macron in the days leading to the election. We identified 18K active bots that pushed the so-called MacronLeaks disinformation campaign. Accounts who engaged with these bots were mostly U.S.-based users with preexisting interest in alt-right topics and alternative news media, rather than French users. Hundreds of bots posting about the French elections were also active in the 2016 U.S. Election discussion, suggesting the existence of a black market for reusable political disinformation bots. The spread of fake news online is another area in which the effect of bots is believed to be relevant (Bessi et al., 2015; Lazer et al., 2018). A study based on 14 million tweets posted during and after the 2016 U.S. presidential election revealed that bots played a key role in the spread of low-credibility content (Shao et al., 2018,b). The study uncovered strategies by which social bots target influential accounts and amplify misinformation in the early stages of spreading, before it becomes viral. Even bots created with good intentions may contribute to the spread of misinformation. After the Boston marathon bombing, for example, bots started to automatically retweet indiscriminately, without verifying the credibility of the posts being retweeted or that of their sources (Gupta, Lamba,  Kumaraguru, 2013), unintentionally amplifying the diffusion of misinformation. Social bots have been used to promote terrorist propaganda and proselytize online extremism. By analyzing a sample of 20,000 Islamic State supporting accounts, Berger and Morgan (2015) found that the terrorist organization was actively using social media and bots to spread its ideology. Abokhodair, Yoo, and McDonald (2015) dissected a botnet that tried to misdirect the online discussion during the Syrian civil war in 2012. Ferrara, Wang, Varol, Flammini, and Galstyan (2016) analyzed a dataset of millions of extremist tweets and found that social bots produced some of the content. 2.3 | Bot detection methods An early attempt by Wang et al. (2012) involved building a crowdsour- cing social bot detection platform. This method proved to be effective, but scalability was a prominent issue: while bots can be multiplied at will at essentially no cost, human detectors cannot. Later efforts therefore mostly leveraged machine learning methods. Approaches based on supervised machine learning algorithms are the most common (Subrahmanian et al., 2016). Supervised approaches depend and often start with the collection of an extensive dataset, with each account labeled as either human or bot. These labels usually come from human annotation (Varol, Ferrara, Davis, et al., 2017), automated methods (e.g., those based on honey pots described ear- lier; Lee et al., 2011), or botnets that display suspicious behaviors (Echeverria  Zhou, 2017a, 2017b). A critical issue with existing data- sets is the lack of ground truth. There is no objective, agreed-upon, operational definition of social bot. One of the factors that explain this is the prevalence of accounts that lie in the gray area between human and bot behavior, where even experienced researchers cannot easily discriminate. Nevertheless, datasets do include many typical bots; using the training labels as proxies for ground truth makes it possible to build practically viable tools (Davis, Varol, Ferrara, Flammini,  Menczer, 2016; Varol, Ferrara, Davis, et al., 2017). The choice of relevant features used to describe entities to be classified is a critical step of machine learning classifiers. Different choices have been considered, but in general six broad categories of features have been identified as relevant for discriminating between human and bot accounts (Varol, Davis, Menczer,  Flammini, 2018): user metadata, friend metadata, retweet/mention network structure, content and language, sentiment, and temporal features. In the case of supervised learning, after extraction and preprocessing, the fea- tures are fed into supervised machine-learning models for training, then the trained models are used to evaluate previously unseen accounts. Most techniques attempt to detect bots at the account level, by processing many social media posts to extract the features listed above. Recently, Kudugunta and Ferrara (2018) proposed a deep neu- ral network approach based on the contextual long short-term mem- ory (LSTM) architecture that exploits content and metadata to detect bots at the tweet level. Contextual features are extracted from user 50 YANG ET AL . metadata and fed as auxiliary input to LSTM deep nets processing the tweet text. The proposed framework, designed to predict whether a given tweet was posted by a human or a bot, exhibited promising per- formance on test benchmarks. While supervised methods have proven to be effective in many cases, they do not perform well at detecting coordinated social bots that post human-generated content (Chen  Subramanian, 2018; Cresci et al., 2017; Grimme et al., 2018). As mentioned earlier, those coordinated bots are not usually suspicious when considered individu- ally. Their detection requires information about their coordination, which becomes available only once the activity of multiple bots is con- sidered. Unsupervised learning methods have been proposed to address this issue. Early attempts combined multiple features via Euclidean distance and then applied clustering or network community algorithms (Ahmed  Abulaish, 2013; Miller, Dickinson, Deitrick, Hu,  Wang, 2014). Cresci, Di Pietro, Petrocchi, Spognardi, and Tes- coni (2016) encoded tweet type or content as strings of symbols, then considered the longest common substring to detect coordination. By comparing the time series of accounts sampled from the Twitter streaming application program interface (API), Chavoshi et al. (2016) built an unsupervised tool called DeBot that is able to find accounts tweeting in synchrony, suggesting they are automated. Chen and Sub- ramanian (2018) adopted a similar method to detect bots by finding accounts tweeting similar content. These unsupervised methods have the advantage of focusing on what accounts (or tweets) have in com- mon rather than what is distinct, but also the disadvantage of having to consider quadratic numbers of account pairs in the worst case. A recent research direction is to test the limits of current bot detection frameworks in an adversarial setting. The idea is to propose methodologies to engineer systems that can go undetected. Cresci, Petrocchi, Spognardi, and Tognazzi (2019) proposed the use of evolu- tionary algorithms to improve social bot skills. Grimme et al. (2018) employed a hybrid approach involving automatic and manual actions to achieve bots that would be classified as human by a supervised bot detection system. Despite the good intention of pointing to weak- nesses in existing systems, this research might also inspire bot crea- tors and give them a competitive advantage. 3 | U S E R E NG A G E M E N T W I T H B O T D E T E C T I O N TO O L S Research efforts aimed at bot detection may be valuable to mitigate the undesired consequences of bot activity, but their efficacy is lim- ited by two factors: limited public awareness of the bot problem, and unwillingness to adopt sophisticated tools to combat it. The success of research efforts also critically depends on the capacity to adapt to ever changing and increasingly sophisticated artificial accounts. This, in turn, depends on the ability to engage the public and collect the feedback it provides. It is therefore important to understand how the public adopts, interacts with, and interprets the results of bot detec- tion tools. This perspective is often neglected by research on social bots. In this section, we use Botometer—a bot detection tool devel- oped at Indiana University—as a case study to illustrate efforts aimed at understanding the needs of the users and how their feedback can help improve tools. Botometer is based on a supervised machine learning approach (Davis et al., 2016; Varol, Ferrara, Davis, et al., 2017). Given a Twitter account, Botometer extracts over 1,000 features relative to the account from data easily provided by the Twitter API, and produces a classification score called bot score: the higher the score, the greater the likelihood that the account is controlled completely or in part by software, according to the algorithm. Because some of the features are based on English-language content, the bot score is intended to be used with English-language accounts. To evaluate non-English accounts, Botometer also provides a language-independent score that is produced by a classification model trained excluding linguistic fea- tures. Botometer additionally reports six subscores, each produced by a model based on a distinct subset of features. The score names refer to the feature classes: user meta-data, friends, content, sentiment, network, and timing. The subscores are provided to help users identify which features contribute to the overall score. The earliest version of Botometer (known as “BotOrNot”) became available to the public in May 2014. Free access is offered through both a web interface (botometer.org) and an API. In the past 4 years, Botometer has constantly increased its basin of adoption, and has pro- vided data for a number of influential studies, including by Vosoughi, Roy, and Aral (2018) and the Pew Research Center (Wojcik et al., 2018). Even more importantly, Botometer is used by many regular Twitter users. Currently, it handles over a quarter million requests every day (Figure 1), while the website receives over 500 daily visits. Botometer also supports several third-party tools, including browser extensions, bot-hunting bots, and localized versions in non-English- speaking countries. Between August and October 2018, we conducted a user experi- ence survey with participants recruited among the visitors of the Bot- ometer website. The survey contained two questions with required answers and a few optional ones. It also allowed respondents to enter some free-text comments. We collected usable answers from 731 par- ticipants; they are listed in Figure 2 together with the questions. A few interesting facts emerge. First, more than one third of participants FIGURE 1 Number of daily requests handled by Botometer YANG ET AL . 51 use Botometer at least weekly, implying that detecting bots is becom- ing a recurring need for at least some users (Figure 2(a)). Second, over 80% of the users believe Botometer is accurate in classifying bots and humans (Figure 2(b)). Third, over 80% of the users find the bot scores presented by Botometer easy to understand (Figure 2(c)). Although these numbers are encouraging, we are aware of self-selection bias, as respondents of the survey tend to be active users of Botometer. Finally, users seem to be equally worried by false positives (humans misclassified as bots) and false negatives (bots misclassified as humans) as shown in Figure 2(d). A more nuanced picture of this last observation emerges when these results are disaggregated into frequency-of-usage categories: Figure 3 shows that occasional users care more about false positives. A typical example of usage that might elicit this concern is when an individual checks whether their own account looks like a bot. On the other hand, frequent users care more about false negatives. For example, an advertiser interested in the number of fake followers of a celebrity paid to endorse a product may be more worried about missing bots. Botometer is based on Random Forest classifiers (Breiman, 2001). The score produced by such a model is the fraction of trees that clas- sify the account under examination as a bot. This interpretation of the bot score is obscure to the typical user, who is unlikely to have back- ground knowledge in machine learning. Because a bot score is defined in the unit interval, it is tempting to interpret it as the probability that the account is a bot—if the score is 0.3, say, then 30% of accounts with similar feature values are bots. Another interpretation is that such an account is 30% automated. Judging from early user feedback, such incorrect interpretations were prevalent. Despite an explanation offered on the website, the confusion persisted. To address this con- cern and bridge the gap between classifier output and user expecta- tions, in the next section we describe three changes in how scores are presented: a non-linear re-calibration of the model, a linear rescaling of scores to the [0,5] range, and the introduction of a new statistics called complete automation probability (CAP). These changes were incorporated into the latest version of Botometer (v3), launched in May 2018, together with other improvements described in Section 5. Users seem to appreciate these changes, as shown in Figure 4. 4 | BOT SCO RE INT ERPRETAB I LITY 4.1 | Model calibration As previously described, the raw output of the random forest classifier is not a probability. Consider the classifier C and two Twitter accounts A and B such that C(A) = x and C(B) = y with x < y. Here we can say that account A is less likely to be a bot than account B, but we cannot say that account A is x% likely to be a bot. While this is not a problem (a) (b) (c) (d) FIGURE 2 Botometer user experience survey questions and responses. Questions (a) and (b) are required; others are optional FIGURE 3 Responses to the Botometer user experience survey question about error type concerns (Figure 2(d)), grouped by frequency of usage. Respondents who use Botometer daily or weekly are considered frequent users 52 YANG ET AL . for measures like area under receiver operating characteristic curve (AUC), which quantifies the probability that a classifier will rank a ran- domly chosen bot higher than a randomly chosen human, it does pre- sent an obstacle when interpreting bot scores. Proper contextualization of the raw classifier output requires comparison to the rest of the scores in the training set. For example, maximum-likelihood estimation is employed by Varol, Ferrara, Davis, et al. (2017) to find the appropriate cutoff for binary classification: the best score above which an account is classified as a bot and below which it is classified as human. This threshold approach is useful for scientists counting bots in a collection (Gorwa, 2017; Haustein et al., 2016; Shao, Ciampaglia, et al., 2018; Stella et al., 2018; Suárez-Ser- rato, Roberts, Davis,  Menczer, 2016), but it is not intuitive for humans evaluating a single account. A calibrated classifier C0 is one whose output can be interpreted as a probability, that is, one in which C0 (A) = x means that the classifier estimates as x% the probability that account A is a bot. As a corollary, the binary classification threshold of a well-calibrated model is 50%. Calibrated models are ideal from the point of view of interpreting their results. To generate such a calibrated model C0 from our classifier C, we want a calibration function F such that F _x000E_ C = C0 . To calibrate Botometer, we employed Platt's scaling (Niculescu- Mizil  Caruana, 2005), a logistic regression model trained on classifier outputs. Figure 5 presents the initial model outcomes and the cali- brated scores. Note that this mapping shifts scores within the unit interval but preserves order, therefore leaving the AUC unchanged. The figure also shows reliability diagrams for raw and calibrated scores (DeGroot  Fienberg, 1983). We split the unit interval into 20 bins; an instance in the training dataset is assigned to a bin based on its pre- dicted (raw) score. For each bin, the mean predicted score is com- puted and compared against the fraction of true positive cases. In a well-calibrated model, the points should align with the diagonal. Observe that the blue line on the right side of Figure 5 is steepest in the middle of the range; this is because most uncalibrated bot scores fall near the middle of the unit interval. Since users, when presented with a single uncalibrated bot score, do not know that most scores fall into this relatively narrow range, they are misled into perceiving uncertainty about classification of most accounts. The flatter red line in the plot shows that each bin has approximately the same number of scores in the calibrated model. As mentioned earlier, the percentage of bot accounts is estimated around 9–15% (Varol, Ferrara, Davis, et al., 2017), which means the probability of a randomly selected account being automated is low. The calibrated score cannot reflect this fact, since it is a likelihood estimation based on our roughly balanced (half humans, half bots) training datasets. In the next subsection we offer a Bayesian posterior probability that overcomes this problem. The bot score presented on the current Botometer website (v3) is the calibrated likelihood score, linearly transformed to a [0, 5] scale in order to differentiate it from the probability that an account with that score is a bot. The API, whose users tend to be more advanced, also provides uncalibrated scores. 4.2 | Complete automation probability Users of Botometer are asking the question: “Is this account a bot, or not?” While model calibration makes the bot scores easier to interpret, a calibrated model still lacks one additional piece of information nec- essary to answer this “bot or not” question: the background level of bots in the population. To see why, consider that the less likely a claim, the more evidence is required to accept it. In our case, the rarer the bots, the higher the score needed to have confidence that an account is a bot. This connection between likelihood and background probability is formalized by Bayes' theorem. The “bot or not” question can be expressed mathematically as P(Bot| S), the conditional proba- bility that an account is a bot given its bot score S. Applying Bayes's rule allows us to rewrite the conditional probability as: P BotjSð Þ = P Botð Þ P SjBotð Þ P Sð Þ : ð1Þ We call P(Bot| S), the quantity we are after, the posterior probability. The prior probability P(Bot) is the background probability that any ran- domly chosen account is a bot. The P SjBotð Þ P Sð Þ term is called the evidence. It compares the likelihood that a bot has score S, P(S| Bot), with the probability that any account has that score. Since this is a ratio of functions of the score S, it is not affected by transformations of S as done in model calibration. Therefore we use raw, uncalibrated scores in our calculations. To calculate the posterior probability, we can expand the denomi- nator in the evidence term of Equation 1: P Sð Þ = P SjBotð ÞP Botð Þ + P SjHumanð ÞP Humanð Þ = P SjBotð ÞP Botð Þ + P SjHumanð Þ 1 − P Botð Þð Þ: ð2Þ The task is then to obtain distributions for the likelihoods P(S| Bot) and P(S| Human). The model's training data provides empiri- cal distributions of scores for both humans and bots (Figure 6); density estimation can be used to find a probability density function likely to produce a particular empirical distribution. Binning is the simplest approach to density estimation, sometimes employing a sliding win- dow. However, this approach proved unsuitable because quantization FIGURE 4 Botometer user experience survey result: Comparison between Botometer v2 and v3 YANG ET AL . 53 artifacts in the Botometer classifier output lead to discontinuities in the density functions. The existence of these artifacts suggests curve-fitting or kernel density estimation (KDE) methods. KDE is widely used for generating distribution functions from empirical data, but the commonly used implementations assume unbounded support, that is, a theoretically infinite range of possible bot score values. Since the support of our distribution is necessarily the unit interval, the use of KDE in this case leads to biased estimates near the interval boundaries (Leblanc, 2012). Seeking to avoid this systematic bias, Botometer uses curve fit- ting, choosing among curves that naturally admit a bounded support. One well-known candidate meeting this criterion is the beta distribu- tion. However, three different fitting algorithms (maximum-likelihood, method of moments, and negative log-likelihood) all failed to produce adequate fits. This is likely due to the shape of the P(S| Human) curve, with its mode near, but not at, zero. Ultimately we chose a lesser- known family of curves for our fit: Bernstein polynomials (Babu, Canty,  Chaubey, 2002). Implementation is straightforward as com- puting values of a polynomial curve is simple, even one with high degree. Experiments showed that a 40th-degree polynomial fit pro- duced satisfactory results (Figure 6). With likelihood curves generated from our classifier testing data, it is straightforward to calculate the evidence term in Bayes' rule. The other term is the prior P(Bot), the background probability of a given account being a bot. This is necessary because the training data does not include information on how common bots are in the wild. Failure to take the prior into account leads one to overestimate the number of bots in a sample due to humans being more common than bots, FIGURE 5 Calibration of the bot scores. The mapping function projects raw classifier outputs to calibrated sco","Yang, K. C., Varol, O., Davis, C. A., Ferrara, E., Flammini, A., & Menczer, F. (2019). Arming the public with artificial intelligence to counter social bots. Human Behavior and Emerging Technologies, 1(1), 48-61."
SCOP_297,"ClaimPortal: Integrated monitoring, searching, checking, and analytics of factual claims on twitter","We present ClaimPortal, a web-based plat form for monitoring, searching, checking, and analyzing English factual claims on Twitter. We explain the architecture of ClaimPortal, its components and functions, and the user interface. While the last several years have witnessed a substantial growth in interests and efforts in the area of computational fact-checking, ClaimPortal is a novel infrastructure in that fact-checkers have largely skipped factual claims in tweets. It can be a highly powerful tool to both general web users and fact checkers. It will also be an educational re source in helping cultivate a society that is less susceptible to falsehoods. While it currently focuses on politics-related tweets, it will be ex tended to include more general factual claims.",Computer Science,Proceeding,2019,N,Y,5,"The spreading of falsehoods on the web has ad- verse effects on a myriad of aspects in our society. Politicians are doubling down on claims that are demonstrably false because of the safety net that fake news” affords them. These efforts to manip- ulate and distort public opinions in order to gain political leverage can have negative effects on a democracy, and they can even result in the poten- tial manipulation of democratic election results. At news organizations such as The Washing- ton Post, New York Times and FactCheck.org, professional fact-checkers take on the hard bat- tle to counter misinformation and disinformation. They vet claims by analyzing relevant data and documents and publishing their verdicts. For in- stance, PolitiFact.com gives factual claims truth- fulness ratings such as true, half true, false, and even pants on fire”. However, there is simply far more misinformation on the web than what fact- checkers can keep up with. The process of fact- checking is laborious and intellectually demand- ing, as it takes the professionals about one day to research and write a typical article about a fac- tual claim (Hassan et al., 2015a). This difficulty leaves many harmful claims unchecked, since fact- checking organizations can only use their limited resources to focus on national events and promi- nent figures. This problem of unchecked claims is exacer- bated on social media. On the one hand, it is un- likely fact-checkers are able to check every social media post, due to limited resources and the sheer volume of data. 1 On the other hand, a large num- ber of false claims, likely much more than those in traditional media, are being spread through so- cial media. This can be due to the compounded effect of several factors: social media platforms have become increasingly important to public fig- ures and organizations in engaging with voters and citizens; mobile devices have brought an age in which sharing and disseminating information is easy for anyone, including both malicious and un- intentional creators of falsehoods; the falsehoods are further replicated and amplified by social me- dia bots and clickbait articles. The consequence can be devastating. For instance, a recent study reports that a sample of 140,000 Twitter users in the battleground state of Michigan shared as many junk news items as professional news during the final ten days of the 2016 election, each constitut- ing 23% of the web links they shared on Twitter in that period. 2 In this paper we present ClaimPortal, a web- based platform for monitoring, searching, check- ing, and analytics of factual claims on Twit- ter. ClaimPortal is available at. ClaimPortal con- tinuously collects tweets and monitors factual claims embedded in tweets. It is integrated with fact-checking tools, including a claim matcher which finds known fact-checks matching any given tweet, a claim spotter which scores each claim and the corresponding tweet based on their check-worthiness, i.e., how important it is to fact- check them. ClaimPortal provides an intuitive and convenient search interface that assists its users to sift through these factual claims in tweets us- ing filtering conditions on dates, twitter accounts, content, hashtags, check-worthiness scores, and types of claims. ClaimPortal also provides simple analytics and visualization tools for discovering patterns pertinent to how certain twitter accounts make claims, how different types of claims are dis- tributed, and so on. The initial call to arms to research on compu- tational fact-checking was made nearly a decade ago (Cohen et al., 2011). The last several years have witnessed a substantial growth in interests and efforts in this arena. These efforts tackle vari- ous fronts, from detecting important factual claims that are worth checking (Hassan et al., 2015b; Jimenez and Li, 2018), to using databases for discerning factual claims’ robustness (Wu et al., 2017) and truthfulness (Ciampaglia et al., 2015; Shi and Weninger, 2016; Jo et al., 2019), to build- ing end-to-end fact-checking systems (Babakar and Moy, 2016; Hassan et al., 2017a,b), and vi- sualizing the spread of claims (Shao et al., 2016). ClaimPortal is a novel infrastructure in that fact- checkers have largely skipped factual claims in tweets, especially those from less prominent ac- counts, due to limited resources. 2 System Architecture and Components 2.1 System Architecture ClaimPortal is composed of a front-end web based GUI, a MySQL database, an Elasticsearch 3 search engine, an API, and several decoupled batch data processing components (Figure 1). The system operates on two layers. The front-end presentation layer allows users to narrow down search results by applying multiple filters. Keyword search on tweets is powered by Elasticsearch which is cou- pled with querying the database to provide addi- tional filters. Additionally, it provides numerous visualized graphs. The back-end data collection and computation layer performs pre-processing elasticsearch of tweets, computing check-worthiness scores of tweets using the public ClaimBuster API (Hassan et al., 2017a), Elasticsearch batch insertion, de- tecting claim types of tweets, and finding similar fact-checked claims for each tweet, using Claim- Buster API. ClaimPortal stays up-to-date with current tweets by periodically calling the Twitter REST API. Figure 1: ClaimPortal system architecture. 2.2 Monitoring, Processing, and Storing Tweets ClaimPortal at this moment focuses on politically- charged tweets, but will be expanded to eventu- ally cover all types of tweets. We curated a list of prominent Tweet handles in U.S. politics that include but are not limited to house representa- tives and senators in the Congress, governors, city mayors, U.S. Cabinet members, other government officials, and political teams of news media. We then made use of the user timeline endpoint of the Twitter REST API to navigate through each user’s timeline and collected their tweets. More specifically, we navigated through the historic data of a user’s timeline, which is a one-time process. We then keep our data up-to-date by continuously monitoring newly posted tweets. As of April 10, 2019, ClaimPortal monitors 3,200 Twitter handles and has collected approximately 3.3 million tweets after being deployed in mid-January 2019. We are working on substantially expanding the curated list of Twitter handles. Claim Type FrameNet Frames Conflict Invading, Attack, Explosion, Destroying, Hostile encounter, Use firearm, Shoot projectiles, Downing, Protest, Political actions Life Giving birth, Being born, Death, Killing, Forming relationships, Cause harm, Personal relationship, Dead or alive Movement Self motion, Inhibit movement, Travel, Departing, Arriving, Visiting, Motion, Cause motion, Bringing Transaction Import export scenario, Commerce buy, Commerce sell, Getting, Commerce pay, Borrowing, Giving Business Activity start, Conquering, Endeavor failure, Intentionally create, Business closure, Locale closure Contact Meet with, Discussion, Come together, Communication, Contacting, Communication means, Text creation, Request Personnel Take place of, Get a job, Hiring, Appointing, Removing, Firing, Quitting, Choosing, Becoming a member, Change of leadership Justice Arrest, Imprisonment, Detaining, Extradition, Breaking out captive, Try defendant, Pardon, Appeal, Verdict, Sentencing, Fining, Execution, Releasing, Notification of charges Comparison Comparing two entities, Comparing at two different points in time Quantity Change position on scale, Creating, Causation, Cause change of position on a scale, Occupy rank, Ratio Stance Taking sides, Opinion, Be in agreement on assessment, Vote, Oppose and Support Consistency Speech Statement, Affirm or deny, Telling Table 1: Claim types and their corresponding FrameNet frames. Frames in red color are created by us. ClaimPortal’s back-end layer focuses on data processing and storage. The Twitter REST API provides us with the necessary data. However, the system does not require all of it. In fact, a lot of the API’s response is discarded to keep our database small and yet sufficient enough to pro- vide all necessary information for the portal. This is achieved through the ClaimPortal API. The API is a web service designed using Python and the Flask 4 micro-framework. It provides end points for loading tweets on the GUI, search for hashtags, and search for users in applying from-user and user-mention filters. Based on the keyword search and filters requested by a user, the API queries the database to find the resulting list of tweet IDs and returns the list as a JSON response. A tweet ID is a unique number assigned to a tweet by Twitter. By using Twitter’s card API 5 the system dynami- cally populates the latest activity of a tweet at the front-end, based on its ID. The MySQL database has several normalized tables. For each tweet the database stores its text, when it was created, and who tweeted it. The database also stores information about re-tweets and quoted-tweets, hashtags and URLs mentioned in the tweets, and information about the accounts mentioned in the tweets. ClaimPortal uses Elasticsearch to support key- word search over the stored tweets. Since Elastic- search is equipped with incremental indexing, the system periodically feeds Elasticsearch the delta tweets since last update for indexing. For this the system uses a decoupled background batch process that takes care of incrementally inserting tweets and updating the Elasticsearch index. docs/tweets/optimize-with-cards 2.3 Claim Spotter In ClaimPortal, each tweet is given a check- worthiness score which denotes whether the tweet has a factual claim of which the truthfulness is im- portant to the public. This score is obtained by probing the ClaimBuster API,6 a well-known fact- checking tool, developed by our research group, that is being used by professional fact-checkers on a regular basis (Adair et al., 2019). Claim- Buster (Hassan et al., 2017a; Jimenez and Li, 2018) is a classification and ranking model trained on a human-labeled dataset of 8,000 sentences from past U.S. presidential debates. The Claim- Buster API returns a check-worthiness score for any given text. The score is on a scale from 0 to 1, ranging from least check-worthy to most check- worthy. The background task of probing Claim- Buster API for getting scores for tweets is another batch process, in parallel with the tweet collection and the Elasticsearch indexing processes. 2.4 Detecting Claim Types ClaimPortal uses tweets to gain insights into fac- tual claims that are being spread, by whom, how often, and whether they are true. To answer these questions we categorize tweets by the types of fac- tual claims they promote. We employed a collec- tion of FrameNet frames (Baker et al., 1998) and created several new frames specifically for factual claims. We then adopted the study of mapping frames to event types (Spiliopoulou et al., 2017). 2.4.1 Frame detection FrameNet is a linguistic resource for English com- prised of 1,224 manually established semantic frames. Each frame provides information about both the linguistic and the semantic structure of a type of event, situation, object, or relation along with its participants. The participants, called (a) (b) Figure 2: (a) ClaimPortal user interface. (b) Similar fact-checks for the highlighted tweet in Figure (a). frame elements, are frame-specific semantic roles that provide additional information. Each frame is evoked by a set of lexical units, or words, which are a composition of the lemma and meaning of the word. We created new frames after conducting a sur- vey of existing fact-checks from PolitiFact 7 and followed it by grouping together semantically and syntactically similar factual claims from these fact-checks. If a group of claims did not share a common existing frame, we created a new frame for it. Details of these purposely created new frames can be found in (Arslan et al., 2019). The corpus of the newly-defined frames along with their annotated exemplary sentences is publicly available. 8 We used open-sesame (Swayamdipta et al., 2017), a recurrent neural network based frame- se- mantic parser, to detect all possible frames a tweet can potentially hold. We retrained open-sesame on FrameNet 1.7 dataset after extending it with annotated sentences for the newly defined frames. Open-sesame works as a pipeline of several tasks: target identification (detecting all lexical units), frame identification (detecting all frames in a sen- tence), and argument identification. 2.4.2 Claim type mapper In (Spiliopoulou et al., 2017) eight ACE event types were listed along with their mapped frames: Business, Conflict, Contact, Justice, Life, Move- ment, Personnel, and Transaction. To accommo- date the new frames explained in Section 2.4.1, we extended this list by introducing four new event types, namely Comparison, Quantity, Stance, and Speech, and their corresponding frames (Table 1). In ensuing discussion, we refer to these event types as claim types, for simplicity of terminology. More specifically, Comparison is for claims that show entities involved in some sort of comparisons based on some criteria, Quantity presents claims with quantities, Stance is for claims that have en- tities with viewpoints towards issues, events, etc., and Speech is for claims that communicate some messages in the written or spoken form. A script identifies the claim types of each tweet by map- ping identified frames to their corresponding claim Figure 3: Examples of visualizations on ClaimPortal website. types. A tweet can have multiple claim types. 2.5 Claim Matcher Claim matching is an important step in the work- flow of fact-checking. Given a factual claim, it aims at finding identical or similar claims from a repository of existing fact-checks. The premise is that public figures keep making the same false claims. While politicians may refrain themselves from making outright false claims to avoid being fact-checked, oftentimes they even double down after their false claims are debunked. 9 ClaimPortal leverages the claim matching func- tion in the ClaimBuster API. The fact-check repository is composed of the Share-the-facts 10 fact checks as well as fact checks collected from several fact-checking organizations like PolitiFact, Snopes, factcheck.org, Washington Post, etc. The system measures the similarity between a claim and a fact-check based on the similarity of their tokens. An Elasticsearch server is deployed for searching the repository based on token similarity. 3 User Interface Features ClaimPortal enables a user to sift through the tweets using multiple filters. The important fil- ters are as follows. (1) Keyword search: It al- lows users to make a text-based search by key- words such as climate change”. (2) Hashtags: It allows users to further filter tweets by hashtags such as #116thCongress” or #2020”. (3) Claim type: It enables users to search for tweets with a specific claim type, e.g., Conflict or Stance. (4) From: It looks for tweets posted by a particu- lar user handle, e.g., @realDonaldTrump”. (5) Mentions: The search results can be filtered fur- ther by user mentions (i.e., using @” to tag a user in a tweet, e.g., @POTUS”). (6) ClaimBuster score: ClaimPortal also offers a slider to filter re- sults based on a ClaimBuster score range. The re- sult tweets are automatically updated as the slider is moved. (7) Date range: Additionally, the portal offers a date picker to filter tweets based on their creation dates. Figure 2a shows ClaimPortal user interface with the search results of a sample query. The sample query contains the following filtering conditions: a keyword climate change, a claim type Stance, a range of ClaimBuster score from 0.3 to 1.0, and a date range from January 1, 2019 to April 1, 2019. Moreover, the ClaimPortal shares previously fact-checked claims with users by dis- playing matching fact-checks after a tweet’s card view is clicked at. Figure 2b depicts the matching fact-checks of the highlighted tweet in Figure 2a. 4 Analytics and Visualizations We work to make ClaimPortal the repository where one can find all factual claims made on Twitter. It can be a powerful tool for a diverse group of users. It enables web users to explore and analyze factual claims in tweets at scale. We use analytics and visualizations to shed more light on the importance of ClaimPortal and bring the hidden patterns in the data to light. For instance, a user can compare tweets from different political groups in detail based on check-worthiness of their claims and variety of their claims. Figures 3d and 3e compare Democratic Senators and Republican Senators based on the types of claims they made and check-worthiness of their claims. Figure 3a depicts the spread of all claim types made by dif- ferent group of politicians in the past one year and Figure 3b shows the distribution of tweets over five ClaimBuster score ranges made by different group of U.S. politicians such as the 2020 presi- dential election candidates.","Majithia, S., Arslan, F., Lubal, S., Jimenez, D., Arora, P., Caraballo, J., & Li, C. (2019, January). ClaimPortal: Integrated monitoring, searching, checking, and analytics of factual claims on twitter. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations."
SCOP_339,Detection and visualization of misleading content on Twitter,"The problems of online misinformation and fake news have gained increasing prominence in an age where user-generated content and social media platforms are key forces in the shaping and diffusion of news stories. Unreliable information and misleading content are often posted and widely disseminated through popular social media platforms such as Twitter and Facebook. As a result, journalists and editors are in need of new tools that can help them speed up the verification process for content that is sourced from social media. Motivated by this need, in this paper, we present a system that supports the automatic classification of multimedia Twitter posts into credible or misleading. The system leverages credibility-oriented features extracted from the tweet and the user who published it, and trains a two-step classification model based on a novel semi-supervised learning scheme. The latter uses the agreement between two independent pretrained models on new posts as guiding signals for retraining the classification model. We analyze a large labeled dataset of tweets that shared debunked fake and confirmed real images and videos, and show that integrating the newly proposed features, and making use of bagging in the initial classifiers and of the semi-supervised learning scheme, significantly improves classification accuracy. Moreover, we present a Web-based application for visualizing and communicating the classification results to end users.",Information Science,Article,2018,Y,Y,42,"Popular social media platforms such as Twitter and Face- book are nowadays an integral part of the journalistic and This work has been supported by the REVEAL and InVID projects, under Contract Nos. 610928 and 687786, respectively, funded by the European Commission. B Symeon Papadopoulos papadop@iti.gr Christina Boididou christina.mpoid@gmail.com Markos Zampoglou markzampoglou@iti.gr Lazaros Apostolidis laaposto@iti.gr Olga Papadopoulou olgapapa@iti.gr Yiannis Kompatsiaris ikom@iti.gr 1 Urban Big Data Centre, Glasgow, UK 2 CERTH-ITI, Thessaloniki, Greece news diffusion process. This is not only due to the fact that these platforms have lowered the barrier for citizens to contribute to news generation and documentation with their own content, but also due to the possibilities they offer for rapidly disseminating news to one’s network of contacts and to broader communities. These new capabilities with respect to publishing and sharing content have led to the uncontrolled propagation of large volumes of news content over social net- works. It is now possible for a news story published by an individual to reach huge numbers of readers in very short time. This is especially true for cases where multimedia con- tent (images, videos) is involved. Those often undergo faster and wider sharing (and sometimes become viral) due to the fact that multimedia is easy to consume and is often used as evidence for a story. The high volume and dissemination speed of news- relevant social media content creates big challenges for the journalistic process of verification. On the one hand, news organizations are constantly looking for original user- generated content to enrich their news stories. On the other hand, having very little time at their disposal to check the veracity of such content, they risk publishing content that 72 International Journal of Multimedia Information Retrieval (2018) 7:71–86 is misleading or utterly fake, which would be detrimental to their credibility. For instance, in the case of a breaking story (e.g., natural disaster, terrorist attack), there is a mas- sive influx of reports and claims, many of which originate from social media. It is exactly this setting where the risk of falsely accepting misleading content as credible is the high- est. As misleading (or, for the sake of brevity, fake), we con- sider any post that shares multimedia content that does not faithfully represent the event that it refers to. This could, for instance, include (a) content from a past event that is reposted as being captured in the context of a currently unfolding similar event, (b) content that is deliberately manipulated (also known as tampering, doctoring or photoshopping), or (c) multimedia content that is published together with a false claim about the depicted event. Figure 1 illustrates a famous” example of a fake photograph that is often recy- cled after major hurricanes and supposedly depicts a shark swimming in a flooded freeway. It is noteworthy that despite this being a well-known case, there are numerous people who still fall for it (as attested by the number of retweets in each case). In contrast, as real, we define posts that share content that faithfully represents the event in question, and can there- fore be used in the context of news reporting. There are also in-between cases, such as for instance, posts that debunk fake content or refer to it with a sense of humor. Since those posts are quite obvious for human investigators, but rather hard for automatic classification systems, we consider them to be out of the scope of this work. The impact of fake content being widely disseminated can be severe. For example, after the Malaysia Airlines flight MH370 disappeared on March 2014, numerous fake images that became viral on social media raised false alarms that the plane was detected.1 This deeply affected and caused emotional distress to people directly involved in the inci- dent, such as the passengers’ families. In another case, on April 2013, a fake tweet was posted by the Associated Press account, which had been hacked for that purpose, stating that the White House had been hit by two explosions and that Barack Obama was injured.2 This caused the SP 500 index to decline by 0.9%, which was enough to wipe out $ 130 billion in stock value in a matter of seconds. Examples such as the above point to the need for methods that can identify misleading social media content. One of the first such attempts [9] used a supervised learning approach, in which a set of news-related tweets were annotated with respect to their credibility and then used to train a model to distinguish between the two classes; experiments were con- ducted on a dataset collected around trending news stories 1 snopes.com/photos/airplane/malaysia.asp. 2 www.theguardian.com/business/2013/apr/23/ap-tweet-hack-wall- street-freefall. and annotated with the help of crowd workers, leading to an accuracy of approximately 86%. However, this level of performance was achieved by performing feature selection on the whole dataset (i.e., both training and test) and by a cross-validation approach that did not ensure full indepen- dence between the events included in the training and test sets, respectively. Furthermore, some of the employed credi- bility features, such as the retweet tree of a tweet, are hardly applicable in a real-time setting. Follow-up research on the problem [10] suffered from similar issues, i.e., the leaking” of information from the training set into the test set, thus giving an optimistic sense of the achievable classification accuracy. In this paper, which offers an extended presentation and more thorough treatment of our previous work [7], we present an approach that moves beyond the supervised learning paradigm for classifying social media content into credible (real) or misleading (fake). The proposed approach uses a variety of content-based and contextual features for the social media post in question and builds two classification models that are used to produce two independent first-level predic- tions regarding the credibility of the post. At a second step, a top-level classifier leverages these first-level predictions on unseen” content for retraining the best of the first-level models, following a semisupervised learning paradigm. In that way, the resulting model is well tuned to the special characteristics of the unseen content and produces more con- fident predictions. Experiments on a public annotated corpus of multimedia tweets demonstrate the effectiveness of the proposed approach. Additionally, we propose a Web-based user interface for visualizing and communicating the result of automatic analysis to end users. The contributions of this work include the following: (1) the use of a feature set for the representation of users and tweets, extending the ones used by previous studies [1,9,10]; (2) the application of an agreement-based retraining scheme, previously proposed in [36] for the task of polarity classifi- cation, which allows the model to adapt to new, unknown datasets; (3) an extensive experimental study on a large annotated corpus of tweets investigating the impact of the proposed novelties and comparing with state-of-the-art meth- ods; (4) a Web-based application that allows users to test our approach for verification, and to further investigate the role of different features on the verification result. 2 Related work The presented work focuses on the problem of misleading social media content detection, and more specifically on Twitter posts (tweets) that are accompanied by multimedia content. More precisely, given a single tweet that claims to provide information on an event and contains an image or International Journal of Multimedia Information Retrieval (2018) 7:71–86 73 Fig. 1 Examples of fake shark image that was posted several times after major hurricanes in the USA (depicted posts refer to Sandy, Matthew, Harvy and Irma) video to support the claim, our task is to return an estimate of its credibility. Furthermore, given that news professionals are generally reluctant to trust black box” systems, a second objective is to be able to communicate the system’s output by illustrating which features matter most toward the final estimate. Finally, for the system to be applicable in the real world, it is important to ensure generalization across differ- ent events, i.e., to make sure that the system can adapt to new content. Given the above definition, the examined problem is related but distinct to several other problems. Hoax detection [17] is the problem of debunking entire stories posted on the Web. Thus, it deals with larger amounts of text than a single social media post, and it is typically not backed by multime- dia evidence. A similar problem is rumor detection. A rumor is an unverified piece of information at the time of its publica- tion. Typically, rumors do not directly correspond to a single piece of text or a social media post, but rather to a collection of items that disseminate it. Zubiaga et al. [43] present a survey of approaches for rumor detection, including veracity classi- fication and the collection and annotation of rumor-focused datasets from social media. Finally, a related problem is auto- mated fact-checking, which pertains to the classification of sentences into non-factual, unimportant factual, and check- worthy factual statements [12]. Fact-checking methods rely on structured knowledge from databases, such as FreeBase and DBpedia, which contain entities, events, and their rela- tions. The above problems are distinct from the one examined in this paper. For instance, hoax detection and fact-checking typically operate on different types of inputs than social media posts and commonly concern claims that can be verified via a combination of database cross-checking and reasoning. On the other hand, rumor detection operates on social media content, but considers collections of posts. In contrast, the focus in this paper is on the problem of veri- fying individual social media posts, typically posted in the context of an unfolding newsworthy event. This is an impor- tant differentiating factor, especially in the context of the first moments after a claim (expressed by an individual post) circulates in social media, when there is little or no contex- 74 International Journal of Multimedia Information Retrieval (2018) 7:71–86 tual information available (e.g., comments responding to the claim, networks of retweets). The particular problem studied in this paper was the focus of the Verifying Multimedia Use” benchmarking task, which was organized in the context of MediaEval 2015 [2] and 2016 [4]. According to the official task definition, given a tweet and the accompanying multimedia item (image or video) from an event that has the profile to be of interest in the international news, return a binary decision representing ver- ification of whether the multimedia item reflects the reality of the event in the way purported by the tweet”. In a comparative study that we recently conducted [6], we present a detailed comparison among three high-performing approaches on the problem, among which is the approach presented here. The typical methodology for detecting a misleading social media post is to extract a number of features from it, and clas- sify it using a machine learning algorithm. Typical features can be text-based, such as linguistic patterns or the presence of capital letters and punctuation, user-based, i.e., infor- mation extracted from the profile of the user account who made the post such as age or number of followers/friends, or interaction-based, such as the number of responses to the post. As mentioned in the introduction, the work by Castillo et al. [9] is one of the earliest attempts on the problem. The approach attempted to assess credibility at the event/topic level, i.e., produce a credibility score for an entire set of tweets discussing one event. The extracted features included text-based (e.g., tweet length, fraction of capital letters), user-based (e.g., account age, number of followers), topic- based (number of tweets, number of hashtags in the topic), and propagation-based, i.e., features describing a tree cre- ated from the retweets of a message. Besides the critique that the training and test cases were not entirely independent during the training/cross-validation process, the fact that the approach operates on the event level instead of the tweet level means it is not flexible enough for our task. However, many of the features are directly applicable to our task as well. Similarly, Vosoughi et al. [38] use text-, user-, and propagation-based features for rumor verification on Twit- ter. In a work that is directly comparable to the one presented here, Gupta et al. [10] train a system on a set of features in order to classify between tweets sharing fake images and tweets sharing real images on a dataset of tweets from Hurri- cane Sandy. In that way, tweet classification is used as a first step toward verifying the associated images. However, as mentioned in the introduction, the separation between train- ing and test cases was not adequate for reliably assessing the generalization ability of the method. In a similar work, O’Donovan et al. [22] performed an analysis of the distri- bution of various features within different contexts to assess their potential use for credibility estimation. However, their analysis remains preliminary in the sense that they only ana- lyze feature distributions and not their effectiveness on the classification task. In our work, we move one step further by directly analyzing the performance of different configu- rations and variations of our approach. More recently, Wu et al. [39] presented a classifier trained on posts from the Chi- nese microblogging platform Sina Weibo. Besides typical features, the paper presents a propagation tree” that models the activity following a post (reposts, replies). This, however, is only applicable long time after a post is published, once a sufficiently large propagation tree is formed. Another recent approach is that of Volkova et al. [37], where Twitter posts are classified into suspicious” versus trusted” using word embeddings and a set of linguistic fea- tures. However, the separation between the two classes is made based on the source, i.e., by contrasting a number of trusted accounts to various biased, satirical, or propaganda accounts. This approach likely ends up classifying the writ- ing styles of the two distinct types of account, while in our case no distinction between trusted and non-trusted accounts was made during model building. Similarly, Rubin et al. [29] use satirical cues to detect fakes, which only applies to a spe- cific subset of cases. Another category of methods attempt to include image features in the classification, under the assumption that the image accompanying a post may carry distinct visual characteristics that differ between fake and real posts [14,34]. While this assumption may hold true when contrasting verified posts by news agencies to fake posts by unverified sources, it certainly cannot assist us when compar- ing user-generated fake and real posts. One typical example is fake posts that falsely share a real image from a past event and claim that it was taken from a current one. In this case, the image itself is real and may even originate from a news site, but the post as a whole is fake. Since we are dealing with multimedia tweets, one seem- ingly reasonable approach would be to directly analyze the image or video for traces of digital manipulation. To this end, the field of multimedia forensics has produced a large num- ber of methods for tampering detection in images [23,31,42] and videos [24] in the recent years. These include looking for (often invisible) patterns or discontinuities that result from operations such as splicing [42], detecting self-similarities that suggest copy–move/cloning attacks [31], or using near- duplicate search to build a history of the various alterations that an image may have undergone in its past (image phylo- genies”) [23]. However, such methods are not well suited for Web and social media images, for a number of rea- sons: – Splicing detection algorithms are often not effective with social media images, as these typically undergo numer- ous transformations (resaves, crops, rescales), which eliminate the tampering traces. International Journal of Multimedia Information Retrieval (2018) 7:71–86 75 – Building an image phylogeny requires automatically crawling the Web for all instances of an image, which is an extremely costly task. – It is highly likely that an image may convey false infor- mation without being tampered. Such is the case, e.g., of posting an image from a past event as breaking news, or of misrepresenting the context of an authentic image. Therefore, an image disseminating false information in social media may no longer contain any detectable traces of tam- pering, or it may even be untampered in the first place. For that reason, we turn to the analysis of tweet- and user-based features for verification. Finally, an important aspect of the problem is not only to be able to correctly classify tweets, but also to present verification results to end users in a manner which is under- standable and can be trusted by end users. Currently, there exist a few online services aiming to assist professionals and citizens with verification. The Truthy system [26] is a Web service that tracks political memes and misinformation on Twitter, aiming to detect political astroturfing, i.e., organized posting of propaganda disguised as grassroots user contri- butions. Truthy collects tweets, detects emerging memes, and provides annotation on their truthfulness based on user manual annotation. RumorLens [28] is a semiautomatic plat- form combining human effort with computation to detect new rumors in Twitter. TwitterTrails [20] tracks rumor propaga- tion on Twitter. There also exist some fully automatic tools, such as TweetCred [11] which returns credibility scores for a set of tweets, and Hoaxy [30], a platform for detecting and analyzing online misinformation. Finally, with respect to analyzing multimedia content, there are two notable tools: (a) the REVEAL Image Verification Assistant [41], which exposes a number of state-of-the-art image splicing detection algorithms via a Web-user interface, and (b) the Video News Debunker [35], which was released by the InVID project as a Chrome plug-in, to assist investigators in verifying user- generated news videos. 3 Misleading social media content detection Figure 2 depicts the main components of the proposed frame- work. It relies on two independent classification models built on the training data using two different sets of features: tweet- based (TB) and user-based (UB). Model bagging is used to produce more reliable predictions based on classifiers from each feature set. At prediction time, an agreement-based retraining strategy is employed, which combines the outputs of the two bags of models in a semisupervised learning man- ner. The verification result is then visualized to end users. The training of classification models and a set of feature dis- tributions that are used by the visualization component are based on an annotated set of tweets, the so-called Verification Corpus, which is further described in Sect. 4. The implemen- tation of the framework and the corpus are publicly available on GitHub. 3,4 3.1 Feature extraction and processing The design of features used in our framework was carried out following a study of the way in which news professionals, such as journalists, verify content on the Web. Based on rel- evant journalistic studies, such as the study of Martin et al. [19], and the Verification Handbook [32], as well as on pre- vious similar approaches [9,10], we defined a set of features that are important for verification. These are not limited to the content itself, but also pertain to its source (Twitter account that made the post) and to the location where it was posted. We decided to avoid multimedia forensics features follow- ing the conclusion of our recent study [40] that the automatic processing of embedded multimedia on Twitter remove the bulk of forensics-relevant traces from the content. This was also confirmed by our recent MediaEval participation [3,5], where the use of forensics features did not lead to noticeable improvement. The feature extraction process produces a set of TB and UB features for each tweet, which are presented in Table 1. Tweet-based features (TB): we consider four types of feature related to tweets: (a) text-based, (b) language- specific, (c) Twitter-specific, and (d) link-based. (a) Text-based These are extracted from the text of the tweet, and include simple characteristics (length of text, number of words), stylistic attributes (number of ques- tion and exclamation marks, uppercase characters), and binary features indicating the existence or not of emoti- cons, special words (please”) and punctuation (colon). (b) Language-specific These are extracted for a prede- fined set of languages (English, Spanish, German), which are detected using a language detection library. 5 They include the number of positive and negative sentiment words in the text using publicly available sentiment lex- icons: For English, we use the list by Jeffrey Breen, 6 for Spanish the adaptation of ANEW [27], and for Ger- man the Leipzig Affective Norms [15]. Additional binary features indicate whether the text contains personal pro- nouns (in the supported languages), and the number of detected slang words. The latter is extracted using lists 3 github.com/MKLab-ITI/computational-verification. 4 github.com/MKLab-ITI/image-verification-corpus. 5 code.google.com/p/language-detection/. 6 github.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107. 123 76 International Journal of Multimedia Information Retrieval (2018) 7:71–86 Fig. 2 Overview of the proposed framework. MV stands for majority voting Table 1 Overview of verification features Tweet-based features (TB) User-based features (UB) Text-based User-specific #words* has please” #friends* has location length of text* has colon #followers* has existing location #question marks* contains happy emoticon* follower–friend ratio* has bio description #exclamation marks* contains sad emoticon* #tweets tweet ratio contains question mark* #uppercase chars* #media content account age contains exclamation mark* has profile image is verified* has header image #times listed* has a URL* Language-specific Link-based (common for TB and UB) #pos senti words* contains 1st pers.pron.* WOT score alexa country rank #neg senti words* contains 2nd pers.pron.* in-degree centrality alexa delta rank #slangs contains 3rd pers.pron.* harmonic centrality alexa popularity #nouns readability alexa reach rank Twitter-specific #retweets* #mentions* #hashtags* #URLs* has external link Link-based features are extracted in the TB case for external links that tweets may share, and in the UB case for the URL included in the account profile. Features with an asterisk were proposed in [1,10] and will be referred to as Baseline Features (BF), while the full feature set (union of BF and newly proposed ones) will be referred to as Total Features (TF) of slang words in English7 and Spanish. 8 For German, no available list was found and hence no such feature is computed. Moreover, the number of nouns in the text was also added as feature, and computed based on the Stanford parser only for English [16]. Finally, we use the 7 onlineslangdictionary.com/word-list/0-a/. 8 www.languagerealm.com/spanish/spanishslang.php. Flesch Reading Ease method9 to compute a readability score in the range [0: hard to read, 100: easy to read]. For tweets written in languages where the above features cannot be extracted, we consider their values missing. (c) Twitter-specific These are features related to the Twitter platform, including the number of retweets, hash- 9 github.com/ipeirotis/ReadabilityMetrics. International Journal of Multimedia Information Retrieval (2018) 7:71–86 77 tags, mentions, URLs and a binary feature expressing whether any of the URLs points to external (non-Twitter) resources. (d) Link-based These include features that provide infor- mation about the links that are shared through the tweet. This set of features is common in both the TB and UB sets, but in the latter it is defined in a different way (see link-based category in UB features). For TB, depending on the existence of an external URL in the tweet, its reli- ability is quantified based on a set of Web metrics: (i) the WOT score, 10 which is a way to assess the trust on a website using crowdsourced reputation ratings, (ii) the in-degree and harmonic centralities,11 computed based on the links of the Web graph, and (iii) four Alexa met- rics (rank, popularity, delta rank and reach rank) based on the rankings API.12 User-based features (UB): These are related to the Twitter account posting the tweet. We divide them into (a) user- specific and (b) link-based features. (a) User-specific These include the user’s number of friends and followers, the account age, the follower– friend ratio, the number of tweets by the user, the tweet ratio (number of tweets/day divided by account age) and several binary features: whether the user is verified by Twitter, whether there is a biography in his/her profile, whether the user declares his/her location using a free text field, and whether the location text can be parsed into an actual location, 13 whether the user has a header or profile image, and whether a link is included in the profile. (b) Link-based In this case, depending on the exis- tence of a URL in the Twitter profile description, we apply the same Web metrics as the ones used in the link-based TB features. If there is no link in the pro- file, the values of these features are considered to be missing. After feature extraction, the next steps include prepro- cessing, cleaning, and transformation. To handle the issue of missing values on some of the features, we use linear regres- sion for estimating their values: We consider the attribute with the missing value as a dependent (class) variable and apply linear regression for numeric features. The method cannot support the prediction of Boolean values, and hence 10 www.mywot.com/. 11 wwwranking.webdatacommons.org/more.html. 12 data.alexa.com/data?cli=10url=google.com. 13 Using: github.com/socialsensor/geo-util. those are left missing. Only feature values from the train- ing set are used in this process. Data normalization is also performed to scale the numeric feature values to the range [− 1, 1]. 3.2 Building the classification models We use the TB and UB features to build two independent clas- sifiers (CL1 , CL2 , respectively), each based on the respective set of features. To further increase classification accuracy, we make use of bagging: We create m different subsets of tweets from the training set, including equal number of sam- ples for each class (some samples may appear in multiple subsets), leading to the creation of m instances of CL1 and CL2 (m = 9 in our experiments). These are denoted as CL11 , CL12, . . . CL1m and CL21, CL22, . . . CL2m , respectively, in Fig. 2. The final prediction for each of the test samples is cal- culated using the average of the m predictions. Concerning the classification algorithm, we tried both logistic regression (LR) and Random forests (RF) of 100 trees. 3.3 Agreement-based retraining A key contribution of the proposed framework is the intro- duction of an agreement-based retraining step (the fusion block in Fig. 2) as a second-level classification model for improving the generalization ability of the framework to new content. The agreement-based retraining step was motivated by recent work on social media sentiment analysis that was demonstrated to effectively address the problem of out-of- domain polarity classification [36]. In our implementation, we combine the outputs of clas- sifiers CL1, CL2 as follows: For each sample of the test set, we compare their outputs, and depending on their agreement, we divide the test set in the agreed and disagreed subsets. The elements of the agreed set are assigned the agreed label (fake/real) assuming that it is correct with high likelihood, and they are then used for retraining the best performing of the two first-level models (CL1, CL2)14 to reclassify the disagreed elements. Two retraining techniques are investi- gated: The first is to use just the agreed samples to train the CL classifier (denoted as CLag ), while the second is to use the entire (total) set of initial training samples extending it with the set of agreed samples (denoted as CLtot ). The goal of retraining is to create a new model that is tuned to the specific data characteristics of the new content. The result- ing model is expected to predict more accurately the values of the samples for which CL1, CL2 did not initially agree. In the experimental section, we test both of the above retraining variants. 14 The selection is based on their performance on the training set during cross-validation. 78 International Journal of Multimedia Information Retrieval (2018) 7:71–86 Fig. 3 Snapshot of the Tweet Verification Assistant interface. Given a tweet, a user can explore the verification result, including the extracted feature values and their distribution on the Verification Corpus 3.4 Verification result visualization The main idea behind the visualization of the produced veri- fication output is to present it along with the list of credibility features that were extracted from the input tweet and the user account that posted it, and to give to end users the option to select any of these features and inspect its value in relation to the distribution that this feature has for real versus fake tweets, as computed with respect to the verification corpus (Sect. 4). Figure 3 depicts an annotated screenshot of this applica- tion, which is publicly available. 15 In terms of usage, the investigator first provides the URL or id of a tweet of inter- est, and then the application presents the extracted tweet- and user-based features and the verification result (fake/real) for the tweet in the form of a color-coded frame (red/green, respectively) and a bar. It also offers the possibility of inspect- ing the feature values in the central column. By selecting a feature, its value distribution appears at the right column, separately for fake and real tweets (side by side). Moreover, a textual description informs the user about the percentage of tweets of this class (fake or real) that have the same value for this feature. In that way, the investigator may better under- stand how the verification result is justified based on the individual values of the features in relation to the typical” values that these features have for fake versus real tweets. 15 reveal-mklab.iti.gr/reveal/fake/. 4 Verification corpus Our fake detection models are based on a publicly avail- able verification corpus (V C) of fake and real tweets that we initially collected for the needs of organizing the MediaEval 2015 Verifying Multimedia Use (VMU) task [2]. 16 This con- sists of tweets related to 17 events (or hoaxes) that comprise in total 193 cases of real images, 218 cases of misused (fake) images and two cases of misused videos, and are associated with 6,225 real and 9,404 fake tweets posted by 5,895 and 9,025 unique users, respectively. The list of events and some basic statistics of the collection are presented in Table 2. Several of the events, e.g., Columbian Chemicals, Passport Hoax and Rock Elephant, were actually hoaxes, and hence all content associated with them is fake. Also, for several real events (e.g., MA flight 370), no real images (and hence no real tweets) were included in the dataset, since none came up as a result of the data collection. Figure 4 illustrates four example cases that are characteristic of the types of fake in the corpus. These include reposting of past images in the context of a new event, computer generated imagery, images accompanied by false claims, and digitally tampered images. The set of tweets T of the corpus was collected with the help of a set of keywords K per event. The ground truth labels (fake/real) of these tweets were based on a set of online arti- cles that reported on the particular images and videos. Only 16 The V C was since then expanded with new data that was used as part of the VMU 2016 task. However","Boididou, C., Papadopoulos, S., Zampoglou, M., Apostolidis, L., Papadopoulou, O., & Kompatsiaris, Y. (2018). Detection and visualization of misleading content on Twitter. International Journal of Multimedia Information Retrieval, 7(1), 71-86."
SCOP_349,Maverick: A system for discovering exceptional facts from knowledge graphs,"This paper presents Maverick, a system for discovering exceptional facts about entities in knowledge graphs. Maverick is built upon a beam-search based algorithmic framework which we proposed in a research paper that is published in SIGMOD 2018. In this demonstration proposal, we showcase an end-to-end system that includes a user-facing portal and a cache server. In Maverick, an exceptional fact about an entity of interest is modeled as a context subspace pair, in which the subspace is a set of attributes and the context is defined by a graph query pattern of which the entity is a match, together with other matching entities. The entity is exceptional among the entities in the context, with regard to the sub space. The portal allows users to search entities in a knowledge graph and explores exceptional facts about the entities of interest. It presents exceptional facts to users in forms of natural language sentences and illustration charts, for better interpretability of the discovered exceptional facts. The cache server stores intermediate computation results, such as pattern evaluations, exceptionality calculations, and candidate patterns. It is built for sharing computation across entities, such that repetitive computation across entities can be avoided.",Computer Science,Proceeding,2018,N,Y,2,"Knowledge graphs such as DBpedia, Freebase, Wikidata, and YAGO record properties of and relationships between real-world entities. These data are used in numerous applications, including search, recommendation, and business intelligence. Given an en- tity in a knowledge graph, we propose a system that discovers ex- ceptional facts about the entity. Informally, such exceptional facts separate the entity from many other entities. We have witnessed a lot of such facts in published news articles: ∗The bulk of the work was done while the author was at UT- Arlington. This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Proceedings of the VLDB Endowment, Vol. 11, No. 12 Copyright 2018 VLDB Endowment 2150-8097/18/8. DOI: 1. Denzel Washington followed Sidney Poitier as only the sec- ond black to win the Best Actor award.” (abcnews.go.com) 2. This was Brazil’s first own goal in World Cup history ...” ( yahoo.com) 3. Hillary Clinton becomes first female presidential nominee.” (chicagotribune.com) An exceptional fact consists of three components: an entity of interest, a context, and a set of qualifying attributes. In each excep- tional fact, among all entities in the context, the entity of interest is one of the few or even the only one that bears a particular value combination on the qualifying attributes. For example, in the above statement 1, the entity of interest is Denzel Washington, the context is the Academy Award Best Actor winners, and the qualifying at- tribute is ethnicity. Discovery of exceptional facts is useful to important applications such as computational journalism [1], recommendation systems, and data cleaning. a) In fact-finding [6, 4], journalists are interested in monitoring data and discovering attention-seizing factual state- ments such as the aforementioned examples. These facts help make news stories substantiated and interesting, and they may even be- come leads to news stories. b) In fact-checking [3], for vetting the statements made by humans, fact-checkers at news organizations such as The Washington Post, CNN, and PolitiFact can compare the statements with automatically-discovered facts. For example, an algorithm may find that Hillary Clinton is the second female pres- idential nominee, which contradicts with the statement 3 above. 1 c) Exceptional facts can help promote friends, news, products, and search results in various recommendation systems. d) When the discovered facts are inconsistent with known truth or apparent com- mon sense, it reveals incomplete data or data errors. Such insights aid knowledge base cleaning and completion. For example, the above statement 3 may be generated using an incomplete source that misses the nomination of Victoria Woodhull. To the best of our knowledge, there is no previous system for discovering exceptional facts about entities in knowledge graphs. The two most related areas are outlier detection in graphs and out- lying aspect mining. Duan et al. [2] and Vinh et al. [5] discussed the differences between these two areas. They achieve different goals. Outlier detection searches for all outlying objects among a set of objects. Outlying aspect mining, however, focuses on only one given object and returns the subspaces of attributes in which the object is relatively outlying, regardless of its true degree of outly- ingness. In terms of objectives and problem modeling, the excep- tional fact discovery problem is closer to outlying aspect mining than outlier detection. However, it focuses on graph data. In con- trast, existing outlying aspect mining methods assume a single rela- 1The first female presidential nominee was Victoria Woodhull, accord- ing. tional table and cannot be effectively applied to knowledge graphs, as explained in [7]. This paper presents Maverick, a system for discovering excep- tional facts about entities in knowledge graphs. Maverick is built upon the beam-search based framework we proposed in [7]. In Maverick, an exceptional fact about an entity of interest is mod- eled as a context-subspace pair, in which the subspace is a set of attributes and the context is defined by a graph query pattern of which the entity is a match, together with other matching entities. The entity is exceptional among the entities in the context, with regard to the subspace. The search spaces of both patterns and sub- spaces are exponentially large. Maverick conducts beam search on the patterns which uses a match-based pattern construction method to evade the evaluation of invalid patterns. It applies two heuris- tics to select promising patterns to form the beam in each itera- tion. Maverick traverses and prunes the subspaces organized as a set enumeration tree by exploiting the upper bound properties of exceptionality scoring functions. In this demonstration proposal, we showcase an end-to-end sys- tem that includes a user-facing portal and a cache server, in addi- tion to the algorithmic framework in [7]. The portal allows users to search entities in a knowledge graph and explores exceptional facts about the entities of interest. It provides functionalities such as entity auto-completion to aid users in searching and exploration. More importantly, Maverick presents exceptional facts to users in the forms of natural language sentences and illustration charts, for better interpretability of the discovered facts. The cache server is built for improving the performance of Maverick, which is achieved by avoiding repetitive computation across entities. Intermediate re- sults, such as pattern evaluations, exceptionality calculations, and candidate patterns, are stored in the cache server, so that excep- tional facts of similar entities can be efficiently discovered. 2. CONCEPTS Knowledge graph, Patterns, Matches A knowledge graph G(VG, EG) is a set of RDF triples with node set VG ⊆ I and edge set EG ⊆ VG × I × VG, where I is the universe of Internationalized Resource Identifiers (IRIs). A pattern is a weakly connected graph P (VP , EP ), where VP ⊆ I ∪ V, EP ⊆ VP × I × VP , and V is the universe of variables. A subgraph M of G is a match to pattern P if M is edge-isomorphic to P and, for each non-variable node v in P , its counterpart in the match has the same identifier. Context A context is a set of entities defined by a pattern and a variable in the pattern. The entities are those in the matching patterns that correspond to the variable. This definition implies that the entities in a context share some common characteristics, e.g., players who play for Brazil”. Attributes Given an entity v, its attributes Av is the union of its incoming and outgoing edge labels: Av = {(l, ←) | ∃(x, l, v) ∈ EG} ∪ {(l, →) | ∃(v, l, x) ∈ EG}. Given an attribute a, the attribute value v.a of an entity is the set of other incidents of the related edges. A subspace A is a subset of Av . Exceptionality function Given a context C of an entity v and a subspace A, an exceptionality scoring function χ(v, A, C) ∈ R measures entity v’s degree of exceptionality with regard to A in comparison with other entities in C. Without loss of generality, we assume the range of χ is [0, 1], with larger χ implying greater ex- ceptionality. We will demonstrate three exceptionality functions as described in [7]: Isolation, Outlierness, and One-of-the-Few. Isolation is based on the principle of minimum description length Figure 1: The framework of Maverick. (MDL). An entity is peculiar if the description length of its at- tributes is longer than others. Outlierness is based on the distribu- tion of attribute values. An entity receives a high score if it has rare attribute values while a lot of other entities share common attribute values. The crux of One-of-the-Few is that a factual claim about an entity is interesting when equally or more significant claims can be made about only few other entities. For example, Denzel Washington is one of the only two black who won the Best Actor award.” Exceptional Fact Discovery Problem Given a knowledge graph G, an entity of interest v0, an integer k, and an exceptionality scor- ing function χ, the problem of exceptional fact discovery is to find the top-k exceptional facts about v0. 3. SYSTEM OVERVIEW Figure 1 illustrates the components of Maverick. While the three components Context Evaluator (CE), Exceptionality Evaluator (EE), and Pattern Generator (PG) work together in an iterative fashion of beam search to discover exceptional facts, which are highly-scored context-subspace pairs, component Cache Server (CS) caches (in- termediate) results of PG, CE and EE to improve efficiency by avoiding repetitive computation, and component Fact Reporter re- ports highly-scored context-subspace pairs to users in natural lan- guage sentences as well as charts. Intuitively, the process can be viewed as nested loops. The outer loop enumerates contexts, while the inner loop enumerates subspaces for each context. Given the entity of interest v0, while subspace enumeration in the inner loop enumerates the subsets of Av0 , the outer loop enumerates contexts by patterns. Conceptually, Maverick organizes all the possible con- texts as a partial order on patterns, i.e., a Hasse diagram, in which each node is a pattern and each edge represents the subsumption (subgraph-supergraph) relationship between the two patterns. The essence of the outer loop is thus a traversal of the search space of patterns. At each iteration, Maverick maintains a beam B of a fixed size w. The beam consists of heuristically the best w patterns (e.g., P2, P3 in Figure 1 where w = 2). For each pattern P in B, component CE obtains the matches MP to the pattern and the correspond- ing contexts C of v0. For each context C in C (e.g. C1 in Fig- ure 1), component EE finds the top-k scored subspaces according to a given exceptionality scoring function χ. Component PG finds the children of the visited pattern based on its matches. Since there are usually much more children than what the beam size w allows, PG applies a set of heuristics to prune the child patterns. Each child pattern is given a score that measures how promising it is accord- ing to the heuristics. The best w patterns among all the children of patterns in B will become the new beam B, which is the input to the next iteration, e.g., {P7, P9} in Figure 1. When the algorithm terminates, Maverick returns the k context-subspace pairs with the highest exceptionality scores. Below we explain these components in more detail. Context Evaluator (CE) This component is responsible for ob- taining the matches to a given pattern as well as the corresponding contexts. CE employs a graph query system to take a pattern as the input and return all the matches to the pattern. Although CE in our implementation uses Neo4j as the graph query system, CE is agnostic to the choice of the specific query processing system. Exceptionality Evaluator (EE) For each context C of the en- tity of interest v0, this component finds the k subspaces A with the highest χ(v, A, C) scores. Section 4.1 briefly discusses how EE uses a set enumeration tree to avoid exhaustively enumerating exponential number of possible subspaces. Specifically, Maverick exploits the upper bound properties of exceptionality scoring func- tions to guide the traversal of the set enumeration tree. Pattern Generator (PG) At each iteration of the beam search on patterns, this component finds the children of each visited pattern P in the current beam. A child pattern, if not pruned (see Section 4.3), is given a score that measures how promising it is according to a few heuristics. Among all the children of the patterns in the current beam, the w children with the highest scores are returned to form the new beam, where w is the predefined beam width. The new beam becomes the input to the next iteration. Cache Server (CS) This component stores and reuses the inter- mediate results of the aforementioned components, to avoid repeti- tive computation in discovering facts about different entities. Enti- ties usually share common characteristics, for example, entities of the same type usually have the same set of attributes and share a lot of common patterns. As a result, CS can improve the efficiency of the system by caching the pattern evaluation results in compo- nent CE, the exceptionality calculations in component EE, and the search of candidate patterns in component PG. Fact Reporter (FR) This component presents exceptional facts of the given entity to users in form of natural language sentences as well as charts. In order to help users interpret the exceptionality of each fact, FR collects statistics about the qualifying attribute val- ues as well as the context, such as the attribute value distributions, context size and so on. Furthermore, FR provides summaries of the statistics in natural language sentences to improve the interpretibil- ity of the exceptional facts. 4. ALGORITHMS To discover the exceptional facts about an entity, we must ex- plore two extremely large search spaces, one of attribute subspaces and the other of patterns. In this section, we briefly explain how we approach the challenges. 4.1 Finding Top-k Subspaces EE applies a set enumeration tree to avoid exhaustively enumer- ating subspaces. Each node in the tree is a subspace—a subset of v’s attributes Av . The children of a node correspond to various su- persets of the node’s associated attributes. The gist is to explore the set enumeration tree using heuristic search methods such as best- first search and to prune branches that are guaranteed to not con- tain highly-scored subspaces. What is particularly challenging is that an exceptionality scoring function χ usually does not have the downward closure property with respect to subspace inclusion, i.e., χ(v, A, C) can be greater than, less than, or equal to χ(v, A′, C) for any A′ ⊇ A. EE uses upper bounds on the exceptionality scor- ing function χ to allow for pruning of the set enumeration tree.The set enumeration tree nodes (i.e., subspaces) are visited in the de- scending order of their upper bounds. If the upper bound score of a node is not greater than the score of the current k-th ranked subspace, the node and all its children are pruned. 4.2 Match-based Construction of Patterns In the search of context, Maverick organizes the candidate pat- terns as a Hasse diagram, in which each node is a pattern and each edge represents the subgraph-supergraph relationship between the two nodes. To construct the child patterns of a pattern P , a simple approach is to enumerate all possible ways of expand- ing P by adding one more edge. A major drawback of this ap- proach is it may construct many invalid patterns that do not have any match. Some invalid patterns can be easily recognized by re- ferring to the schema graph of the data. However, chances are most of the schema-abiding patterns are still invalid because they do not have matching instances in the data graph, given the sheer diversity of a knowledge graph. The system will evaluate such patterns in vain to get empty results in order to realize they are invalid. To avoid evaluating invalid patterns, we propose a match-based pattern construction method. Instead of constructing the child pat- terns by directly expanding pattern P , this method expands the matches of P and constructs the child patterns from the expanded matches. It guarantees to construct only valid patterns and evade the evaluation of invalid patterns. Specifically, given a match M of P , it finds each of its weakly connected supergraphs by adding an edge that exists in the data graph G and is adjacent to a node in M . The supergraphs are then used to generate P ’s child patterns by optionally replacing the newly added node with a variable. 4.3 Candidate Pruning and Candidate Rank- ing To ensure efficiency, PG employs two pruning rules to exclude irrelevant patterns and to avoid repeated constructions of patterns from certain type of parent patterns: 1) exclude a pattern if it does not define any context for the entity of interest v0; 2) expand a pat- tern only if the new edge has at least one variable. The rationale behind the first rule is, for discovering exceptional facts about v0, a pattern is relevant only if it defines a context for v0. Enforcing the second rule reduces the number of candidate patterns by eliminat- ing the patterns that define same contexts of v0. To further limit the number of candidates, PG applies two scor- ing heuristics for selecting promising patterns to visit: Optimistic hopt and Convergent hconv . hopt simply uses the exceptionality score upper bound of P , which essentially selects the patterns that define large contexts. The hconv score of P is a weighted sum of the upper bound of P (for any subspace) and the best score of the parent pattern P ′. Compared with hopt, hconv is potentially both more efficient and more effective. It can be more efficient since it may favor child patterns that define smaller contexts. Such child patterns usually can be evaluated more efficiently since they have less matches. It can be more effective since it discards child patterns that define contexts where the entity of interest may not be exceptional, based on the highest score of the context-subspace pairs for the parent pattern. 5. USER INTERFACE AND DEMONSTRA- TION PLAN We will demonstrate Maverick using two knowledge graphs: Film and FIFA World Cup. A demonstration video can be found a Entity search page b Exceptional fact page Figure 2: The main user interface of Maverick. in the supplementary materials. Film, a subgraph of Freebase, con- tains two Freebase domains: Film and Award. The knowledge graph FIFA World Cup is constructed from https:// www.fifa.com/ worldcup/ . Figure 2 shows the main user interface of Maverick when it is applied on Denzel Washington in Film. Some functions in the user interface are still under development, e.g., the natural lan- guage interpretation of exceptional facts. We are also constantly adding more features to the system. When a user visits the Maverick system, it shows a landing page which has a banner and a search box. The landing page is iden- tical to the top part of Figure 2a. The banner allows the user to choose a knowledge graph to explore. The search box allows the user to search entities by names. The search box is also equipped with auto-completion to help users search entities more efficiently. Figure 2a shows a screenshot when a user enters Denzel” in the search box. The page shows the descriptions of a list of matching entities to help the user choose the entity of interest. Below the de- scription of each entity, it provides a link to the page of exceptional facts of the entity, which will be described below. When there are many entities in the search result, the system provides a pagination bar at the bottom of the page for the user to navigate through the search results. Figure 2b shows a screenshot of the exceptional fact page of en- tity Denzel Washington. The exceptional fact page of a given entity displays a list of exceptional facts about the entity as well as the entity’s description. We will demonstrate exceptional facts based on 3 different exceptionality functions: Isolation, Outlierness, and One-of-the-Few. In the system, we will show top-5 exceptional facts from each exceptionality function. Each exceptional fact is shown as a fact card in Figure 2b. There are four elements in a fact card: interpretation, exceptionality score, context graph, and subspace chart. The interpretation is usually a sentence or two describing the fact. The exceptionality score is a real value between 0 and 1 indicating how exceptional the fact is. The context graph shows a graph pattern. The graph pattern along with the orange node together define the context of the entity. The subspace chart is a set of tangent circles. The outermost circle rep- resents the context. Each nested circle represents the entities sat- isfying the conjunctions of the qualifying attributes represented by outer circles. For example, let the set of qualifying attributes be {a1, a2, a3}, and the entity of interest be v0, then there are four circles in the subspace chart. From outermost to innermost, the cir- cles represent entities V0 = C, V1 = {v ∈ C | v.a1 = v0.a1}, V2 = {v ∈ C | v.a1 = v0.a1 ∧ v.a2 = v0.a2}, and V3 = {v ∈ C | v.a1 = v0.a1 ∧ v.a2 = v0.a2 ∧ v.a3 = v0.a3}, respectively, where C is the context. The size of each circle reflexes the cardi- nality of the corresponding set of entities in logarithmic scale.","Zhang, G., & Li, C. (2018). Maverick: a system for discovering exceptional facts from knowledge graphs. Proceedings of the VLDB Endowment, 11(12)."
SCOP_365,A context-aware approach for detecting worth-checking claims in political debates,"In the context of investigative journalism, we address the problem of automatically identifying which claims in a given document are most worthy and should be prioritized for fact-checking. Despite its importance, this is a relatively understudied problem. Thus, we create a new corpus of political debates, containing statements that have been fact-checked by nine reputable sources, and we train machine learning models to predict which claims should be prioritized for fact-checking, i.e., we model the problem as a ranking task. Unlike previous work, which has looked primarily at sentences in isolation, in this paper we focus on a rich input rep resentation modeling the context: relationship between the target statement and the larger context of the debate, interaction between the opponents, and reaction by the moderator and by the public. Our experiments show state-of-the-art results, outperforming a strong rivaling system by a margin, while also confirming the importance of the contextual information.",Computer Science,Proceeding,2017,Y,N,52,"The current coverage of the political landscape in the press and in social media has led to an unprece- dented situation. Like never before, a statement in an interview, a press release, a blog note, or a tweet can spread almost instantaneously and reach the public in no time. This proliferation speed has left little time for double-checking claims against the facts, which has proven critical in politics, e.g., during the 2016 presidential campaign in the USA, which was arguably impacted by fake news in social media and by false claims. Figure 1: Information verification pipeline. Investigative journalists and volunteers have been working hard trying to get to the root of a claim and to present solid evidence in favor or against it. Manual fact-checking has proven very time- consuming, and thus automatic methods have been proposed as a way to speed-up the process. For in- stance, there has been work on checking the factu- ality/credibility of a claim, of a news article, or of an information source (Castillo et al., 2011; Ba et al., 2016; Zubiaga et al., 2016; Ma et al., 2016; Hardalov et al., 2016; Karadzhov et al., 2017a,b; Nakov et al., 2017). However, less at- tention has been paid to other steps of the fact- checking pipeline, which is shown in Figure 1. The process starts when a document is made public. First, an intrinsic analysis is carried out in which check-worthy text fragments are identi- fied. Then, other documents that might support or rebut a claim in the document are retrieved from various sources. Finally, by comparing a claim against the retrieved evidence, a system can de- termine whether the claim is likely true or likely false. For instance, Ciampaglia et al. (2015) do this on the basis of a knowledge graph derived from Wikipedia. The outcome could then be pre- sented to a human expert for final judgment.1 1As of present, fully automatic methods for fact checking still lag behind in terms of quality, and thus also of credibility in the eyes of the users, compared to what high-quality man- ual checking by reputable sources can achieve, which means that a final double-checking by a human expert is needed. 267 In this paper, we focus on the first step: predicting check-worthiness of claims. Our contributions can be summarized as follows: 1. New dataset: We build a new dataset of manually-annotated claims, extracted from the 2016 US presidential and vice- presidential debates, which we gathered from nine reputable sources such as CNN, NPR, and PolitiFact, and which we release to the research community. 2. Modeling the context: We develop a novel approach for automatically predicting which claims should be prioritized for fact- checking, based on a rich input representa- tion. In particular, we model not only the textual content, but also the context: how the target claim relates to the current segment, to neighboring segments and sentences, and to the debate as a whole, and also how the op- ponents and the public react to it. 3. State-of-the-art results: We achieve state-of- the-art results, outperforming a strong rival- ing system by a margin, while also demon- strating that this improvement is due primar- ily to our modeling of the context. We model the problem as a ranking task, and we train both Support Vector Machines (SVM) and Feed-forward Neural Networks (FNN) obtain- ing state-of-the-art results. We also analyze the relevance of the specific feature groups and we show that modeling the context yields a signifi- cant boost in performance. Finally, we also ana- lyze whether we can learn to predict which facts are worth-checking with respect to each of the in- dividual media sources, thus capturing their bi- ases. It is worth noting that while trained on po- litical debates, many features of our model can be potentially applied to other kinds of information sources, e.g., interviews and news. The rest of the paper is organized as follows: Section 2 overviews related work. Section 3 de- scribes the process of gathering and annotating the debates dataset. Section 4 describes our super- vised approach to predicting fact-checking wor- thiness, including the explanation of the model and the information sources we use. Section 5 includes the evaluation and discusses the results. Section 6 provides further analysis. Finally, Sec- tion 7 presents the conclusions and outlines some lines for future research. 2 Related Work The previous work that is most relevant to our work here is that of (Hassan et al., 2015), who developed the ClaimBuster system, which assigns each sentence in a document a score, i.e., a num- ber between 0 and 1 showing how worthy it is for fact-checking. The system is trained on their own dataset of about 8 thousand debate sentences (1,673 of them worth-checking), annotated by stu- dents, university professors, and journalists. Un- fortunately, this dataset is not publicly available, and contains sentences without context as about 60% of the original sentences had to be thrown away due to lack of agreement. In contrast, we develop a new publicly-available dataset,2 based on manual annotations of politi- cal debates by nine highly-reputed fact-checking sources, where sentences are annotated in the con- text of the entire debate. This allows us to explore a novel approach, which focuses on the context. Note also that the ClaimBuster dataset is an- notated following guidelines from (Hassan et al., 2015) rather than a real fact-checking website; yet, it was evaluated against CNN and PolitiFact (Has- san et al., 2016). In contrast, we train and evaluate directly on annotations from fact-checking web- sites, and thus we learn to fit them better. Beyond the document context, it has been pro- posed to mine check-worthy claims on the Web. For example, Ennals et al. (2010a) searched for linguistic cues of disagreement between the author of a statement and what is believed, e.g., falsely claimed that X”. The claims matching the patterns go through a statistical classifier, which marks the text of the claim. This procedure can be used to acquire a corpus of disputed claims from the Web. Given a set of disputed claims, (Ennals et al., 2010b) approached the task as locating new claims on the Web that entail the ones that have already been collected. Thus, the task can be conformed as recognizing textual entailment, which is analyzed in detail in (Dagan et al., 2009). Finally, Le et al. (2016) argued that the top terms in claim vs. non-claim sentences are highly overlapping, which is a problem for bag-of-words approaches. Thus, they used a Convolutional Neu- ral Network, where each word is represented by its embedding and each named entity is replaced by its tag, e.g., person, organization, location. 2The dataset and the source code are available in GitHub: https://github.com/pgencheva/claim-rank268 Medium 1st 2nd VP 3rd Total ABC News 35 50 29 28 142 Chicago Tribune 30 29 31 38 128 CNN 46 30 37 60 173 FactCheck.org 15 45 47 60 167 NPR 99 92 91 89 371 PolitiFact 74 62 60 57 253 The Guardian 27 39 54 72 192 The New York Times 26 25 46 52 149 The Washington Post 26 19 33 17 95 Total annotations 378 391 428 473 1,670 Annotated sentences 218 235 183 244 880 Table 1: Number of annotations in each medium for the 1st, 2nd and 3rd presidential and the vice- presidential debates. 3 The CW-USPD-2016 Corpus on US Presidential Debates We created a new dataset called CW-USPD-2016 (check-worthiness in the US presidential debates 2016) for finding check-worthy claims in context. In particular, we used four transcripts of the 2016 US election: one vice-presidential and three pres- idential debates. For each debate, we used the publicly-available manual analysis about it from nine reputable fact-checking sources, as shown in Table 1. This could include not just a statement about factuality, but any free text that journalists decided to add, e.g., links to biographies or be- havioral analysis of the opponents and modera- tors. We converted this to binary annotation about whether a particular sentence was annotated for factuality by a given source. Whenever one or more annotations were about part of a sentence, we selected the entire sentence, and when an an- notation spanned over multiple sentences, we se- lected each of them. Ultimately, we ended up with a corpus of four debates, with a total of 5,415 sentences. The agreement between the sources was low as Table 2 shows: only one sentence was selected by all nine sources, 57 sentences by at least five, 197 by at least three, 388 by at least two, and 880 by at least one. The reason for this is that the different media aimed at annotating sentences according to their own editorial line, rather than trying to be exhaus- tive in any way. This suggests that the task of predicting which sentence would contain worth- checking claims will be challenging. Thus, below we focus on a ranking task rather than on absolute predictions. Moreover, we predict which sentence would be selected (i) by at least one of the media, or (ii) by a specific medium. Agreement Number of Cumulative Level Sentences Sum 9 1 1 8 6 7 7 5 12 6 19 31 5 26 57 4 40 97 3 100 197 2 191 388 1 492 880 Total number of sentences: 5,415 Table 2: Agreement between the media repre- sented as the number of sentences that n out of nine providers identified as worth-checking. Note that the investigative journalists did not select the check-worthy claims in isolation. Our analysis shows that these include claims that were highly disputed during the debate, that were relevant to the topic introduced by the moderator, etc. We will make use of these contextual dependencies below, which is something that was not previously tried in related work. 4 Modeling Check-Worthiness We developed a rich input representation in order to model and to learn the check-worthiness con- cept. The feature types we implemented operate at the sentence- (S) and at the context-level (C), in ei- ther case targeting segments by the same speaker.3 The context features are novel and a contribution of this study. We also implemented a set of core features to compare to the state of the art. All of them are described below. 4.1 Sentence-Level Features ClaimBuster-based (1,045 S features; core): First, in order to be able to compare our model and features directly to the previous state of the art, we re-implemented, to the best of our abil- ity, the sentence-level features of ClaimBuster as described in (Hassan et al., 2015), namely TF- IDF-weighted bag of words (998 features), part- of-speech tags (25 features), name entities as rec- ognized by Alchemy API4 (20 features), sentiment score from Alchemy API (1 feature), and number of tokens in the target sentence (1 feature). 3We define a segment as a maximal set of consecutive sen- tences by the same speaker without intervention by another speaker or by the moderator. 4http://www.ibm.com/watson/ alchemy-api.html269 Apart from providing means of comparison to the state of the art, these features also make a solid contribution to the final system we build for claim- worthiness estimation. However, note that we did not have access to the training data of Claim- Buster, which is not publicly available, and we thus train on our own dataset. Sentiment (2 S features): Some sentences are highly negative, which can signal the presence of an interesting claim to check, as the two example sentences below show (from the 1st and the 2nd presidential debates): Trump: Murders are up. Clinton: Bullying is up. We used the NRC sentiment lexicon (Mohammad and Turney, 2013) as a source of words and n-grams with positive/negative sentiment, and we counted the number of positive and of negative words in the target sentence. These features are different from those in the CB features above, where these lexicons were not used. Named entities (NE) (1 S feature): Sentences that contain named entity mentions are more likely to contain a claim that is worth fact-checking as they discuss particular people, organizations, and locations. Thus, we have a feature that counts the number of named entities in the target sentence; we use the NLTK toolkit for named entity recog- nition (Loper and Bird, 2002). Unlike the CB fea- tures above, here we only have one feature; we also use a different toolkit for named entity recog- nition. Linguistic features (9 S features): We count the number of words in each sentence that belong to each of the following lexicons: Language Bias lexicon (Recasens et al., 2013), Opinion Negative and Positive Words (Liu et al., 2005), Factives and Assertive Predicates (Hooper, 1974), Hedges (Hy- land, 1998), Implicatives (Karttunen, 1971), and Strong and Weak subjective words. Some exam- ples are shown in Table 3. Feature Name Examples Bias capture, create, demand, follow Negatives abnormal, bankrupt, cheat, conflicts Positives accurate, achievements, affirm Factives realize, know, discover, learn Assertives think, believe, imagine, guarantee Hedges approximately, estimate, essentially Implicatives cause, manage, hesitate, neglect Strong-subj admire, afraid, agreeably, apologist Weak-subj abandon, adaptive, champ, consume Table 3: Linguistic features and examples. Tense (1 S feature): Most of the check-worthy claims mention past events. In order to detect when the speaker is making a reference to the past or s/he is talking about his/her future vision and plans, we include a feature with three values— indicating whether the text is in past, present of fu- ture tense. The feature is extracted from the verbal expressions, using POS tags and a list of auxiliary verbs and phrases such as will, have to, etc. Length (1 S feature): Shorter sentences are generally less likely to contain a worth-checking claim.5 Thus, we have a feature for the length of the sentence in terms of characters. Note that this feature was not part of the CB features, as there length was modeled in terms of tokens, but here we do so using characters. 4.2 Contextual Features Position (3 C features): A sentence on the bound- aries of a speaker’s segment could contain a reac- tion to another statement or could provoke a reac- tion, which in turn could signal a worth-checking claim. Thus, we added information about the posi- tion of the target sentence in its segment: whether it is first/last, as well as its reciprocal rank in the list of sentences in that segment. Segment sizes (3 C features): The size of the segment belonging to one speaker might indi- cate whether the target sentence is part of a long speech, makes a short comment or is in the middle of a discussion with lots of interruptions. The size of the previous and of the next segments is also important in modeling the dialogue flow. Thus, we include three features with the sizes of the pre- vious, the current and the next segments. Metadata (8 C features): Worth-checking claims often contain accusations about the opponents, as the example below shows (from the 2nd presidential debate): Trump: Hillary Clinton attacked those same women and attacked them viciously. Clinton: They’re doing it to try to influence the elec- tion for Donald Trump. Thus, we use a feature that indicates whether the target sentence mentions the name of the opponent, whether the speaker is the moderator, and also who is speaking (3 features). We further use three binary features, indicating whether the target sentence is followed by a system message: applause, laugh, or cross-talk. 5One notable exception are short sentences with nega- tions, e.g., Wrong., Nonsense., etc.270 4.3 Mixed Features The feature groups in this subsection contain a mixture of sentence- and of contextual-level fea- tures. For example, if we use a discourse parser to parse the target sentence only, any features we extract from the parse would be sentence-level. However, if we parse an entire segment, we would also have contextual features. Topics (300+3 S+C features): Some topics are more likely to be associated with worth-checking claims, and thus we have features modeling the topics in the target sentence as well as in the sur- rounding context. We trained a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003) on all political speeches and debates in The Amer- ican Presidency Project6 using all US presidential debates in the 2007–2016 period7. We had 300 topics, and we used the distribution over the topics as a representation for the target sentence. We fur- ther modeled the context using cosines with such representations for the previous, the current, and the next segment. Embeddings (300+3 S+C features): We fur- ther modeled semantics using word embeddings. We used the pre-trained 300-dimensional Google News word embeddings by Mikolov et al. (2013) to compute an average embedding vector for the target sentence, and we used the 300 coordinates of that vector. We also modeled the context as the cosine between that vector and the vectors for three segments: the previous, the current, and the following one. Discourse (2+18 S+C features): We saw above that contradiction can signal the presence of worth-checking claims, and contradiction can be expressed by a discourse relation such as CON- TRAST. As other discourse relations such as BACKGROUND, CAUSE, and ELABORATION can also be useful, we used a discourse parser (Joty et al., 2015) to parse the entire segment, and we focused on the relationship between the target sen- tence and the other sentences in its segment; this gave rise to 18 contextual indicator features. We further analyzed the internal structure of the tar- get sentence —how many nuclei and how many satellites it contains—, which gave rise to two sentence-level features. 6http://www.presidency.ucsb.edu/ debates.php 7https://github.com/paigecm/ 2016-campaign Contradictions (1+4 S+C features): Many claims selected for fact-checking contain contra- dictions to what has been said earlier, as in the ex- ample below (from the 3rd presidential debate): Clinton: [. . . ] about a potential nuclear competition in Asia, you said, you know, go ahead, en- joy yourselves, folks. Trump: I didn’t say nuclear. We model this by counting the negations in the target sentence as found in a dictionary of nega- tion cues such as not, didn’t, and never. We further model the context as the number of such cues in the two neighboring sentences from the same segment and the two neighboring segments. kNN (2+1 S+C features): We used three more features inspired by k-nearest neighbor (kNN) classification. The first one (sentence-level) uses the maximum over the training sentences of the number of matching words between the testing and the training sentence, which is further mul- tiplied by -1 if the latter was not worth-checking. We also used another version of the feature, where we multiplied it by 0 if the speakers were differ- ent (contextual). A third version took as a training set all claims checked by PolitiFact (excluding the target sentence). 5 Experiments and Evaluation 5.1 Experimental Setting We experimented with two learning algorithms. The first one is an SVM classifier with an RBF ker- nel.8 The second one is a deep feed-forward neural network (FNN) with two hidden layers (with 200 and 50 neurons, respectively) and a softmax output unit for the binary classification. We used ReLU (Glorot et al., 2011) as the activation function and we trained the network with Stochastic Gradient Descent (LeCun et al., 1998). The models were trained to classify sentences as positive if one or more media had fact-checked a claim inside the target sentence, and negative oth- erwise. We then used the classifier scores to rank the sentences with respect to check-worthiness.9 We tuned the parameters and we evaluated the per- formance using 4-fold cross-validation, using each of the four debates in turn for testing while train- ing on the remaining three ones. 8The RBF kernel was clearly superior to a linear kernel in our initial experiments. 9We also tried using ordinal regression, and SVM-perf, an instantiation of SVM-struct, to directly optimize precision, but none of them yielded improvements.271 For evaluation, we used ranking measures such as Precision at k (P @k) and Mean Average Preci- sion (MAP). As Table 1 shows, most media rarely check more than 50 claims per debate. NPR and PolitiFact are notable exceptions, the former go- ing up to 99; yet, on average there are two claims per sentence, which means that there is no need to fact-check more than 50 sentences even for them. Thus, we report P @k for k ∈ {5, 10, 20, 50}.10 MAP is the mean of the Average Precision across the four debates. The average precision for a debate is computed as follows: AvPrec = ∑n k=1(P (k) × rel(k)) number of relevant utterances (1) where n is the number of sentences to rank in the debate, P (k) is the precision at k and rel(k) = 1 if the utterance at position k is worth-checking, and it is 0 otherwise. We also measure the recall at the R-th position of returned sentences for each debate. R is the number of relevant documents for that debate and the metric is known as R-Precision (R-Pr). 5.2 Results Table 4 shows the performance of our models when using all features described in Section 4: see the SVMAll and the FNNAll rows. In order to put the numbers in perspective, we also show the results for five increasingly compet- itive baselines. The first one is a random base- line. It is then followed by an SVM classifier based on a bag-of-words representation with TF- IDF weights learned on the training data. Then come three versions of the ClaimBuster system: CB-Platform refers to the performance of Claim- Buster using the scores obtained from their on- line demo,11 which we accessed on December 20, 2016, and SVMCBf eat and FNNCBf eat are our re- implementations of ClaimBuster using their fea- tures, which we then use in our SVM or FNN clas- sifiers trained on our dataset. We can see that, as expected, all systems per- form well above the random baseline. The three versions of ClaimBuster also outperform the TD- IDF baseline on most measures. 10Note that as far as the difference between the P@k met- rics (especially between 5 and 10) is in terms of a few sen- tences, the deviation between them can seem large, while caused by a few correctly/wrongly predicted sentences. 11http://idir-server2.uta.edu/ claimbuster/demo Moreover, our reimplementations of ClaimBuster are better than the online platform in terms of MAP. This is expected as their system is trained on a different dataset and it may suffer from test- ing on slightly out-of-domain data. At the same time, this is reassuring for our implementation of the features, and allows for a more realistic com- parison to the ClaimBuster system. More importantly, we can see that both the SVM and the FNN versions of our system, when trained with all features, consistently outperform all three versions of ClaimBuster on all measures. This means that the extra information coded in our model, mainly more linguistic, structural, and con- textual features, has an important contribution to the final performance. We can further see that the neural network model, FNNAll, clearly outperforms the SVM model for this task: consistently on all metrics. As an example, with the precision values achieved by FNNAll, the system would rank on average 4 pos- itive examples in the list of its top-5 choices, and also 14-15 in the top-20 list. Considering the re- call at the first R sentences, we will be able to en- counter 43% of the total number of check-worthy sentences. This is quite remarkable given the dif- ficulty of the task. As a next step of the evaluation, we perform error analysis of the decisions made by the Neu- ral Network that uses all available features. We present examples of False Positives (FP) and False Negatives (FN): 1 FP Clinton:He actually was sued twice by the Justice Department. 2 FP Clinton:Five million people lost their homes. 3 FP Clinton:There’s no doubt now that Russia has used cyber attacks against all kinds of organiza- tions in our country, and I am deeply con- cerned about this. 4 FP Trump:Your husband signed NAFTA, which was one of the worst things that ever happened to the manufacturing industry. 5 FN Trump:This is one of the worst deals ever made by any country in history. 6 FN Trump:Well, nobody was pressing it, nobody was caring much about it. 7 FN Trump:So Ford is leaving. 8 FN Trump:It was taken away from her. Regarding the false positive examples, we can conclude that they could be also interesting for fact-checking, as they make some questionable statements. The list of false negatives contains sentences which belong to a whole group of anno- tations and some of them are not check-worthy on their own such as the eighth example. Some of the272 System MAP R-Pr P@5 P@10 P@20 P@50 Baselines Random .164 .007 .200 .125 .138 .160 TF-IDF .314 .333 .550 .475 .413 .360 CB Platform.317 .349 .500 .550 .488 .405 SVMCBf eat.360 .393 .400 .425 .525 .495 FNNCBf eat .357 .379 .500 .550 .550 .510 Systems (using all features) SVMAll .395 .406 .650 .725 .588 .565 FNNAll .427 .432 .800 .725 .713 .600 Table 4: Evaluation results: our full systems (SVM and FNN) vs. a number of base- lines:random and a TF-IDF baselines, also Claim- Buster from the platform, and our two reimple- mentations thereof. false negatives, though, need to be fact-checked and our model missed them such as sixth and sev- enth examples. An interesting observation is that we have two sentences, making the same state- ments using different wording - fourth and fifth sentences. On the one hand, the annotators should have labeled both of the sentences in the same manner, and on th other hand, our model should have also labeled them equally. Finally, we can conclude that the false posi- tives of our ranking system also make good candi- dates for credibility verification and demonstrate that the system has successfully extracted com- mon patterns for check-worthiness. This way, the top-n list will contain mostly sentences which need to be further checked. Given the discrep- ancies and the disagreement between the annota- tions, a further cleaning of the corpus might be needed to prevent missing important check-worthy statements. 6 Discussion In this section, we present some in-depth analysis and further discussion. 6.1 Individual Feature Types Table 5 shows the performance of the individ- ual feature types described in Section 4, when training using our FNN model, and ordered by their decreasing MAP score. We can see that em- beddings perform best (MAP=.357, P@50=.495), which shows that modeling semantics and the sim- ilarity of a sentence against its context is quite im- portant. Then comes kNN with MAP of .313 and P@50 of .455. The high performance of this fea- ture reveals the frequent usage of statements which resemble already fact-checked ones. In the case of S or CFeat. Group MAP R-Pr P@5 P@10 P@20 P@50 S+C Embeddings .357 .380 .450 .525 .488 .495 S+C kNN .313 .322 .800 .725 .612 .445 S Linguistic .308 .333 .450 .450 .463 .430 S Sentiment .260 .277 .550 .400 .288 .315 C Metadata .256 .268 .350 .300 .388 .370 S Length .254 .350 .350 .375 .400 .340 S NEs .236 .251 .250 .275 .313 .280 S+C Contradiction .222 .222 .400 .275 .288 .260 C Segment size .217 .231 .100 .150 .150 .245 C Position .212 .230 .100 .075 .175 .230 S+C Discourse .205 .206 .200 .300 .325 .255 S+C Topics .180 .178 .000 .000 .013 .085 Table 5: Performance of each feature group in iso- lation, using the FNN system. Results sorted by decreasing MAP score. false claims, this can be considered as a testimony for the existence of a post-truth era (Davies, 2016). Then follow two sentence-level features, lin- guistic features and sentiment, with MAP of .308 and .260, and P@50 of .430 and .315. This is on par with previous work, which has focused primar- ily on similar sentence-level features. Then fol- low a contextual feature: Metadata (MAP=.256, P@50=.370). And two sentence features: length and named entities, with MAP of .254 and .236, and P@50 of .340 and .280. At the bottom of the table we find position, a general contextual feature with MAP of .212 and P@50 of .230, followed by discourse and topics. 6.2 Effect of Context Modeling Next, we study the impact of the contextual fea- tures. Table 6 shows the results when using all fea- tures vs. excluding the contextual features vs. us- ing the contextual features only. We can see that the contextual features have a major impact on per- formance: excluding them yields major drop for all measures, e.g., MAP drops from .427 to .385, and P@5 drops from .800 to .550. The last two rows in the table show that using contextual fea- tures only performs about the same as CB Plat- form (which uses no contextual features at all). System MAP R-Pr P@5 P@10 P@20 P@50 All .427 .432 .800 .725 .713 .600 All, no contextual .385 .390 .550 .500 .550 .540 Only contextual .317 .404 .725 .563 .465 .465 CB Platform .317 .349 .500 .550 .488 .405 Table 6: Impact of the contextual features on the overall performance (FNN system).273 System MAP R-Pr P@5 P@10 P@20 P@50 PolitiFact (PF) CB Platform .154 .213 .200 .300 .238 .210 NN (train on PF) .218 .274 .450 .325 .300 .270 NN (train on all) .213 .246 .400 .350 .375 .290 NPR CB Platform .144 .186 .200 .225 .225 .180 NN (train on NPR) .193 .216 .550 .475 .350 .255 NN (train on all) .208 .250 .500 .450 .375 .255 The New York Times (NYT) CB Platform .103 .250 .250 .163 .135 NN (train on NYT) .136 .178 .250 .225 .188 .135 NN (train on all) .136 .169 .150 .200 .163 .160 The Guardian (TG) CB Platform .084 .128 .100 .100 .125 .140 NN (train on TG) .121 .156 .250 .225 .200 .155 NN (train on all) .128 .185 .100 .150 .188 .165 FactCheck (FC) CB Platform .081 .213 .150 .125 .100 .115 NN (train on FC) .081 .098 .050 .125 .088 .085 NN (train on all) .115 .149 .100 .125 .125 .140 CNN CB Platform .082 .096 .150 .125 .088 .085 NN (train on CNN) .079 .076 .100 .100 .100 .090 NN (train on all) .095 .087 .000 .075 .088 .100 Chicago Tribune (CT) CB Platform .053 .032 .050 .050 .038 .065 NN (train on CT) .087 .118 .150 .150 .175 .105 NN (train on all) .092 .098 .150 .075 .100 .090 ABC CB Platform .065 .066 .150 .125 .088 .080 NN (train on ABC) .059 .068 .050 .050 .100 .060 NN (train on all) .088 .090 .150 .150 .113 .100 Washington Post (WP) CB Platform .048 .056 .050 .075 .050 .045 NN (train on WP) .102 .098 .200 .175 .113 .080 NN (train on all) .076 .751 .200 .100 .075 .080 Table 7: Training on the target medium vs. train- ing on all media when testing with respect to a par- ticular medium (FNN system). 6.3 Mimicking each Particular Source In the experiments above, we have been trying to predict whether a sentence is check-worthy in general, i.e., with respect to at least one source; this is how we trained and this is how we evalu- ated our models. Here, we want to evaluate how well our models perform at finding sentences that contain claims that would be judged as worthy for fact-checking with respect to each of the individ- ual sources. The purpose is to see to what extent we can make our system potentially useful for a particular medium. Another interesting question is whether we should use our generic system or we should retrain with respect to the target medium. Table 7 shows the results for such a comparison, and it further compares to CB Platform. We can see that for all nine media, our model outperforms CB Platform in terms of MAP and P@50; this is also true for the other measures in most cases. Moreover, we can see that training on all data is generally preferable to training on the target medium only, which shows that despite the sizable disagreement between the different media, they do follow some common principles for selecting what is check-worthy; this means that a general sys- tem could serve journalists in all these nine, and possibly other, media. One exception is Washing- ton Post, where our system performs better when trained only on the single source, which is an in- dicator of the difference between Washington Post and the rest sources. Overall, our model works best on PolitiFact, which is a reputable source with fact checking as their primary expertise. We also do well on NPR, NYT, Guardian, and FactCheck; this is quite encouraging. 7 Conclusions and Future Work We have developed a novel approach for automat- ically finding worth-checking claims in political debates, which is an understudied problem, de- spite its importance. Unlike previous work, which has looked primarily at sentences in isolation, here we have focused on the context: relationship be- tween the target statement and the larger context of the debate, interaction between the opponents, and","Gencheva, P., Nakov, P., Màrquez, L., Barrón-Cedeño, A., & Koychev, I. (2017, September). A context-aware approach for detecting worth-checking claims in political debates. In Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017 (pp. 267-276)."
SSAFC_003,Verifying Text Summaries of Relational Data Sets,"We present a novel natural language query interface, the AggChecker, aimed at text summaries of relational data sets. The tool focuses on natural language claims that translate into an SQL query and a claimed query result. Similar in spirit to a spell checker, the AggChecker marks up text passages that seem to be inconsistent with the actual data. At the heart of the system is a probabilistic model that reasons about the input document in a holistic fashion. Based on claim keywords and the document structure, it maps each text claim to a probability distribution over associated query translations. By efficiently executing tens to hundreds of thousands of candidate translations for a typical input document, the system maps text claims to correctness probabilities. This process becomes practical via a specialized processing backend, avoiding redundant work via query merging and result caching. Verification is an interactive process in which users are shown tentative results, enabling them to take corrective actions if necessary. We tested our system on 53 publicly available articles containing 392 claims. Our tool revealed erroneous claims in roughly a third of test cases. Also, AggChecker compares favorably against several automated and semi-automated fact-checking baselines.",Computer Science,Article,2018,N,Y,2,"We present a tool for verifying text summaries of relational data sets. Our tool resembles a spell checker and marks up claims that are believed to be erroneous. We focus on natural language claims that can be translated into an SQL query and a claimed query result. More precisely, we focus on claims that are translated into aggregation queries on data subsets. Hence the name of our system: AggChecker. Our analysis shows that this claim type is at the same time very common and error-prone. The following example illustrates the concept. In contrast to prior work [22], our focus is not mostly on adversarial fact checking. We also want to support text authors (e.g., data journalists or scientists) or third persons, collaborating with authors (e.g, a lector or reviewer), in cre- ating accurate data summaries. The motivation for using our tool is similar to the motivation for using a spell checker in those cases. However, publishing erroneous numbers can in some cases have more serious consequences than spelling mistakes (e.g., corrections or even retractions, erroneous numbers might even have legal consequences if they appear in business reports). Hence, the need for specialized verifica- tion tools. Note that some of our assumptions, e.g. having access to the data set associated with text, are motivated by this focus (even though we also report results on determining suitable data sets via our tool later in this paper). Example 1. Consider the passage There were only four previous lifetime bans in my database - three were for re- peated substance abuse” taken from a 538 newspaper arti- cle [13]. It contains two claims that translate into the SQL queries SELECT Count(*) FROM nflsuspensions WHERE Games = ‘indef’ (with claimed result ‘four’) and SELECT Count(*) FROM nflsuspensions WHERE Games = ‘indef’ AND Category = ‘substance abuse, repeated offense’ (with claimed result ‘three’) on the associated data set. Our goal is to automatically translate text to queries, to evaluate Table 1: AggChecker: primary design choices and un- derlying motivation. Component Design Choice Goal Text Analysis Keyword-based High-recall heuristic Processing Engine Batch-Optimized High- throughput verification User Interface Interactive Integrate user feedback Claim Checker Probabilistic Model Leverage het- erogeneous feedback Expectation Maxi- mization Learning Exploit Seman- tic Correlation those queries, and to compare the evaluation result against the claimed one. Internally, the system executes the following, simplified process to verify a claim. First, it tries to translate the natural language claim into an SQL query reflecting its semantics. Second, it executes the corresponding query on the rela- tional database. Third, it compares the query result against the value claimed in text. If the query result rounds to the text value then the claim has been verified. Otherwise, the claim is considered erroneous. Color markup indicates the verification result to users. Additionally, users may obtain in- formation on the verification process and can take corrective actions if necessary (similar to how users correct erroneous spell checker markup). The most challenging step is of course the translation of a natural language claim into an SQL query. Among the challenges we encountered when studying real-world test cases are the following. First, the claim sentence itself is often missing required context. This context can only be found when analyzing preceding paragraphs or headlines (assuming a hierarchical input text document). Second, claim sentences often contain multiple claims which make it hard to associate sentence parts to claims. Third, claim sentences are often long and contain parts which do not immediately correspond to elements in the associated query. This makes it hard to map the claim sentence parse tree to a similar SQL query tree. Fourth, data sets often contain entries (e.g., abbreviations) that are not found immediately in the claim text. Altogether, this makes it hard to map claim sentences unambiguously to database elements. The high-level design of AggChecker is motivated by those challenges. We acknowledge that translating text claims to queries, based on text analysis alone, is inherently unreli- able. Hence, we seek to exploit additional signals to reduce our uncertainty in translation. First, as opposed to natural language querying, we are not only given a query but also a claimed, numerical result. The probability that the result of a random query matches a claimed number is typically low. Also, correct claims are in practice more likely than incorrect claims for the type of claim we are considering (as evaluated in more detail later). This makes query candidates whose result matches the claimed value more likely as claim translations. Of course, to exploit this signal, we first need to execute the candidate query. Second, as we analyze in more detail later, claims in the same text document are often simi- lar. Authors tend to use the same aggregation functions, and similar aggregates and predicates, in their claims throughout a document. Hence, if we can translate a few claims with high confidence, it can help us to translate the others. Third, if all else fails, it might be necessary to get help from users. The goal is of course to limit user intervention to the most difficult cases (similar to a spell checker, which needs correc- tive action only very occasionally). Also, we want to make the most out of the user’s time by transferring” feedback we obtain for one claim to others. AggChecker is designed to exploit various signals in claim to query translation. Table 1 gives an overview of its compo- nents, the primary design choices made in each component, and the motivation for doing so. In order to exploit addi- tional signals in translation, we first need to obtain a space of possible query translations for each given claim. As rank- ing and filtering steps follow, this space can be large but it needs to contain the correct query to enable a successful translation. Hence, we use a high-recall (but low precision) heuristic for analyzing the input text and matching claim text to database entries. This heuristic is based on keyword matching and the input document structure (we exploit doc- ument structure to associate claims with keywords from other document parts). As discussed before, a first signal comes from executing query candidates and comparing their results to claimed values. To enable us to verify the large query candidate space, resulting from the first stage, we use an execution engine that is tailored for executing batches of similar queries with high throughput. To enable users to take corrective actions if necessary, AggChecker features an interactive user interface that exploits partial verification results to minimize overheads for users (e.g., by showing likely translations for single-click feedback). AggChecker exploits heterogeneous features for claim to query translation. We need to integrate feedback from different sources in a principled manner to come to a ten- tative verification result. We use a probabilistic model to do so, integrating feedback from text analysis, query ex- ecutions, user feedback, and (as discussed next), semanticResearch 3: Information ExtractionSIGMOD ’19, June 30–July 5, 2019, Amsterdam, Netherlands300 correlations between claims. We assume (and experimentally verify later) that documents typically have a common theme, which can be represented by an a-priori probability distri- bution over query fragments (e.g., aggregation functions, specific columns in the data set, etc.). Knowing the docu- ment theme helps in query translations and knowing (some) query translations helps in inferring the document theme. This circular dependency motivates an iterative expectation- maximization approach, in which we infer document theme and likely translations at once, thereby exploiting semantic correlations between claims. Our contributions lie in the high-level design of the system (which is novel and tailored to our specific scenario), as well as in the design of each single component. We evaluated our system on a variety of real-world test cases, containing 392 claims on relational data sets. Our test cases cover diverse topics and derive from various sources, reaching from Wikipedia to New York Times articles. We generated ground truth claim translations by hand and con- tacted the article authors in case of ambiguities. We identified a non-negligible number of erroneous claims, many of which are detected by our system. We compare against baseline systems and perform a user study. The user study demon- strates that users verify documents significantly faster via AggChecker than via standard query interfaces. In summary, our original scientific contributions are the following: • We introduce the problem of translating natural lan- guage claims on relational data to SQL queries, without using prior training or manual annotations. • We propose a first corresponding system whose design is tailored to the particularities of our scenario. • We compare our system against baselines in fully au- tomated checking as well as in a user study. The remainder of this paper is organized as follows. We formalize our problem model in Section 2. Next, we give an overview of our system in Section 3. The following three sec- tions describe specific components of our system: keyword matching, probabilistic reasoning, and massive-scale candi- date query evaluations. After that, we present experimental results in Section 7. Finally, we compare against related work in fact-checking and natural language query interfaces in Section 8. In the appendix, we provide more experimental results and list all our test cases. 2 PROBLEM STATEMENT We introduce our problem model and related terminology. We generally assume a scenario where we have a relational Database together with a natural language Text summarizing it. The relational database might be optionally associated with a data dictionary (mapping database elements such as columns and values to text descriptions). The text document may be semi-structure, i.e. it is organized as a hierarchy of sections and subsections with associated headlines. Also, the text contains Claims about the database. Definition 1. A Claim is a word sequence from the input text stating that evaluating a query q on the associated data- base D yields a rounded result e. We focus on SQL queries with numerical results (e ∈ R). We call q also the Matching Query or Ground Truth Query with regards to the claim. A claim may be a sentence part or a sentence (one sentence may contain multiple claims). A claim is Correct if there is an ad- missible rounding function ρ : R → R such that the rounded query results equals the claimed value (i.e., ρ(q(D)) = e). We currently consider rounding to any number of signifi- cant digits as admissible. The approach presented in the next sections can be used with different rounding functions as well. We focus on Simple Aggregate Queries, a class of claim queries defined as below. Definition 2. A Simple Aggregate Query is an SQL query of the form SELECT Fct(Agg) FROM T1 E-JOIN T2 ... WHERE C1 = V1 AND C2=V2 AND ..., calculating an aggregate over an equi-join between tables connected via primary key- foreign key constraints. The where clause is a conjunction of unary equality predicates. Claims of this format are very popular in practice and at the same time error-prone (see Section 7). Currently, we support the following aggregation functions: Count, Count Distinct, Sum, Average, Min, Max, Percentage, and Con- ditional Probability1 (we plan to gradually extend the scope). The ultimate goal would be to perform purely Auto- matic Aggregate-Checking (i.e., given a text document and a database, identify claims automatically and decide for each one whether it is correct). This would however require near- perfect natural language understanding which is currently still out of reach. Hence, in this paper, we aim for Semi- Automatic Aggregate-Checking in which we help users to verify claims without taking them out of the loop completely. Definition 3. Given input ⟨T , D⟩, a text T and a database D, the goal of Semi-Automatic Aggregate-Checking is to identify claims and to map each claim c to a probability distribution Qc over matching queries. This probability dis- tribution can be exploited by a corresponding user interface to quickly verify text in interaction with the user. The quality of a corresponding approach can be measured based on how 1For conditional probability, we assume that the first predicate is the condi- tion and the rest form the event. That is, (SELECT ConditionalProbabil- ity(Agg) FROM T1 E-JOIN T2 ... WHERE C1 = V1 AND C2=V2 AND ...) = (SELECT Count(Agg) FROM T1 E-JOIN T2 ... WHERE C1 = V1 AND C2=V2 AND ...) * 100 / (SELECT Count(Agg) FROM T1 E-JOIN T2 ... WHERE C1 = V1).Research 3: Information ExtractionSIGMOD ’19, June 30–July 5, 2019, Amsterdam, Netherlands301 Figure 1: Overview of AggChecker system. often the top-x likely query candidates in Qc contain the matching query. 3 SYSTEM OVERVIEW Figure 1 shows an overview of the AggChecker system. The input to the AggChecker consists of two parts: a relational data set and a text document, optionally enriched with HTML markup highlighting the text structure. The text contains claims about the data. Our goal is to translate natural lan- guage claims into pairs of SQL queries and claimed query results. The process is semi-automated and relies on user feedback to resolve ambiguities. Finally, we enrich the input text with visual markup, identifying claims that are incon- sistent with the data. For each newly uploaded data set, we first identify rele- vant query fragments (see Figure 2(c)). The system focuses on Simple Aggregate Queries as defined in Section 2. Query frag- ments include aggregation functions, aggregation columns, or unary equality predicates that refer to columns and values in the data set. We associate each query fragment with key- words, using names of identifiers within the query fragment as well as related keywords that we identify using Word- Net [ 11, 35]. We index query fragments and the associated keywords via an information retrieval engine (we currently use Apache Lucene [17]). Next, we parse the input text using natural language anal- ysis tools such as the Stanford parser [33]. We identify po- tentially check-worthy text passages via simple heuristics and rely on user feedback to prune spurious matches. Then, we associate each claim with a set of related keywords (see Figure 2(d)). We use dependency parse trees as well as the document structure to weight those keywords according to their relevance. We query the information retrieval engine, indexing query fragments, using claim keywords as queries. Thereby we obtain a ranked set of query fragments for each claim. Query fragments with relevance scores form one out of several inputs to a probabilistic model. This model maps each text claim to a probability distribution over SQL query candidates, representing our uncertainty about how to trans- late the claim (see Figure 2(e)). The model considers the document structure and assumes that claims in the same document are linked by a common theme. The document theme is represented via model parameters capturing the prior probabilities of certain query properties. We infer doc- ument parameters and claim distributions in an iterative expectation-maximization approach. Furthermore, we try to resolve ambiguities in natural language understanding via massive-scale evaluations of query candidates. The Ag- gChecker uses evaluation strategies such as query merging and caching to make this approach practical (we currently use Postgres [19] to evaluate merged queries). We typically evaluate several tens of thousands of query candidates to verify one newspaper article. Example 2. Figure 2 provides a concrete running example demonstrating the inputs and outputs of the main compo- nents. Figure 2(a) depicts the raw relational data where query fragments and their associated keywords are extracted as in Figure 2(c). Figure 2(b) illustrates a text passage from a 538 newspaper article [13 ]. It contains three claimed results (colored in blue) where we focus on the claimed result ‘one’ in this example. In Figure 2(d), we extract relevant keywords for this claimed result and weigh them based on the text structure. Then, we calculate relevance scores for pairs of query fragments and claims based on their keywords. The probabilistic model takes into account the relevance scores as well as two other inputs to infer the probability distribution over query candidates. Figure 2(e) captures this concept. First, ‘Keyword Probability’ is derived from the relevance scores. Second, ‘Prior Probability’ encapsulates the model parame- ters that embrace all claims in the text document in a holistic fashion. Third, green or red color under ‘Evaluation Result’ shows whether the query result matches the value claimed in text. Lastly, ‘Refined Probability’ illustrates the final prob- ability distribution over query candidates, considering all three inputs. After expectation-maximization iterations have converged, the system verifies the claim according to the query with the highest probability. We provide more detailed explanations in each section where all following examples refer to this figure. After an automated verification stage, the system shows tentative verification results to the user. Claims are colored based on their probability of being erroneous (see Figure 3(a)).Research 3: Information ExtractionSIGMOD ’19, June 30–July 5, 2019, Amsterdam, Netherlands302 (a) Raw relational data. (b) Text document. Claims colored in blue. We focus on claimed result ‘one’. (c) Query fragments. (d) Keywords and their weights (specific to claimed result ‘one’). (e) Probability distribution and evaluation results of query candidates. Green indicates a match between query result and claimed result while red indicates the opposite (specific to claimed result ‘one’). Figure 2: Running example of AggChecker system. (a) Marked up claims after initial processing. (b) Query description shown upon hovering. (c) Select query from top-5 likely candidates. (d) Construct query by selecting fragments. Figure 3: Screenshots from AggChecker interface. Users can hover over a claim to see a natural language de- scription of the most likely query translation (see Figure 3(b)) and may correct the system if necessary. Alternatively, users may pick among the top-k most likely query candidates (see Figure 3(c)) or assemble the query from query fragments with high probability (see Figure 3(d)). 4 KEYWORD MATCHING In the first processing phase, we extract query fragments and claim keywords from the inputs and match them together to calculate relevance scores. 4.1 Keyword Matching Overview We calculate relevance scores for pairs of claims and query fragments. The higher the relevance score, the more likely the fragment to be part of the query matching the claim. We consider aggregation functions, aggregation columns, and predicate parts as query fragments. Given an input database, we can infer all potentially relevant query fragments (i.e., we introduce an equality predicate fragment for each literal in the database, an aggregation column fragment for each column containing numerical values etc.). Furthermore, we can associate query fragments with relevant keywords (e.g., the name of a literal, as well as the name of the contain- ing column and synonyms for a fragment representing an equality predicate). On the other side, we can associate each claim in the input text with relevant keywords, based on the document struc- ture. Having query fragments and claims both associated with keyword sets, we can use methods from the area of information retrieval to calculate relevance scores for spe- cific pairs of query fragments and claims. For instance, we use Apache Lucene in our current implementation, index- ing keyword sets for query fragments and querying with claim-specific keyword sets. While keyword-based relevance scores are inherently imprecise, they will form one out of several input signals for the probabilistic model described in the next section. The latter model will associate each claim with a probability distribution over query candidates. 4.2 Indexing Query Fragments When loading a new database, we first form all potentially relevant query fragments. Function IndexFragments (we describe its implementation without providing pseudo-code) traverses the database in order to form query fragments that could be part of a claim query. We consider three types of query fragments: aggregation functions, aggregation columns, and equality predicates. All aggregation function specified in the SQL standard are potentially relevant (we could easily add domain-specific aggregation functions). We consider allResearch 3: Information ExtractionSIGMOD ’19, June 30–July 5, 2019, Amsterdam, Netherlands303 1: // Extract keywords for claim c from text T . 2: function ClaimKeywords(c,T ) 3: // Initialize weighted keywords 4: K ← ∅ 5: // Add keywords in same sentence 6: for word ∈ c.sentence do 7: weiдht ← 1/TreeDistance(word, c) 8: K ← K ∪ {⟨word, weiдht⟩} 9: end for 10: // Add keywords of sentences in same paragraph 11: m ← min{1/TreeDistance(k, c)|k ∈ c.sentence} 12: K ← K ∪ {⟨k, 0.4m⟩|k ∈ c.prevSentence} 13: K ← K ∪ {⟨k, 0.4m⟩|k ∈ c.paraдraph.f irstSentence} 14: // Add keywords in preceding headlines 15: s ← c.containinдSection 16: while s , null do 17: K ← K ∪ {⟨k, 0.7m⟩|k ∈ s.headline.words} 18: s ← s.containinдSection 19: end while 20: return K 21: end function Algorithm 1: Extracts a set of keywords for a claim. numerical columns in any table of the database as aggrega- tion columns (in addition, we consider the all column” * as argument for count aggregates). Finally, we consider all equality predicates of the form c = v where c is a column and v a value that appears in it. We associate each query fragment with a set of relevant keywords. Keyword sets are indexed via an information re- trieval engine (together with a reference to the corresponding fragment). We associate each standard SQL aggregation func- tion with a fixed keyword set. The keywords for aggregation columns are derived from the column name and the name of its table. Column names are often concatenations of multiple words and abbreviations. We therefore decompose column names into all possible substrings and compare against a dictionary. Furthermore, we use WordNet to associate each keyword that appears in a column name with its synonyms. The keywords for an equality predicate of the form c = v are derived from the column name c (and from the name of the containing table) as well as from the name of value v. Finally, the AggChecker also offers a parser for common data dictio- nary formats. A data dictionary associates database columns with additional explanations. If a data dictionary is provided, we add for each column the data dictionary description to its associated keywords. 4.3 Extracting Keywords from Text Next, we associate each claim in the input text with a weighted set of keywords. More precisely, we iterate over each number in the input text that is likely to represent a claimed query Document Sections Subsections Paragraphs Sentences . . . . . . . . . . . . . . . . . . Claim Sentence Figure 4: Keyword context of claim sentence: keyword sources in red, the claim sentence in blue. result. We describe in Section 3 how they are identified. Algo- rithm 1 associates each such claim with weighted keywords, extracted from the containing text. First, we consider key- words in the claim sentence itself (i.e., the sentence in which the claimed result number is found). One sentence might contain multiple claims and we must decide what keywords are most relevant to one specific claim. For that, we con- struct a dependency parse tree of the claim sentence. We make the simplifying assumption that sentence parts are more closely related, the lower their distance (i.e., number of tree edges to traverse) is in the parse tree. Hence, for each numerical aggregate representing the result of a claim, we weight the surrounding keywords based on their distance from the numerical aggregate in the dependency tree (de- noted by TreeDistance in Algorithm 1). Considering keywords in the same sentence is often in- sufficient. In practice, relevant context is often spread over the entire text. We exploit the structure of the text document in order to collect potentially relevant keywords. Our cur- rent implementation uses HTML markup but the document structure could be easily derived from the output format of any word processor. We assume that the document is structured hierarchically into sections, sub-sections etc. For a given claim sentence, we walk up” that hierarchy and add keywords in all headlines we encounter. In addition, we add keywords from the first and preceding sentences in the same paragraph. Figure 4 illustrates keyword sources for an example claim. Example 3. To provide a concrete example, we refer to the paragraph in Figure 2(b). The second sentence contains two claimed results (‘three’ and ‘one’) that translate into queries of the form: SELECT Count(*) FROM T WHERE Games = ‘indef’ AND Category = V. We illustrate two difficulties associated with these claims. First, there are two claims in one sentence. The system needs to distinguish keywords that are more relevant to each claim. Let’s consider the keyword ‘gambling’. According to the dependency parse tree of the second sentence, the distance from ‘three’ to ‘gambling’ is two while the distance from ‘one’ to ‘gambling’ is one. Then, we assign weightsResearch 3: Information ExtractionSIGMOD ’19, June 30–July 5, 2019, Amsterdam, Netherlands304 by taking the reciprocal of the distance (see Figure 2(d)). This helps the system to understand that ‘gambling’ is more related to ‘one’ than ‘three’. Second, no keyword in the second sentence explicitly refers to the restriction Games = ‘indef’. Rather, it can be implicitly inferred from the context where only the first sen- tence has the keywords ‘lifetime bans’. Thereby, considering the keyword context of a claim sentence enables us to iden- tify important and relevant keywords from other parts of the text. In Section 7, we conduct an experiment to measure the effect of keyword context (see Figure 8). 4.4 Constructing Likely Query Candidates Having associated both, query fragments and claims, with keywords, we can map claims to likely query candidates. We indexed keyword sets associated with query fragments in an information retrieval engine. For a given claim, we use the associated keyword set to query that information retrieval engine. The returned results correspond to query fragments that are associated with similar keywords as the claim. Fur- thermore, each returned query fragment is associated with a relevance score, capturing how similar its keywords are to the claim-related keywords. Combining all returned query fragments in all possible ways (within the boundaries of the query model described in Section 2) yields the space of claim- specific query candidates. Each candidate is characterized by a single aggregation function fragment, applied to an aggregation column fragment, in the SQL select clause. In addition, each candidate is characterized by a set of unary equality predicates that we connect via a conjunction in the SQL where clause. The SQL from clause can be easily inferred from the other query components: it contains all ta- bles containing any of the columns referred to in aggregates or predicates. We connect those tables via equi-joins along foreign-key-primary-key join paths. 5 PROBABILISTIC MODEL We map each natural language claim to a probability distri- bution over matching SQL queries. Based on the most likely query for each claim, we can decide which claims are likely to be wrong and focus the user’s attention on those. 5.1 Probabilistic Model Overview Our probabilistic model is based on a fundamental property of typical text documents (we quantify this effect in Appen- dix A): text summaries tend to have a primary focus. The claims made in a text are not independent from each other but typically connected via a common theme. If we find out the common theme, mapping natural language claims to queries becomes significantly easier. 1: // Calculate for each claim in C a distribution over 2: // matching queries on database D using relevance 3: // scores S via expectation maximization. 4: function QueryAndLearn(D, C, S) 5: // Initialize priors describing text document 6: Θ ←Uniform 7: // Iterate EM until convergence 8: while Θ not converged yet do 9: // Treat each factual claim 10: for c ∈ C do 11: // Calculate keyword-based probability 12: Qc ←TextProbability(S, Θ) 13: end for 14: // Refine probability via query evaluations 15: Q ←RefineByEval({Qc |c ∈ C}, C, D) 16: // Update document-specific priors 17: Θ ←Maximization({Qc |c ∈ C}) 18: end while 19: return {Qc |c ∈ C} 20: end function Algorithm 2: Learn document-specific probability dis- tribution over queries and refine by evaluating query candidates. We represent the common theme as a document-specific probability distribution over queries. We use that distribution as a prior when inferring likely queries for each claim. Be- yond the prior distribution, the likelihood of queries depends on the keyword-based relevance scores that are associated with each claim (we described how those relevance scores can be calculated in the last section). We face a circular dependency: if we had the document theme, we could use it as prior in our search for the most likely query for each claim. On the other side, if we had the most likely query for each claim, we could infer the docu- ment theme. This motivates an expectation-maximization approach [9] in which model parameters (describing here the document-specific query distribution) and values of latent variables (describing claim-specific query distributions) are iteratively refined, using tentative values for one of the two to infer estimates for the other. Algorithm 2 describes how the AggChecker infers prob- ability distributions over query candidates. Starting from a uniform document distribution (captured by parameter Θ whose precise components are described in the following), the algorithm iterates until convergence. In each iteration, a claim-specific probability distribution over query candidates is calculated for each claim, based on the relevance scores provided as input and the model parameters. Strong evidence that a query candidate matches a natural language claim can be obtained by evaluating the query and comparing its result to the claimed one. However, evaluating queries on potentially large data sets may lead to significantResearch 3: Information ExtractionSIGMOD ’19, June 30–July 5, 2019, Amsterdam, Netherlands305 Specific to Claim 1 Specific to Claim n p Model parameter V Latent Variable V Observable Variable pf 1 . . . pa1 . . . pr 1 . . . Q1 Qn. . . S1 E1 Sn En Figure 5: Simplified probabilistic model for query in- ference: parameters describe prior probabilities of query characteristics, claim queries (Qc ) are latent while relevance scores (Sc ) and evaluation results (Ec ) are observable. processing overheads. Myriads of queries are possible for a given data set and we cannot execute all of them. This is why we use a preliminary, claim-specific query distribution to select promising query candidates for execution. In the next section, we describe efficient processing strategies enabling us to execute hundreds of thousands of query candidates during learning. The evaluation of promising candidates is encapsulated in Function RefineByEval in Algorithm 2. The result is a refined probability distribution over query candidates for a given claim that takes evaluation results into account. Finally, the model parameters are updated based on the claim-specific distributions. In the following subsections, we provide details on our probabilistic model. A simplified version of the model is illustrated in Figure 5. 5.2 Prior Probabilities We need to keep that model relatively simple for the fol- lowing reason: having more parameters to learn typically requires a higher number of iterations until convergence. In our case, each iteration requires expensive data processing and hence we cannot afford an elevated number of steps. We therefore introduce only parameters that describe the probabili","Jo, S., Trummer, I., Yu, W., Wang, X., Yu, C., Liu, D., & Mehta, N. (2019, June). Verifying text summaries of relational data sets. In Proceedings of the 2019 International Conference on Management of Data (pp. 299-316)."
SSAFC_005,Use of bot and content flags to limit the spread of misinformation among social networks: a behavior and attitude survey,"The COVID-19 infodemic is driven partially by Twitter bots. Flagging bot accounts and the misinformation they share could provide one strategy for preventing the spread of false information online. This article reports on an experiment ( N  = 299) conducted with participants in the USA to see whether flagging tweets as coming from bot accounts and as containing misinformation can lower participants’ self-reported engagement and attitudes about the tweets. This experiment also showed participants tweets that aligned with their previously held beliefs to determine how flags affect their overall opinions. Results showed that flagging tweets lowered participants’ attitudes about them, though this effect was less pronounced in participants who frequently used social media or consumed more news, especially from Facebook or Fox News. Some participants also changed their opinions after seeing the flagged tweets. The results suggest that social media companies can flag suspicious or inaccurate content as a way to fight misinformation. Flagging could be built into future automated fact-checking systems and other misinformation abatement strategies of the social network analysis and mining community",Computer Science,Article,2021,N,Y,9,"The COVID-19 crisis, which led to much of social life migrating online, has contributed to an infodemic, where information of varying quality quickly spreads in social media networks around the world. While ideally high-quality health information would be shared from credible sources and saturate social networks, misinformation about COVID- 19 poses a significant public health risk during a global pan- demic (O’Connor and Murphy 2020; Barau et al. 2020). As World Health Organization Director General Tedros Adhanom Ghebreyesus remarked, We’re not just fighting a pandemic; we’re fighting an infodemic” (The Lancet Infec- tious Diseases 2020). Twitter provides a significant source of COVID-19 misinformation (Yang et al. 2020). In one analysis, almost 25% of COVID-19-related tweets contained some misinformation (Kouzy et al. 2020). Much of this mis- information spreads through bots, automated accounts that often share false or conspiracy-based information in order to amplify a political message. According to one analysis of COVID-19 information on Twitter, bot accounts share a high volume of tweets linking to low-credibility sources (Yang et al. 2020). Analysis has also revealed that high bot score accounts are used to promote political conspiracies and divisive hashtags alongside with COVID-19 content” (Ferrara 2020 p. 17), while accounts likely run by humans focus more on health and public welfare. The spread of misinformation on Twitter has been noted with alarm by scholars even before COVID-19 (Zubiaga and Ji 2014; Waszak et al. 2018; Sommariva et al. 2018). When reviewing the state of medical information on social media, Wang et al. (2019) conclude that misinformation is abundant on the internet and is often more popular than accurate information” (p. 7), while Chen and colleagues (2018) found that medical misinformation spread more broadly on Twitter than accurate information. For instance, misleading information about Zika on Twitter was more popular than accurate posts (Sharma et al. 2017). With the rise in AI, bots spread much of this disinformation, oftenSocial Network Analysis and Mining (2021) 11:321 332 Page 2 of 15 contributing significantly to the spread of low-credibility content (Shao et al. 2018). Twitter bots played a dispro- portionate role in spreading and repeating misinformation” about the U.S. presidential election 2016 (Shao et al. 2017, p. 1), hold a small but strategic role in Venezuelan politi- cal conversations” (Forelle et al. 2015 p. 1), and retweet anti-vaccination information (Broniatowski et al. 2018), especially to receptive users (Yuan et al. 2019). Correcting Twitter misinformation remains a huge public health challenge, both in and beyond the COVID-19 crisis. This challenge exists because people are notoriously dif- ficult to persuade when they hold false or conspiratorial beliefs (Gruzd and Mai 2020; Rice 2020) and because some analyses suggests that on Twitter, COVID-19 misinformed communities are denser, and more organized than informed communities, with a possibility of a high volume of the misinformation being part of disinformation campaigns” (Memon and Carley 2020 p. 1). Scholars have increased calls for research into combating misinformation online. Chou et al. (2018) called for research that develops and tests interventions in response to online misinformation. According to Pagato et al. (2019), research must address the following questions: How does health (mis)information spread, how does it shape attitudes, beliefs and behavior, and what policies or public health strategies are effective in disseminating legitimate health information while curb- ing the spread of health misinformation?” (2019, p. 1). Wei et al. (2016) describe the challenges that undesirable users” create for using Twitter as a medium for understanding the cultural landscape” and helping the response to important events and crises (p. 51). Misinformation is often defined in a way that allows for its automatic detection. Dhar et al. (2016) describe misinfor- mation as a rumor; pushing that definition further, Tsugawa and Ohsaki (2017) identify misinformation with the concept of flaming” where falsehoods become viral when expressed in negative terms; by using a sentiment analysis, Tsugawa and Ohsaki then identified possible misinformation. Dewan and Kumaraguru (2017), on the other hand, focused on the motives of those who shared the misinformation, describing it as a tool of cybercriminals perpetuating a scam or hoax. Another approach to identifying misinformation uses auto- mated fact-checking, which focuses on a direct comparison of the message to a known, credible outside source. Thorne and Vlachos (2018), using this definition, look at the state of natural language processing and journalistic sources to see where there are gaps in the automated fact-checking process. Each definition provides benefits for the automated identifi- cation and tracking of misinformation to monitor the health of social networks. While the work of social network analy- sis and mining scholars is of great importance for address- ing the COVID-19 infodemic, the second step in addressing misinformation in social networks is what is done once a message (tweet, FB post, etc.) is identified as a problem. Many social network platforms like Facebook, particu- larly those located in societies which emphasize the impor- tance of freedom of expression, may feel uncomfortable outright banning or censoring posts (Kang and Isaac 2019). Instead, flagging posts from a questionable source or flag- ging information that is known to miss what credible sources are saying is a common approach. Yet, does flagging misin- formation or a questionable source sway social media users if they already believe the information being flagged? In response to these calls and the special theme of this issue, which asks for strategies to mitigate and fact check COVID-19 misinformation, this article reports on a novel, branching survey experiment (N = 299) that tested how par- ticipants responded to tweets featuring conspiracy theories about the official count of COVID-19 deaths in the United States. Participants first viewed a tweet that aligned with their existing opinion about the COVID-19 death tallies and then saw the same tweet with a flag indicating that the tweet was generated by a bot and then saw a flag warning that the tweet contained false information. The results suggest that both flags significantly decrease participants’ willingness to engage with tweets and may change some participants’ minds about COVID-19 misinformation. Social media plat- forms can use this information in their approaches to help combat a COVID-19 infodemic. This finding is an important contribution to social network analysis and mining so that the warnings from automated detection techniques can be crafted into persuasive messages that will motivate users to be cautious during the COVID-19 infodemic. 2 Literature review 2.1 Human perception of messages shared by bots People tend to trust content attributed to AI authors less than they trust content attributed to humans (Waddell 2018). This makes sense, as users often rely on the authority of a Twitter account to separate reliable and unreliable informa- tion (Zubiaga and Ji 2014). However, studies tend to find that people only mistrust AI-generated content under certain conditions. Readers did not assign higher credibility scores to human-written vs. bot-written news articles when they did not know who wrote the story, but they considered stories labeled as written by humans more credible and readable (Graefe and Bohlken 2020). Adding low-confidence indica- tors to AI-generated content decreases participant trust, but high-confidence indicators do not increase trust (Bruzzese et al. 2020). Research of participants who viewed tweets labeled as coming from either a CDC Twitterbot or a human working at the CDC found that a Twitterbot is perceived as Social Network Analysis and Mining (2021) 11:321 3 Page 3 of 15 32 a credible source of information” (Edwards et al. 2014, p. 374). Participants gave similar credibility scores for a set of 10 Airbnb profiles regardless of whether they thought they were human or computer generated; however, when partici- pants engaged with a set of 10 profiles and received infor- mation that some of the profiles were human generated and some were AI generated, they gave lower trustworthiness scores to profiles they assumed were AI generated (Jakesch et al. 2019). 2.2 Correcting misinformation on social media Many studies find that interventions to correct misinfor- mation on Twitter work to reduce misperceptions. Giv- ing people accuracy nudges before they consider sharing COVID-19-related information significantly improves their truth discernment, suggesting that nudging people to think about accuracy is a simple way to improve choices about what to share on social media” (Pennycook et al. 2020). Labeling information as rumor caused participants to con- sider it less important than information labeled as news (Oh and Lee 2019). Correcting misinformation about the Zika virus on Twitter by providing a source lowered mispercep- tions in participants (Vraga and Bode 2017a, b), as did cor- recting conspiracy theories about Zika (Lyons et al. 2019). Corrections can be effective coming from either algorithms or other platform users and can even affect individuals with high levels of conspiracy beliefs (Bode and Vraga 2017). WhatsApp messages from civil society organizations in Zimbabwe can correct COVID-19 misperceptions and affect positive changes in social distancing behavior (Bowles et al. 2020). Corrections from government agencies were more effective than corrections from other users (Vraga and Bode 2017a, b; van der Meer and Jin 2020), though other research has found that comments about Twitter content being fake news were more effective coming from other users than as a disclaimer from a social media platform (Colliander 2019). In an experimental situation where participants saw a fake news story on Facebook about a nonprofit organization along with a refutation from the nonprofit, denial created higher credibility for the nonprofit than comments attacking the source of the fake news (Vafeiadis et al. 2019). However, attempts to correct misinformation can some- times work against their intended effect (Lewandosky et al. 2012), especially in individuals who accept conspiracy theories (Miller et al. 2016). In two experiments designed to combat Zika and yellow fever misinformation in Brazil, Carey et al. (2020) found partial success for interventions to correct health myths, but also concluded that current approaches to combating misinformation and conspiracy theories about disease epidemics and outbreaks may be ineffective or even counterproductive” (p. 9). A meta-anal- ysis of attempts to correct misinformation online (Wang et al. 2019) finds that although interventions to correct misperceptions are proven effective at times, efforts to retract misinformation need to be carried out with caution in order to prevent backfiring” (p. 7). 2.3 Research gap The experiment reported here contributes to this ongoing investigation of methods for best countering and correcting the spread of misinformation on social media. Specifically, we make two unique contributions to this effort. First, while most research randomly assigns participants into experimental groups, this study assigned participants to conditions based on their previous beliefs about COVID- 19 misinformation. Participants who believed COVID-19 death tallies were over- or undercounted saw tweets con- firming their beliefs. (Those with no opinion or who felt the counts were accurate were randomly sorted into the over- or undercounted groups.) This approach allowed us to test the impact of flags on audiences sympathetic to the misinformation in the tweets and also allowed us to more directly test for backfire effects sometimes associ- ated with message correction. This methodology specifi- cally responds to calls by Lewandosky et al. (2012) to test whether retractions fail to reduce reliance on misinfor- mation specifically among people for whom the retrac- tion violates personal belief” (p. 118) and Wang (2019) to understand the role of belief systems on the intention to spread misinformation” (p. 1). Second, the experiment employs a sequence of two flags, telling users that the tweet is a suspected bot and then informing users that the tweet contains misinformation. This approach allows us to test the influence of these flags individually and together and represents a more sustained fact-checking approach. 2.4 Research questions This study answers the following research questions: RQ1 Does a flag that the tweet is written by a bot change participants’ engagement with and attitudes about the tweet? RQ2 Do flags that the tweet is both written by a bot and contains misinformation change participant’s engage- ment with and attitudes about the tweet? RQ3 Are participants capable of changing their opinion after viewing flags that a tweet was shared by a bot and contained misinformation? RQ4 What personal experiences and attitudes are asso- ciated with the respondent’s willingness to change their Social Network Analysis and Mining (2021) 11:321 332 Page 4 of 15 opinion about coronavirus numbers after viewing the flagged tweets? 3 Method 3.1 Research context Data were collected over a three-day period from Sep- tember 8, 2020, through September 10, 2020. On Septem- ber 10, 2020, the USA had a total of 6,366,986 cases of COVID-19, including 183,950 reported deaths from the virus (US Historical Data). At the time data were collected for this study, there was no scientific evidence indicat- ing these numbers were incorrect. However, posts sharing false and misleading information, including suggestions that official COVID-19 numbers from the CDC were being either over- or underreported, were abundant on social media platforms such as Facebook and Twitter during this time period (Ebrahimji 2020; Kouzy et al. 2020). In the month prior to the current study, the then current President of the United States shared a tweet that was removed by Twitter for reporting false coronavirus statistics (Quinn 2020). The removed tweet claimed the CDC had quietly updated coronavirus numbers and suggested prior COVID- 19 deaths were being overreported. 3.2 Participants We collected a total of 332 initial responses from partici- pants using Amazon’s Mechanical Turk (MTurk) between September 8, 2020 and September 10, 2020. We choose MTurk for recruitment because its participants have been found representative of the general US population (Levay et al. 2016; McCredie and Morey 2019; Redmiles et al. 2019), especially the general Internet-using population (Keith et al. 2017). Further, MTurk participants tend to accept and take seriously experimental conditions at roughly the same rate as lab experiment participants (Thomas and Clifford 2017). According to Ford (2017), one major problem with MTurk results comes from speeders,” participants who rush through answers to get paid as quickly as possible. To help combat this issue, our survey included three attention check questions embedded throughout the survey where participants were asked to select a specific response option. We dropped 33 subjects from our final dataset for failing to correctly respond to all three attention check questions, resulting in a final sample of 299 individuals. The final sample was on average 35 years old (M = 35.49, SD = 10.03), primarily male (59.9% male, 39.5% female, 0.6% other or prefer not to say), and White (White = 75.95%, Asian = 10.0%, Black/African-Amer- ican = 7.0%, Hispanic/Latinx = 2.3%, Native Ameri- can = 1.7%, biracial = 2.1%, and other = 0.7%). Our sample is a good reflection of Twitter’s users: 30.9% are between the age of 25 and 34, and the majority of Twitter’s users are male (Clement 2020a, 2020b). 3.3 Survey instrument Participants were first presented with four statements and asked to select the one that best described their view of coronavirus case reporting data from the U.S. federal gov- ernment. The four statements were: (1) there is underre- porting—actual numbers are higher than reported numbers; (2) there is overreporting—actual numbers are lower than reported numbers; (3) there is accurate reporting—actual numbers are consistent with reported numbers; and (4) I do not have an opinion regarding coronavirus numbers. Participant responses to this question were used to assign participants to either the overreporting tweet condition or the underreporting tweet condition. One novel aspect of this study is that we presented respondents with the tweets that were aligned with respondents’ current beliefs. For respond- ents who did not have an opinion on reporting or believed the numbers were accurate, they were randomly assigned to either the overreporting or underreporting tweet condition. Participants were then asked questions to assess their atti- tudes and behaviors before being presented with one of two fabricated tweets claiming coronavirus deaths are being mis- reported, either overreported (See Fig. 1) or underreported (See Fig. 2). After viewing the tweet, respondents were asked to respond to items to assess their attitudes regarding the tweet’s credibility. After the initial tweet was presented to the respondent, they were then shown the tweet again, this time flagged with a statement which read Caution: Sus- pected Bot Account. Learn More.” Respondents were then asked to assess the flagged tweet’s credibility. Respondents were ultimately shown the tweet a third time with a second flag added which read Caution: Tweet contains misinforma- tion about the novel coronavirus. Learn more.” After being presented with the tweet with two flags, respondents were asked to assess the tweet’s credibility. Tweets were created using the Tweetgen.com service (beta-0.3.2 2020) and used identical share and like numbers for all versions. The num- bers were chosen to neither appear very low nor very high to keep the participants focused on the content flags. Our methodology follows other recent studies (Borah and Xiao 2018; Wasike 2017; Lim and Lee-Won 2017; Oeldorf-Hirsch et al. 2020; Scott et al. 2020; Solnick et al. 2020) that test the effects of static representations of Twitter and Facebook posts. Social Network Analysis and Mining (2021) 11:321 3 Page 5 of 15 32 Fig. 1 Tweets for overcounted coronavirus numbers condition Fig. 2 Tweets for undercounted coronavirus numbers condition Social Network Analysis and Mining (2021) 11:321 332 Page 6 of 15 3.4 Measures Participant preventative behaviors were assessed by ask- ing respondents how frequently (never, sometimes, often, or always) they engaged in seventeen different behaviors designed to reduce the risk of catching the coronavirus. Behaviors included avoiding nonessential shopping, fre- quently washing hands for 20 seconds, cleaning regularly touched surfaces with disinfectant, and limiting gatherings to fewer than 10 people. To assess respondents’ fears related to the coronavirus, we asked respondents to indicate their level of agreement using a 5-point Likert-type scale for three statements designed to capture their coronavirus health-related concerns. These statements included: I am scared that I might contract coronavirus,” I am scared that someone in my family will contract coronavirus,” and I fear that if I or someone in my family gets coronavirus, we will face complications that require hospitalization.” Respondents were asked to report the number of hours they spent on social and news media both before the coro- navirus and in the 30 days prior to completing the survey. To determine time spent on social media, respondents were asked to report the number of hours spent per day on social media, while time spent on news media was collected by asking respondents to report the number of hours they spent watching/reading news for each time period. Respondents were presented with 22 different news sources (including a write-in other” option) and asked to indicate where they received their news. Respondents were allowed to select multiple options from the list. Major televi- sion and social media sites were listed separately (e.g., CNN, Fox News, Facebook, Twitter), while less frequently con- sumed sources or regional media were presented using cat- egories such as Local Television News” or Liberal News Websites (Mother Jones, the Nation).” Respondents were also given the option of selecting news information directly from President Trump through either President Trump Tweets” or President Trump White House briefings.” Anomie, or the breakdown in belief in social bonds, was assessed using the nine item GSS Anomie Scale (Smith et al. 2019). Sample items include Most people don’t care what happens to others” and A person must live pretty much for today.” Respondents responded using a 5-point Likert-type scale. The Cronbach’s alpha of this scale was 0.845. To assess the extent to which respondents generally believe and engage in conspiratorial thinking we used the Conspiracy Mentality Questionnaire (CMQ; Bruder et al. 2013). The scale consisted of five items (e.g., there are secret organizations that greatly influence political decisions) and used a 5-point Likert-type scale with strongly agree and strongly disagree anchors. Cronbach’s alpha for the CMQ was 0.832. Government trust was measured using the Citizen Trust in Government Organizations’ scale (Grimmelikhuijsen and Knies 2017). Respondents were presented with nine state- ments and asked the extent to which they agree or disagree with each statement using a 5-point Likert-type scale. Sam- ple items include the federal government is capable” and the federal government is honest.” The overall Cronbach’s alpha for this scale was 0.959. To assess respondent attitudes regarding tweet credibility, we adapted items used to evaluate Twitter posts first used by Vraga and Bode (2017a, b). After viewing each tweet, respondents were asked to evaluate the tweet as being use- ful, interesting, trustworthy, credible, biased, accurate, or relevant using a 5-point Likert-type scale. Additionally, respondents were asked to indicate how they would inter- act with the tweet by responding to four questions to gauge likely behaviors in regard to the tweet. These behaviors included following the Twitter account, retweeting the tweet, liking the tweet, and searching for additional information related to the tweet. After reading the tweets, respondents were presented with a cognitive dissonance measure developed by Metzger et al. (2020) to determine the impact of viewing attitude- challenging information, such as flagging a tweet that shares respondents’ beliefs, on feelings of dissonance. This was measured using a nine-item 5-point Likert-type scale. Cron- bach’s alpha for the scale was 0.638. Respondent religiosity was captured using a 3-item meas- ure developed by Barnett et al. (1996). Respondents were presented with three statements (e.g., ""my religion is very important to me"") and asked to respond to each statement using a 5-point Likert-type scale. The Cronbach’s alpha for this scale was 0.930. Given the nature of COVID-19, basic respondent health information related to the virus was also collected for this study. Respondents were asked to report whether they, some- one in their household, a family member, close friend or acquaintance, or coworker had been diagnosed with COVID- 19. Respondents were also asked to indicate whether they suffered from any of the preexisting conditions that increased the risk of severe illness from COVID-19 (Centers for Disease Control and Prevention 2020). Demographic data including age, gender, ethnicity, and highest degree completed were collected from all respondents. 3.5 Analysis Data were analyzed using Kruskal–Wallis, ANOVA, Chi- squared test for independence, independent t-tests, and Pear- son correlations using IBM SPSS 26 (2020). Graphs were created in Microsoft Excel (2016). Social Network Analysis and Mining (2021) 11:321 3 Page 7 of 15 32 4 Results 4.1 Association between belief in COVID‑19 numbers and preventative behaviors To identify whether there is an association between the participants’ belief about the accuracy of COVID-19 mortality figures and preventive behaviors, we performed a Kruskal–Wallis test and found that there was a statisti- cally significant difference in hand washing (H3 = 15.653, p = 0.001), avoiding touching the face (H 3 = 15.407, p = 0.002), avoiding using cash when making purchases (H3 = 13.725, p = 0.003), limiting gatherings to fewer than 10 people (H3 = 33.311, p < 0.001), working from home (H3 = 16.313, p = 0.001), avoiding nonessential shopping (H3 = 22.595, p < 0.001), monitoring news about corona- virus (H3 = 11.326, p = 0.01, practicing social distancing (H3 = 24.511, p < 0.001), using electronic communication to avoid meeting with people in person (H3 = 27.78, p < 0.001), wearing a mask (H3 = 43.923, p < 0.001), staying at home unless shopping for core needs (H3 = 16.195, p = 0.001), and quarantining from others if symptoms appear (H3 = 18.879, p < 0.001). The average preventative behavior score for each coronavirus case count group is shown in Fig. 3. For those who have personally experienced COVID-19 themselves or had someone they know, either a friend or family member, contract the disease, they were more likely to take a clear position on the COVID-19 mortality count; the unsure” respondents were those without personal expe- rience (H3 = 13.998, p = 0.003). A Kruskal–Wallis test also showed that those who fear contracting the coronavirus, fear that their family will contract the virus, and that they or their family may face complications were more likely to believe the COVID-19 numbers were accurate or undercounted compared to those who believe the numbers are overes- timated (H3 = 20.063, p < 0.001; H3 = 18.732, p < 0.001; H3 = 15.649, p = 0.001). 4.2 Impact of Twitter flags on change in belief about COVID‑19 numbers A series of paired t-tests show how individual respondents changed their opinions about the Twitter accounts after a flag was placed on the tweet. The first flag warned partici- pants that the tweet was shared by a suspected bot account. After being flagged as a potential bot, participant’s perspec- tive on the Tweet’s credibility changed for every measure except bias, which remained consistent regardless of the Twitter flags. In responding with their desire to follow the Twitter account, the version with no flag was higher than the bot flag (t298 = 8.638, p < 0.001) and the bot–misinformation flag (t298 = 9.443, p < 0.001). For willingness to retweet this tweet, the unflagged tweet was rated more highly than the bot flag (t298 = 5.165, p < 0.001) and the bot–misinformation flag (t298 = 5.819, p < 0.001). Willingness to like the tweet followed the same pattern from unflagged to a reduced will- ingness after the tweet was flagged as a bot (t298 = 5.862, p < 0.001) and then as misinformation (t 298 = 8.581, p < 0.001). The next set of questions dealt with the percep- tion of the tweet. The unflagged tweet was rated more highly for willingness to seek more information compared to the bot flag (t298 = 6.177, p < 0.001) and the bot–misinformation flag (t298 = 8.793, p < 0.001), respectively; the same was true for usefulness (t298 = 6.113, p < 0.001; t298 = 9.43, p < 0.001), interest (t298 = 6.199, p < 0.001, t298 = 8.318, p < 0.001), trust- worthiness (t298 = 6.304, p < 0.001; t298 = 9.349, p < 0.001), credibility (t298 = 7.977, p < 0.001; t298 = 10.439, p < 0.001), accuracy (t298 = 6.264, p < 0.001; t298 = 11.581, p < 0.001), and relevance (t 298 = 6.942, p < 0.001; t298 = 9.412, Fig. 3 Differences in preventative behaviors based on belief in COVID-19 Count Social Network Analysis and Mining (2021) 11:321 332 Page 8 of 15 p < 0.001). The one aspect that remained the same despite the flags as a bot or as a bot–misinformation was bias: t298 = -0.452, p = 0.652; t298 = -0.951, p = 0.342. The aver- age rating for each flag condition is shown in Fig. 4. A Chi-squared test of independence shows that expo- sure to the series of tweets with flags changed the opin- ions of certain participants at a statistically significant rate: χ29 = 462.360, p < 0.001. Those who were unsure or believed the count is accurate were more likely to switch their perspective to the tweet they viewed and agree with it even after it was flagged as a bot and cautioned it contained misinformation. For those who were initially unsure of the count’s accuracy, 80% remained unsure and 20% switched to saying the numbers are overcounted after seeing the over- count tweet. For those who believe the numbers are accurate, 78% held that position, but 12% changed their opinion to match the tweet they saw, while 9% adjusted their opinion against the tweet they saw. Those who believed the num- bers are overcounted were most susceptible to changing their opinions after seeing the cautionary flags; 73% continued to believe numbers were overcounted, with 5% becoming unsure, 11% saying the numbers are accurate, and 12% say- ing the numbers are undercounted. Those who believe the numbers are undercounted were the most dependable in their belief with 88% stating the count is underreported, 4% say- ing it is accurate, and 8% saying it is overcounted. 4.3 Characteristics based on belief in COVID‑19 count accuracy Using an ANOVA test, we found that there was a differ- ence between respondents’ view of the COVID-19 mortal- ity count and their score on the cognitive dissonance scale (F3,295 = 3.437, p = 0.017). Those who believed the count is overstated averaged 0.33 less than those who believe the count is accurate and averaged 0.232 less than those who believe the count is undercounted. There were also differ- ences in the conspiracy scale (F3,295 = 3.21, p = 0.023) and trust in the government (F3,295 = 11.068, p < 0.001). Those who believe the count is overstated had a higher average conspiracy score by 0.308 compared to those who think the number is accurate and by 0.33 for those who believe the count is undercounted. The participants who believe the number is undercounted did not trust the government compared to the other groups with an average difference of 0.608 for those who think the number is overstated and an average difference of 0.77 for those who think the number is accurate. An additional ANOVA test revealed that religiosity and political affiliation were also associated with differences in belief. On a seven-point scale, the overcount participants were 1.1 points more conservative on average than the undercount participants; the accurate count participants were 1.14 points more conservative than the undercount participants (F3,295 = 8.227, p < 0.001). Those who believe the numbers are accurate or overcounted also agreed more strongly to the statement I am very religious,” My reli- gion is very important to me,” and I believe in God” when compared to the undercount position (F3,295 = 9.610, p < 0.001). 4.4 Characteristics for changed opinion Anomie, or the breakdown in belief in social bonds, was higher in those who changed their mind (3.38) than in those who didn’t (3.12) (t297 = − 2.147, p = 0.033). Trust in government was higher in those who changed their mind (3.57) than those who did not (2.99) (t100.028 = − 4.18, p < 0.001). Fig. 4 Change in rating after tweets flagged Social Network Analysis and Mining (2021) 11:321 3 Page 9 of 15 32 Those who changed their mind had more formal educa- tion (5.21) than those who did not change their mind (4.68) (t118.965 = − 4.111, p < 0.001). Those willing to change their mind also had more pre- existing conditions (2.54) compared to those unwilling to change their mind (1.7) (t297 = − 2.197, p = 0.029). Lastly, those willing to change their mind were less con- cerned about the economy being negatively impacted from the coronavirus (3.16) than those who did not change their mind who had higher levels of concern (3.8) (t 297 = 3.9, p < 0.001). 4.5 News media consumption A Chi-squared test of independence showed that cer - tain news media outlets had an impact on what the par- ticipant believed. The number of participants from each group who read or view the following news resources are shown in Fig. 5. Fox news (χ2 = 12.191, p = 0.007), One America News Network (χ2 = 13.379, p = 0.004), National Newspapers (χ2 = 11.495, p = 0.009), Liberal News Web- sites (χ 2 = 8.641, p = 0.034), Conservative News ","Lanius, C., Weber, R., & MacKenzie, W. I. (2021). Use of bot and content flags to limit the spread of misinformation among social networks: a behavior and attitude survey. Social Network Analysis and Mining, 11(1), 1-15."
SSAFC_017,ExFaKT: A Framework for Explaining Facts over knowledge graphs and Text,"Fact-checking is a crucial task for accurately populating, updating and curating knowledge graphs. Manually validating candidate facts is time-consuming. Prior work on automating this task focuses on estimating truthfulness using numerical scores which are not human-interpretable. Others extract explicit mentions of the candidate fact in the text as an evidence for the candidate fact, which can be hard to directly spot. In our work, we introduce ExFaKT, a framework focused on generating human-comprehensible explanations for candidate facts. ExFaKT uses background knowledge encoded in the form of Horn clauses to rewrite the fact in question into a set of other easier-to-spot facts. The final output of our framework is a set of semantic traces for the candidate fact from both text and knowledge graphs. The experiments demonstrate that our rewritings significantly increase the recall of fact-spotting while preserving high precision. Moreover, we show that the explanations effectively help humans to perform fact-checking and can also be exploited for automating this task",Computer Science,Article,2019,N,Y,0,"Motivation and Problem. Knowledge Graphs (KGs) are large col- lections of factual triples of the form ⟨subject predicate object⟩ (SPO) about people, companies, places, etc. Projects like BabelNet [27 ], DBpedia [ 5 ], Wikidata [ 36 ] and YAGO [ 34] have constructed KGs with millions of entities and billions of facts. However, KGs also contain doubtful if not incorrect SPO triples, as they are partly built by automatic information extraction, crowd-sourcing, or methods for KG completion using rules [ 15 ] or embeddings [ 28 , 30 , 38]. This incurs the problem of validating if an SPO triple is correct or not, a task that is often referred to as fact checking or truth discovery [ 23]. Traditionally, fact checking has been performed manually by human reviewers but this is time-consuming. Therefore, with the increase of false facts on the Web, the automation of fact checking is gaining more attention. Methods for automatic fact checking Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). WSDM’19, February 11-15th, 2019, Melbourne, Australia © 2019 Copyright held by the owner/author(s). ACM ISBN 978-x-xxxx-xxxx-x/YY/M (e.g., [ 17 , 23 , 25 , 29 , 31 ]) proceed in two steps. First, they perform fact spotting: searching for occurrences of a fact candidate, such as ⟨Sadiq_Khan citizenOf UK⟩, and possible alternatives, such as ⟨Sadiq_Khan citizenOf Pakistan⟩, in a variety of Web sources. This is done by expanding the predicate into paraphrases (e.g., has nationality, has passport”,. . . , etc.) and searching for it jointly with the S and O arguments of the triple (usually entities with a set of alias names). The second step of fact checking is the inference of the truth value of the candidate fact based on the retrieved evidence or counter-evidence. Numerical scores produced by fully automated methods are not adequate whenever KG curators are required to make the final de- cision. For humans, such scores are hard to understand or justify without explanations. Some approaches (e.g., [17]) attempt to show the sources used in estimated scores as explanation. Yet, these evi- dences result from the fact spotting step which is purely syntactic: matching exactly S, P and O in one of their paraphrased forms. Unfortunately, this has often insufficient evidence, since textual sources are incomplete and biased in what is stated explicitly. For ex- ample, the citizenship of London’s mayor Sadiq Khan would rarely be mentioned. Moreover, some KG predicates (e.g., influencedBy) are ambiguous, and their interpretation is domain-specific. Proposed Approach. To better support KG curators in deciding the correctness of the candidate facts, we propose a novel frame- work for finding semantically related evidence in Web sources and the underlying KG, and for computing human-comprehensible expla- nations for facts. We refer to our framework, as ExFaKT (Explaining Facts over KGs and Text resources). The key for detecting semantic evidence is intensional back- ground knowledge in the form of rules, specifically, Horn rules of the form H ← B1, B2, . . . , Bn . For example, citizenOf (X, Y ) ← mayorOf (X, Z), locatedIn(Z, Y ) intuitively states that mayors of cities are normally citizens of coun- tries where these cities are located. Such rules can be specified by humans or automatically extracted from KGs using rule mining methods such as [ 16 , 37 ]. As the latter may fall short of covering all interesting situations, hand-crafted rules are a valuable asset. We performed a user study with undergraduate students who were novices to KGs, and obtained a good number of rules with low error rate in less than an hour (20 minutes of user instruction, 30 minutes of rule specification). WSDM’19, February 11-15th, 2019, Melbourne, Australia M. H. Gad-Elrab et al. We utilize rules to rewrite a fact-spotting query into more fre- quently stated and thus easier-to-spot related facts. This way, we counter the reporting sparseness and bias. Moreover, rules can en- code domain-specific knowledge to better cope with ambiguous predicates. Finally, rules combine knowledge from both textual Web sources and the KG. For example, a rule could find the mayors of cities in news articles and look up the countries of cities in the KG. Given a set of rules and a query for a fact candidate, ExFaKT uti- lizes the rules to rewrite the query into a set of subqueries. Whenever we find evidence that the body of the rule holds, this strengthens the credibility of the head. This process creates semantic traces that explain, in a human-readable format, why a fact is likely true (or false). Such interpretable evidence advances the state of the art and is our main contribution. ExFaKT adopts a top-down strategy for efficient query rewriting. In our setting, evaluating rule body atoms involves external data sources (KG or Web), which makes the task challenging. We develop optimization techniques to this end. Contributions. Our contributions are summarized as follows: • We introduce ExFaKT, a framework for computing semantic traces for facts in question from both KG and an implicit external source in the form of a text corpora by utilizing Horn rules. • We develop optimization strategies, whose target is an automatic search for an effective rewriting plan based on our cost model. • We evaluate ExFaKT over real-world KGs and rules from var- ious domains. The experiments show the effectiveness of our rewriting strategy, the benefits of the computed explanations in supporting human fact checkers, and the viability of exploiting explanations in automated fact-checking. A separate user study demonstrates that useful rules can be hand-crafted by novice users, after brief instructions, at low cost. ExFaKT code and data are available at. 2 PROBLEM STATEMENT In this section we provide background on KGs and rules as well as describe the problem of using them along with text in computing human-comprehensible explanations for facts in question. Knowledge Graphs. Let E and R be fixed sets of entities and relations respectively. A knowledge graph (KG) G is a repository of unary and binary facts (i.e., p(s) and p(s, o)) encoding domain knowl- edge (e.g., director (nolan), influencedBy(nolan, lucas)), where p ∈ R is a relation (predicate) and s, o ∈ E are entities (constants). Rules. Our goal is to exploit both KG and text resources in order to construct an explanation that can either support or reject a given input fact. To do so, we use rules, which we define below by borrowing well-known concepts from the Datalog language [1]. Rules are expressions of the form H ← B1, . . . , Bn (1) where H, B1, . . . , Bn are the atoms of the rule, i.e., expressions of the form p(X ) or p(X , Y ), where p ∈ R and X , Y are either enti- ties or variables. We refer to head(r) = H and body(r)={B1, ..., Bn } respectively as the head and body of r . Example 2.1. For r from Section 1, head(r) = {citizenOf (X, Y )}, and body(r) = {mayorOf (X, Z), locatedIn(Z, Y )}. □ Rules encode commonsense knowledge, e.g., capitals are located in countries or constraints, e.g., a person cannot be born in two places. Given a rule r of the aforementioned form (1) and a set I of facts, we define the set of facts inferred by r from I as r (I ) = {Hθ | B1θ, . . . , Bnθ ∈ I }, (2) where θ is a postfix operator which substitutes variables with con- stants. We denote by ε empty substitutions, i.e., qε = q for any q. Moreover, Π(I ) = Ðr ∈Π r (I ) is the extension to inferences produced by all rules in Π. Output of multiple rule executions is recursively defined by setting Π0(I ) = I and Πi+1(I ) = Π(Ðj ∈ {0, ...,i } Πj (I )). The set Π∞(I ) (called closure) contains all possible inferences de- rived using Π on I . Fact Spotting. Let textspot be a textual fact-spotting procedure, which gets as input an atom q and a set T of textual documents such as Wikipedia (textspot(q, T )), and outputs a set of tuples of the form ⟨θ, s⟩, where qθ is a fact spotted in the text and s is a textual string containing this fact. Example 2.2. For the query q = directed(lucas, star_wars) and the text T = {G. Lucas, the director of Star Wars, signs ..., Nolan got inspired by Star Wars, Star Wars 1977 directed by Lucas..}, textspot(q, T ) returns {⟨θ, s = G. Lucas, the director of Star Wars..⟩, ⟨θ, s =Star Wars 1977 directed by Lucas..⟩}, where θ = ε. □ We define the set of all facts involving entities from E and re- lations from R which can be extracted from the text T using the syntactic fact spotting procedure textspot by IT = {p(s, o) | s, o ∈ E, p ∈ R ∧ textspot(p(s, o), T ) , ∅}. Fact Explanations. Given a fact in question, a KG, text and rules, our goal is to compute a set of semantic traces or explanations as we call them for a given fact, which are formally defined as follows: Definition 2.3 (Explanation). Given a fact q, a KG G, text corpus T and a rule set Π, a set E ⊆ G ∪ IT of facts is an explanation for q w.r.t. Π, G, T if q ∈ Π∞(E). The presence of unstructured textual resources in the input makes the problem of computing explanations particularly chal- lenging. Naturally, a given fact q might have multiple explanations or a single trivial one, i.e., q itself. Obviously, explanations as de- fined above may be subsumed by others. Among all explanations ideally we aim at computing non-trivial ones that are (D1) concise, i.e., contain a small number of atoms; (D2) close to the query, i.e., obtained by using few rules; (D3) reliable, i.e., contain as many facts from the KG as possible, since KGs are usually more reliable than text. 3 EXFAKT FRAMEWORK Our framework relies on sets of rules to compute explanations using the content of KGs and other textual resources. While a variety of high-quality KGs is readily available, choosing good rulesets and fact-spotting procedures is a problem that needs more attention. Before dwelling into the details of our ExFaKT framework we first discuss the realization of these prerequisites. 3.1 Prerequisites Rule Acquisition. In general, rules can be either automatically extracted using tools such as [16 , 37 ], or manually specified by ExFaKT: A Framework for Explaining Facts over Knowledge Graphs and Text WSDM’19, February 11-15th, 2019, Melbourne, Australia Algorithm 1: Algorithm for computing explanations. Input: fact q, KG G, text corpus T, ruleset Π, nonegative parameter max _dept h for ensuring termination Output: set of explanations O 1 function explain(q, G, T, Π) 2 dept h[q] ← 0; st atus[q] ← TODO; P ← { {q } }; O ← { } 3 while P , ∅ do 4 Pick explanation E from P (i.e., E ∈ P ) 5 P ← P \ {E } 6 if st atus[д] =FOUND for all д ∈ E then 7 O ← O ∪ {E } ▷We found a valid explanation. 8 else 9 Pick an atom д from E s.t. st atus[д] =TODO 10 N T ← pr ocess_дoal (д, E \ {д }, G, T, Π) 11 P ← P ∪ N T 12 return O 13 function process_goal(д, E, G, T, Π) 14 O ← ∅ 15 Σ ← bind (д, G, T) 16 T R ← {д } 17 for σ ∈ Σ do 18 a ← дσ 19 dept h[a] ← dept h[д]; st atus[a] ← FOUND 20 if sour ce[σ ] =KG then T R ← T R \ {a } 21 O ← O ∪ {Eσ ∪ {a } } 22 for дr ∈ T R s.t. dept h[дr ] < max _dept h do 23 O ← O ∪ r ewr it e(дr, E, Π) 24 return O domain experts as in the context of ontology engineering [ 4]. We performed experiments to evaluate the quality and feasibility of both cases by using rules constructed by pre-existing systems and manually curated rulesets. In this last case, we have also conducted a pilot experiment to ensure the feasibility of manual rule construc- tion where we asked non-experienced participants to create useful rules. From these experiments, we observed that adequate rules can be produced not only by KG curators (which are our target group), but also by non-experts (more details are reported in Sec. 4). Realization of Fact Spotting. ExFaKT utilizes the syntactic fact spotting subroutine textspot, defined above. In practice, similar to [25 , 31 ], it is implemented relying on a textual-search-based method, in which the SPO query q is converted to textual repre- sentation (i.e., verbalization) using paraphrasing dictionaries for relations such as PATTY [ 26 ] and entity-mentions dictionaries for entity name aliases. Then q with its paraphrasing is issued to get the documents containing it. Finally, a named entity recognizer, e.g., [13 ], is utilized to collect entities from the documents and com- pute the substitution θ . This versatile approach is easily extendable without extensive training, but conceptually any alternative fact spotting method can be likewise applied in our work. 3.2 Computing Explanations We are now ready to describe our procedure for computing expla- nations using all the information that can be extracted from the KG and text. Computing the entire Π∞(G ∪ IT ) from scratch is not fea- sible, as it requires the extraction of all possible facts from T . Thus, we proceed backwards, i.e., we start from the input fact, and check whether any of the rules can potentially produce such derivation. If so, then we move to the body of the rule, and search for possible instantiations for each body atom. This triggers a recursive process, where the new input is constituted by the body atoms. The recursion stops in case no rules can be found or all atoms are instantiated either by facts in the KG or by text. In this last case, the rules produce new derivations which are returned to earlier recursive calls. Notice an important difference between our setting and existing applications of this procedure: While current applica- tions of backward reasoning assume that all the input is contained in the same database, in our case some facts are located in the KG (i.e., typically stored using indexed data structures), while others must be extracted from text on-the-fly using textspot. Therefore, our method needs to be particularly careful in choosing the next subquery, in order to prevent expenses in the retrieval procedures which might compromise an interactive usage of the system. We can identify two key operations in such a recursive process, which we call bind and rewrite. The first retrieves answers for a given query from the underlying data sources, while the second rewrites a query into subqueries. We formally specify them below. Bind. Prior to defining bind, we introduce some formal notations. Let д be an atom, G a KG, and T a text corpus. We define ΣG (д) = {σ | дσ ∈ G} and ΣT (д) = {σ | textspot(дσ , T ) , ∅} as the sets of substitutions that result in answers to the query д from the KG and text respectively. Every substitution σ ∈ ΣG (д) is annotated with the metadata source[σ ] =KG, while every σ ′ ∈ ΣT (д) is annotated with source[σ ′] =TEXT; moreover, each σ ′ is annotated with another metadata text[σT ] which contains the string that mentions д in T (as returned by textspot). Metadata of substitutions is passed to atoms they substitute, e.g., if text[σ ] =X then text[дσ ] =X. In our procedures, we use another two types of metadata, status[.], to mark atoms which are already processed, and depth[.], to record the number of rewritings that led to the atom at hand. Finally, metadata is not considered for the equality of substitutions and atoms, i.e. two atoms can be equivalent even if they are annotated with different metadata. Definition 3.1. The function bind(д, G, T ) receives as input an atom д, a KG G, and a text corpus T . It returns in output the set Σ = {ΣG (д) ∪ {ΣT (д) \ ΣG (д)}}. Example 3.2. Let us assume that д = directed(lucas,Z), G = {directed(lucas, star_wars)} and T =Along with Star Wars, Lucas has co-directed the films Raiders of the Lost Ark, Temple of Doom,...”. Then, ΣG = {σ1}, ΣT = {σ2, σ3}, where σ1 = {Z → star_wars}, σ2 = {Z → raiders_of _ . . .}, σ3 = {Z → temple_of _ . . .}. Also, source[σ1] =KG, source[σ2] = source[σ3] =TEXT, and text[σ2], text[σ3] store string coordinates of films in T . □ Rewrite. The function rewrite moves the search space of the an- swers for a given query from the rule’s head to its body by rewriting an input query into a respective set of subqueries. Definition 3.3. The function rewrite(g,E,Π) receives as input an atom д, a set of atoms E and a program Π. It computes the set Bд = {body(r )σдσE |r ∈ Π ∧ head(r )σд = д} where σд is a sub- stitution that unifies the rule head with g and propagates the substituted terms to the rule body, while σE renames among the rest of the body variables those that appear in E into fresh ones. Moreover, for each atom a in Bд the function sets status[a] =TODO, depth[a] = depth[g] + 1 and returns the set {E ∪ a | a ∈ Bд }. The following example illustrates the work of rewrite. WSDM’19, February 11-15th, 2019, Melbourne, Australia M. H. Gad-Elrab et al. Example 3.4. For the input д = inspiredBy(nolan, Z), E = {isDirector(nolan), directed(Y , Z)} and Π={r2 : inspiredBy(X, Y ) ← liked(X, Y ), isArtist(X )}, the function rewrite(g, E, Π) first computes Bд = body(r2 )σgσE, where σд = {X → nolan, Y → Z } and σE = ε, since σд already handled all variables appearing in body(r2 ). Finally, in the output we obtain E = {isDirector(nolan), directed(Y , Z), liked(nolan, Z), isArtist(nolan)}, where for every atom a ∈ Bд , the status is assigned as status[a]=TODO, and the depth as depth[a] = depth[g] + 1. □ Main Procedure. We give a formal and self-contained description of our method in Algorithm 1 and then show an example to ease the understanding. Our procedure takes as input a query q, a KG G, a text corpus T , a rule set Π and a global parameter max_depth specifying a maximum number of allowed rewritings to ensure termination, and as output provides a set O of explanations for q. Initially, the status of the input query is set to TODO, and the set P of potential explanations is initialized with {q}. In (3) the algorithm iterates over the set P of potential explanations. Explanations E ∈ P, all of whose atoms have status FOUND, are moved to the output set O in (7). The atoms with the status TODO of other explanations are processed by the procedure process_goal. This procedure takes as input an atom д and first retrieves all substitutions of variables to constants that result in answers to д in the KG and text using bind function and copies д into the set T R (to be rewritten). Then, it iterates over the retrieved substitutions and sets the depth of every obtained answer (a) to the one of д and the status to FOUND in line (19). After that the answer is added to the existing atom in the current explanation (E), which is included in O (line 21). Lastly, process_goal rewrites the input query using the rules in the program by invoking the function rewrite. Notice that T R might be empty: This occurs if д is a fact, which was verified in the KG. In this case, we do not need to rewrite it, since the fact is already explicitly stated; thus we remove it from T R in line (20). Note that if no restriction is put on the form of allowed rules then Algorithm 1 might not terminate. To avoid this, we bound the number of rewritings using the max_depth parameter, so that the algorithm always terminates. As output, Algorithm 1 returns explanations for the input candidate, which is a property formally stated in the following theorem: Theorem 3.5. Let G be a KG, T a text corpus, Π a program, q an input fact, and O = explain(q, G, T , Π). If E ∈ O then E is an explanation of q w.r.t. Π, G, T . We illustrate the work of Algorithm 1 with an example. Example 3.6. Consider the following input to Algorithm 1: • q = influencedBy(nolan, lucas) • T is a text corpus composed of Wikipedia articles • G = {directed(lucas, star_wars), isDirector(nolan), directed(lucas, amer_graffiti)} • Π = {r1, r2}, where r1 : influencedBy(X, Y )←isDirector(X ), directed(Y , Z), inspiredBy(X, Z); r2 : inspiredBy(X, Y )←liked(X, Y ), isArtist(X ). At the start of the algorithm the query q is the only available can- didate explanation to be processed. We have that q < G, moreover, assume that textspot(q, T ) = ∅. Then the set P of potential explana- tions is {{isDirector(nolan),directed(lucas,Z),inspiredBy(nolan,Z)}}. It holds that isDirector(nolan) ∈ G, hence rewrite is not applied to the first atom of the explanation, and we move to directed(lucas, Z). Applying bind on this atom results in the updated set of candidate explanations P = {{isDirector(nolan), directed(lucas, star_wars), inspiredBy(nolan, star_wars)}, {isDirector(nolan), directed(lucas, amer_graffiti), inspiredBy(nolan, amer_graffiti)}}. We start process- ing the first explanation, and since its first atoms are in G we move to the third one, which is inspiredBy(nolan, star_wars). Provided that this atom is spotted in the text, we obtain the following expla- nation E1={isDirector(nolan),directed(lucas, star_wars), inspired(nolan, star_wars)}. As it contains only found atoms, E1 is ready for addition to the final output set O. However, since the last atom in E1 was found in text, the algo- rithm still rewrites it, seeking for further evidence. In this case, r2 is used for rewriting leading to E2 = {isDirector(nolan), directed(lucas, star_wars), liked(nolan, star_wars), isArtist(nolan)}. Assuming that the last two atoms of E2 are spotted in T , we get E2 as the second explanation to be in the output set O. Analogously, we process the second explanation candidate in the set P, i.e., {isDirector(nolan), directed(lucas, amer_graffiti), inspiredBy(nolan, lucas)}. □ After the explanations have been computed, a decision on the validity of the fact in question needs to be made based on the collected evidence. If this operation is performed by a human, the obtained explanations provide interpretable assistance. If the fact validity is assessed by an automated procedure, the explanations can be used as input features. Optimizations. Often, we do not need to calculate all explanations; few relevant ones are sufficient for establishing the truth value of a fact in question. To improve the performance, we introduce anytime behavior and incrementally collect new explanations as they are added to the output set (line 7 of Algorithm 1). In this new setting, we can stop the algorithm whenever it has returned satisfactory explanations, but it is crucial that the most relevant explanations based on the criteria (D1)-(D3) from Section 2 are computed first. Example 3.7. If influencedBy(nolan, lucas) from Ex. 3.6 was found in text, then E0 = {influencedBy(nolan, lucas)} would be the most concise explanation, since it contains only a single atom. Moreover, for E1 and E2 from the same example we have that E1 is closer to the query than E2, since less rules were used to produce E1. If there is another explanation E3 which equals to E2 but with star_wars substituted by raiders_of . . . and directed(lucas, raiders_of . . .) is found in text, it holds that E2 is more reliable than E3, since E2 contains fewer atoms from text sources. □ Our optimizations along these lines affect two operations: the explanation selection and the atom selection criteria. Explanation Selection Criterion. In line 4, Algorithm 1 selects one explanation to be processed from those in P. To prioritize the processing of promising explanations we change the order in which they are picked based on the following criteria: (i) we favor shorter explanations, i.e., explanations with the lowest number of atoms (D1); (ii) we favor explanations produced with fewer rewritings by picking the query with the smallest depth value (D2). ExFaKT: A Framework for Explaining Facts over Knowledge Graphs and Text WSDM’19, February 11-15th, 2019, Melbourne, Australia Atom Selection Criterion. In line 9 of Algorithm 1, a naive strat- egy would always pick the first atom inserted in E resulting in huge search space. To counter this, we first select atoms without variables. Then, we select atoms with some constants and keep the ones without any constant to be processed last. Moreover, to favor explanations that can be proven from the KG (D3), we prefer atoms with some KG substitutions. 4 EVALUATION After describing our experimental setup in Section 4.1, we start in Section 4.2 with evaluating the effectiveness of ExFaKT with respect to the increase in the coverage of the collected evidences while preserving their precision (as shown later in Section 4.3). Then, we present our user-study which targets assessing the quality of the explanations and their readability and usefulness for human- reviewers in Section 4.3. Later in Section 4.4, we demonstrate the effectiveness of ExFaKT on automatically mined rules used as input. Moreover, a showcase for integrating ExFaKT into the pipeline of a fully automated fact-checking system is given in Section 4.5. Finally, Section 4.6 reports the results of our study on the feasibility of manual rule construction. Runtime experiments are reported in the supplementary material, and all datasets and rules are available at. 4.1 Experimental Setup Datasets. We conducted our experiments on two datasets: • YAGO-based benchmark, which consists of 300 true candidate facts, uniformly distributed over six relations (listed in Table 1). The instances of the first three relations were randomly selected from YAGO [ 34 ]. The other three relations do not appear in the KG; hence, their instances were semi-automatically curated using simple logical rules. Then, 50% of the facts used during their creation were removed at random; thus, intentionally introducing incompleteness in the KG and hence the need for textual sources. • DBpedia-based benchmark is a subset of the dataset proposed by [33 ] containing facts over the predicates, for which AMIE, a state-of-the-art rule mining system [ 15], managed to learn at least 5 rules having them in the head. This benchmark contains four predicates with a total of 1763 correct facts. Rules. For each benchmark we constructed a rule set. For the YAGO-based benchmark, we selected rules from the top-ranked ones mined by AMIE from YAGO, and added further rules with new predicates that do not exist in the KG. We refer to these new predicates as text-based, as they need to be verified from the text. This way we obtained 20 rules on average for each head predicate. For the DBpedia-based benchmark we used the rules mined by AMIE from DBpedia without altering them. Each head predicate in the dataset obtained a set of 100 related rules on average. This setup is designed to study the case of fully relying on automatically learned rules. Knowledge Graph. We used YAGO3 [34] as KG in all experiments. YAGO3 contains around 5.5M facts and 35 relations. This KG is geared towards precision rather than recall, allowing us to treat it as a trusted resource. Table 1: Recall of baselines vs. ExFaKT configurations B-Wiki B-Web KG Wiki Web KG+Wiki KG+Web influences 0.30 0.24 0.00 0.38 0.88 0.42 0.92 isPolitOf 0.02 0.16 0.26 0.18 0.88 0.42 0.92 wroteMusic 0.08 0.28 0.00 0.10 0.72 0.24 0.78 mayorOf 0.66 0.9 0.00 0.66 0.90 0.66 0.90 actedWith 0.26 0.52 0.18 0.26 0.60 0.54 0.94 countryWon 0.18 0.38 0.00 0.18 0.38 0.70 0.92 Total 0.25 0.41 0.07 0.29 0.73 0.50 0.90 Text Corpora. As for text, we experimented with two different sources: (i)Wiki which contains 5.5M Wikipedia articles, whose textual parts were split into sentences and indexed as separate documents using Elasticsearch [18 ] and (ii)Web constructed relying on the Bing API for searching in Web pages. Baselines. We compared against three baselines: • B-Wiki: a method that syntactically spots candidate facts and their paraphrases in the Wiki corpus. • B-Web: a method that retrieves fact occurrences in the Web using the Bing search API and post-filters the results to obtain the relevant text snippets (this baseline was extracted from [31 ]). • B-Search: a simulation for a user issuing verbalized versions of candidate facts to commercial search engine and retrieving the top-5 results. Configurations. To analyze the influence of the various sources, we ran ExFaKT with the following configurations: • KG: Rules are used over the KG facts only. • Wiki: Rules are used only with syntactic fact spotting over the Wiki corpus, i.e., no KG facts are exploited. • Web: Rules are used only with fact spotting over the Web, i.e., no KG facts are used. • KG+Wiki: Rules used together with both Wiki corpus and KG. • KG+Web: Both Web corpus and KG are used with the rules. In all of the experiments, we set the parameter max_depth to 5 unless otherwise stated. 4.2 Explanations Coverage Experimental Details. In this experiment, we show the effective- ness of ExFaKT in retrieving more explanations for candidate facts. We used the YAGO-based dataset and compared all five configura- tions of our method against the baselines. We computed the recall of each configuration as the ratio of the queries for which at least one explanation was retrieved. Results. Table 1 reports the recall for each predicate in the dataset, and the recall of the whole dataset as the total. These results show that ExFaKT configured with KG+Wiki or KG+Web almost doubled the recall of the baselines B-Wiki and B-Web respectively, whereas rules on the KG alone (KG) have the worst recall. Similarly, config- urations over text corpora alone, namely Wiki or Web could not compensate the absence of the KG. Observe that since mayorOf is prominent enough to be easily spotted in the text, there is no increase in the recall for this predicate. In contrast, our method is particularly successful for countryWon, WSDM’19, February 11-15th, 2019, Melbourne, Australia M. H. Gad-Elrab et al. Table 2: Examples of explanations produced by ExFaKT Fact candidates Explanations countryWon(guatemala, nobel) isCitizenOf (miguel_asturias, guatemala), source = TEXT, text=Asturias is a Nobel Prize-winning Guatemalan poet. . . ” hasWonPrize(miguel_asturias, nobel), source = KG influences(s_fitzgerald, r_yates) wrote(s_fitzgerald, the_great_gatsby), source = KG read(r_yates, the_great_gatsby), source = TEXT, text=Yates, called The Great Gatsby the most nourishing novel he read.” since the baselines fail to spot facts over this newly created pred- icate. Overall, the results demonstrate that while neither the KG nor textual sources alone are sufficient to collect strong evidences, their combination doubles the recall, and is doubtlessly the best configuration. Moreover, comparing KG+Wiki with KG+Web shows that increasing the size of the textual corpus enhances the results. Examples. As anecdotal evidence, Table 2 shows two examples of candidate facts for which our approach managed to compute supporting explanations even though fact spotting failed to find direct mentions for them. The first fact to be spotted is about the country Guatemala and the predicate country has won prize. Our method was able to find positive evidence for Guatemala winning the Nobel prize by spotting the Nobel laureate Miguel Asturias, and combining this information with the fact that he is a citizen of Guatemala. In the second example, we were able to extract an evidence about the influence of a writer on another one by spotting the fact that the latter read books written by the former. Note that here the relation read is not present in the KG but mentioned in the Wikipedia text. 4.3 Explanations Quality and Usefulness In this experiment, we evaluate the quality of the retrieved explana- tions by estimating the precision of the results and their readability based on human judgment. Experimental Details. We designed a Mechanical Turk [ 6] task to collect human judgments on the quality of the computed explana- tions. To facilitate readability, we translated our task into a natural language question. The participants were shown a candidate fact and some extracted explanation (i.e., in a human readable format), and their task was to judge the correctness of the fact relying only on the provided information. Since it may be hard for non-experts to give a solid judgment, the participants were asked to choose one out of five answers: (i) For sure, yes, (ii) Probably, yes, (iii) Proba- bly, no, (iv) For sure, no, or (v) Can not judge. As a confirmation, they were also asked for their explicit feedback on the usefulness of the provided information. Participants were provided with a set of instructions and clarification examples covering all possible cases. Each record was assigned to 5 different participants. We evaluated the top-5 explanations produced by ExFaKT’s con- figurations KG+Wiki and KG+Web","Gad-Elrab, M. H., Stepanova, D., Urbani, J., & Weikum, G. (2019, January). Exfakt: A framework for explaining facts over knowledge graphs and text. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining (pp. 87-95)."
SSAFC_024,Tackling Fake News Detection by Interactively Learning Representations using Graph neural networks,"Easy access, variety of content, and fast widespread interactions are some of the reasons that have made social media increasingly popular in today’s society. However, this has also enabled the widespread propagation of fake news, text that is published with an intent to spread misinformation and sway beliefs. Detecting fake news is important to prevent misinformation and maintain a healthy society. While prior works have tackled this problem by building supervised learning systems, automatedly modeling the social media landscape that enables the spread of fake news is challenging. On the contrary, having humans fact check all news is not scalable. Thus, in this paper, we propose to approach this problem interactively, where human insight can be continually combined with an automated system, enabling better social media representation quality. Our experiments show performance improvements in this setting.",Computer Science,Article,2021,N,Y,7,"Over the last decade, an increasing number of peo- ple access news online (Amy Mitchell, 2016), of- ten using social networking platforms to engage, consume and propagate this content in their social circles. Social networks provide easy means to dis- tribute news and commentary, resulting in a sharp increase in the number of media outlets (Ribeiro et al., 2018), and a rapid spread of content. In par- ticular, false news stories tend to spread at lightning speeds, and due to the volume, cannot be checked manually. An alternative to fact-checking claims, which is arguably easier to scale, is to focus on their source, and ask who can you trust? Prior works have formulated this as a traditional classification problem using techniques such as feature-based SVM’s (Baly et al., 2018, 2020), and more recently Graph Neural Networks (GNNs) (Li and Goldwasser, 2019; Shu et al., 2019; Han et al., 2020; Nguyen et al., 2020), which create a better representation of social media interactions. Graphs often consist of nodes corresponding to news sources (associated with a discrete factuality level - high, low, or mixed), the articles they release, and their social context, corresponding to social media users engaging and sharing information in their networks. GNNs can utilize this information by using edge interactions to create node represen- tations contextualized by their graph neighbours. This leads to a stronger representation of the com- plex information landscape on social media that enables fake news to spread, allowing it to be better detected. For this reason, we adopt graphs as our automated framework (1). Despite the success of these works, fake news detection is still a challenging research problem and human performance is significantly higher than fully automated systems (Shaar et al., 2020). Clearly, having humans fact check every informa- tion source is not scalable. Thus, our goal in this pa- per is to explore a different form of interaction with humans, where they can provide advice (Mehta and Goldwasser, 2019) to the automated system. Advice corresponds to localized judgements (pro- vided through natural language) that help charac- terize the content and social interactions associated with sources. These judgements, associated with article and social media user nodes, are then prop- agated through the information graph using the GNN, allowing the system to take advantage of it to improve it’s representation. As advice is not providing source labels directly, which is a time- consuming process requiring a global view of the source’s interactions, it is scalable. For example, one challenging aspect of the prob- lem is that low-factuality (fake news”) sources may not always propagate false information (some of the articles they publish may be factual), and 47Figure 1: Information Graph capturing interactions between news sources, articles, and engaging users. Advice is added to the information graph by adding new nodes/edges(teal) based on the advice type (news spreader or relevant claims). Advice then provides in- formation that can useful to clear up the complex social space the graph is modeling. vice-versa (leading to model confusion). Human interaction, in the form of advice, can help clean up some of this uncertainty, by identifying claims containing egregious falsehoods. The model could then use this information and trust sources making these claims less. We refer to this form of advice, mapping a specific article to known falsehoods as relevant claim advice. In another case, referred to as news spreader advice, a human could inform the system that a user that is spreading a sources’ arti- cles frequently spreads lies, which would increase the likelihood that that source and any other source this user spreads articles from are fake. Fig 1 shows how both of these advice types can be seamlessly added to an information graph. In this work, we show that our protocol in which humans iteratively provide these types of advice by interacting with the model (even after it is trained) improves overall fake news detection performance. In summary, we formulate fake news source detec- tion as a reasoning problem over an information graph. We then suggest an interactive learning based approach for incorporating human knowl- edge as advice to clean up uncertain graph deci- sions, which allows us to better learn and reason on this graph. Finally, we perform experiments that demonstrate that this setup leads to performance improvements on fake news source detection. 2 Model 2.1 Graph Creation and Training We start with defining our social context informa- tion graph. It consists of sources (S), articles they publish (A), and Twitter users that interact with sources/articles (U ). Our goal is fake news source factuality classification. Each node in the graph is represented by a high dimensional feature vec- tor, (similar to prior work (Baly et al., 2018, 2020; Nguyen et al., 2020)) to provide knowledge to the model that can be utilized when learning the graph embedding. Source and user feature vectors are cre- ated by concatenating embeddings based on their Twitter profiles (SBERT + features, details in Ap- pendix A.2.1). Sources also can include YouTube profile embeddings. Articles are represented by the encoding text into a SBERT RoBERTa embedding. Our graph is formed by first adding all the sources as individual nodes. We then scrape and add up to 300 articles (ai) for each source, connect- ing each with an edge to the source that published it (e = {si, aj }). Next, we add social context to the graph via Twitter users that interact with sources. We add up to 5000 users that follow sources, and users that tweet links to any articles in the graph within a 3 month period of the article being pub- lished (e = {si, uj }, e = {ai, uj }). Users that follow/engage with sources are likely to be aligned with/propagating the view of the sources, and mod- eling this can be useful. Finally, in order to capture the social interactions between users in the graph, which is critical to capturing fake news propagation on social media, we scrape up to 5000 followers of each Twitter user and make an edge between a pair of existing users if one user follows another. In order to learn the information captured by our information graph, we train a GNN to learn an initial embedding, on top of which we will ap- ply the interactive protocols (Sec 2.2). As a node embedding function, we utilize Relational Graph Convolutional Networks (R-GCN) (Schlichtkrull et al., 2018) (they can handle the social media re- lationships well). We achieve meaningful repre- sentations and capture factuality of the different nodes in our graph by optimizing the Node Clas- sification (NC) objective of Fake News Detection. After obtaining the source representations os from the R-GCN, we pass them through the softmax ac- tivation function σ and then train using categorical cross-entropy loss: Lnc = − ∑C i=1 yilog(σ(os)) where the C classes for yi are either high, mixed, 48or low factuality, and s is the current source. 2.2 Advice Protocols We now describe the two advice protocols we uti- lize in this paper. As mentioned in the introduction, in this work, we define advice as a form of human provided judgement (typically provided through natural language) about intermediate relationships in the information graph, that cleans up the space of complex judgements made by the GNN, allowing us to better capture the challenging landscape on so- cial media that enables fake news to spread ( Fig 1). Advice is provided by humans interactively and continuously, so that the process is scalable (not many judgements are needed, and they can always be provided, even after the system is deployed). In this way, our advice protocols provide a mech- anism for humans to interact with the automated graph system. We use two forms of advice: 2.2.1 Relevant Claim Advice When a human provides relevant claim advice, they have some prior knowledge about a certain claim (or news statement), and are telling this information (the claim and what their belief about its’ factuality is) to the model . For example, a human may know that a certain claim is not factual (perhaps many users on social media spread it and thus the human has seen it before). The human would then pro- vide this claim and a message about its factuality through natural language. Once a human has provided advice in the form of a claim that may be relevant, the model must de- cide which articles (if any) the claim is relevant for. Once it does so, it can add a new node in the graph for the claim (represented similar to the article text node with SBERT RoBERTa embedding), and con- nect it to the relevant article(s), allowing the advice knowledge to easily propagate through the graph (either by re-training the GNN or using the trained GNN to embed the advice node appropiately → we evaluate both setups in Sec 3). This automated setup allows for minimal effort needed from the human, making the advice simple to provide. To do this, first, the model filters a subset of sources (a process which we call filtering), whose articles could be candidates to receive advice. As mentioned earlier, advice cleans up complexities in the information graph, so these sources are ones which the model predicts the label of with low con- fidence (we rank Softmax scores for this). Then, for each filtered source’s article, the model decides if the claim provided by the human is relevant, by analyzing content in two ways. (1) First, a heuris- tic is used to determine if the advice and article are talking about the same event. To do this, the model extracts the entities from the advice claim and the article (we use the FLAIR tagger (Akbik et al., 2019)), and determines if any of them over- lap. If they do, the model also checks the date the advice claim was made, and makes sure it is within a one week period of the article being published. (2) Then, to further check content relevance, we use an entailment model (Parikh et al., 2016) and a sentence selection model (Nie et al., 2019) to check if any sentences from the article (chosen by the sen- tence selection model) entail the advice claim. If they do, the chance that the two are talking about similar content is higher. If there is an entailment, the advice statement d node is connected to the arti- cle a with an edge. All advice is also connected to a special label node h, m, or l, representing ‘high’, ‘low’, or ‘mixed’ factuality, based on the advice label (which is provided by the human), so that the model can easily represent that information. In our interactive process, which we evaluate in Sec 3.2, a human can continuously provide relevant claims (through natural language) based on knowl- edge they posses as advice, and through the process described above, the model can determine which articles to use it for (thus connecting the advice in the graph). In this way, the human interacts with the system to clear up potential confusion about certain articles, which propagates via the graph through sources and users, to lead to better fake news detection performance. 2.2.2 News Spreader Advice When providing news spreader advice, the human informs the system that a certain user is a bad ac- tor, meaning that they frequently spread lies. This knowledge would increase the likelihood that arti- cles this user tweets, and other users they interact with, are also non-factual. The user is then con- nected via an edge to a special ‘low’ factuality node, signifying to the model the set of users that are deemed to not be trusted. 2.2.3 Simulating Advice In this preliminary work, we simulate the two pre- vious forms of human provided advice by collect- ing data from fact-checking websites (PolitiFact, Snopes, USA Today, The Washington Post) and Twitter (details in Appendix A.1). For relevant 49Model Performance Acc Macro F1 # of Advice M1 : Majority class 52.43 22.93 - M2 : Best Model from (Baly et al., 2020) 71.52 67.25 - M3 : Our replication of (Baly et al., 2020) 69.38 63.63 - M4 : Node classification (NC) 65.76 55.97 - M5 : Relevant Claim All Advice 69.02 61.89 29,673 M6 : Relevant Claim Advice Filtering 25% 68.09 60.28 17,305 M7 : Relevant Claim Advice Selection 70.54 62.61 4,106 M8 : Relevant Claim Advice Filtering + Selection 50% + 50% 68.56 60.80 4,106 M9 : Relevant Claim All Advice Match Label 76.36 70.89 8,677 M10 : News Spreader Only Bad 67.40 59.57 2,643 M11 : News Spreader Bad 50% 65.91 58.65 1,350 M12 : News Spreader Bad 50% + 50% 66.35 58.26 2,643 Table 1: Final results. claim advice, we scrape all claims fact-checked by these websites and their factuality scores, and use that. This simulates humans providing advice in the real world, as a claim and some factuality in- sight about it are given. For news spreader advice, we use the Twitter API to determine all users that have been suspended since we initially collected our dataset, and use them as our news spreaders. Twitter manually suspended most of these users af- ter the storming of the US capitol, so using this data allows us to accurately simulate human advice. Although in this work we did not explicitly ask users to provide advice based on our learned graph model, our approximation of human advice that we collected was provided by human experts, and is thus relatively close to real advice that a human could provide. Relevant claims advice is based on real news claims that experts have associated factu- ality labels with, and Twitter manually suspended the users we used for news spreader advice. 3 Experiments 3.1 Dataset and Collection To evaluate our model’s ability to predict the factu- ality of news medium, we used the Media Bias/Fact Check (MBFC) dataset (Baly et al., 2018, 2020) (859 sources, each labeled on a 3-point scale based on their factuality: low, mixed, and high). We pro- vide graph statistics in App. A. 3.2 Fake News Classification Table 1 shows our results. We average our mod- els on all 5 data splits released by (Baly et al., 2020), using 20% of the training set sources as a development set, and report results on accuracy and Macro F1-score for fake news source classifi- cation. We compare our advice protocol models to the baseline-graph based model trained only on node classification (NC - no advice provided, M4). For completeness, we included the results of the SOTA (Baly et al., 2020) (M2), as well as replication of their setup using the data we scraped (and their code). Our results are worse than their released performance, so we hypothesize that their data on our setup may lead to better overall performance. For relevant claim advice, we evaluate settings in which we provide all the advice we scraped (29,673 statements - M5), where we provide advice only to the bottom 25% of sources that our model is not confident on during train/dev/test time (M6, all advice that passes the event filter is used → at least one entity in the articles title matches the advice claim and the dates are within one week of each other), and where we provide advice to all sources and make sure articles pass the event + entailment + sentence selection criteria (M7 - full setup in Sec 2.2.1). In all these setups, the advice is provided on the best model in M4, and then pa- rameters are reset and the model is re-trained to learn how to incorporate the advice. M8 is different and more interactive, as advice is first provided on the bottom 50% of confident sources based on the protocol in Sec 2.2.1, then the model is re-trained. Then, the rest of the advice is provided as in M7, except this time the model isn’t retrained. This simulates advice being continuously provided in- teractively by the human in the real world, and performance still improves. In this setting, as no re-training of the model is necessary, advice can be 50quickly utilized. All setups improve performance from the baseline, and using the filtering + sen- tence selection approach (M7) leads to the best per- formance, showing that the content of the advice matching the article matters. Thus, in the future when humans provide advice that is more likely to match the content of the articles, it is likely that we will see further performance improvements. Fur- ther, it is likely that less advice will need to be provided to see improvements. For completeness, in M9 we also evaluate an up- per bound, where advice provided by the human would be 100% accurate, i.e. the human would only provide advice that matched the article label (article label based on the label of the source). Finally, we evaluate news spreader advice, first when all news spreaders are told to the model (M10), then when only 50% are (M11), and finally when 50% are told, the model is retrained, and then the rest are provided (M12, simulating true interaction). In all advice models, performance improves from the NC baseline, showing that these types of advice can be helpful to the model. Furthermore, once the advice is provided, we can add more (M8, M12), and still see performance improvements with- out having to retrain the model, demonstrating a true interactive scenario, where a human can contin- uously be interacting with an automated system. In addition, providing advice as a localized judgement is simple and easier than labelling an entire source, so large amounts of advice can be collected from different experts to improve results. In the future, when we experiment with humans providing advice that is more content relevant (not simulating), the amount of advice needed could also decrease. 3.3 How Does Advice Help? In this section, we analyze a few specific cases of how relevant claim advice is used by the model to improve performance. In one case, an article from a news source labeled as spreading fake news was discussing how a Democratic leader would become Vice President if the President was impeached. Our model incorrectly predicted the factuality of this source. However, an advice claim from Snopes stating that the 25th amendment would not lead to this Democratic candidate immediately becom- ing Vice President was able to push the prediction of the source in the appropriate direction. In an- other case, advice that a specific former President was the first to speak against the current President was provided through PolitiFact with a False label, pushing a different source towards the fake news label. 4 Summary and Future Work In this paper, we proposed an approach to tackle fake news detection interactively by designing a protocol for a graph based system to continuously solicit human advice, and take advantage of it to improve overall information quality, which en- ables better fake news detection performance. We showed the benefits of two forms of advice (rele- vant claims and news spreaders), provided either all at once or continuously. In the future, we plan to have humans actually provide this advice, and explore other advice types. 5 Acknowledgments We thank the anonymous reviewers of this paper for all of their vital feedback. This works was partially supported by an NSF CAREER award IIS- 2048001. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the NSF. 6 Ethics Statement To the best of our knowledge no code of ethics was violated throughout the experiments done in this paper. We reported all hyper-parameters and other technical details necessary to reproduce our results. For space constraint we moved some of the technical details to the Appendix section which is submitted with this manuscript. The results we reported supports our claims in this paper and we believe it is reproducible. Any qualitative result we report is an outcome from a machine learning model that does not represent the authors’ personal views. Any results that we discussed on the data we used did not include account information and all results are anonymous. We anonymized the Twitter, article, and advice (Politifact, Snopes, USA Today, The Washington Post) data we collected to respect the privacy policy of the various websites and user data. While our overall approach does rely on user insights, each advice statement provided does not directly affect the final prediction, so a system receiving advice for fake news detection can not be easily manipulated.","Mehta, N., & Goldwasser, D. (2021, August). Tackling Fake News Detection by Interactively Learning Representations using Graph Neural Networks. In Proceedings of the First Workshop on Interactive Learning for Natural Language Processing (pp. 46-53)."
SSAFC_028,"TruthBot: An Automated Conversational Tool for Intent Learning, Curated Information Presenting, and Fake News Alerting","We present TruthBot, an all-in-one multilingual conversational chatbot designed for seeking truth (trustworthy and verified information) on specific topics. It helps users to obtain information specific to certain topics, fact-check information, and get recent news. The chatbot learns the intent of a query by training a deep neural network from the data of the previous intents and responds appropriately when it classifies the intent in one of the classes above. Each class is implemented as a separate module that uses either its own curated knowledge-base or searches the web to obtain the correct information. The topic of the chatbot is currently set to COVID-19. However, the bot can be easily customized to any topic-specific responses. Our experimental results show that each module performs significantly better than its closest competitor, which is verified both quantitatively and through several user-based surveys in multiple languages. TruthBot has been deployed in June 2020 and is currently running.",Computer Science,Article,2021,N,Y,28,"Social media has endowed us with a massive volume of information, which has both useful, not-so-useful, and misleading content (Vraga and Tully, 2019; Shu et al., 2017). This is worrisome since a usual netizen’s social media time is significant. According to eMarketer1, US adults use social media for an average of 12 hours per day. During the crisis situations like a global pandemic, people desperately look for solutions and end up consuming unverified information from various online resources including social media. It is observed that the users often trust the messages they see on social media and instant messengers. A study by Bridgman et al. (2020) shows that those who receive most of the news from social media are more likely to believe falsehoods about COVID-19. It is also observed that the social media has taken over the role of a news platform and now ranks just behind television (Kohut et al., 2010) which may mask true information for certain users. To help such users reach to the correct information, technology needs to be developed within the social media and instant messengers itself. On the other hand, popularity of chatbots among the Internet users has increased signif- icantly in the past few years2 (GrandViewResearch, 2017). Studies on US-based users have shown that 86% of them prefer chatting with a chatbot than a human agent (Forbes.com, 2019). A study by Brandtzaeg and Følstad (2017) shows that the major motivations for using chatbots are their timely and efficient assistance for information which improves pro- ductivity. Hence, the prospect of a chatbot to truth-seek information is bright, and serves as the motivation of this paper. 1.0.1 State-of-the-art and limitations. The current fact-checking apps require the users to enter the suspicious messages in those apps to fact-check or ask the users to read the current fact-checking articles to find out if their answers lie within the articles. Such apps are not very useful, since suspicious messages typically come via social media platforms or instant messengers and the cost of switching apps or read verified messages to find out the truth is quite high. The other kind of solutions that involve forwarding service (e.g., to WhatsApp business accounts) for fact-checking are typically manually verified and responded by a team of journalists. The chatbot approach for fact-checking is very limited as we discuss in Section 2. However, those approaches also do not (a) consider a complete truth-seeking design that provides holistic information on a topic, rather take care of certain frequently asked questions on a topic (e.g., COVID-19), and (b) handle low-resource languages. Note that we consider the term truth-seek to be more general than fact-check. While fact-checking tries to classify a piece of news to be true or not, truth-seeking provides a complete information against a query. For instance, the query can be a general question about a topic or a news article about which a user is only partially aware or an area- based infection statistics of a disease. The scope of truth-seeking, therefore, subsumes fact-checking as we discuss in the following section. 1.1 Proposed Solution: TruthBot. We introduce TruthBot, a truth-seeking chatbot that (a) provides answers to topic-specific frequently asked questions (FAQ), (b) returns articles after searching on the web, (c) re- sponses to custom queries (e.g., area-wise infection statistics for COVID-19), in addition to (d) fact-checking news articles. TruthBot uses a deep neural network to identify the intent of the user’s query. However, if the confidence of the identification is not significant, it consults with the user through a conversation to make sure that the right intent is conveyed to the chatbot. If the query topic falls in any of the four classes discussed above, it triggers the corresponding module and retrieves the answer. The modules have dependencies, e.g., if the FAQ module cannot respond to the query, it automatically searches for the query in the fact-check or Google search modules. The details are in Section 3. 1.2 Evaluation. We evaluate TruthBot in three aspects: B We measure the response accuracy by finding the relevance of the content of the response with the content of the original query. Query: monkeys are being tortured in laboratories to find COVID-19 vaccine IFCN chatbot Response: (1) Claim rated False: A vaccine for coronavirus was available since 2001. Fact checked by Newsmeter.in on 04/26/2020 This is a dog vaccine wrongly claimed as being a vaccine for COVID-19. (provides the link) (2) Claim rated False: A WhatsApp forward claims that the COVID-19 vaccine is ready, was announced by U.S. President Donald Trump, and will be launched next Sunday ... Fact checked by Newschecker on 03/23/2020 (provides the link) TruthBot Response: (1) (provides link to news which reads) Fact check: This disturbing viral video of animal cruelty is not of COVID-19 vaccine test A disturbing video purportedly showing animal cruelty circulating on social media claims: this is how monkeys are being tortured in laboratories to test the COVID-19 vaccine. Viral video showing animal torture is from last year reportedly from a German laboratory ... (2) (provides link to news which reads) Ferrets are among the animals researchers are infecting with the Covid-19 virus as they search for a suitable species to use in tests ... Table 1: An example query and response obtained from two chatbots – IFCN (a baseline) and TruthBot (our method). B We conduct a survey on the satisfaction of the chatbot regarding the topic it handles (COVID-19 in our example). B We also conduct another survey with a different population on the user interface about the usefulness, ease of use, credibility, precision, and value of the chatbot. Table 1 shows a typical example of the responses returned by the international fact-checking network (IFCN) chatbot on WhatsApp3 vis-a-vis TruthBot, where the latter is much more precise against the query. 1.3 Our contributions. The contributions of this paper can be summarized as follows: B Unlike other fact-checking chatbots, TruthBot is a truth-seeking bot, which performs four tasks (Section 3.3 to Section 3.6) via a conversational AI engine (Section 3.2) which identifies the intent of a query and guides the user through a piece of curated information or gets the intent clarified from the user. B The chatbot identifies the query language and responds in that language (Section 3.7). This is supported for the 108 languages that Google translate is capable of handling (we use the Google translate service for this task), which is a major advantage of using TruthBot, particularly for low-resource languages. B Experiments (Section 4) show that the response accuracy for all types of queries is much better than that of the IFCN chatbot, which also does a query-based fact- checking and is the closest in its design to our chatbot. B TruthBot has been operational since June 2020 and, it is currently accepting queries related to COVID-19. 1.4 Reproducibility We provide all data, code, and survey results separately. 2 Related work We discuss the related studies briefly in two aspects – fake news detection and task-specific chatbots. Detecting malicious content on social media such as fake news, misinformation, disin- formation, rumors, cyberbullying has been one of the primary research agenda in natural language processing and social computing. Due to the abundance of literature, we would like the readers to point to two recent survey articles by Zhou and Zafarani (2018) and Bondielli and Marcelloni (2019). Existing approaches can be broadly divided into four cat- egories – (i) knowledge-based, which verifies whether the given news is consistent with a knowledge base (true knowledge) Pan et al. (2018); (ii) style-based, which employs various stylometry analysis modules to check the similarity of the writing style of a given news with existing fake news Zhou et al. (2019); (iii) propagation-based, which explores the propagation of a given news on social media to check how it correlates with the usual fake information diffusion Bian et al. (2020); and (iv) source-based, which examines the trustworthiness of the source (the account) from which the news has started propagating Zhou and Zafarani (2019). Another direction of research relevant to the current study deals with the use of chatbots as an assistant for different applications (Lin et al., 2020; Radziwill and Benton, 2017; Følstad et al., 2018). Though chatbots originated in the 1960s, its use as an AI assistant and widespread acceptance is a recent phenomenon (Shum et al., 2018). The use of chatbots for fake news detection is quite a recent development. The approach closest to this paper is by the international fact-checking network (IFCN) (IFCN, 2020), that has tied up with WhatsApp to develop a fact-checking chatbot on that platform for people to fact-check news articles. In this paper, we provide case-studies and a comparison of outputs of various queries of our chatbot vis-a-vis the IFCN chatbot. TruthBot is quite different from all chatbot approaches to true information retrieval, since it is not limited to fact-checking alone, rather provides a complete information on a topic in multiple languages. 3 Architecture of TruthBot The objective of TruthBot is to bring all relevant truth-seeking activities on a topic within the scope of a single chatbot. Users can interact with TruthBot to get (a) general infor- mation regarding the topic, (b) fact-check potentially fake news, (c) general information on search queries from Google search, and (d) response from other custom information mod- ules. The current setup of TruthBot is tuned to provide all such information regarding Figure 1: Schematic workflow of TruthBot. COVID-19 pandemic, with the custom information module being the infection and death statistics of a city, district, state, or country. However, it is perfectly general to be tuned to any other topic with a little customization. We present the basic architecture of TruthBot in this section. Fig. 1 shows the graphical outline of the working units of TruthBot. 3.1 User queries The beta-version of the bot has been operational since June, 2020, and has attracted 696 unique users (who either have directly used or are informed about the bot till August, 2020) on the three platforms – WhatsApp, Facebook Messenger, and Telegram. For the analyses presented in this paper, we used about 1300 queries which were received till July, 2020.4 We manually classified these queries into six classes – frequently asked questions (FAQ) on a specialized topic (COVID-19 in our case), fake information verification (FAKE), general search for a news (GEN), some custom information (area-wise infection, death statistics– AREASTAT–in our case of COVID-19), greeting, and spam. Some of the queries, e.g., “warm saline water gargling can cure coronavirus”, were classified into multiple classes, as this example can be classified as FAQ, FAKE, and GEN. This manual labeling task took about 30 person-hours since one needed to read every query and use human justification to classify them. This process generated the training set (the JSON file with these classification is available in the supplementary material) which we used to classify new queries. 3.2 Conversational AI approach TruthBot is designed to help users check the truth of a news or a message. The focus is to provide the least cognitive load to the user and understand the intent of her queries before proceeding on to searching the information. This is important since the trajectory of the 4Collectively we got 2270 different queries sent to our bot till September 9, 2020. The performance of the bot will improve as this training set becomes richer. Figure 2: Deep neural network for query classification (number of nodes is representative). search for the different tasks TruthBot is capable of doing is quite different. The query is tokenized and stemmed into root words to create a bag of words. The collection of such words and the query classes are used to train the Query Classification Unit (QCU). This unit classifies the intents of the queries into the six classes discussed in Section 3.1 using a deep neural network (DNN). Any new query is soft-classified into these classes using the softmax scores. If the scores are significant—the threshold has been tuned based on manual inspection of the classification—then the action corresponding to that class is invoked (e.g., it calls the module Fact Check if it is classified as a fake information verification query). If the scores of classification are not significant, then it returns to the user and provides a selection choice of the intent to make a focused truth-search of the query. A couple of examples of how the conversation unfolds in this method are provided in Fig. 3. Fig. 2 shows the schematic representation of QCU. The performance metrics of this classification for different classes are shown in Table 2. Metric (Shu et al., 2017) FAQ FAKE GEN Precision 0.44 0.22 0.58 Recall 0.61 0.08 0.64 F1 score 0.51 0.12 0.61 Accuracy 0.67 0.602 0.68 Table 2: Performance metrics of the DNN classification. 3.3 Topic-specific frequently asked questions (FAQs) The purpose of this module is to answer some standard questions users may have on the topic TruthBot is tuned to. Since standard answers to these FAQs are already available, it is possible to create a knowledge-base from which these questions can be directly answered and reduce the latency of the responses. In the following subsection, we discuss the case study of the FAQs on COVID-19 that this bot is currently tuned to. Figure 3: Examples of the conversation with the user when TruthBot is confident (left) and not completely confident (right). 3.3.1 Case study: COVID-19. For COVID-19, websites like WHO, several government health departments (e.g., the Indian Ministry of Health and Family Welfare provide a large collection of FAQs and myth-busters. We first create a knowledge-base by web-scraping those articles from these websites. Fig. 4 shows the flowchart of this module. The Text Scraping Unit (TSU) consists of several Python libraries. The TSU downloads the HTML content of the target webpage (e.g., the FAQ page of CDC5) by sending an HTTP request to the associated URL and uses it to construct a parse tree using html.parser. Figure 4: Architecture of the FAQ layer. The useful FAQs and the paragraphs answering each FAQ are extracted from the parse tree and stored in the local knowledge-base. To augment the local knowledge-base, we also employ automatic question generation from the answer texts. This is done such that a slightly rephrased question gets a better similarity with one of the questions in our knowledge-base. The answer, however, remains same for all the questions generated from that text. The most straightforward means for this task is answer-aware question generation Lopez et al. (2020). In answer-aware question generation, the model is presented with the answer and a passage to generate a question for that answer considering the passage as the context. As the answer-aware models require answers for generating questions, we first split the passage into sentences and select a subset based on named-entity recognition and noun-phrase extraction. We have used a text-to-text transformer model Raffel et al. (2019) for question generation. The model is fine-tuned in a multitask way using task prefixes with the help of SQuADv1 dataset Rajpurkar et al. (2016) along with pre-trained transformers (specifically seq-2-seq models) on paragraphs that provide answers to COVID-19 related FAQs extracted from online databases. When a user sends the query, we need to find the appropriate question that matches this query. We use BERT Devlin et al. (2018) fine-tuned on CORD-19 dataset6 to generate the contextual embedding for each question sentence in the corpus. Then, we use cosine similarity to match a query with one of the questions in the local knowledge-base. A cosine similarity score of 0.85 is used as an empirically determined threshold for the correctness of the matchings. If a query scores above this threshold, we return the answer corresponding to the matched question in the corpus. Otherwise, the query is transferred to Fact Check module. 3.4 Custom information query Several topic-specific custom modules can be added to TruthBot. The purpose of these modules varies with the kind of topic we tune the chatbot to. As an example, we explain one such custom information module for COVID-19 below. 3.4.1 Case study: COVID-19. Currently, TruthBot is running a custom information module, called AREASTAT. The goal of this module is to provide the user with the infection, death, recovery statistics of a city, state, or country. Currently, the bot responds to the finer AREASTAT queries on Indian states and cities. For other countries, it provides country-level aggregate information. However, this can be easily extended with an appropriate database providing the finer information for the cities and states of other countries. After the intent of the query is classified as AREASTAT, it queries the databases with the name of the city, state, or country extracted from the query. The following workflow is repeated at most twice until the result is found. B The name of the place is extracted from the query. B The place is looked up in three databases – state wise, district wise, and country wise in order. Figure 5: Custom information query (AREASTAT) layer. B If a match is found in the database, the results of the number of COVID-19 cases, partitioned into confirmed, recovered, active, and deceased are sent to the user. B If no match is found in the database, the user is sent a message asking to enter only the name of the place to ensure that there is no spelling mistake. 3.4.2 Verifying if the query is about AREASTAT. The AREASTAT queries follow a pattern, e.g., “what are the number of cases in 〈place name〉”, “number of COVID-19 cases in 〈place name〉”, “how many people are infected with covid in 〈place name〉” etc. The query is first processed for the common phrases such as “number of cases” or “how many” followed by the place name. However, since the database for area-wise statistics available to us is only for COVID-19, we need to distinguish similar queries that ask the statistics for a different topic, e.g., the death in a cyclone. We first identify if the query has any COVID-19 related keywords along with the phrases discussed above. Once the query is confirmed to be a COVID-19 statistics query, we detect the place name using the usual patterns of such AREASTAT questions. There are two ways in which the statistics query may not be responded to: (a) if the statistics asked is not related to COVID-19, or (b) if the place name is not available in the database. In the former case, the query is transferred to the GEN module for a Google search of the query. In the latter, the user is responded with a message asking to re-enter the place correctly. Statistics databases. The covid19india API7 crowdsources the latest number of COVID-19 cases from various sources pertaining to each state and city of India. For the countries other than India, the data is scraped from the Worldometers website8 at the time when the query is received. Figure 6: Architecture of Fact Check module. 3.5 Fake news alerting Several responsible news agencies bust fake news and post such analysis on their dedicated fact-check section of their websites. If a query is classified as FAKE, it is processed through the Fact Check module that uses ‘Google Fact Check Claim Search API’ to search for the already fact-checked claims by sending an HTTP request. A typical url for calling the API consists of the query, language code, page size and an API key. An API call can return with a nested dictionary (denoted by FCR) of multiple objects corresponding to the number of fact-checked claims available on different fact-checker websites (e.g., Alt News9). For each of the retrieved results, we store the TEXT, TEXTUAL RATING, and URL attributes. For each result FCRi, we assign a relevance score rsi that computes the semantically matching similarity (based on cosine similarity) between the contextual embedding of the TEXT attribute of FCRi and the contextual embedding of the query. The results in FCR sorted based on their relevance scores and the top-k (k ≤ 3) results are displayed to the user if their relevance scores exceed a predefined (empirically determined) cutoff. If the query language is a language other than English, the results are displayed after translating to the original language using the Language Processor module. If the API call does not return any result or the relevance scores of all the returned results are below the cutoff, the query is forwarded to the google-search module. A regional language query is forwarded to the Google search module in its original language. The flow-chart for the Fact Check module is shown in Fig. 6. 3.6 Google search articles When a query is classified as GEN or the FAQ and Fact Check module does not produce any response to a query in FAQ or FAKE category respectively, the query is forwarded to the Google Search module that retrieves results by performing Google search using the google 10 Figure 7: Consolidated architecture of Google search and summarizer layers. search library. The query is fed to the search() method of Google search library and the top two urls (already sorted based on their relevance to the query) are retrieved. For each of the two URLs obtained in the previous step, we scrape the webpage so that we can perform summarization using that scraped data. After scraping the url, the content is summarized using TextRank Mihalcea and Tarau (2004), a graph-based summarization algorithm. To compute the edge weights of the graph used in TextRank, we use three different similarity metrics 1. Cosine similarity between sentences using TF-IDF model, 2. Longest common substring between any two sentences, 3. BM25 / Okapi-BM25 (Robertson and Zaragoza, 2009), a bag-of-words ranking func- tion used for information retrieval tasks. The obtained summary and the url are displayed to the user. If the original query is in a low-resource language and the scraped data of the webpage is in the same language, no translation is required. However, if the obtained webpages are in English, then the language processor (LP) module (see Fig. 1) translates the summary into the language of the original query before displaying it to the user. 3.7 Extension to low-resource languages One distinguishing factor of TruthBot is that it is capable of responding in multiple lan- guages. This is particularly helpful for the low-resource language users since the social media and instant messengers have significant user-base in these languages but technologies for various applications (including true information finding applications) are not well-developed for them. We use the language detection and translation service of Google translate10 in the QPU and language processor modules in Fig. 1. In QPU, if the detected language is different from English, then that information is stored, the query is translated into English 10https://translate.google.com 0.02 l 0.055 l l 7.4e−06 FAKE FAQ GEN TruthBot IFCN Bot TruthBot IFCN Bot TruthBot IFCN Bot 0.00 0.25 0.50 0.75 1.00 Response AccuracyFigure 9: Comparison of Response Accuracy for FAKE, FAQ, and GEN queries. The numbers on the square brackets on the top of the plots denote the p-values of the t-test for statistical significance. 4.1 Evaluation dataset To evaluate TruthBot, we curated a dataset consisting of 79 queries. The queries be- longed to four categories - FAQ, FAKE, GEN, and AREASTAT. The FAQ queries were manually curated from WHO and CDC which provide general information on COVID- 19. For FAKE and GEN queries, we collected most recent (as of August 10, 2020) fake information and true news articles on COVID-19 from different media outlets (e.g., Single sentence queries that conveyed the main message were curated from the collected articles. We curated the AREASTAT queries manually. Supplementary materials accompany the list of queries and the responses of the two chat- bots. In the quantitative comparison of the response accuracy, we used the first three categories, since the textual similarity between AREASTAT queries and responses is not informative about accuracy. 4.2 Evaluating response accuracy First, we wanted to evaluate the accuracy of TruthBot in retrieving the correct article available in the online or offline databases in response to the specific query. For this, we designed a metric called ‘response accuracy’, which measures the similarity of the retrieved response against the actual query. We used cosine similarity between the main text of the retrieved response and the actual query for computing response accuracy. Results of TruthBot were compared against that of the chatbot by IFCN. For IFCN chatbot, the sentence following the phrase “Claim rated false” or “Claim rated misleading” was selected as the main text of the response. For TruthBot, for FAKE queries, the sentence followed by the phrase “Claim” was selected as the main text, whereas for FAQ and GEN queries, the complete response was compared against that of the actual query. Fig. 9 compares the response accuracy of TruthBot against that of IFCN chatbot for different classes of queries. For FAKE queries, average response accuracy of TruthBot was 0.74 as compared to 0.56 for IFCN chatbot. Since IFCN is only meant for fake news alerting, the response p < 2.22e−16 p < 2.22e−16 l l l p < 2.22e−16 l l l l l llll p < 2.22e−16 Areastat FAKE FAQ GEN TruthBot IFCN Bot TruthBot IFCN Bot TruthBot IFCN Bot TruthBot IFCN Bot 1 2 3 4 5 User RatingFigure 10: Comparison of user satisfaction while using TruthBot and IFCN chatbot based for AREASTAT, FAKE, FAQ and GEN queries. The results are based on a user survey. The numbers on the square brackets on the top of the plots denote the p-values of the t-test for statistical significance. that it produces for FAQ and GEN queries may not be meaningful. However, since the response accuracy measures the textual similarity between the response and the query, it also indicates whether relevant articles were retrieved by the chatbot. For FAQ and GEN queries, TruthBot’s average response accuracy was 35.64% and 68.2% better than that of IFCN chatbot. 4.3 Chatbot satisfaction survey We conducted a human-centric evaluation to determine how satisfied a user would be with the responses received from TruthBot as compared to IFCN chatbot. The subjects were chosen through an open call sent via email and 64 users participated in this survey. The age-group of the participants were: 10-30 years (80.3%), 31-50 years (16.3%), and above 50 years (3.4%). Among them, 20.2% were female and 79.2% were male participants. Each user was assigned 10 queries (2 AREASTAT, 3 FAKE, 2 FAQ and 3 GEN) and the corresponding responses from TruthBot and IFCN chatbot. The queries were randomly chosen from the set of queries described in Section 4.1 so that we obtain multiple ratings (by multiple users) for a single query. Each user was instructed to rate the response of each bot based on how satisfied (1 for least and 5 for most) they were with the response obtained from a bot for each of their assigned queries. The result of the user survey is shown in Fig. 10. For each of the query classes, TruthBot achieved much better user rating as compared to IFCN chatbot. For AREASTAT, FAQ and GEN, median user rating of TruthBot was 5 as compared to the median rating of 2 for IFCN chatbot. Even for FAKE queries (for which IFCN chatbot is specialized), TruthBot achieved better rating (median rating 4 as compared to 3 for IFCN chatbot). 4.4 User interface study Finally, we also compared TruthBot against the IFCN bot based on the feedback from the users regarding different aspects of the user interface - Usefulness, Ease of Use, Credibility, Precision, and Value (interaction design.org, 2020). We obtained responses from 51 users Usefulness Credibility Value Ease of Use Precision 1 2 3 4 5 Scores TruthBot IFCN botFigure 11: Comparison of the GUIs of the two chatbots along different metrics. for this survey. In the survey, the users had to rate the bots for each of the above aspects. Possible ratings were from 1 (worst) to 5 (best). The result of the user survey is shown in Fig. 11. For each of the above aspects, TruthBot obtained better ratings compared to that of IFCN bot. Specifically, for Usefulness and Precision, mean rating for TruthBot was 2 times better than that IFCN bot on an average. The p-values of the t-test for statistical significance on all the aspects were < 0.00001. 5 Summary and Future Work We presented a multilingual and multipurpose chatbot that provides almost all aspects of information on a topic a user can be interested in. Such chatbots will be a single-point destination for users during situations such as COVID-19. Performance evaluation and user survey (as exemplified for COVID-19) also demonstrate the superiority of the bot over the existing options (IFCN chatbot) in providing multi-faceted well-searched information on a specific topic. The bot can be customized for other topics too. A primary goal of this paper is to introduce and exhibit the concept of such holistic information dissemination through a chatbot and provide evaluations to show its efficacy. The chatbot still depends on a number of external databases. While this may not be completely avoidable, but in our future work, we plan to make some of these information to be cached in our local databases to reduce the dependencies on such external factors, which helps both in the response latency and reliability. We also plan to expand the scope of the chatbot to image and multimedia messages, which are often sources of misinformation.","Gupta, A., Varun, Y., Das, P., Muttineni, N., Srivastava, P., Zafar, H., ... & Nath, S. (2021). TruthBot: An Automated Conversational Tool for Intent Learning, Curated Information Presenting, and Fake News Alerting. arXiv preprint arXiv:2102.00509."
SSAFC_038,Credibility Development with knowledge graphs,"Detection of misinformation online requires understanding both the sources and content of information. While a variety of supervised learning methods have been proposed for automated fact-checking with respect to the information content of media, the source is usually not taken into account. To address this gap in existing methods, we describe a novel framework for validating online content based on a knowledge graph of media content and an attribution graph of media sources. This approach enables decision makers to identify factual information and supports counter disinformation operations by tracing the spread of disinformation across reliable and unreliable outlets. We have found that tracking knowledge provenance is critical to assessing the credibility of that knowledge. In addition to building a knowledge graph of fact triples (subject, verb, object), we construct an attribution graph composed of links between all extracted facts and their sources on which we apply our main credibility reasoning mechanism, belief propagation. Analysis of credibility based on sources best captures reliable knowledge generation processes such as science, legal trials, and investigative reporting. In these domains there is a process for identifying experts and coming to consensus about the validity of claims to establish facts. Our method models these processes in news media by considering the relations between credible information and reliable sources.",Computer Science,Proceeding,2019,Y,N,0,"Reputable academic and journalistic institutions have traditionally operated under a peer-reviewed, expert distillation of credible editorial content. In con- trast, online media creation is as rampant and unpoliced as online media con- sumption, which is often accompanied by unreliable judgment of content credibil- ity by the individual consumer. Even good-intentioned and well-informed human purveyors of open online media are highly susceptible to cognitive biases that mislead their judgments about what content and sources are credible. Respon- sibility for verifying content is shared between readers, publishers, editors, and authors in online rriedia. Denying our collective responsif›i1ity to mitigate this problem is damaging because misinformation can lead to severe political, social, and econorriic consequences that cannot be ignored. Technical means to aid humans in content verification are essential to protecting the utility of online media. Verifying the integrity of inforrriation content and sources is challenging in the digital age due to the volurrie of data and variety of sources. We descrifie a unified framework of modeling human understanding of the world by distilling media content into a set of facts in a kllowledge graph, which is a graph that contains real-world entities as nodes and the relations between them as edges. As it applies to evelit uliderstanding, we view knowledge elelnents aS facts derived from media sources, where they demonstrate relations between two entities, such as France is part of the European Union”. The knowledge graph that we compose embeds two core types of knowledge: global and local. Global knowledge is composed of facts that are invariant with respect to time such as Democracy is a form of government” and forms the background knowledge necessary for understanding the world. Local knowledge is more situational and temporally localized such as The Foreign Minister held a press conference in Brussels”, and relates to specific knowledge about an event or set of events. We distinguish these two forms of knowledge, and acquire global knowledge from open source ontologies and knowledge bases. Local knowledge is extracted from online content such as online news articles, social media, and public statements. When humans assess the credibility of a new piece of information they natu- rally scrutinize the source of the information: Do I trust where this information came from?”. Along these lines, our source and fact credibility model quantifies the confidence associated with each fact and the reliability of each source'. When modeling the credibility of facts and sources, we evoke the intuition that facts are credible if corroborated by many credible sources, and sources are credible if they corroborate many credible facts. Prior work has demonstrated that a source centric analysis of credibility can be more effective than content based methods [3]. Our approach builds a network of facts and sources, called the ottrihution, Crept, and assigns a probabilistic hefie/ to each vertex in this graph. By propagating these beliefs we solve for a confidence in every source and every fact. Interpreting this solution provides a quantitative measure of credibility.  2  Literature Review The predominant approach to automated fact checking and credibility assess- ment in news media is that of supervised machine learning [10]. While methods in this area [7, 9, 11] contain unique variations, in general they frame the problem as a text classification problem, where features of specific text spans, including words, phrases, sentences, and entire documents, are extracted and utilized for ˡ We use this fact ts credt6Je as a synonym for we have confidence in this Jact., and a source zs ref(nfife if most of the facts it generates are credible.  Credibility Development with Knowledge Graphs   3ti  labeling facts or claims with a discrete category such as credible or not credi- ble. Another technique [5] iiames the problem as a textual similarity problem, where new text spans are compared with previously seen spans that are known to be credible or not credible. In practice, both groups of approaches are hin- dered from the same issues that face all supervised learning problems, namely the requirement of large labeled datasets that tlTO OX OIlSlve to acquire and main- tain. As applied to the specific problem of credibility assesslnent however, a key drawback of these approaches is that each text span is considered in an isolated context where application of backgroulid knowledge to the reasoning process is not involved. This is a critical limitation, because statements that are inher- ently false can be worded in a Inanner that appears credible [10]. Models trained on this task in this manner are able to compute a result over the shallow lin- guistic features that the words in the content provide, but do not consider the source from which the information originates. On the other hand, our approach described herein only requires prior credibilities assigned to source nodes in the attribution graph, bypassing the need for large quantities of labeled data. Fur- thermore, being a source-centric technique for credibility assessment, it is not hindered by ambiguity that arises from sole consideration of linguistic features in the content.  3  Building and Extending the Knowledge Graph Often in the effort to ascertain whether information is credible, a person will draw upon an extensive amount of background knowledge to start the assessment. In our framework, a knowledge graph therefore serves as a surrogate for human memory, recall, and reasoning. The knowledge graph is used to organize and store information as it becomes available in media. In order to model the credibility of both data and sources we use an attribution graph to compute credibility using belief propagation. The posterior beliefs represent credibility scores that quantifies the trustworthiness of the information relative to its source(s).  3.1  Global Knowledge Resources Humans analyze new information through the lens of personal experience but also against the backdrop of historical, cultural, and political situations, thus analysis requires inclusion of global background knowledge. We leverage Babel- Net [8], which is a multi-lingual encyclopedic dictionary and semantic network of entities, concepts, and relations. BabelNet integrates knowledge of concepts and entities from 47 distinct knowledge bases such as WordNet, Wikipedia, Wikidata, Wikitionary, and OmegaWiki. It contains nearly 6.1M concepts, 9.7M entities, and 1.3B lexico-semantic relations among them. Tens of millions of entries pos- sess various forms of multi-modal metadata as well such as images and phonetic details.  3ti   J.P. Fairbanks ct al.  3.2  Local Knowledge Extraction Online media content arrives in a variety of’ unstructured, multi-modal formats including text, image, video, and audio. Although our fact parsing methods operate on text directly, non-text elements within content could be extracted and represented as text prior to parsing and insertion lllto t)lO knowledge graph. While this extension is out-of-scope in our current efforts, we propose applying object recognition and irriage captioliing [12] to produce textual representations of images and videos, and speech to text models to address audio captures [2]. A particular challongo elicoulitered iIi extractilig knowledge from unstruc- tured text is that of word sense disambiguation, or resolving discrete text spans to their correct lexical Ineanings. Hi the absence of large labeled datasets to build supervised learning models from, we apply an approach that exploits relations between named entities in BabelNet to perform these mappings. An algorithm that performs entity linking with respect to this knowledge base is accessible through a service called Babelfy [6]. To in order to extend its capability, we augmented the original Babelfy algorithm by including a number of accuracy- improving heuristics, while preserving the core algorithm.  3.3  Knowledge Representation Global and local knowledge extractions are referred to as facts in the knowledge graph and are assimilated in the form of tuples. Representing information as facts in the knowledge graph enables reasoning over the knowledge that humans bring to their interpretation of a new piece of information from documents, imagery, or other media. Figure 1 shows a notional sample of the knowledge graph and how to interpret the information in it. Each vertex represents an entity and each edge represents a relationship between two of them. Edges in this graph are labeled with proba- bilities representing likelihood and confidence, along with a binary label of local or global. Consider the fact tuple ( Uoccine, prevents, Polio). The edge weight is a numerical value between [0, 1] that quantifies the likelihood that the local fact {Uocc?ne, presents, polio) is true. The edge colors denote the categorical labels of confidence in the likelihoods given by the edge weight, which conditions the belief a fact is true on the credibility of supporting evidence and the reputation of the source. An edge's line style indicates the type of knowledge to which the relation belongs. A dotted line denotes local knowledge that is susceptible to change such as (Polio, is, Durable), and a solid line denotes global knowledge such as (Polio, is a, Disease). The time the event occurred I and a decay rate p are also affixed to each fact edge. These attributes can be used to modulate the likelihood that the event is still true given any passage of time. For events that last a long time, as is the case of global knowledge, the decay rate p should be smaller. While temporal information is not used in our application of belief propagation, it is useful when manipulating the knowledge graph in an applied setting.  Credibility Development with Knowledge €iraphs   37            prevents   Vaccine    Edga Sources:  Likelihoods     Global  Disease   (”  ""’ t = 1952,p = let 8)  (w = 0.QS. I = l9%.p = ie — 4) Very Likely   w E [@, 1] Even Chance  w E (e, g) Unknown   •=:   is     z  y (w = 0.98, t = 1850, p = 0) Disability  Curabls High                                     Moderate   Low Unknown I e [0, +m) - time ersnt occurred p C [0, +‹x›) - fact decay rate  Fig. 1. Knowledge Graph representation and Edge Encoding, each vortex is an entity with facts represented as edges. Each edge has a likelihood (weight), confidence (color), type (dashed for local and solid for global), time of occurrence t, and decay rate p. Here o, I C (0, 1). (Color figure online)  4  Credibility Assessment Our approach for credibility reasoning is based on assessing information sources and facts relative to their sources. A separate representation called the attribu- tion graph is constructed between facts and source vertices. Belief Propagation is applied to infer the credibility of all facts and sources in this graph. Sources and their credibility scores are stored in a separate database for later analysis and retrieval while fact credibility scores are encoded as edge weight attributes in the knowledge graph, which we denote as confidences in Fig. 1. We summarize this credibility assessment process in the following steps: 1. Process New Data: Ingest multimedia documents and extract facts 2. Update Attribution Graph: Build the graph linking facts and their sources 3. Assess Source-Fact Pairs: Run Belief Propagation on the attribution graph 4. Update Sources Database: Store sources and credibility in separate database 5. Update Knowledge Graph: Assign fact credibility scores as edge attributes.  4.1  Building the Attribution Graph The attribution graph is bipartite with two types of vertices: sources and facts. Sources are retrieved from the database while the facts are derived from the knowledge graph. Specifically, each fact tuple (subject, verb, ob₃ec!) from the knowledge graph is encoded as a single fact vertex in the attribution graph. Edges are added connecting a fact vertex to each of the source vertices that provided that fact.  38   J.P. Fairbanks ct al.  Prior to running belief propagation, fact and source vertices are assigned a prior probability of being credible and is denoted as Q(z) where z is a random viiriable denoting the credibility score. There are rriany factors that influence the prior probability. In general, when assessing the trustworthiness of a piece of information, people consider factors such as how it was collected, its level of corripleteness, and its age in addition to the source. When the source is a digital media asset such as an image, documollt, video, or audio recording, then detailed information such as metadata and hyperlink information can be used to determine a prior estimate of credibility. Our method Cilli augment any existing credibility assessment technique by taking the output of that technique as the prior distributioli for belief propagation.  4.2  Calculating on Confidence We apply belief propagation (BP) on the attribution graph as the primary rea- soning mechanism for credibility for all observed sources and facts. Belief Prop- agation algorithms have been developed successfully for cybersecurity problems in computer networks to classify malware and detect infected machines [1]. The applied intuition here is that if a fact is corroborated by many credible sources, then it will be credible, and if a source corroborates many credible facts then it is reliable. Beliefs are adjusted for every vertex in the graph as based on the entire history of the sources and the facts they provide. The BP algorithm, illustrated in Fig. 2, requires that every source and fact vertex in the attribution graph is assigned a credibility value at the start, which is the prior probability of belonging to the target class label. More formally, this approach treats each fact or source vertex as a random variable z; C (0, 1) and the prior probability, denoted by ‹p(z,), represents the n pNori belief that vertex I belongs to class z;. Iterations of the belief propagation algorithm are interpreted as passing messages denoted by m•h(:s₃ ), which is the message from vertex i to vertex j about vertex ₁’s likelihood of being in class xJ . The message update equation is given in Eq. 1 below.    The function y,j (z„ x₃) is a hyper-parameter that determines the conditional probability that if a neighboring node I is of class zi, then its neighbor 1 will be of class x₃. Table 1 shows the 2 x 2 affinity matrix y. Smaller choices for c in assumes homophily of the labels. Upon convergence, the BP algorithm solves for the posterior beliefs of credi- bility for each vertex in the attribution graph. Beliefs are denoted as h(z;), and the belief update equation is given in Eq. 2, where Z is a normalizing constant:  (2)  Credibility Development with Knowledge Ciraphs   39  SourEe Vertex Confidence Label Indicator                 Low Fact Vertex Confidence Label Indicator  Fig. 2. Belief propagation on the attribution graph ta)ses prior credibility estimates (left) and the links between sources and facts to solve for posterior credibility estimates (right). Credibility scores are tied back to knowledge graph edges for those facts.  Tahle 1. Edge potentials between neighboring nodes   1—r r e  l—e   For fact vertices, the posterior belief informs the confidence level associated with that fact in the knowledge graph. For source vertices, the posterior belief represents the overall credibility of facts supplied by that source and is stored for later retrieval and analysis but is not used in the knowledge graph.  4.3   flelief Propagation on a Toy Example Figure 3 shows the attribution graph constructed on a synthetic dataset of 5 source nodes and 8 fact nodes with class labels color-coded according to the included legend. There are a total of 5 test nodes with credibility class labels stripped: ( 1 4 J3 J6 / ). Note, labeled nodes have a prior estimate of credibility equal to 0.9 if credible and 0.1 if not credible while unlabeled nodes are initialized with a uniform prior of 0.6 across both classes. In this simulation, eps = 0.1 for the edge potential function y;₃ (z', x₃) with a maximum number of iterations set to 10. Table 2 shows the resulting prior and posterior beliefs for a class label of credible, where a higher posterior indicates a stronger probability that the vertex is credible. The last column shows the ground truth class labels where 1 encodes credible and 0 encodes nolcredible.  4()    .I. P. Fairbanks ct al.   I      fS  s»      6 Class Labels        Fig. 3. Toy attribution graph demonstrating intuition of the belief propagation rithm upon convergence  5  Experiments Several experiments were designed in order to demonstrate theoretical and empirical performance of our proposed credibility assessment method. The first experiment creates synthetic data using graph generators to analyze the ability of belief propagation to correctly classify reliable sources. The second experi- ment demonstrates the performance of belief propagation to correctly classify the credibility of sources and facts using an actual news dataset and reports ROC curves using 3-fold cross-validation.  5.1  Belief Propagation Reliability on a Synthetic Dataset The simulated experiment generates synthetic graph data with known proper- ties and runs our algorithm on such data to measure accuracy. The simulation consists of a set of fact extractors that each generate edges from a known, dis- tinct distribution over (U x U), each having a tunable parameter. For exam- ple, the distribution over pairs of vertices is uniform, so that each extractor is an Erdos Renyi (ER) graph generator. These generators would then be joined with a Bernoulli distribution with probability p, to indicate the accuracy of the extractors. Each extractor would have a different probability of generating a false positive edge. Since our method cannot generate edges that the extractors failed to find, we are more concerned with eliminating spurious facts reported by the extractors than with creating edges the extractors failed to find. The goal  Credibility Development with Knowledge Graphs   41 Table 2. Prior grid posterior beliefs o1’ creclibility ve ground truth for toy example   Vertex si 82 ss s‹ st Jt J2 /n  /; /6 /i /s  Prior 0.500 0.010 0.900 0.TOO 0.010 t).9t)t) 0.010 0.ii00  b.b1b 6.566 0.010 0.500  Posterior 0.81fi 0.114 0.979 0.044 0.024 t).894 0.031 0.883  b.b34 6.115 0.024 0.135  Truth 1 0 1 0 0 1 0 1 1 b 0 0 0   of this simulation is to show that the attribution graph beliefs identify the least reliable extractors, ie. those with the highest probability P of emitting spurious facts. Figure 4 shows that the BP method is a reliable method of recovering the source reliability ranking. Here p, written rho on the charts, is defined as the Spearman rank correlation between the observed BP score of the information extractor and the ground truth reliability of that extractor. Figure 4 shows that as the problem gets larger the method is able to acquire a better estimate of the reliabilities of the extractors. This makes sense because in this example there is a fixed number of extractors (6) and the number of samples is growing. This additional information per sample enables more accurate estimates of the source reliabilities.  5.2  Credibility Assessment on the Fake News Challenge Dataset The Fake News Challenge (FNC) dataset is based on the emergent dataset [4] originally curated at Columbia's School of Journalism in order to address the task of rumor-debunking. Creators of the Fake News Challenge released this dataset for the classification task of stance detection, which they believe to be a critical milestone to mitigate the problem of fake news in media. The FNC dataset consists of 843 news articles and 9ñ8 headlines with a total of 4,518 headline-article pairs with a label of agree, disagTee, discuss, or unrelated. We re-purposed the FNC dataset in order to demonstrate credibility assessment of sources and facts using real news articles. Using this dataset, we report on two such experiments below and refer to them as the simplified model and the complex model.  42    .I. P. Fairbanks ct al.      0.6             0    500      1000     1500 number of edges 2000    Z500  Fig. 4. Simulation correlation of source reliability as a function of the sise of the graph in edges  Table 3. Publisher class metrics and reporting accuracy Publisher label  Class percentage  hlean acc Median acc Credible     0.545        0.891    0.900 Not credible   0.455        0.053    0.054 Experimental Dataset. The FNC dataset was re-purposed for our experi- ments by first filtering the set of article-headline pairs to only include pairs labeled with a stance of oqree or disagree. Next, we assigned groups of articles as being sourced from a single entity, notionally equivalent to a puhilsher. A total of 55 different publishers were created with an average of 13.163 articles per ptlbli8her, minimtlm of 6 and maximum of 22. In both experiments, each publisher entity represents a source vertex in the attribution graph. In the simplified model, each headline (ie claim) represents a fact vertex and an edge occurs between a source and a fact if the publisher (source) owns at least one article that agrees with the given headline (fact). Note that each headline has a ground truth label of credible or nolcredible and publishers may possess articles that agree with a headline that is not true. Therefore a priori estimates of publisher credibility are based on the accuracy of their reporting, that is an average of the number of claims they agreed with that were true. Table 3 shows publisher reporting accuracy metrics for each target class. Further, the distribu- tion of credible and not credible headlines is 0.ñ44 and 0.356, respectively. Note, this model does not rely on facts extracted from the articles but rather leverages the headlines as a proxy for facts present in the articles with the assumption  Credibility Development with Knowledge Graphs   43  that an article headline is a concise sumrriary of the central claim present in an article. This model demonstrates proof-of-concept for our proposed credibility assessment methodology and reports perforrriance on a simplified experiment with a predictable outcorrie. In the complex rriodel, a set of fact (s•Oect, rerh, ^)ect) tuples are extracted for each article and are represented as fact vertices in the attribution graph, in accordance with Sect. 3.1. In this model, an edge occurs between a source and a fact if the publisher owns the article that produced the extracted fact tuple. A priori estimates of publisher credibility are dervied as described above. A priori estimates of fact credibility were derived based on the given headline labels using a textual eiitailnient prediction model. Details of this process are described in more detail below.  Simplified Model: Belief Propagation Between Sources and Headlines. In the attribution graph, each publisher entity represents a source vertex and each headline (or claim) represents a fact vertex. An edge in the attribution graph occurs between a source and a fact if the publisher (source) owns at least one article that agrees with the given headline (fact). In this setup, the attribution graph has a total of 55 source nodes and 783 fact nodes, which constitutes a majority of the 958 available headlines from the original FNC dataset. In this experiment, 3-fold cross-validation was performed by randomly select- ing 1/’3 of source and fact vertices to be unlabeled (i.e. test vertices) with the remaining 2/3 of vertices retaining their credibility labels. Figure 5 shows the ROC curves for the test vertices in each of the 3 folds. Note, it is assumed that performance will degrade at the granular fact level that is used in the complex model. The next section reports on performance where the attribution graph is built between publishers (source vertices) and extracted facts from articles (fact vertices).  Complex Model: Belief Propagation Between Sources and Fhcts. The simplified model described above builds an attribution graph between publish- ers of articles and headlines, which act as a proxy for the facts contained in the articles. We view the simplified model's credibility assessment performance as a best-case” scenario as we expect the noise generated by the fact extraction process to degrade performance. In contrast, the complex model builds the attri- bution graph between publishers of articles and the facts extracted from those articles. The main challenges in creating an attribution graph between sources and extracted facts is knowing (1) how to resolve fact vertices (i.e. identify facts as the same vertex in the attribution graph if they support the same claim) and (2) what the a priori class labels are for each extracted fact pcredible or nolcredible). To construct the attribution graph, each article is processed through our in- house fact-extraction pipeline which leverages state-of-the-art NLP models from spaCy and AllenNLP. In order to determine a priori estimates of credibility for each extracted (cubject, verb,  O  !) fact tuple, we use a textual entailment  44   J.P. Fairbanks ct al.  Table 4. Extracted Fact arid Headlirie Eritailrrient Examples  Headline: Rare meteorite impact causes blast iri Nicaragua's capital, Managua” Fact: a irieteorite pluiriirietirig to Earth caused A blast riear the Nicaraguan capital rity ref Ma,na,gua, run fiaturr)a,y might' Predictions: [Entailirient: 0.957, Contradictiori: 0.002, Neutral: 0.041s  Headline: Texas Truck Wirids Up iri Syria With Islamic Militants” Fact: the truck had different owners since auction before ending up in the hands of Islamic terrorists” Predictions: [Entailment: 0.fi 19, Contradiction: 0.01G, Neutral: 0.3Go]  Headline: Dylan Thomas Finds Tropical Spider Burrowed Under Skin” Fact: Doctors retrieved a tropical spider from The 21 year old's abdomen” Predictions: [Entailment: 0.561, Contradiction: 0.097, Neutral: 0.342]  Headline: eBay is planning an Apple Watch app""’ Fact: At least one of the big boys planning on developing an app for the Apple Watch” Predictions: [Entailment: 0.954, Contradiction: 0.009, Neutral: 0.037]  Headline: Breaking: Soldier shot at National War Memorial in Ottawa” Fact: a gunman shot A uniformed soldier at the Canadian War Memorial on Wednesday morning' Predictions: [Entailment: 0.529, Contradiction: 0.007, Neutral: 0.464]   prediction model to generate predictions for each fact relative to their paired headlines. The model returns a probability distribution for each headline/fact pair with labels of entoifment, conlradiclion, and seutro/ that quantifies the amount of agreement held between a headline and a fact. For each headline, all fact pairs that generate an entailment prediction of at least 50% are resolved to the same fact vertex in the attribution graph and assumes the given headline credibility class label. The complex model is equally a demonstration of both (l) the reliability of our fact extraction generators and (2) credibility assessment performance using a real news dataset at the fact-granular level. Table 4 shows examples of extracted facts (here joined together as a single string for readability) and headline pairs that have an entailment prediction of greater than or equal tO 50%. Similar to the simplified model, 3-fold cross-validation is executed on this new attribution graph with a randomized 1 /3 of the vertices remaining unlabeled as test vertices. Figure 6 shows performance for each fold on the attribution graph composed between publishers (sources) and their extracted facts. The performance does decrease slightly, which was expected, but is still respectable.  Credibility Development with Knowledge €iraphs   4s  Receiver Operating Characteristic   0.8  O6   0.4   0.2  0.0 ^ Fold 0(area = 0.932)                   Fold 1(area = 0.958) x                    Fold 2(area = 0.932) 0.0      0.2      0.4      0.6      O8 False Positive Rate  Fig. 5. ROCcurve for simplified model  While this performance analyzes the extent to which extracted information is congruent with the main article claim represented by the headline, it does not provide a measure of the total quality of the facts that were extracted. Further experiments must be developed that assimilates into the attribution graph those extracted facts which are predicted to refute (contradiction) or are unrelated (neutral) to their paired headlines. With respect to the simplified model, the attribution graph constructed for the complex model captures 53 ’55 original publishers but only preserves 243/783 headline claims with the facts that are extracted. This reduction of publisher and  Receiver Operating Characteristic   0.8          02  0.0  Fold 0(area = 0.868)                   Fold 1(area - 0.872) Fold 2(area - 0.860)  0.0 O2   0.4      0.6 False Positive Rate 0.8 L.0 Fig. 6. ROC curve for the complex model  4ti   J.P. Fairbanks ct al.  headline representation in the attribution graph could f›e a consequence of either (1) a lack of complete (sv b)ec!, uer’#, obbec!} tllples extracted from articles and (2) extracted fact tuples frorri articles that do not capture the Inain claim of the headline as quantified by textual entailrrient. The first reason represents a challenge in reliable fact extraction and the second represents a challenge iii reli- ably scrutinizing the meaning of information without human intervention, froth of which are inherent in the domain of kllowledge represelltation and reasoning.  6  Conclusions We have presented a novel Inethod to rapidly ingest and judge the credibility of information and sources by building a knowledge graph and applying belief propagation on an attribution graph. Our method works at the granularity of individual facts extracted from online media and is capable of handling, in addi- tion to text, a variety of data formats including imagery, video, and audio with the addition of neural processing methods. Instead of merely passing through credible data and discarding the non-credible, our system quantifies measures of data integrity and assigns weights to each fact and source. As such, our method has the advantage of being transparent and interpretable to the user in that it incorporates human readable relationships between entities and events extracted directly from data sources. Future work should build on the experiments addressed herein to develop more holistic and larger datasets that includes both global and local knowledge sources on which to test the proposed method. In addition to questioning the source, we also aim to address another natural question that scrutinizes the content of information: °How does this new information relate to existing knowl- edge?”. Pairwise similarity measures in graphs are traditionally used to solve the link prediction task, which is to recommend new connection in a network. Therefore, models of structural support for facts based on the link prediction problem could address this question by characterizing the structural context in which a new fact would occur.","Fairbanks, J. P., Fitch, N., Bradfield, F., & Briscoe, E. (2019, February). Credibility Development with Knowledge Graphs. In Multidisciplinary International Symposium on Disinformation in Open Online Media (pp. 33-47). Springer, Cham."
SSAFC_041,The Fake News Challenge: Stance Detection using Traditional machine learning Approaches,"Fake news has caused sensation lately, and this term is the Collins Dictionary Word of the Year 2017. As the news are disseminated very fast in the era of social networks, an automated fact-checking tool becomes a requirement. However, a fully automated tool that judges a claim to be true or false is always limited in functionality, accuracy and understandability. Thus, an alternative suggestion is to collaborate a number of analysis tools in one platform which help human fact checkers and normal users produce better judging based on many aspects. A stance detection tool is a first stage of an online challenge that aims to detect fake news. The goal is to determine the relative perspective of a news article towards its title. In this paper, we tackle the challenge of stance detection by utilizing traditional machine learning algorithms along with problem specific feature engineering. Our results show that these models outperform the best outcomes of the participating solutions which mainly use deep learning models.",Computer Science,Proceeding,2018,Y,Y,0,"Fake news is one of the controversially discussed is- sues lately. New York Times defines it as ”a made- up story with an intention to deceive”1. Moreover, propaganda, conspiracy theories and other false sto- ries have always been used in the media for a second gain like monetizing, political goals and opinion ma- nipulation. Online services such as factcheck.org and PolitiFact.com perform manual fact checking to filter fake news. The current online environments like social media create powerful tools to spread false stories extensi- vely. As a result, journalists and fact checkers with their current strategies cannot label fake stories in real time before they are out of control. Automating those strategies is one solution to speed up the procedure. This kind of issues is considered to fit a machine lear- ning task (Markowitz and Hancock, 2014; Hardalov et al., 2016; Jin et al., 2017). Until lately, the work on fighting fake news is handled in many separate projects and studies. Howe- ver, organizations like FullFact.org suggests to open collaborations between these projects to build a plat- form that provides a collection of tools to handle the different aspects of fact checking routines2. Similarly Fake News Challenge (FNC-1), which is an on-line competition, also suggests a solution for fake news detection to be composed by a collection of automa- ted tools to support human fact checkers and speed up their processes. Stance detection is among the col- lection of these tools 3. Stance detection has been proven to be useful in disinformation detection. (Jin et al., 2016) applied the stance to analyze the credibility propagation for news verification through building connections bet- ween micro-blogs (tweets) as supporting or denying each others’ viewpoints. (Qazvinian et al., 2011) use the stance observed in tweets in a Belief Classifica- tion to classify false and true rumors, even though rumors checking is found to be different from news checking. Stance detection for fact checking in the emerging news has mostly been investigated in micro- blogs. The stance detection task presented by FNC-1 is about predicting the stance of one piece of text (news body) towards another (news headline). Particularly, it should predict whether the news body has the stan- ces Unrelated, Discuss, Agree or Disagree to a news headline. Most of the teams participated in the FNC- Masood R. and Aker A. The Fake News Challenge: Stance Detection using Traditional Machine Learning Approaches. DOI: 10.5220/0006898801280135 In Proceedings of the 10th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (KMIS 2018), pages 128-135 ISBN: 978-989-758-330-8 Copyright c© 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved including the winner team used deep learning appro- aches to solve the task. Although deep learning is a powerful technique and it has shown a great success in various tasks, the deep architectures is said to be short on providing more understandable results in terms of features extracted by the deep architecture and their performances. This drawback is the motivation in our work. Thus, instead of deep learning we use traditi- onal machine learning approaches along with the ap- propriate feature selection/engineering and show that we can beat the deep learning approaches in an at- tempt to provide useful information about engaged fe- atures and their performances. Thus our contributions are as follows: • We provide a solution for the FNC-1 task using traditional machine learning algorithms. • We extract a range of different features that are useful for the stance detection task. • We perform feature analysis and discuss different experimental training and testing settings which were important to obtain state-of-the-art results. • We achieve a score of 82.1%4 which is currently the best score achieved for the fake news stance detection task. We first discuss related work, and then in Section 3 we describe the data and the scoring system. In Section 4 we introduce our method including the fe- atures and the machine learning approaches used to learn the models. Then, in Section 5, we discuss ex- perimental settings followed by the results discussion in Section 6 and conclusion in Section 7. 2 RELATED WORK The problem that is introduced in this paper was pu- blished first as a pure stance detection tool within a plan to employ it in a wider fake news detection plat- form. Many researches proposed methods which em- ploy stance in disinformation detection. The veracity of claims were also predicted using the stance of arti- cles and the reliability of their sources (Popat et al., 2017). Stance features were also employed in de- tecting the credibility of rumours, which are also de- fined to be unverified claims (Enayet and El-Beltagy, 2017). Moreover, Using Tweets publishing time and stances as the only features to model the veracity of tweets using Hidden Markov Models achieved high accuracy (Dungs et al., 2018). Some other cases that 4This score is calculated using the FNC-1 scoring sy- stem targeted rumours used also stance features to identify their veracity (Zubiaga et al., 2018). Detecting stance of news articles is the most rela- ted work to our task. On this line the work of Fer- reira  Vlachos addresses rumor debunking based on stance. The aim is to estimate the stance of a news he- adline towards its paired claim as Observing, For or Against. Linguistic features are extracted from each claim and headline pair (Ferreira and Vlachos, 2016). The FNC-1 task extends the work of Ferreira  Vlachos to predict the stance of a complete news arti- cle (body of an article) towards a title or headline pai- red with that article. For this task results of first three top systems have been announced. The first ranked team5 approach is based on a 50/50 weighted average ensemble combining a gradient-boosted decision tree model fed with text based features from the headline and the body pair, and a deep learning model based on one dimensional Convolutional Neural Network (CNN) with Google News pre-trained vectors. Unlike most of the approaches used by the parti- cipating teams described above we employ traditional machine learning techniques along with feature en- gineering. We investigate several features and deter- mine their contribution towards the task. We also ex- periment with different training settings. Overall we show that our approach leads to slightly better results than those reported by deep learning strategies. 3 THE FAKE NEWS CHALLENGE The fake news challenge (FNC-1) is a machine lear- ning task which is a contribution between AI commu- nity, journalists and fact-checkers. It forms a basis for fighting fake news and aims to develop tools towards fake news detection. One of the tools is a stance de- tection tool which is the first interest of the challenge. The challenge is about predicting the stance of a news article (body of the article) towards its paired title or headline. In the following sections we describe the data and the scoring mechanism of FNC-1. 3.1 Data The data used in the competition was extracted from Craig Silverman’s Emergent dataset 6 which is part of a research project that employs rumor tracking in de- tecting misinformation. The dataset consists of 2595 articles that relates to 300 claims (headline) so that for each claim there are between 5 to 20 articles. These articles are labeled manually by journalists as agree, disagree or discuss the claims they are paired with. The FNC-1 organizers mixed and matched the ar- ticle bodies and their headlines, and used the labels relative to the claims. They got 75,119 labeled pairs as the following: • Unrelated: The topic of the headline is different from the topic of the article body. • Discuss: The body observes the headline’s claim neutrally without taking a position. • Agree: The body confirms the headline’s claim. • Disagree: The body refutes the headline’s claim. The resulted pairs were divided by FNC-1 orga- nizers into 49,972 pairs as training data and 25,147 pairs for testing. The training dataset was a match be- tween 1648 unique headlines and 1683 unique article bodies, whereas the test dataset is a match between 880 unique headlines and 904 unique article bodies with no overlaps between the splits. In addition, the test data used to finally evaluate the competitors was supplied with an additional 266 pairs that the organi- zers derived and labeled using Google News articles. The headline’s length ranged between 1 - 40 words with an average of 11 words. While the article body length ranged between 3 - 4800 words with an average of 350 words. The training dataset is highly unbalan- ced with class distribution as the following: 73.13% unrelated, 17.83% discuss, 7.36% agree and 1.68% disagree. 3.2 FNC-1 Scoring System and Baseline Classifier FNC-1 scoring system adds 0.25% score for each pair classified correctly as Unrelated. The score is incre- ased by 0.25% if the pair was related and was classi- fied as any of Discuss, Agree or Disagree classes. If the pair was correctly classified as Discuss, Agree or Disagree, the score is increased to 0.75%. We consi- der the approach that won the FNC-1 as our baseline system. This system scored 82.02% according to the FNC-1 scoring system. 4 METHOD In our methodology we apply traditional machine le- arning approaches, specifically, L1-Regularized Lo- gistic Regression provided by LibLINEAR (Fan et al., 2008) using WEKA (Hall et al., 2009) and Random Forest classifier from the same WEKA toolkit. Both approaches rely on feature engineering. Our feature engineering focuses on the article content and tries to find parts of it that would best describe the stance the article has towards the headline. The data provided for the FNC-1 stance detection task is limited to articles’ text with no reference to sources, writers or any explicit meta data. Given this, the features we relied on are only linguistic features. In the following sections we describe our features in detail. 4.1 Headline Features • Headline Length (H-Len). This is equal to the number of words in the headline. • Headline Contains Question Mark (H-Q). A feature indicating whether a headline contains a question mark or not (0 or 1). 4.2 Article Content Features We split each article content into a heading, middle, and tail parts based on the sentences7. The motiva- tion behind this splitting is that most news articles are written in a specific style in which the article begin- ning (heading) introduces the main argument(s) that the entire article wants to convey to the users, the body part (middle) provides more detailed informa- tion about the argument(s) made earlier and a conclu- sion towards the end (tail) summarizing what is de- tailed in the body. We have experimented with diffe- rent splitting strategies however, dividing the entire article into first 5 sentences (heading), 4 sentences from the tail and 10% of the middle sentences (min. 2 sentences)8 gave us best performance. In the fol- lowing we explain the features extracted from these parts. • Bag of Words (BoW): We extract uni-grams and bi-grams from the heading and tail parts of the ar- ticle. However, we retain only the 500 most occur- ring n-grams and delete all the remaining ones. • Root Distance (Root-Dist): This feature is calcu- lated similar to the study of (Ferreira and Vlachos, 2016). However, for our case we compute 3 dif- ferent features (feature vectors), i.e. one for the heading part, one for the middle and one for the tail part. For each sentence in each part we parse it using Standard CoreNLP parser and compute its root distance to pre-collected words list obtained from related work (Discuss or Refute words). 7Sentence splitting has been performed using The Stan- ford CoreNLP tools (Manning et al., 2014) 8We start taking from the median, then left and right of it until we have reached our threshold.KMIS 2018 • Sentiments: For each sentence in each article part we compute its sentiment score. The tool used is Stanford Sentiment (Socher et al., 2013) which gi- ves each sentence a score between 0 (high positi- vity) to 4 (high negativity). • Sentence Length (Sentence-Len): This feature indicates the maximum and the average length of the sentences in the respective article parts. • Punctuation Count (Punct): We use several punctuation such as dot, comma, etc. and for each of them we compute how many times it appears in the entire article (not only in the three parts). • Lemma Count: We remove all stop words from the headline and lemmatize the remaining words. For each sentence in each article part we count the occurrences of each lemma that also appears in the headline and take the sum of all lemma occur- rence counts as a lemma count feature. We also do this for the entire article regardless of the men- tioned article split boundaries. • Character Grams (Ch-Grams): We build sets of character sequences of lengths 4, 8 and 16 from the headline. For each character sequence set we count how many times the sequences appear in each sentence of each article part. Each sentence is assigned three count values each indicating how many times the sentence includes any sequence from the respective length category. We use lem- matized text before building the character sequen- ces and also remove all the stop words. • Word2Vec Similarity (W2Vec-Sim): For this fe- ature, a vector space representation of both the headline and each article part is computed using word embedding (Mikolov et al., 2013). For the embedding, we used Google’s Word2Vec pre- trained words and phrases from Google News 9. Once the embedding vectors are obtained we compute the cosine similarity between the given vectors. • Word Grams (N-Grams): This is similar to the Ch-Grams feature however, instead we take word sequences of lengths 2, 4, 8 and 16. • Hypernyms Similarity (Hyp-Sim): We use WordNet 3.1 (Miller, 1995) and collect hyper- nyms from the first synset of nouns and verbs. The nouns and verbs are taken from the headline, article heading and article tail. For the collected hypernyms we build word embedding vectors and compute similarities between title-article heading and title-article tail using cosine. 9https://code.google.com/archive/p/word2vec/ • Cosine Similarity (Cos-Sim): This feature com- putes the cosine similarity of the headline to each sentence in each article part. The vector values are word counts. Before computing we take lemmas of the words and remove stop words. • Paraphrase Alignment (ppdb): This feature captures an alignment score calculated between two texts depending on the Paraphrase Database (Pavlick et al., 2015) and the Kuhn-Munkres al- gorithm (Kuhn, 1955; Munkres, 1957). It is com- puted between words from the headline and words from a sentence in each article part. This feature is calculated similar to (Ferreira and Vlachos, 2016). • Subject, Verb and Object Triples Entailment (SVO): This feature indicates the entailment re- lations between the subject, verb and object tri- ples of the headline and each sentence in each ar- ticle part. The entailment is again found using the paraphrase database (Pavlick et al., 2015). This feature is computed as in (Ferreira and Vlachos, 2016) work but instead of indicating the entail- ment with 0 or 1 we count how many sentence in each article part have this entailment relationship. • Negation (Neg): We use the Hungarian algorithm (Kuhn, 1955) to align words between the headline and words from each sentence from the article. Then we check for each aligned word pairs whet- her one of them is the negation of the other accor- ding to (Ferreira and Vlachos, 2016). Each sen- tence is assigned a counter indicating how many negated pairs it contains. We compute this feature for each sentence in the entire article. • Word Overlap Score (W-overlap): For this fe- ature we compute an overlap score between the headline and the body’s heading as well as bet- ween headline and tail. The method is based on extracting all possible sub-strings from these parts and then finding the longest matching sub-strings. The score is calculated by summing up the square lengths of these matches. • Bias Count (Bias): Based on a bias lexicon as in (Recasens et al., 2013) and (Allen et al., 2014), we compute how many bias lexicon entries appear in the entire article as well as in the headline. 5 EXPERIMENTAL SETTING As noted in section 3.1, the data has four different class labels: Unrelated, Discuss, Agree and Disagree. We trained our classifiers so that they predict one of these four labels. However, the performance of theThe Fake News Challeng resulting models were below the baselines10. After manual inspection of the data we realized that the ar- ticles labeled differently were similar in tone towards the headline and thus difficult for a multi-class labeler to predict the right class. Furthermore, the data is un- balanced and contains mostly Unrelated pairs and re- latively few pairs from the other classes. To overcome these issues we experimented with different training strategies without modifying the training and testing settings defined by FNC-1: • 2-Steps Classifier: We first train the classifier to distinguish only between Unrelated and Related pairs, where Related represents all the categories Discuss, Agree, Disagree. Next, we train a second classifier on the pairs labeled with Discuss, Agree, Disagree. For testing, we first run the first classi- fier on the entire testing data. Any article-headline pair classified as Related is further analyzed with the second classifier to further classify it as Dis- cuss, Agree or Disagree. • 3-Steps Classifier, Setting 1: We further split the classification of the Related classes and cre- ate three classifiers. We keep the first step as it is in the previous 2-step classifier setting (classifica- tion for Unrelated and Related. Then we train a classifier to predict the classes Discuss and Non- Discuss, where Non-Discuss category stands for the original categories Disagree, Agree. For the third step we use a 2-way classification for the re- maining categories Disagree, Agree. For testing we again run first the first classifier to split the data into Unrelated and Related catego- ries. After, the Related data pairs are further clas- sified to obtain Discuss and Non-Discuss article- headline pairs. Finally, for the Non-Discuss pairs we further detail their actual classes using the third classifier and obtain the Disagree Agree clas- ses. • 3-Steps Classifier, Setting 2: We keep the first step as it is, but we used a 2-way classification in the second step for the categories Disagree, Non- Disagree. The Non-Disagree category represents the original categories Discuss, Agree. For the third step we use a 2-way classification for the re- maining categories Discuss, Agree. In all settings for the first two steps we use an L1- regularized logistic regression classifier (Fan et al., 2008). For the third step we use a Random Forest classifier with 100 trees (Breiman, 2001). In each step we used different sets of features. Table 1 shows the features used in each step. 10Classifier predicting all classes led to 77.04% FNC-1 score. Table 1: Classifier steps and the features used in each step. 1st step 2nd step 3rd step W-Overlap H-Q H-Q Lemma Count BoW BoW Ch-Grams Root-Dist Root-Dist N-Grams Neg Neg Cos-Sim SVO SVO Hyp-Sim Sentiments Sentiments PPDB PPDB PPDB W2Vec-Sim W2Vec-Sim Sentence-Len Bias Bias Punct 6 RESULTS AND DISCUSSION Our overall results are shown in Table 2. We report, as in FNC-1 challenge, the results using the FNC-1 score. We also compute accuracy. From the table we see that the best results are obtained with the 3-step classifiers and setting 2. However, we found no diffe- rence in terms of significance to our other settings.11 From the table we also see that the performance of our classifier (3-steps classifier setting 2) is better than the one of the best system participated in the FNC-1 task. Both FNC-1 as well as accuracy figures are better than those of the best performing baseline.12 Tables 3 and 4 show the confusion matrices of the best baseline and our 3-steps classifier with setting 2. According to the matrices, the 3-steps classifier in set- ting 2 predicts more correct Discuss, Disagree, Unre- lated pairs. The baseline, on the other hand, performs better on the Agree class. 6.1 Features Analysis As shown in Table 5 best results are obtained when all features are used. We aimed to understand the con- tribution of each feature to the overall results. Thus we removed a feature at a time, trained the classifiers with the remaining features and tested on the testing data. The difference in results are captured using pai- red t-test and a p-value of p < 0.002813 In the results we see only a significance drop when we remove the BoW feature, in all other settings the results are not significantly different from when there is no feature 11Significance test is performed using student t-test. 12Again in the results we did not find any indication for significance. 13When conducting multiple analyses on the same depen- dent variable, the chance of achieving a significant result by pure chance increases. To correct for this we did a Bon- ferroni correction on the p-value. Results are reported after this correction.KMIS 2018 Table 2: N-Steps classifiers and Baseline. Winning Baseline 2-Step Classifier 3-Steps Classifier setting 1 3-Steps Classifier setting 2 Unrelated 0.98 0.98 0.98 0.98 Discuss 0.76 0.76 0.75 0.76 Agree 0.54 0.52 0.49 0.52 Disagree 0.04 0.05 0.07 0.1 Accuracy 89.1 89.1 88.8 89.18 FNC- Score 82.02 82.0 81.53 82.10 Table 3: Best baseline Confusion Matrix. A, DA, DC and U stands for Agree, Disagree, Discuss and Unrelated respecti- vely. A DA DC U A 1114 17 588 184 DA 275 13 294 115 DC 823 6 3401 234 U 35 0 203 18111 Table 4: 3-Steps Classifier with setting 2 Confusion Matrix. A, DA, DC and U stands for Agree, Disagree, Discuss and Unrelated respectively. A DA DC U A 947 29 799 128 DA 181 39 343 134 DC 589 28 3558 289 U 10 2 219 18118 omission. However, in all removal cases there is a moderate drop in the results indicating that every fea- ture has some contribution to the final results. We also removed combinations of features from the entire set of features used in our final model to show the effect of more than one feature removed at once. The selection of different combinations is cho- sen according to the relatedness of features. We list them in groups: • Group A: Is a group of features used in the first step for distinguishing Related and Unrela- ted classes, namely Ch-grams, N-grams, Lemma Count and W-overlap (see Table 1 for the set of features used in the first step). When removing Group A features, the number of correctly clas- sified instances as Unrelated reduces the most (from 18118 to 18034), hence reducing the cor- rectly classified instances as Discuss. See confu- sion matrices in Tables 6 and 4 for comparison. • Group B: This group holds features related to si- milarity and entailment, namely PPDB, Hyp-Sim, W2Vec-Sim and Cos-Sim. They have lower effect on Related and Unrelated but greater effects on the Agree (reduction from 947 to 913) and Dis- agree (reduction from 39 to 26). See confusion Table 5: Accuracy, and FNC-1 score when using all featu- res compared to results when removing features one by one accordingly. Features Accuracy FNC-1 Score All Features 0.891 82.1 - BoW* 0.870 78.53 - Lemma Count 0.888 82.07 - Ch-Grams 0.888 81.42 - N-Grams 0.890 82.00 - Hyp-Sim 0.891 81.93 - Cos-Sim 0.890 81.86 - W2Vec-Sim 0.891 82.01 - ppdb 0.890 81.70 - w-overlap 0.891 82.02 - H-Len 0.891 81.97 - Root-Dist 0.888 81.57 - SVO 0.889 81.59 - Neg 0.891 81.85 - Sentiments 0.887 81.37 - Bias 0.891 82.00 - Punct 0.889 81.66 - Sentence-Len 0.887 81.41 - Tittle-Q 0.891 81.86 - group A 0.885 81.19 - group B 0.886 80.87 - group C 0.888 81.38 - group D 0.890 81.89 Table 6: Group A: Features without Ch-grams, N-grams, Lemma Count and W-overlap. A DA DC U A 934 27 797 145 DA 180 42 342 133 DC 561 25 3493 385 U 16 1 298 18034 matrices 7 and 4. • Group C: This group contains SVO, Neg, Root- Dist and PPDB features. Confusion matrix 8 shows that by removing these features, the cate- gories Disagree and Discuss are mostly affected. • Group D: This group contains Punct, Bias, Sentence-Len and T-Quest features. Removing this combination has a greater effect on the Agree category. See Table 9.The Fake News Challenge: Table 7: Group B: Features without PPDB, Hyp-Sim, W2Vec-Sim and Cos-Sim. A DA DC U A 913 30 783 177 DA 165 26 328 178 DC 563 30 3485 386 U 9 1 243 18096 Table 8: Group C: Features without SVO, Neg, Root-Dist and PPDB. A DA DC U A 939 26 793 145 DA 212 29 310 146 DC 626 35 3486 317 U 24 1 212 18112 Figures for the accuracy and FNC-1 metrics after re- moving these group features are shown in Table 5. Overall the removal of all group features lead to de- crease in performance. However, similar to the single features cases the decreases are only moderate wit- hout significance relevance. 6.2 Discussion Overall we have seen that our 3-step classifier in set- ting 2 outperforms the state-of-the-art system that par- ticipated in the FNC-1 challenge. Although the diffe- rences in the results are only moderate, nevertheless, they show that it is possible to beat state-of-the-art results with feature engineering as well as traditional machine learning approaches. Furthermore, tackling the problem in hand with such an approach has the advantage that, unlike deep learning approaches, ena- bles feature extraction and later feature analysis. In our case, we carefully picked our features and investi- gated settings including finding article parts and clas- sification steps where they shine best. Feature analysis shows that removing any single feature leads to some drop in performance compared to the results when all features are used. The signifi- cant drop happens when we remove the BoW feature. The BoW feature includes uni-grams and bi-grams ex- tracted from the article heading as well as from the article tail. Thus, it aims to capture what is in those article parts in terms of vocabulary. Those areas of the article introduce and summarize arguments. The chance is very high that they capture the claim intro- duced in the headline. Indeed the results confirm this phenomenon with a significant drop when removing this feature. We also grouped features and removed them al- together from the complete feature set. The overall drop in terms of performance was moderate. Howe- Table 9: Group D: Features without Punct, Bias, Sentence- Len and T-Quest. A DA DC U A 886 26 863 128 DA 173 34 356 134 DC 556 27 3592 289 U 14 2 215 18118 ver, in the confusion matrices we have seen that each feature group has its strength in a specific category or class. Group A features help in the relatedness task (step one of the classification) whereas the ot- her groups find their shining points at later steps and address Agree, Disagree and Discuss classes. Finally we performed error analysis on the final classifier results. We observed the following points: 1. There is ambiguity in Disagree definition. Exam- ple: pair: {headline: ”Justin Bieber Helps De- fend Russian Fisherman...”, body ID: 2373}. This pair’s correct class is ”Disagree”, but it is classi- fied as Unrelated by our classifier. In this exam- ple there is no mention of Justin Bieber. The ar- ticle itself is about a Fisherman being attacked by a bear. However, there is no disagreement about the topic that is introduced in the headline. Thus according to the definition for the category Dis- agree, this pair should be classified as Unrelated. 2. Detecting disagreement is hard in some cases be- cause it depends on the implicit meaning of the ar- ticle. As an example, the pair:{Headline: ”People Actually Believed Argentina’s President Adopted A Jewish Boy...”, body ID: ”2382”,} This pairs correct classification is Disagree, but it is classi- fied as Discuss by our classifier. The article talks about passing a law to stop some act of Argen- tina’s people and it does not refute explicitly what it is in the headline. 3. Detecting unrelated titles to their paired articles is critical when the article uses most of the words mentioned in the title. 4. In most cases there is no clear indications for dif- ferentiating between the classes Agree and Dis- cuss which makes them hard to judge by our clas- sifier. Most of the classifier errors are due to this phenomenon. See Table 4. 7 CONCLUSIONS In this paper we re-investigated the Fake News first challenge of stance detection using traditional ma- chine learning and feature engineering approach.KMIS 2018 Using this method we scored better than the first win- ner’s deep learning model. We performed feature analysis by removing a fea- ture at a time but also groups of features. Any removal led to moderate performance drop. The significance drop happened when the BoW feature was removed. This feature contains uni-grams and bi-grams extrac- ted from the article heading and article tail. As dis- cussed both parts either introduce or summarize argu- ments and are likely to capture what is said in the he- adline. Overall every feature plays a role in the clas- sification. We showed that some features play role in the first step (distinguishing between related and unre- lated pairs) and others play at discriminating between agree, disagree and discuss classes. Our immediate future work will be to use stance to perform judgments about fake news. We will investi- gate how stance can be integrate for the fake news classification.","Masood, R., & Aker, A. (2018). The Fake News Challenge: Stance Detection using Traditional Machine Learning Approaches. In KMIS (pp. 126-133)."
SSAFC_060,Assessing the News Landscape: A Multi-Module Toolkit for Evaluating the Credibility of News,"Today, journalist, information analyst, and everyday news consumers are tasked with discerning and fact-checking the news. This task has became complex due to the ever-growing number of news sources and the mixed tactics of maliciously false sources. To mitigate these problems, we introduce the The News Landscape (NELA) Toolkit: an open source toolkit for the systematic exploration of the news landscape. NELA allows users to explore the credibility of news articles using well-studied content-based markers of reliability and bias, as well as, filter and sort through article predictions based on the user's own needs. In addition, NELA allows users to visualize the media landscape at different time slices using a variety of features computed at the source level. NELA is built with a modular, pipeline design, to allow researchers to add new tools to the toolkit with ease. Our demo is an early transition of automated news credibility research to assist human fact-checking efforts and increase the understanding of the news ecosystem as a whole.",Computer Science,Proceeding,2018,Y,Y,4,"Understanding and analyzing the news landscape has became a pri- ority for researchers across many disciplines. The production and This paper is published under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. WWW ’18 Companion, April 23–27, 2018, Lyon, France © 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY 4.0 License. ACM ISBN 978-1-4503-5640-4/18/04. consumption of news in today’s media landscape favors clicks and attention, as opposed to in-depth analysis. This drive for attention has lead to the emergence of a large number of media sources with ever increasing visibility. These sources operate under different in- centives: from benign to opportunistic and malicious. Those sources which are partisan or malicious in intent employ a wide-range of tactics to make their message heard. They employ tactics such as re- porting incorrect information, using emotionally charged language, manipulative titles, and mixing true news with fake news. Fake news stories and hyper-partisan news coverage are thought to have influenced various key elections worldwide. This, coupled with the well-known susceptibility of individuals to false and misleading information [ 5 ], has lead to the increasing need for tools that assist researchers, journalists, and every day individuals in the analysis of news. Supporting this notion, in a 2017 agenda for fake news research, Lazer et al. argue that we ”need to translate existing re- search into a form that is digestible by journalist and public-facing organizations [ 4].” However, given the complexity, the problem requires multi-faceted solutions and a better understanding of the wide-range of news sources. In addition, tools should be able to quickly evaluate sources to decide where to dedicate fact-checking efforts (before an article’s spread). To address these problems, we introduce the The News Land- scape (NELA) Toolkit: an open source toolkit for the systematic exploration of the news landscape, through a unique combination of (a) real data from news sources and social media, (b) state-of-the-art tools that predict different factors of credibility, and (c) visualization tools to compare a large number of media sources across differ- ent axes. Specifically, NELA is made up of multiple independent modules, in which users can scrape news articles for article-level predictions or explore source-level characteristics using the built-in NELA data set. In this demonstration, we discuss the first release of the toolkit, and briefly discuss its utility using an initial 7 months of news data from 92 sources across the reliability and bias spectrum. 2 DESCRIPTION OF THE DEMO To use the NELA Toolkit, visit the NELA Toolkit website (nelatoolkit. science). The homepage provides two choices Check a News Arti- cle or Compare News Sources. Under Check a News Article users can provide a url to a news article or manually enter news article text. The tool then performsTrack: DemonstrationWWW 2018, April 23-27, 2018, Lyon, France235 Figure 1: NELA Toolkit architecture several predictions on the article: reliability, political impartiality, title objectivity, text objectivity, and several online community interest predictions. Each of these predictions is displayed as a probability and each article with associated predictions are entered into a table. As more article entries are provided, this table can be sorted and filtered by different predictions using the table filters menu at the top of the page. Further, more details about the article and analysis of the article can be found by clicking on the entry in the table. The ultimate goal of this page is to allow journalist and information analyst to quickly filter articles down to ones that need to be fact-checked or are of interest. Under Compare News Sources users can explore and compare a variety of news sources using content-based features. Specifically, users can select multiple features, sources, and a time range to visualize on a 2-dimensional scatter plot. For example, a user can select reading complexity for the x-axis and negative sentiment for the y-axis using the chart setting menu on the left side of the page. They can then select any number of sources from our data set and a data range over which to explore. The tool will then generate a scatter plot of the selected sources for comparison. If a user wants more details about a source, they can double-click the source bubble in the scatter plot. This detailed page will show source metadata, credibility predictions, and Facebook engagement over time. These details can also be found on the View All Sources page. The overall architecture of the toolkit can be found in Figure 1. Due to lack of space and the many parts of the toolkit, we do not provide screenshots. We encourage readers to visit the NELA Toolkit website (nelatoolkit.science), watch our demo walk-through (nelatoolkit.science/help), or check out our code-base (goo.gl/cSpWmp). 3 DATA Every module in the NELA toolkit is based on real news data. To create a general news data set, we first gather a wide variety of sources using multiple lexicons (opensources.co, Wikipedia) and studies [2]. These news sources include: mainstream sources, satire sources, maliciously false sources, political blogs, and some rela- tively unknown sources. Each news source’s website or RSS feed is scraped twice a day, everyday, between April 2017 and October 2017, totalling in 92 sources and 136K articles. To control for topic, we only collect news from politics pages and feeds. The complete list of sources currently in the data set can be found on the NELA toolkit website. From this general news data set, two subsets are created to build a reliability labeled news data set and a bias labeled news data set. Specifically, we use OpenSources (www.opensources.co/), an expert-curated news source lexicon, to create 4 groups of sources: reliable news, unreliable news, biased news, and unbiased news (Table 2). Opensources has 12 different tags: fake, satire, extreme bias, conspiracy, rumor, state, junk science, hate speech, clickbait, unreliable, political, and reliable. We use the fake and conspiracy tags to create our unreliable group and the bias and political tags to create our biased group. The articles from each labeled source are used in training and testing the two machine learning models, discussed in Sections 4.1 and 4.2. It is important to note this ground truth is a previous behavior- based ground truth rather than a correctness-based ground truth. In other words, if a news source has been found to publish many fake articles in the past, they are an unreliable source, or if a news source has been found to be hyper-partisan many times in the past, they are a biased source. We choose this method for two primary reasons: (1) reliability and bias can be labeled quickly over time, allowing for our tool to be retrained as the news changes. Currently, fact-checking (or biased-checking) articles is a very slow and selec- tive process. Hence, fact-checked data for algorithm training can be very small and time specific, making trained classifiers difficult to maintain over time. (2) We can reasonably classify fake articles using this method. Explicitly, on a small fact-checked, correctness labeled test set (of 100 articles), the reliability labeled classifier per- forms well in detecting fake news as unreliable and real news as reliable (with 90% accuracy). However, our predictions are built to predict the type of source a news article is coming from, not the specific nature of the claims in an article. This notion is further discussed in Section 4. This data will continue to be collected for use in the toolkit and its later release. 4 MODULES IN THE NELA TOOLKIT In this section, we will briefly discuss the basic research behind each module in the NELA Toolkit. 4.1 Reliability prediction The first module predicts the reliability of a user-selected news article. Given a url, the tool scrapes the title and body content from the web page. After the news article is scraped, it is passed through a feature computation pipeline, which computes a large set of content-based features. These features primarily come from [ 2 , 8 ], but are also influence by other studies on persuasion [ 7]. Due toTrack: DemonstrationWWW 2018, April 23-27, 2018, Lyon, France236 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0 True Positive Rate NECO17+CIKM16 (area = 0.89) NECO17 (area = 0.89) CIKM16 (area = 0.76) POS (area = 0.69)0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0 True Positive Rate NECO17+CIKM16 (area = 0.92) NECO17 (area = 0.91) CIKM16 (area = 0.80) POS (area = 0.80)Reliable vs. Unreliable Unbiased vs. Hyper-partisan Table 1: ROC curves for each feature set using a Random Forest machine learning model, where NECO17 is from [2], CIKM16 is from [8], and POS is a standard Part-Of-Speech feature set. space restrictions, descriptions of these features can be found on the NELA Toolkit website. After features are computed, they are passed through a feature selection module, which selects the best features for the reliability prediction based on a previously com- puted variance analysis. Once feature selection is done, the single feature vector, representing the user-selected article, is passed to our machine learning model. The reliability model is a Random Forest classifier trained on news sources labeled by previous be- havior, discussed in section 3. To make the ground truth stronger, we also require news sources in the unreliable category to have published more than 1 completely false article according to online fact checkers (eg. snopes.com, politifact.com, etc.). In the current implementation, we trained the classifier on 4504 articles and tested it on 1130 articles, achieving 0.89 ROC AUC (refer to Table 1). The final output of the classifier is a probability of being reliable rather than a strict binary classification. To do this, we use the mean predicted class probabilities from the trees in the forest. This probability is then colored based on the strength of the prediction (where green is strongly reliable, red is strongly not reliable, and yellow is an edge case). This design choice allows for some notion of certainty or uncertainty in the algorithms predictions. News is inherently not a two-class problem, rather a spectrum between the two-classes; hence, it is important to show the user when a data point is near the edge of the decision boundary. Each result is entered into a sort-able and filterable table to allow for batch article analysis. For example, if an analyst is given a large number of news articles to assess, they can use the NELA Toolkit to quickly filter down to the most interesting articles. 4.2 Bias and subjectivity prediction The next module is made up of two independent classifiers: (1) a Random Forest classifier trained on content-based features to pre- dict hyper-partisan articles, (2) a Naive Bayes classifier trained on objective and subjective labeled sentences. Just as in the reliability module (Section 4.1), a user provides a url, and the title and body content is scraped from the web page. The content is then passed through both feature computation and model-specific feature selec- tion pipelines. The first classifier in this module is very similar to our reliability module, only differing in the data and features selected. The features are based on several studies on news and political bias in text [ 2, 9 ] and the labeled data is discussed in Section 3. The sources are balanced between politically right and politically left hyper-partisan sources. In the current implementation, we trained the classifier on 6158 articles and tested it on 1539 articles, achieving 0.92 ROC AUC (refer to Table 1). The final output from this classifier is a probability of an article being classified as impartial. The second classifier in this module is more generic than the previous, focusing on sentence level objectivity. Specifically, the classifier will provide a probability of being objective for both the title and body of the news article independently. The separation of title and body allows for a finer-grain analysis of title dynamics. This classifier is built using a Naive Bayes model that is trained on 10K sentences from Pang and Lee 2004 [ 6 ], and it achieves a 92% 5-fold cross-validation accuracy. The final outputs of this classifier are the probability of being objective for both the title and body text. The results from both classifiers are also added to the sort-able and filterable table for quick batch analysis. Reliable/Unbiased sources Unreliable sources Hyper-partisan sources Associated Press Infowars Brietbart PBS Liberty News Young Cons NPR Natural News RedState CBS Alt Media Syndi- cate The Blaze USA Today DC Clothesline CNS BBC Newslo Bipartisan Report New York Times Ending the Fed Occupy Democrats The Guardian Daily Buzz Live Daily Kos Intellihub Shareblue Freedom Daily Politicus USA Table 2: Sources used in each category 4.3 Community interest prediction Our next module is built to predict which online groups are in- terested in an article using news communities on reddit.com. To build this module, we first collect recent posts from 4 news com- munities (r/new_right, r/esist, and r/conspiracy). Once these posts are collected, we extract the top 25% of posts by their rank- ing score (roughly upvotes minus downvotes). These posts can be considered the most popular or most widely accepted by the community during the time slice collected. The news article in each post is scraped and content-based features are computed [ 3]. We compare r/news (a general interest community) to the other three subreddits (specific interest communities). Specifically, us- ing these features, we train 3 binary classifiers to predict articles as r/news interest or (r/new_right, r/esist, r/conspiracy) interest. Each classification is shown as a probability, similar to the other modules in the toolkit. In the current implementation, we trained each classifier on 2000 articles and tested each on 500 articles, achieving 0.77 ROC AUC on average. These community interest models are in a very early stage of development. Currently these models are based solely on newsTrack: DemonstrationWWW 2018, April 23-27, 2018, Lyon, France237 content features, but could be significantly improved with topic, source, or community-specific features. In addition, more in-depth feature analysis can provide insights into community differences and similarities. For example, it may be that highly emotional or subjective articles are popular in both r/new_right and r/esist, but the articles differ in slant (due to selection bias, framing bias, etc.). Automatic methods to capture these various types of bias in a general news setting could significantly improve our accuracy. We leave these improvements to future work. 4.4 Feature-based source visualizations Our last module analyzes the news at a source-level granularity, rather than an article-level granularity. Using our data set (refer to Section 3), we computed 260 content-based features [ 1, 2, 8 , 9, 11 ] on each article. Users can pick a set of news sources, a time frame, and 2 to 4 features to visualize on a 2-dimensional plane. This visualization provides a quick and easy comparison of individual sources or clusters of sources. Further, we provide meta data for each source, which can be accessed by clicking on a source bubble in the visualization. The meta data includes: (1) Percentage of articles that were predicted as reliable using our reliability model (2) Percentage of articles that were predicted as impartial using our bias model (3) Top phrases for each month using Autophrase [10] (4) The year the source was founded and the country of origin, if known (5) Facebook shares, reactions, and comments over time As data is collected, this module will be updated to reflect the current predictions and articles from each source, allowing for users to explore changes in sources over time.","Horne, B. D., Dron, W., Khedr, S., & Adali, S. (2018, April). Assessing the news landscape: A multi-module toolkit for evaluating the credibility of news. In Companion Proceedings of the The Web Conference 2018 (pp. 235-238)."
SSAFC_067,Progress Toward  the Holy Grail ” : The Continued Quest to Automate Fact-Checking,"Two years ago at this conference we issued what we said was a call to arms” to advance automated fact-checking [1]. We said the Holy Grail” was a completely automated fact-checking platform that can detect a claim as it appears in real time, and instantly provide the voter with a rating about its accuracy.” We acknowledged that goal may remain far beyond our reach for many, many years to come,” but we called on the journalism and computer science communities to redouble their efforts to make progress. Since then there has been remarkable progress and the Holy Grail” is no longer a distant dream. Although computer scientists and journalists still have significant hurdles to overcome, recent advances with the creation of a global database of structured factchecks and fact-checking tools such as ClaimBuster and iCheck have laid a groundwork for additional advances in the next few years. As we noted in our 2015 paper, fact-checking is a growing form of journalism. According to the Duke Reporters’ Lab, the number of fact-checkers has nearly doubled from 64 in 2015 to 126 today. Fact-checking is uniquely suited for automated journalism because the individual articles have value weeks after they have been published because of the tendency of government officials to repeat political claims. During live events such as debates and speeches, fact-checking organizations have typically relied on editors and reporters to manually match new statements with previously published fact-checks. But in a fully automated system, the statements could be detected and, if they had been previously fact-checked, a link or summary of the conclusion could pop up in real time for the reader. A promising development in this effort is the creation of a schema to identify fact-checking articles. This project, which is led by members of our team from Google and Duke University, has created a global open standard known as ClaimReview so that organizations can identify the people and statements they are checking, as well as their conclusion about the accuracy of claims. Google and Bing are now using the schema for search results. Publishers can generate the schema from their content management systems or use the Share the Facts” widget",Computer Science,Proceeding,2017,Y,Y,28,"Two years ago at this conference we issued what we said was a call to arms” to advance automated fact-checking [1]. We said the Holy Grail” was a completely automated fact-checking platform that can detect a claim as it appears in real time, and instantly provide the voter with a rating about its accuracy.” We acknowledged that goal may remain far beyond our reach for many, many years to come,” but we called on the journalism and computer science communities to redouble their efforts to make progress. Since then there has been remarkable progress and the Holy Grail” is no longer a distant dream. Although computer scientists and journalists still have significant hurdles to overcome, recent advances with the creation of a global database of structured fact- checks and fact-checking tools such as ClaimBuster 1 and iCheck 2 have laid a groundwork for additional advances in the next few years. As we noted in our 2015 paper, fact-checking is a growing form of journalism. According to the Duke Reporters’ Lab, 3 the number of fact-checkers has nearly doubled from 64 in 2015 to 126 today. Fact-checking is uniquely suited for automated journalism because the individual articles have value weeks after they have been published because of the tendency of government officials to repeat political claims. During live events such as debates and speeches, fact-checking organizations have typically relied on editors and reporters to manually match new statements with previously published fact-checks. But in a fully automated system, the statements could be detected and, if they had been previously fact-checked, a link or summary of the conclusion could pop up in real time for the reader. A promising development in this effort is the creation of a schema to identify fact-checking articles. This project, which is led by members of our team from Google and Duke University, has created a global open standard known as ClaimReview so that organizations can identify the people and statements they are checking, as well as their conclusion about the accuracy of claims. Google and Bing are now using the schema for search results. Publishers can generate the schema from their content management systems or use the Share the Facts” widget developed by the Duke Reporters’ Lab. The schema provides a consistent way for search engines to identify and index fact- checking articles. It also solves a problem that has bedeviled anyone who has tried to develop apps: the mishmash of ways that different publishers present their fact-checks. The database of fact-checks identified by the schema creates tremendous potential for automation projects because it could potentially include every fact-check article published around the world. Another promising area for automation is to assist journalists with repetitive and time-consuming tasks such as identifying factual claims. Every day fact-checkers and their college interns have difficulty keeping up with the flood of new factual claims from legislative debates, TV talk shows and other news coverage. ClaimBuster, a tool developed by our team at the University of Texas at Arlington, addresses this need by automating the process of finding factual claims to check. ClaimBuster can do the work of many college interns by quickly analyzing voluminous transcripts and identifying claims that journalists are most interested in checking. In the past two years, we have refined ClaimBuster and have begun to deploy it for daily use by journalists. In Australia, it is used for daily analysis of Hansard, the proceedings of the Australian parliament. 4 In the United States, we are using ClaimBuster to analyze the transcript of a cable news channel and identify the most check-worthy” claims. Once a check-worthy claim is identified, we look for ways to help journalists check it. There are many possibilities for automation, and one focus of our team at Duke University, Google, and the University of Texas at Arlington is checking claims based on data or statistics. These claims are often vague and may be factually correct, but they can still mislead by cherry-picking” partial and biased vantage points of the data. We have developed a tool for perturbation analysis,” which puts the claim into a larger context by automatically exploring a large number of alternative vantage points of the data, in order to evaluate claim qualities such as fairness, robustness, and uniqueness in a principled manner. As a proof of concept, we have developed a website called iCheck and released it to the public in September 2016. The website analyzes the voting records of the U.S. Congress from January 2009 to September 2016, and lets visitors compare how legislators vote with party majorities and the president, and more importantly, explore how the comparison stacks up under Computation + Journalism Symposium, October 2017, Evanston, Illinois USA B. Adair et al. 2 different contexts—over time, among groups of peers, and for key votes” identified by lobbying/political organizations. While the Holy Grail” of fully automated fact-checking still poses significant challenges—some requiring more research and investments over a long term—we believe that some aspects of automated fact-checking are ready for prime time and can deliver substantial benefit to the journalists and the public. In the rest of the paper, we describe our progress, discuss lessons learned, and outline our vision of next steps. 2 ClaimBuster Since December 2014, the team at the University of Texas at Arlington has been building ClaimBuster [2-5], a claim-spotting tool for assisting fact-checkers in discovering factual claims that are worth checking. ClaimBuster monitors the plethora of places where politicians and others make political claims such as interviews, speeches and debates. It gives each sentence a score that indicates how likely it is the sentence contains an important factual claim that should be checked. In this way, ClaimBuster provides a priority ranking on the sentences. The ranking helps fact-checkers avoid having to read massive transcripts and efficiently focus on the top-ranked claims. ClaimBuster’s claim spotter was tested in real-time during the live coverage of all primary election and general election debates for the 2016 U.S. presidential election. Closed captions of the debates on live TV broadcasts, captured by a decoding device, were fed to ClaimBuster, which immediately scored each sentence spoken by the candidates and posted top-scored claims to the project’s website and Twitter account (@ClaimBusterTM). Post- hoc analysis of the claims checked by professional fact-checkers at CNN, PolitiFact.com and FactCheck.org reveals a highly positive correlation between ClaimBuster and journalism organizations in deciding which claims to check. ClaimBuster has also been continuously monitoring Twitter and retweeting the check-worthy factual claims it finds in people’s tweets (see @ClaimBusterTM). Our experience so far suggests a few directions for improving ClaimBuster's accuracy in spotting important factual claims. Currently, the tool scores individual sentences. This is a clear limitation as factual claims may span multiple sentences. Mitigating this limitation entails several natural language processing tasks, including coreference resolution and topic segmentation. Furthermore, structured representation of factual claims is imperative for deep understanding of the claims and thus more accurate spotting of important claims. Such structured representation should capture various aspects of a factual claim, including the domain and topic of the claim, the template of the fact being expressed, the involved entities, and their relationships. It is also crucial to capture the claim’s important elements such as numbers, time points and intervals, comparisons, grouping, and aggregates. ClaimBuster delivers the scores on claims through a variety of channels, including its website, Twitter account, API, and Slackbot. Particularly, the Slackbot allows users to supply their own text, directly as Slack input or through text files in a Dropbox folder, and to receive the claim spotter scores for the sentences in that piece of text. The Slackbot has been published in the public Slack App directory and can be installed from Furthermore, a public ClaimBuster API 5 enables developers to create their own fact- checking applications using ClaimBuster as an underlying service. As part of the team’s next step toward the Holy Grail”, we are extending ClaimBuster into an end-to-end fact-checking assistant for professional fact-checkers. A preliminary version of this extension already produces true-or-false verdicts for certain types of factual claims. Given a factual claim which is scored highly by the aforementioned claim spotting component, ClaimBuster may reach a verdict by a few methods. Particularly, one of the methods is to translate the factual claim into questions and their accompanying answers. It then sends the questions to question- answering systems and compares the returned results with the aforementioned answers. It produces a verdict based on the presence/absence of a discrepancy between these two sets of answers. 3 iCheck We demonstrated iCheck at the 2016 Computation+Journalism Symposium; please see our paper [6] and website for additional details. Here, we focus on summarizing the challenges we identified during this project. As explained earlier, we target number-based claims derived from data or statistics, and we have identified perturbation analysis as a way to formulate the human fact-checking process as a computational problem. Automated perturbation analysis [7] can quickly examine a huge number of different vantage points of data, quantitatively assess various aspects of claim quality, and intelligently suggest counterarguments,” thereby relieving human fact-checkers from the tedious, time-consuming, and error-prone aspects of manual fact-checking. While perturbation analysis has proven to be a remarkable fit for number-based claims, it is by no means a one-size-fits-all solution for all types of claims. For example, checking an assertion that some event occurred or somebody took a particular position on an issue would require different procedures that need to be automated differently. Given the diversity of domains and types of claims, it seems improbable for a single computational approach to be universally effective at automated fact-checking. The process of readying iCheck to the public also taught us valuable lessons. Data extraction, cleaning, and linking took huge amounts of effort. Although we have been blessed with high- quality open-source APIs for the U.S. Congress (we relied heavily on GovTrack.us), a lot of work remained to get other related data for iCheck, such as lists of key votes from various lobbying/political organizations, and properly link them to the congressional voting records. These organizations publish their Progress Toward the Holy Grail”: The Continued Quest to Automate Fact-Checking Computation + Journalism Symposium, October 2017, Evanston, Illinois USA 3 information in different formats and refer to key votes in different ways. References are often incomplete or ambiguous—especially when many roll calls may be associated with the same bill—and linking is further complicated by occasional typos in the data source. While automated data extraction and cleaning techniques have come a long way, they still cannot achieve the accuracy desired for fact-checking. Some of the errors and ambiguities we encountered could only be resolved by input from human experts with intimate knowledge of the Congress. Improving accuracy for specific domains—without a lot of data, let alone expert-labeled data—remains a challenge. Making iCheck user-friendly also required enormous effort. iCheck provides visualization, exploration, and recommendation features, but to make them useful and accurate, a very high degree of customization was necessary. Accuracy is difficult to achieve because the reality always manages to come up with exceptions to assumptions made by analysis and implementation. For example, legislators can switch party affiliations and voting rights of delegates change over time, complicating even simple accounting queries. Recommendation algorithms—for example, for suggesting related claims that are surprising” or best counter” the one being checked—also required lots of expert input and extensive tuning by our developers. Looking back, we ask ourselves whether all the development effort was worthwhile for an application in a specific domain. iCheck was made public in September 2016. While there were some claims during the 2016 elections that perfectly fit iCheck, the bulk of the check-worthy claims in that season turned out to have nothing to do with congressional voting records. As we probably do not have the luxury of developing a system like iCheck from scratch for every single domain, the key question is whether and how we can develop a more general system or a set of tools that work across multiple domains. Since iCheck, our team has been exploring ways to build more general tools that can work with more types of claims and additional domains, while striking some balance between generality and user-friendliness. Recognizing data quality issues, we are also actively doing research on fact-checking in the presence of uncertain data, and developing techniques for prioritizing data cleaning efforts under resource constraints. Overall, it has been a humbling experience for the computer scientists involved in the iCheck project to see the wide gamut of knowledge, skills, and efforts required of human fact-checkers and journalists. Plenty of interesting challenges remain in making automated fact-checking more general and more cost-effective. 4 The ClaimReview Schema and Share the Facts The ClaimReview schema was developed by Jigsaw, a subsidiary of Google, and the Duke Reporters’ Lab in an open process with schema.org. The markup is embedded in articles, providing a consistent way for fact-checkers to identify key elements such as the person or group being checked, the statement and the rating or conclusion. Publishers can use their own content management systems to embed the markup or can use Share the Facts, a free service of the Reporters’ Lab. In addition to the markup, Share the Facts also renders a widget” that can be inserted in an article providing a visual summary. Figure 1: The Share the Facts widget provides a visual summary of a fact-checking article as well as embedding the ClaimReview schema. The widget can be shared on social media and embedded in articles and blog posts like tweets.6 Use of the schema and widget is growing. Approximately 20 fact-checkers around the world are using the ClaimReview schema and an additional 11 are using the Share the Facts widget as of July 2017. Many others have said they plan to adopt one of the two methods in the next six months. Over the past year, Google has announced a series of product features that leverage the ClaimReview schema to surface and highlight fact-checking articles in Google News and search results. In October 2016, the company began identifying articles that contain the markup with a FACT CHECK” tag. 7 Figure 2: In October 2016, Google News began identifying fact-check articles with a unique tag. Computation + Journalism Symposium, October 2017, Evanston, Illinois USA B. Adair et al. 4 In April 2017, Google began highlighting fact-check articles in search results. To distinguish them from other types of content, they were displayed with enriched textual snippets that concisely summarize the findings.8 Figure 3: Google uses the ClaimReview markup to display the fact-check as a rich text snippet in search results. Most recently, in June 2017, Google started presenting a daily collection of fact-checking articles on the desktop homepage of Google News as part of the News Desktop redesign. 9 In July 2017, Bing, the Microsoft search engine, published information for publishers about how to use ClaimReview that said the markup will be used for enhanced captions” in search results.10 An additional benefit of the markup is that structured summaries of the fact-checks can now be collected in a database that provides content for future applications. It is now possible to easily tap into the complete archive of articles by the world’s fact- checkers. 5 Live Pop-Up Fact-Checking We have made some early progress toward the goal of live pop-up fact-checking. When the television networks decided in the 2016 campaign that they were not going to do live fact-checking of presidential debates, the Duke Reporters’ Lab developed FactPopUp, a Chrome browser extension. FactPopUp is a manual tool to present short summaries of fact- checks on top of live video. It uses Twitter and Chrome’s notification feature to display text and images. A fact-checker – for the tests, it was PolitiFact editor Aaron Sharockman – listens to the event and sends a tweet when one of the speakers makes a factual claim that has been previously fact-checked. That triggers a box that pops up on the browser. We conducted beta tests of FactPopUp with mixed results. Our first test, during the final presidential debate of 2016, provided timely fact-checks after the candidates made factual claims. The web video of the debate was delayed about 15 seconds from the live” event on television, which provided sufficient time for the PolitiFact editor to find the relevant fact-check that had been previously published and then tweet it so the pop-up appeared just a few moments after the candidate said the claim on the web video. During the debate, FactPopUp provided about 10-12 high- quality notifications about fact-checks on the candidates. FactPopUp was less successful during the inaugural speech of President Donald Trump. There were only a few factual claims, and the live feed being used for the event happened to be running about 45 seconds behind live television. As a result, the PolitiFact editor triggered the pop-ups when he heard them on live TV, but on the web video, they appeared before Trump actually said the statement. Currently, we are working on the next generation of pop-up fact-checking where the task of matching previously done fact- checks is automated, allowing the public to benefit from the database of fact-checks collected through the ClaimReview schema on a much bigger scale. A user could ask our app to monitor a web page or a video or audio stream for matching claims, or search the database via text or voice. Going beyond keyword searches, our back-end system could make use of any additional contextual signals provided by the app, such as the stream URL being monitored and time into the stream, to improve matching quality. For some streams, the back-end system can obtain additional information useful to matching, such as full-text transcripts and annotations by human experts. Besides leveraging our growing database of fact-checks, our system also syncs with ClaimBuster. Search requests for specific claims indicate users find them check-worthy, and logs of such requests can be used by ClaimBuster as training data to improve its claim identification algorithms. Popularities of claims by request also serve an additional criterion with which human fact- checkers can decide what to check next. Our apps will allow users to subscribe to claims with no existing fact-checks or streams with ongoing fact-checking activities; users are notified as soon as new fact-checks become available. The app can also continue to track published fact- checks, and send any updates and corrections to users. 6 The Challenges and Prospects for Fully Automated Fact-Checking In the past two years we have made significant progress toward the Holy Grail.” ClaimBuster, iCheck and the ClaimReview schema have provided valuable lessons about the next steps toward fully automated fact-checking. The ClaimReview schema provides one key element for the Holy Grail”—a growing database of fact-checks organized as structured data and amenable to automated searching and matching. As we get buy-ins from major technology companies like Google, we are close to offering this part of the Holy Grail” to the public, maximizing the impact of the hard work that human fact-checkers have created. In the near future, we see this direction as one where we can make the most practical gain with the current technology, and our Share the Facts widget and development of a better pop-up” fact-checking app are important first steps. In the longer term, more study is needed on novel mechanisms for introducing the results of fact-checks to Progress Toward the Holy Grail”: The Continued Quest to Automate Fact-Checking Computation + Journalism Symposium, October 2017, Evanston, Illinois USA individuals—especially those with strong prior beliefs who may be less receptive to the results. Regardless of the final delivery mechanism—and there may be many alternatives—the system infrastructure that we are building will serve as a solid foundation. Besides disseminating the results of fact-checks, our work helping journalists produce more fact-checks remains a challenge, but we continue to make good progress and gain new insights. General, end-to-end automated systems are difficult. However, some steps of fact-checking are more amenable to general automation solutions than others. ClaimBuster has identified one sweet spot where AI can help learn what is check-worthy effectively. On the other hand, our experience with iCheck seems to indicate that checking a non-trivial statement automatically starting from just data still requires considerable work. With enough effort, we can probably achieve end-to-end automated fact-checking in specific domains for specific kinds of claims, but generalizing the success to other domains and claim types in a cost-effective manner remains challenging and would require long-term investment in collaborative research between journalists and computer scientists. To continue our research, we have formed the Tech & Check Cooperative, a team that includes our researchers from Duke University, the University of Texas at Arlington and Google, as well as new partners from the Internet Archive and California Polytechnic State University. The Tech & Check Cooperative has received a grant from the Knight Foundation to continue this important research. In addition to developing apps for live fact- checking and expanding the use of ClaimBuster, the team will communicate with other researchers doing work in this area.","Adair, B., Li, C., Yang, J., & Yu, C. (2017). Progress toward “the holy grail”: The continued quest to automate fact-checking. In Computation+ Journalism Symposium,(September)."
SSAFC_077,A Framework for Automated Fact-Checking for Real-Time Validation of Emerging Claims on the Web,"Literally minutes after the Las Vegas shooting, rumors about the identity and the motives of the perpetrator started to circulate on Twitter. Some of the most persistent ones were that the perpetrator was a Muslim convert [1] or a member of Antifa [2]. Moreover, also alleged reactions of people to the shooting received much attention. On a fake Twitter account, a liberal teacher was supposedly hoping that Trump supports were among the victims ([3]). When all the attention is on the event and not all details about the incident are known, propagandists have the opportunity to instrumentalize the event to promote a certain worldview. As the problem of false information being distributed on the web became more severe in the past couple of years, there is also an increased interest in information validation. Thus, fact-checking websites like politifact.com, fullfact.org, and snopes.com are becoming more popular. On these websites, journalists or professional factcheckers are manually resolving controversial claims, by providing a verdict, which is backed up by evidence. Nevertheless, even though manual fact-checking blossoms from the spread of fake news, the approach is rather mitigating the influence of false information rather than solving the problem. The resolution is often done subsequently after a fake news article has spread, although most of the damage is caused when the fake news article is distributed through social networks. In fact, many of the news consumers are not going to review the facts on a story once the spotlight of the media has shifted to a different topic. Thus, real-time fact-checking techniques are required, which would be able to intervene in the proliferation process in the early stages, before the false information goes viral. Many of the issues of manual claim validation can be addressed by automated fact-checking, as it would be possible to validate a large number of articles as they appear on the web automatically. To address the problem, a number of different approaches have been suggested, many of which are based on knowledge bases ([4, 5, 6]). These methods are validating a claim by verifying whether it is consistent with the knowledge base, that is, whether predicates can be found which basically restate the claim or contradict it. Nevertheless, knowledge bases only represent a small portion of all the information available on the web and newly generated knowledge is rare since the updating process requires some time. Thus, in particular for real-time fact-checking, methods based on raw text are more suitable, as they would allow recently published web documents to be incorporated into the validation process. However, claim validation on the basis of raw text has not yet received much attention and only a few studies address these issue. The task 8 in SemEval-2017 [7] was concerned with the problem of validating claims on Twitter. The claim itself was represented by a tweet and the problem was approach in two different settings: In the closed setting, the validation was done only on the basis of the features of the claim tweet itself. In the open setting, external information, in form of related Wikipedia articles and web documents, was provided. Both problem settings turned out to be too challenging for the applied methods since the majority baseline could not be beaten. The participants suggest implementing additional more discriminative features, like those used in sentiment analysis, or discriminative rules to further increase performance. A method for the identification and validation of simple claims about 16 statistical properties of countries is presented in [8]. The authors introduce a distantly supervised approach which is based on a knowledge base, as well as raw text input. The method is able to identify statistical claims with 60% precision and to validate these claims without explicit supervision. The framework for claim validation presented in [9] is to our knowledge the most comprehensive. The authors extracted 4856 claims and the verdicts for these claim from the fact-checking website snopes.com [10]. In order to collect external information for the resolution of the claims, the Google search engine was used. The developed system is able to determine the stance of a text with respect to a given claim, the credibility of sources and the validity of the claim. The authors report 80% accuracy for the claim validation task.",Computer Science,Proceeding,2017,Y,N,0,"Literally minutes after the Las Vegas shooting, rumors about the identity and the motives of the perpetrator started to circulate on Twitter. Some of the most persistent ones were that the perpetrator was a Muslim convert [1] or a member of Antifa [2]. Moreover, also alleged reactions of people to the shooting received much attention. On a fake Twitter account, a liberal teacher was supposedly hoping that Trump supports were among the victims ([3]). When all the attention is on the event and not all details about the incident are known, propagandists have the opportunity to instrumentalize the event to promote a certain worldview. As the problem of false information being distributed on the web became more severe in the past couple of years, there is also an increased interest in information validation. Thus, fact-checking websites like politifact.com, fullfact.org, and snopes.com are becoming more popular. On these websites, journalists or professional fact- checkers are manually resolving controversial claims, by providing a verdict, which is backed up by evidence. Nevertheless, even though manual fact-checking blossoms from the spread of fake news, the approach is rather mitigating the influence of false information rather than solving the problem. The resolution is often done subsequently after a fake news article has spread, although most of the damage is caused when the fake news article is distributed through social networks. In fact, many of the news consumers are not going to review the facts on a story once the spotlight of the media has shifted to a different topic. Thus, real-time fact-checking techniques are required, which would be able to intervene in the proliferation process in the early stages, before the false information goes viral. Many of the issues of manual claim validation can be addressed by automated fact-checking, as it would be possible to validate a large number of articles as they appear on the web automatically. To address the problem, a number of different approaches have been suggested, many of which are based on knowledge bases ([4, 5, 6]). These methods are validating a claim by verifying whether it is consistent with the knowledge base, that is, whether predicates can be found which basically restate the claim or contradict it. Nevertheless, knowledge bases only represent a small portion of all the information available on the web and newly generated knowledge is rare since the updating process requires some time. Thus, in particular for real-time fact-checking, methods based on raw text are more suitable, as they would allow recently published web documents to be incorporated into the validation process. However, claim validation on the basis of raw text has not yet received much attention and only a few studies address these issue. The task 8 in SemEval-2017 [7] was concerned with the problem of validating claims on Twitter. The claim itself was represented by a tweet and the problem was approach in two different settings: In the closed setting, the validation was done only on the basis of the features of the claim tweet itself. In the open setting, external information, in form of related Wikipedia articles and web documents, was provided. Both problem settings turned out to be too challenging for the applied methods since the majority baseline could not be beaten. The participants suggest implementing additional more discriminative features, like those used in sentiment analysis, or discriminative rules to further increase performance. A method for the identification and validation of simple claims about 16 statistical properties of countries is presented in [8]. The authors introduce a distantly supervised approach which is based on a knowledge base, as well as raw text input. The method is able to identify statistical claims with 60% precision and to validate these claims without explicit supervision. The framework for claim validation presented in [9] is to our knowledge the most comprehensive. The authors extracted 4856 claims and the verdicts for these claim from the fact-checking website snopes.com [10]. In order to collect external information for the resolution of the claims, the Google search engine was used. The developed system is able to determine the stance of a text with respect to a given claim, the credibility of sources and the validity of the claim. The authors report 80% accuracy for the claim validation task. 1 Document retrieval Claim Verdict Evidence extraction Claim Stance detection Claim Claim validation Claim Evidence 1 Evidence 2 Evidence 3 Evidence 4 Source 2 Source 3Source 1 Evidence 1 Evidence 2 Evidence 3 Evidence 4 Source 2 Source 3Source 1 Source 2 Source 3Source 1Figure 1: Pipeline for claim validation Nevertheless, despite significant progress in the field of natural lan- guage processing in the past couple of years, a fully automated system for claim validation, which is able to validate newly emerging claims on the web with high accuracy, is not yet feasible. Today’s approaches for automated fact-checking are still restricted in their capabilities and are only trained on small amounts of data. The validation process is very challenging and there are a number of abilities a system must have, such as the incorporation of world knowledge in the validation process or the ability to reason with known facts, which cannot be easily real- ized with today’s machine learning techniques. Thus, our objective, is therefore, to develop a system, which is able to assist a fact-checker in the validation process in order to speed up the procedure rather than taking over the job entirely. In order to address the described challenges, we are proposing a com- prehensive system for claim validation which has the following charac- teristics. For the reduction of the complexity of the problem, we divide the task into several subproblems and tackle them individually. As a result, also the transparency of the system is increased, which enables the fact-checker to comprehend why a particular verdict was predicted on the basis of the intermediate outputs of the subsystems. To address the problem of data sparsity in knowledge bases, we are developing a system which extracts its knowledge from web documents. This would enable the system to assess the veracity of a claim on a wide range of topics. The pipeline of the proposed system is illustrated in Figure 1. In the first step, relevant web documents for the resolution of a given claim, as well as the information about their sources, are retrieved. In the second step, evidence, which supports or refute the claims, is identified in the web documents. The stance of the evidence with respect to the claim is determined in the third step. In the fourth step, the actual claim validation is performed. The generated output of all three previous subsystems serves thereby as an input. Since our objective is to develop a system for automated fact-checking, which is transparent, the identification of evidence in the validation process is one of the main contributions. We are planning to find text snippets which are crucial for the interpretation of the verdict by the fact-checker, as well as for the machine learning model which comes up with the verdict. The development of the system for claim validation is currently in progress, and we have already implemented methods for evidence extraction, stance classification, and claim validation. The machine learning methods are trained on a corpus, which was constructed by crawling the snopes.com website [10]. In contrast to the study presented in [9], in addition to the claims and the verdicts, we have also collected evidence for each claim from the Snopes website, and the documents, from which the evidence have been extracted. Evidence extraction for automated claim validation is considered as a classification problem on the sentence level. We have found that for this task, feature-based classifiers, such as linear models and SVMs, outperform neural networks based on LSTMs ([11]). Nevertheless, even the linear model, which performed best, reached a relatively low F1 score of 55%. We believe that the low performance is due to a low upper bound for the task since the fact-checkers have not ensured that their annotations are reproducible. For stance detection, a feature-based multilayer perceptron ([12]) was used, which was one of the best performing models in the Fake News Challenge stance detection task [13]. We have implemented additional features for the model and have been able to increase the performance from 81.97% to 82.7% on the Fake News Challenge evaluation metric. For the claim validation, different LSTM network structures have been applied. We have found that regular BiLSTM and hierarchical BiLSTM models [14] perform well for the task and even outperform BiLSTM models with different kinds of attention. The highest F1 score of 66% was reached by the BiLSTM model.","Hanselowski, A., & Gurevych, I. (2017, October). A framework for automated fact-checking for real-time validation of emerging claims on the web. In NIPS 2017 Workshop on Prioritising Online Content. Long Beach, USA. url: https://www. k4all. org/wp-content/uploads/2017/09/WPOC2017_paper_6. pdf."
SSAFC_081,Effort-driven Fact Checking,"The Web constitutes a valuable source of information. In recent years, it fostered the construction of large-scale knowledge bases, such as Freebase, YAGO, and DBpedia, each storing millions of facts about society in general, and specific domains, such as politics or medicine. The open nature of the Web, with content potentially being generated by everyone, however, leads to inaccuracies and misinformation, such as fake news and exaggerated claims. Construction and maintenance of a knowledge base thus relies on fact-checking, assessing the credibility of facts. Due to the inherent lack of ground truth information, fact-checking cannot be done in a purely automated manner, but requires human involvement. In this paper, we propose a framework to guide users in the validation of facts, striving for a minimisation of the invested effort. Specifically, we present a probabilistic model to identify the facts for which manual validation is most beneficial. As a consequence, our approach yields a high quality knowledge base, even if only a sample of a collection of facts is validated. Our experiments with three large-scale datasets demonstrate the efficiency and effectiveness of our approach, reaching levels of above 90% precision of the knowledge base with only a third of the validation effort required by baseline techniques.",Computer Science,Proceeding,2019,N,Y,14,"Extracting factual knowledge from Web data plays an important role in various applications. For example, knowledge bases such as Freebase [fre 2017], YAGO [yag 2017] and DBpedia [dbp 2017] rely on Wikipedia to extract entities and their relations. These knowledge bases store millions of facts, about society in general as well as specific domains such as politics and medicine. Facts can be stored in various formats, reaching from unstructured text segments, through formal concepts and relations, to structured statements. In any case, extraction of factual knowledge first yields candidate facts (aka claims), for which the credibility needs to be assessed. Given the open nature of the Web, where content is potentially generated by everyone, fact extraction faces inaccuracies and misinformation, such as fake news and exaggerated claims. Hence, building a knowledge base from multiple Web sources does not only require conflict resolution and data cleansing [Dong et al. 2012], but also calls for methods to ensure the credibility of the extracted facts, especially in domains dealing with sensitive information, such as healthcare [Mukherjee et al. 2014]. To assess the credibility of facts extracted from the Web, automated methods rely on classification [Lehmann et al. 2012] or sensitivity analysis [Wu et al. 2014]. While these methods scale to the volume of Web data, they are hampered by the inherent ambiguity of natural language, deliberate deception, and domain-specific semantics. Consider the claims of ‘the world population being 7.5 billion’ or ‘antibiotics killing bacteria’. Both represent common-sense facts. Yet, these facts have been generated by complex statistical and survey methods and, therefore, cannot easily be inferred from other basic facts. For applications that rely on accurate facts, incorporating manual feedback is the only way to overcome the limitations of automated fact checking. However, eliciting user input is challenging due to several reasons. First, user input is expensive (in terms of time, cost, etc.), so that a validation of all claims is infeasible, even if one relies on a large number of users (e.g., by crowdsourcing [Hung et al. 2013]) and ignores the overhead to resolve disagreement among them. Second, claims are not independent, but connected in a network of Web sources. An assessment of their credibility therefore requires effective propagation of user input between correlated claims. Finally, there is a trade-off between the precision of a knowledge base (the ratio of credible facts) and the amount of user input: The more facts are checked manually, the higher the precision. However, user input is limited by some effort budget in most applications. Against this background, we aim at supporting a user in the validation of facts by means of a pay-as-you-go approach. While user input is incorporated continuously to improve the results of automatic fact checking, our ultimate goal is to instantiate an accurate knowledge base, even if not all claims have been validated. By (i) inferring the credibility of non-validated facts from those that have been validated, and by (ii) guiding a user in the validation process, we reduce the amount of manual effort needed to achieve a specific level of result precision. Both steps, credibility inference and user guidance, are interrelated. On the one hand, inference exploits mutual reinforcing relations between Web sources and claims, which are further justified based on the user input. On the other hand, a user is guided based on the potential effect of the validation of a claim for credibility inference. Our contributions are summarised as follows:2 Tam Nguyen • Fact checking model: §2 proposes a model for fact checking that combines automated credibility inference with input from a user who manually validates facts. We also formulate the problem of effort minimisation in fact checking and introduce an iterative approach to guide a user in the validation process. • Probabilistic credibility inference: §3 presents methods to construct a probabilistic knowledge base, to perform incremental inference based on user input, and to construct a trusted set of facts. • Probabilistic user guidance: §4 develops a strategy to choose the claims for which manual validation is most beneficial, dropping traditional assumptions about the trustworthiness of Web sources. We also combine credibility inference and user guidance in a comprehensive validation process. Furthermore, §5 evaluates our techniques with three large-scale datasets, demonstrating their efficiency and effectiveness. Finally, §6 discusses related work before §7 concludes the paper. 2 MODEL AND APPROACH Setting. We model the setting of fact checking by a set of data sources S = {s1, . . . , su }, a set of documents D = {d1, . . . , dm }, and a set of candidate facts, or short claims, C = {c1, . . . , cn }. A source could be a user, a website, a news provider, or a business entity. A document is often textual and provided by some source (e.g., a tweet, a news item, or a forum posting). Sources, documents, and claims jointly represent a fact database, denoted by a tuple N = ⟨S, D, C⟩. To construct an accurate knowledge base, the goal is to infer labels for every claim in C, where ci = 1 denotes that a fact ci is credible, whereas ci = 0 represents the opposite. User input is modelled by a fact checking function e : C → {0, 1, ⊖}. The label ⊖ denotes that a claim has not yet been validated. Our work employs a probabilistic model, where P (c = 1) denotes the probability that claim c is credible. Then, a probabilistic fact database is a tuple Q = ⟨N , e, P⟩, with e being a fact checking function and P assigning a credibility probability to all claims. The goal of fact checking is a deterministic assignment, a grounding function д : C → {0, 1} that assigns truth values to all claims. It then enables the construction of an accurate knowledge base from the claims that are deemed credible. Effort minimisation. While our objective is to instantiate a trusted set of credible facts, our work combines automated fact checking with input from users validating claims. However, such validation is costly, in terms of user hiring cost and time. Therefore, user input is commonly limited by an effort budget, which leads to a trade-off between validation accuracy and invested effort. Going beyond this trade-off, we aim at minimising the user effort invested to reach a given validation goal. We consider fact checking as an iterative process with a user validating the credibility of a claim in each iteration. This process halts either when reaching a validation goal or upon consumption of the available effort budget. The former relates to the desired result quality, e.g., a threshold on the estimated credibility of the deterministic assignment. The latter defines an upper bound for the number of validations by a user and, thus, iterations of the validation process. Formally, for a fact database N = ⟨S, D, C⟩, conducting fact checking leads to a sequence of deterministic assignments ⟨д0, д1, . . . , дn ⟩, termed a validation sequence. Each дi represents the assignment obtained after the i-th iteration of the validation process. Given an effort budget b and a validation goal ∆, we refer to a sequence ⟨д0, д1, . . . , дn ⟩ as being valid, if n ≤ b and dn satisfies ∆. Let R (∆, b) denote a finite set of valid validation sequences that can be created by instantiations of the validation process. Then, a validation sequence ⟨д0, д1, . . . , дn ⟩ ∈ R (∆, b) is minimal, if for any validation sequence ⟨д′ 0, д′ 1, . . . , д′ m ⟩ ∈ R (∆, b) it holds that n ≤ m. PROBLEM 1 (EFFORT MINIMISATION IN FACT CHECKING). Let ⟨S, D, C⟩ be a fact database and R (∆, b) a set of valid validation sequences for an effort budget b and a goal ∆. The problem of effort minimisation in fact checking is the identification of a minimal sequence ⟨д0, д1, . . . , дn ⟩ ∈ R (∆, b). As detailed above, a validation goal is commonly defined in terms of a threshold on the estimated credibility of the deterministic assignment. However, solving Problem 1 is challenging. Claims are not independent, but subject to mutual reinforcing relations with Web sources and documents. Consequently, the validation of one claim may affect the probabilistic credibility assessment of other facts. Furthermore, sources that try to spread misinformation influence fact checking and may distort the evaluation of a fact’s credibility. Finally, there is a computational challenge and finding an optimal solution to Problem 1 quickly becomes intractable: all permutations of all subsets (of size ≤ b) of claims would have to be explored. Manuscript submitted to ACM Effort-driven Fact Checking 3 Guided fact checking. To address the problem of effort minimisation in fact checking, we consider a process that guides a user in the validation of claims. The general idea of the validation process is summarised as follows: User input shall be sought solely on the ‘most promising’ unverified facts, i.e., those for which manual validation is expected to have the largest impact on the estimated credibility of the resulting deterministic assignment. Let Q = ⟨N , e, P⟩, with N = ⟨S, D, C⟩, be a probabilistic fact database. Then, the validation process continuously updates a deterministic assignment д of truth values to claims by: (1) selecting a claim c for which feedback shall be sought; (2) eliciting user input on the credibility of c, which is represented by the fact checking function, e (c); (3) inferring the implications of the user input on the probabilistic fact database Q; (4) deciding on the deterministic assignment д that captures the facts that are assumed to be credible. To realise the above process, steps (1), (3), and (4) need to be instantiated with specific methods. An example for a straight-forward instantiation would be a validation process that selects a claim randomly; limits the inference to the claim for which feedback has been sought; and decides that a claim c is credible, д(c) = 1, if and only if it holds P (c) ≥ 0.5. In the remainder of this paper, we present methods for a more elaborated instantiation of the above process that exploit the mutual reinforcing relations between Web sources and claims to infer the implications of user input and decide on the deterministic assignment. Furthermore, we also show how to select the claims for which manual validation is most beneficial. 3 PROBABILISTIC FACT CHECKING This section presents a probabilistic model for fact checking (§3.1). Based thereon, we introduce mechanisms for incremental inference (§3.2) and the instantiation of a deterministic assignment (§3.3). 3.1 A Probabilistic Model for Fact Checking Sources of uncertainty. A probabilistic fact database is constructed based on sources and documents, each encoded using a set of features, such as the frequency of updates at a source or linguistic characteristics of a document. Our model abstracts from the specific nature of these features, but takes into account that the trustworthiness of a source and the language quality of a document often have a strong influence on the credibility of the extracted claims. Each source sk is associated with a feature vector ⟨f S 1 (sk ), . . . , f S mS (sk )⟩ of mS source features. In the same vein, ⟨f d 1 (dj ), . . . , f d mD (dj )⟩ is a vector of mD document features assigned to each document dj . Features of sources and documents interact with each other, as well as the credibility of the extracted claims, so that we consider the following relations between them: • Causal relation: Claims are provided in a document by a source. Thus, a claim’s credibility depends on both trustworthiness of the source and the language quality of the document. A claim is more likely to be credible, if it is posted by a trustworthy source using confident and objective language. Yet, the intentions of a source, and thus its trustworthiness, may change over different contexts and hence documents. • Mutual relation: Causal relations are complemented by mutual relations that stem from overlapping sets of a sources, documents, and claims. Multiple documents, potentially from multiple sources, may contain the same claim. For example, if a source disagrees with a claim that is accepted by several sources, it shall be regarded as not trustworthy. To model these relations along with observed variables, we rely on a Conditional Random Field (CRF) model, see Fig. 1. Specifically, causal relations are captured by cliques in the CRF, while mutual relations are modelled by the overlap of cliques, which leads to the factorization of cliques to compute probability distributions. The Conditional Random Field model. We construct a CRF as an undirected graph of random variables S, D, C for sources, documents, and claims. There are three types of edges between vertices, those between a source and a document, between a document and a claim, and between a source and a claim. Hence, the CRF contains cliques of a source, a document, and a claim. Since the same claim can be provided in different documents, by the same or different sources, any random variable can be part in multiple cliques. Manuscript submitted to ACM 4 Tam Nguyen𝒔𝟏 𝒅𝟏 𝒅𝟐 𝒄𝟑𝒄𝟏 𝒔𝟐Sources Documents Claims 𝒄𝟐 𝒅𝟑 causal interaction mutual interactions Fig. 1. A fraction of probabilistic fact database We note that S and P are observed variables, while C represents output variables. Therefore, the model likelihood can be expressed in the form of a conditional distribution: Pr (C |D, S;W ) = 1 Z (D, S;W ) ∏ π ={ci ,dj ,sk } ∈Π φ (ci , dj , sk ;Wπ ) (1) where Π is the set of all possible cliques; ci , dj , sk are a claim, document, and source of a clique π , respectively; Z (D, S;W ) = ∑C ∏π φ (ci , dk , sk ;Wπ ) is a normalization constant to ensure that the sum of probabilities over all configurations of C is equal to one; and W = ⋃π ∈Π Wπ is the set of model parameters, used to control the effects of individual features. Using this model, we would like to compute the conditional distribution of C, given the source features and document features. This is realised by the log-linear model (aka logistic regression) that expresses the log of a potential function as a linear combination of associated features: log φ (ci , dj , sk ;Wπ ) = 1∑ b=0 wπ,b × Ib (ci ) + mD∑ t =1 wD π,t × f D t (dj ) + mS∑ t =1 wS π,t × f S t (sk ) (2) where Ib (ci ) is the indicator function of the current configuration of ci , i.e., Ib (ci ) equals to one, if b = ci , and is zero otherwise. Hence, we have different weights for each configuration of C and Wπ = {wπ,b , wD π,t , wS π,t } is the set of all weights. The above formalisation provides a simple model, in which the features of sources and documents are discriminative indicators for the credibility of the related claims. The weights of different features further enable us to tune their relative importance, as features vary between applications and shall be learned from labelled data. Handling opposing claims. Different Web documents may contain the same claim with opposite stances—support or refute it [Hasan and Ng 2014]—and a source is considered trustworthy, if it refutes an incorrect claim. A model that only captures that a claim was mentioned in a document by a source would neglect this aspect. However, incorporating such information explicitly would over-complicate the model, as the number of Web documents is commonly much larger than the number of claims (see our real-world datasets in §5). We therefore tackle this challenge by introducing an opposing variable ¬c for each claim c. Then, model complexity increases only slightly: C includes opposing claims, W contains a doubled number of parameters, and any document connects only to the positive or negative variable of a claim. However, Manuscript submitted to ACM Effort-driven Fact Checking 5 as c and ¬c cannot have the same truth value, we enforce a non-equality constraint, as follows: Pr (c, ¬c ′) =    0 if c = c ′ Pr (c, ¬c ′|D, S;W ) otherwise. (3) 3.2 Incremental Inference with User Input Using the above formalisation, we need to solve the following optimisation problem to infer model parameters: W ∗ = arg max W log Pr (CL |D, S;W ) = arg max W log ∑ CU Pr (CL, CU |D, S;W ) (4) where CL and CU are sets of labelled and unlabelled claims, respectively. The log-likelihood optimisation is convex, since the logarithm is a monotonically increasing function and the probability distribution is in exponential form. However, the problem becomes intractable due to the exponential number of configurations of CU . Moreover, upon receiving new user input, CL and CU change, so that re-computation is needed. Against this background, we propose an incremental inference algorithm, called iCRF , which follows the view maintenance principle [Blakeley et al. 1986]. Estimation of assignment correctness and model parameters exploits the results of the previous iteration of the validation process, thereby avoiding re-computation. As we will show experimentally, this does not only increase inference efficiency, but also yields a better approximation compared to random probability estimation. The iCRF algorithm implements the conclude function of the validation process introduced in §2. In the z-th iteration of the process, the input is given by the fact database N and the fact checking function ez , which has been updated with the user input received in the z-th iteration. When conducting the validation process, we always maintain the state of the previous iteration as follows: If ˆc is the claim validated in the z-th iteration of the validation process, we rely on the probabilistic fact database Qz−1 = ⟨N , ez−1, Pz−1⟩ to conduct the reasoning. Then, the set of unlabelled and labelled claims is updated, CU z = CU z−1 \ { ˆc} and CL z = CL z−1 ∪ { ˆc}, and the algorithms returns a new probabilistic fact database Qz = ⟨N , ez , Pz ⟩. In each iteration of the validation process, the iCRF algorithm adopts the Expectation-Maximization (EM) principle [Mc- Callum et al . 2005] for inference. The choice for EM is motivated by its generally fast convergence, computationally efficiency, and particular usefulness when the likelihood is an exponential function (i.e., maximising log-likelihood becomes maximising a linear function). Specifically, we infer the labels of the variables CU for unlabelled claims and learn the weight parameters W mutually. By relying on an EM-based approach, we can further naturally integrate user input on the credibility of specific claims. This is a major advantage compared to approaches based on gradient-descent [Mukherjee and Weikum 2015] that optimise model parameters, but do not enable the integration of user input and constraints (e.g., on opposing claims as introduced above). Technically, inference is iterative and alternates between an Expectation step (E-step) and a Maximization step (M-step), until convergence. EM-based inference is conducted in each iteration of the validation process, while each EM iteration updates the model parameters W . Therefore, in the z-th iteration of this process, we obtain a sequence W 0 z ,W 1 z , . . . ,W t z of model parameters and a sequence P0 z , P1 z , . . . , Pt z of credibility probabilities assigned to claims. E-step: We estimate the credibility probabilities from the current parameter values. The first E-step of the z-th iteration of the validation process is based on parameters W 0 z , given as input from the previous iteration of the validation process, i.e., W 0 z = W tz−1 z−1 , with tz−1 as the number of EM iterations in the z − 1-th iteration of the validation process. In the t-th E-step of the z-th step of the validation process, credibility probabilities are computed as follows: (1) Obtain a sequence of samples Ωt by performing Gibbs sampling according to the conditional probability distribution: qt z (CU ) = Pr (CU |CL, D, S;W t z ) ∝ ∏ π ={ci ,dj ,sk } ∈Π Pr t −1 z (c) × φ (ci , dj , sk ;W t z ) (5) We incorporate non-equality constraints (Eq. 3) into Gibbs sampling using an idea similar to [Schmidt 2009], which, based on matrix factorisation, embeds constraints as factorised functions into the Markov chain Monte Carlo process. Manuscript submitted to ACM 6 Tam Nguyen Note that Ωt is a sequence, as any configuration of C can appear multiple times. We weight the influence of causal interactions (i.e., cliques) by the credibility of their contained claims, so that user input is propagated via mutual interactions (i.e., overlaps) between the cliques. (2) Compute the probability from the Gibbs samples, for each claim c for which no user input has been received (ez (c) = ⊖): Pr t z (c) = ∑C ∈Ωt 1c=1 |Ωt | (6) For all other claims (ez (c) , ⊖), the probability is fixed: Pr t z (c) =    1 if ez (c) = 1 0 otherwise. (7) M-step: We compute the new parameter values by maximizing the expectation of log-likelihoods as a weighted average of the probability distribution of current label estimates. That is, in the t-th M-step of the z-th step of the validation process, we have: W t +1 z = arg max W ′ ∑ CU qt z (CU ) log Pr (CL, CU |D, S;W ′) (8) This step is implemented by a L2-regularized Trust Region Newton Method [Lin et al. 2008], suited for large-scale data, where critical information is often sparse (many feature values may be zero). The process converges when the difference between two consecutive estimates of parameters is insignificant. 3.3 Instantiation of Deterministic Assignment After incorporating the user input of the z-th iteration of the validation process, a deterministic assignment may be instantiated by predicting the truth values of all claims. Since claims are not independent, we take the truth configuration with maximal joint probability: дz (CU ) = arg max CU Pr (CU |CL z , D, S;Wz ) (9) However, solving this equation is similar to solving a boolean satisfiability problem. Therefore, we may simply leverage the most recent Gibbs sampling result Ωz obtained during EM for instantiation: дz (CU ) = arg max CU ∑ C ∈Ωz 1CU =C (10) We break ties randomly. For validated claims, the user input is directly incorporated, i.e., дz (CL ) = ez (CL ). 4 USER GUIDANCE IN FACT GUIDING Having discussed a model and techniques for (i) inference based on user input and (ii) instantiation of a deterministic assignment, we now turn to strategies to guide a user in the validation. This section first defines a measure of uncertainty for a probabilistic fact database (§4.1). Subsequently, we introduce two approaches to guide the selection of claims for validation (§4.2 and §4.3), before combining them in a hybrid approach (§4.4). 4.1 Uncertainty of a Probabilistic Fact Database The model of a probabilistic fact database, as constructed above, enables us to quantify the uncertainty related to credibility inference in order to guide a user in the validation process. Let Q = ⟨N , e, P⟩, with N = ⟨C, D, S⟩, be a probabilistic fact database. Recall that Q defines the likelihood of credibility of each claim, i.e., P = ∪c ∈C Pr (c). Then, the overall uncertainty of the database is computed by the Shannon entropy [Shannon 2001] over a set of claims as random variables: HC (Q ) = − ∑ C ∈Ω Pr (C;W ) log Pr (C;W ) (11) where Ω are all possible configurations of C. In our iCRF model, it can be computed exactly by [Reyes 2013; Reyes and Neuhoff 2009]: H (C;W ) = Φ(W ) − EW [t (C)]T W (12) Manuscript submitted to ACM Effort-driven Fact Checking 7 where Φ(W ) = ∑C ∏π φ (ci , dk , sk ;W ) is called the partition function and EW [t (C)] = ∇Φ(W ). Since our model is an acyclic graph with no self statistics, the partition function is computed exactly using Ising methods [Reyes 2013], which run in polynomial time. We can further scale-up uncertainty computation by approximating the entropy in linear time, as follows: HC (Q ) = − ∑ c ∈C [Pr (c) log Pr (c) + (1 − Pr (c)) log(1 − Pr (c))] (13) where the probability of each claim is obtained after the EM iterations (i.e., Eq. 6 and Eq. 7). However, this approximation neglects the mutual dependencies between claims and, thus, potentially underestimates the effects of user input on their credibility. 4.2 Uncertainty-driven User Guidance A first heuristic to guide the selection of claims for validation aims at the maximal reduction in uncertainty under the assumption of trustworthy sources. It exploits the benefit of validating a single claim using the notion of information gain from information theory [Russell and Norvig 2003]. To capture the impact of user input on a claim c, we define a conditional variant of the entropy measure introduced earlier. Informally, it measures the expected entropy of the probabilistic fact database under specific validation input: HC (Q | c) = Pr (c) × HC (Q+ ) + (1 − Pr (c)) × HC (Q− ) (14) where Q+ = conclude(N , e ′) (and Q− = conclude(N , e ′)) is constructed by the incremental learning with e ′(c) = 1 (and e ′(c) = 0) and e ′(c ′) = e (c ′) for c ′ ∈ (C \ {c}). To take a decision on which claim to select, we assess the expected difference in uncertainty before and after incorporating input for a claim. The respective change in entropy is the information gain that quantifies the potential benefit of knowing the true value of an unknown variable [Russell and Norvig 2003], i.e., the truth value in our case: IGC (c) = HC (Q ) − HC (Q | c). (15) The information gain can guide the selection of a claim: we chose the one that is expected to maximally reduce the uncertainty of the probabilistic fact database. This is formalized by a selection function for uncertainty-driven user guidance: selectC (C ′) = arg max c ∈C′ IGC (c) (16) where C ′ ⊆ C is the set of non-validated claims. While this formulation is sound for ranking a whole set of claims, validated claims will be ranked last (since their credibility probability is either one or zero). Also, we do not need to rank the opposing claim ¬c of a claim c, as their conditional entropies in Eq. 14 will be equivalent. 4.3 Source-driven User Guidance User guidance as introduced above assumes that sources are trustworthy—an assumption that is often violated in practice. To tackle this issue, we model source trustworthiness by explicitly aggregating over all claims made by a source. More precisely, the likelihood that a source is trustworthy is measured as the fraction of its claims that are considered as true in the last EM iteration: Pr (s = 1) = ∑c ∈Cs 1c=1 |Cs | (17) where Cs = {c ∈ C |(c, s) ∈ Π} is the set of claims connected to s in the CRF model. Then, the uncertainty of source trustworthiness values is defined as: HS (Q ) = − ∑ s ∈S [Pr (s) log Pr (s) + (1 − Pr (s)) log(1 − Pr (s))] (18) The conditional entropy when a claim c is validated is: HS (Q |c) = Pr (c) × HS (Q+ ) + (1 − Pr (c)) × HS (Q− ) (19) where, as above, Q+ = conclude(N , e ′) (and Q− = conclude(N , e ′)) is constructed by the incremental learning with e ′(c) = 1 (and e ′(c) = 0) and e ′(c ′) = e (c ′) for c ′ ∈ (C \ {c}). Manuscript submitted to ACM 8 Tam Nguyen As for the case of the first heuristic, we further capture the information gain as the difference in entropy and, based thereon, define the selection function for source-driven user guidance: IGS (c) = HS (Q ) − HS (Q |c) (20) selectS (C ′) = arg max c ∈C′ IGS (c) (21) where C ′ ⊆ C is the set of non-validated claims. Again, we do not need to rank opposing claims. 4.4 A Combined Approach to User Guidance There is a trade-off between the application of the uncertainty-driven and the source-driven strategies for user guidance. Focusing solely on the former may lead to contamination of the claims from trustworthy sources by unreliable sources. An excessively source-driven approach, in turn, is undesirable as it may increase the overall user efforts significantly. Therefore, we propose a dynamic weighting procedure that, in each iteration of the validation process, helps to choose among the two strategies. Weighting procedure. Two factors affect the choice of strategies: Ratio of untrustworthy sources. If there is a high number of unreliable sources, the source-driven strategy is preferred. With little user input, detection of unreliable sources is difficult, though, so that the uncertainty-driven strategy is favoured in the beginning. Error rate. The deterministic assignment дi captures the assignments considered to be correct in the i-th iteration of the validation process. If дi turns out to be mostly incorrect, we have evidence of unreliable sources and, thus, favour the source-driven strategy. Both factors combined lead to a dynamic strategy. Initially, with little user input, the strategy is primarily chosen on the error rate of the deterministic assignment. At later stages, the number of inferred unreliable sources becomes the dominant factor. The above idea is formalised based on the ratio of unreliable sources in the i-th iteration of the validation process, which is ri = (|{s ∈ S |Pr (s) < 0.5}|)/(|S |). The error rate of the deterministic assignment is computed by comparing the user input for claim c in the i-th iteration with the truth value that has been assigned to c in дi−1, i.e., in the previous iteration. Here, we leverage the probability Pi−1 (c) of the probabilistic fact database Qi−1 = ⟨N , ei−1, Pi−1⟩, of the previous iteration. If дi−1 assigns a credibility v ∈ {0, 1} to c, the error rate is computed as: εi = 1 − Pri−1 (c = v) (22) Using the ratio of unreliable sources ri and the error rate εi , we define a normalised score for choosing the source-driven strategy: zi = 1 − e−(εi (1−hi )+ri hi ) (23) where hi = (i)/(|C |) is the ratio of user input. This score mediates the trade-off between the error rate εi and the ratio of untrustworthy sources ri by the ratio of user input hi . When the ratio hi is small, the ratio of untrustworthy sources has less influence and the error rate is the dominant factor. When the ratio hi is large, the ratio of unreliable sources becomes a more dominant factor. Validation procedure. Combining the model and inference mechanism introduced in §3 with the above strategies to guide a user, we present our complete validation process for fact checking in Alg. 1. As long as the validation goal is not reached and the user effort budget has not been exhausted (line 5), selection of the claim for which user input shall be sought is done either by the source-driven or the uncertainty-driven strategy. The choice between strategies is taken by comparing factor zi to a random number (line 7), which implements a roulette wheel selection [Goldberg 1989]. The second step (lines 11 and 12) elicits user input for the selected claim and computes the error rate according to Eq. 22. The third step (line 13-16) incorporates the user input. That is, we update the fact checking function ei+1; infer the implications of user input by means of function conclude, which yields a new probabilistic fact database; and decide on the new deterministic assignment дi+1 capturing which facts are considered credible. Further, the trustworthiness of each source is updated and the ratio of unreliable sources ri is calculated to compute score zi+1 (lines 18-19), which is used in the next iteration to choose between the selection strategies. Manuscript submitted to ACM Effort-driven Fact Checking 9 Algorithm 1: Validation process for fact checking input :an probabilistic fact database, Q = ⟨N , e, P ⟩, with N = ⟨C, D, S ⟩, a validation goal ∆, and a user effort budget b. output :the truth assignment д. 1 e0 ← (c 7 → ⊖, c ∈ C ); 2 P0 ← conclude(N , e0 ); 3 д0 ← decide(P0 ); 4 i, z0 ← 0; 5 while not ∆ ∧ i ≤ b do // (1) Selecting a claim to validate 6 x ← r andom(0, 1); 7 if x < zi then // Choosing the source-driven strategy 8 c ← selectS ( {c′ ∈ C | ei (c′) = ⊖}) 9 else // Choosing the uncertainty-driven strategy 10 c ← selectC ( {c′ ∈ C | ei (c′) = ⊖}) // (2) Eliciting user input 11 Elicit user input v ∈ {0, 1} on c; 12 εi = 1 − Pi −1 (c = v ); // Calculate error rate εi // (3) Incorporating user input 13 ei +1 ← (c 7 → v ∧ c′ 7 → ei (c′), c′ ∈ C, c′ , c ) ; 14 Qi +1 ← conclude(N , ei +1 ); 15 дi +1 ← (c′ 7 → decide(Pi +1 ), c′ ∈ C, ei +1 (c′) = ⊖ ∧ c′ 7 → ei +1 (c′), c′ ∈ C, ei +1 (c′) , ⊖) ; 16 i ← i + 1; 17 Update source trustworthiness P r (s ), ∀s ∈ S ; 18 Calculate ratio of unreliable sources ri ; 19 zi +1 = 1 − e− ( εi ( 1− i |C | ) +ri i |C | ) ; 20 return di ; 5 EVALUATION In this section, we evaluate the proposed approach experimentally, using real-world datasets. We first discuss the experimental setup (§5.1), before turning to an evaluation of the following aspe","Nguyen, T. T. (2019). Effort-driven Fact Checking (No. REP_WORK)."
SSAFC_092,User Guidance for Efficient Fact Checking,"The Web constitutes a valuable source of information. In recent years, it fostered the construction of large-scale knowledge bases, such as Freebase, YAGO, and DBpedia. The open nature of the Web, with content potentially being generated by everyone, how- ever, leads to inaccuracies and misinformation. Construction and maintenance of a knowledge base thus has to rely on fact checking, an assessment of the credibility of facts. Due to an inherent lack of ground truth information, such fact checking cannot be done in a purely automated manner, but requires human involvement. In this paper, we propose a comprehensive framework to guide users in the validation of facts, striving for a minimisation of the invested effort. Our framework is grounded in a novel probabilistic model that combines user input with automated credibility inference. Based thereon, we show how to guide users in fact checking by identifying the facts for which validation is most beneficial. More- over, our framework includes techniques to reduce the manual effort invested in fact checking by determining when to stop the validation and by supporting efficient batching strategies. We further show how to handle fact checking in a streaming setting. Our experiments with three real-world datasets demonstrate the efficiency and effectiveness of our framework: A knowledge base of high quality, with a precision of above 90%, is constructed with only a half of the validation effort required by baseline techniques.",Computer Science,Proceeding,2019,N,Y,21,"Extracting factual knowledge from Web data plays an important role in various applications. For example, knowledge bases such as Freebase [3], YAGO [7] and DBpedia [1] rely on Wikipedia to extract entities and their relations. These knowledge bases store millions of facts, about society in general as well as specific domains such as politics and medicine. Independent of the adopted format to store facts, extraction of factual knowledge first yields candidate Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Articles from this volume were invited to present their results at The 45th International Conference on Very Large Data Bases, August 2019, Los Angeles, California. Proceedings of the VLDB Endowment, Vol. 12, No. xxx Copyright 2018 VLDB Endowment 2150-8097/18/10... $ 10.00. DOI: facts (aka claims), for which the credibility needs to be assessed. Given the open nature of the Web, where content is potentially gen- erated by everyone, extraction of claims faces inaccuracies and mis- information. Hence, building a knowledge base from Web sources does not only require conflict resolution and data cleansing [23], but calls for methods to ensure the credibility of the extracted claims, especially in sensitive domains, such as healthcare [48]. To assess the credibility of claims, automated methods rely on classification [41] or sensitivity analysis [66]. While these methods scale to the volume of Web data, they are hampered by the inherent ambiguity of natural language, deliberate deception, and domain- specific semantics. Consider the claims of ‘the world population being 7.5 billion’ or ‘antibiotics killing bacteria’. Both represent common-sense facts. Yet, these facts have been derived from com- plex statistical and survey methods and, therefore, cannot easily be inferred from other basic facts. When relying on accurate facts, incorporating manual feedback is the only way to overcome the limitations of automated fact checking. However, eliciting user input is challenging. User input is expensive (in terms of time and cost), so that a validation of all claims is infeasible, even if one relies on a large number of users (e.g., by crowdsourcing) and ignores the overhead to resolve disagreement among them. Also, claims are not independent, but connected in a network of Web sources. An assessment of their credibility thus requires effective propagation of user input between correlated claims. Finally, there is a trade-off between the precision of a knowledge base (the ratio of credible facts) and the amount of user input: The more claims are checked manually, the higher the precision. However, user input is commonly limited by some budget. This paper presents a comprehensive framework for guiding users in fact checking, adopting a pay-as-you-go approach. We present a novel probabilistic model that enables us to reason on the credibility of facts, while new user input is continuously incorporated. By (i) inferring the credibility of non-validated facts from those that have been validated, and by (ii) guiding a user in the validation process, we reduce the amount of manual effort needed to achieve a specific level of result precision. Credibility inference and user guidance are interrelated. Inference exploits mutual reinforcing relations between Web sources and claims, which are further justified based on user input. Moreover, a user is guided based on the potential effect of the validation of a claim for credibility inference. Efficient user guidance further requires to decide: (i) when to terminate validation to avoid wasting resources on marginal im- provements of the quality of the knowledge base; (ii) how to group claims for batch processing to reduce the impact of set-up costs in validation (a user familiarising with a particular domain); and (iii) how to handle continuous arrival of new data to avoid redundant computation. Our novel model enables us to address these aspects Our contributions are summarised as follows: • Approach to Guided Fact Checking: §2 formalises the setting of fact checking and, based thereon, formulates the problem of ef- fort minimisation. We further introduce an iterative approach to guide a user in the validation process and highlight requirements for its instantiation. • Probabilistic credibility inference: §3 addresses the need for a method to reason on the credibility of facts. We introduce a probabilistic model for fact checking, based on Conditional Random Fields, and show how to perform incremental inference based on user input. Aiming at pay-as-you-go validation, we show how to derive a trusted set of facts based on our model. • Probabilistic user guidance: §4 presents strategies to guide users, i.e., to select the claims for which validation is most beneficial. These strategies target the reduction of uncertainty in our probabilistic model for fact checking. • Complete validation process: §5 combines our mechanisms for credibility inference and user guidance to obtain a comprehen- sive validation process. We also show how to achieve robustness against erroneous user input. • Methods for effort reduction: §6 introduces techniques for early termination of the validation process and batch selection. The former is based on signals that indicate convergence of our prob- abilistic model and, thus, of the quality of the derived knowledge base. The latter selects groups of claims for validation based on the benefit of their joint validation. Since this selection problem turns out to be intractable in practice, we propose a greedy top-k algorithm, which comes with performance guarantees. • Streaming fact checking: §7 shows how to handle continuously arriving data by an adaptation of our validation process that fea- tures stochastic approximation and reuse of model parameters. We evaluate our techniques with three large-scale datasets (§8) of real-world claims. We demonstrate low response times for claim selection (<0.5s) and high effectiveness of guiding users in their val- idation efforts. To obtain a knowledge base of high quality (>90% precision), only a half of the effort of baseline techniques is required. Finally, we review related work (§9) and conclude (§10). 2. GUIDED FACT CHECKING 2.1 Setting We model the setting of fact checking by means of a set of data sources S = {s1, . . . , su}, a set of documents D = {d1, . . . , dm}, and a set of candidate facts, or short claims, C = {c1, . . . , cn}. A source could be a user, a website, a news provider, or a business entity. It provides multiple documents, each often being textual (e.g., a tweet, a news item, or a forum posting) and involving a few claims. The representation of a claim (e.g., unstructured text or an RDF triple) is orthogonal to our model. However, a claim can be referenced in multiple documents, it depends on a specific process for information extraction how the link between claims and documents is established (see §8.1). A claim c ∈ C represents a binary random variable, where c = 1 and c = 0 denote that the claim is credible or non-credible, respectively. In fact checking, however, these values are not known, so that we consider a probabilistic model P , where P (c = 1), or P (c) for short, denotes the probability that claim c is credible. Combining the above notions, the setting of fact checking is a tuple Q = 〈S, D, C, P 〉, also referred to as a probabilistic fact database. A knowledge base is constructed from such a database by deriving a trusted set of facts. We formalise this construction by a grounding function g : C → {0, 1}, labelling claims as credible (g(c) = 1) or non-credible (g(c) = 0). In fact checking, claims are validated manually by a user, which is represented by a binary model of user input. A claim c is either confirmed as credible, which yields P (c) = 1, or labelled as non- credible, so that P (c) = 0. As an example, consider the Snopes dataset [9], a collection of 4856 claims derived from 80421 documents of 23260 sources, such as news websites, social media, e-mails, etc. For instance, this dataset comprises the claim that eating turkey makes people especially drowsy. This claim can be found in documents of various Web sources, among them earthsky.org [2], webmd.com [6], and kidshealth.org [4]. In the Snopes dataset, claims have been validated by expert editors, which corresponds to the user input in our model. It labels the aforementioned example claim as non-credible [5]. 2.2 Effort Minimisation Adopting the above model, the grounding g to derive a trusted set of facts is partially derived from user input. However, manual validation of claims is expensive, in terms of user hiring cost and time. User input is commonly limited by an effort budget, which leads to a trade-off between validation accuracy and invested effort. Going beyond this trade-off, we aim at minimising the user effort invested to reach a given validation goal. We consider fact checking as an iterative process with a user validating the credibility of a single claim in each iteration. This process halts either when reaching a validation goal or upon consumption of the available effort budget. The former relates to the desired result quality, e.g., a threshold on the estimated credibility of the grounding. The latter defines an upper bound for the number of validations by a user and, thus, iterations of the validation process. Formally, given a probabilistic fact database 〈S, D, C, P 〉, fact checking induces a validation sequence, a sequence of groundings 〈g0, g2, . . . , gn〉 obtained after incorporating user input as part of n iterations of a validation process (i.e., any gi is a prediction of the model). Given an effort budget b and a validation goal ∆, a sequence 〈g0, g1, . . . , gn〉 is valid, if n ≤ b and gn satisfies ∆. Let R(∆, b) denote a finite set of valid validation sequences that can be created by instantiations of the validation process. Then, a validation sequence 〈g0, g1, . . . , gn〉 ∈ R(∆, b) as minimal, if n ≤ m for any validation sequence 〈g′ 0, g′ 1, . . . , g′ m〉 ∈ R(∆, b). Problem 1 (Effort Minimisation). Let 〈S, D, C, P 〉 be a probabilis- tic fact database and R(∆, b) a set of valid validation sequences for an effort budget b and a goal ∆. The problem of effort minimi- sation in fact checking is the identification of a minimal sequence 〈g0, g1, . . . , gn〉 ∈ R(∆, b). The validation goal could be the precision of the final grounding gn, estimated by cross validation. Note that, in theory, Problem 1 could have no solution—the effort budget may be too small or the validation goal may be unreachable. However, for practical reasons, there needs to be a guarantee that the validation process terminates. Solving Problem 1 is challenging, mainly for two reasons. First, claims are not independent, but subject to mutual reinforcing rela- tions with Web sources and documents. Consequently, the validation of one claim may affect the probabilistic credibility assessment of other facts. Second, the problem is computationally hard: Finding an optimal solution quickly becomes intractable, since all permutations of all subsets (of size ≤ b) of claims would have to be explored. 2.3 Outline of the Validation Process To address the problem of effort minimisation, we argue that a user shall be guided in the validation of claims. In essence, user input shall be sought solely on the ‘most promising’ unverified facts, i.e., those for which manual validation is expected to have the largest impact on the estimated credibility of the resulting grounding. 2 Let 〈S, D, C, P 〉 be a probabilistic fact database. Our validation process continuously updates the grounding g to validate claims in a pay-as-you-go manner, by: (1) selecting a claim c for which feedback shall be sought; (2) eliciting user input on the credibility of c, which either confirms it as credible or labels it as non-credible; (3) inferring the implications of user input on the probabilistic credibility model P ; (4) deciding on the grounding g that captures the facts that are assumed to be credible. In the above process, steps (1), (3), and (4) need to be instantiated with specific methods. An example for a straight-forward instantia- tion would be a validation process that: • selects a claim c randomly for validation; • limits the inference to claim c, setting either P (c) = 1 or P (c) = 0, not changing P (c′) for any claim c′ 6 = c; • decides that a claim c is credible, g(c) = 1, if and only if it holds P (c) ≥ 0.5. In the remainder, we present methods for a more elaborated instanti- ation of the above process. We introduce a probabilistic model for fact checking that captures the mutual reinforcing relations between Web sources and claims. This enables us to infer the implications of user input beyond the claims that have been validated, and based thereon, decide on the grounding while incorporating the relations between sources and claims. Also, the model enables conclusions on the claims that shall be selected. Unverified claims for which validation is most beneficial for the inference will be chosen. Our model further helps to identify suspicious user input, i.e., claims that may have been validated by mistake. We then address aspects of practical relevance, which are not captured in Problem 1. Validation may converge before the vali- dation goal is reached and the effort budget has been spent. If so, further user input leads to diminishing improvements of the quality of the grounding and the validation process may be terminated. We show how our model enables the detection of such scenarios by decision-support heuristics. In practice, users that validate claims face significant set-up costs, implied by the need to familiarise with claims of a particular domain. It therefore increases user convenience and efficiency if the valida- tion process considers a batch of claims per iteration. We support such batching by a greedy top-k strategy to select a set of claims with a high joint benefit for credibility inference. Moreover, in many applications, new sources, documents, and claims arrive continuously. We thus illustrate how the above process can be lifted to a streaming setting by exploiting online algorithms for inference and reusing parameters of our underlying model. 3. CREDIBILITY INFERENCE This section presents a probabilistic model for fact checking (§3.1), before turning to mechanisms for incremental inference (§3.2) and the instantiation of a grounding (§3.3). 3.1 A Probabilistic Model for Fact Checking Sources of uncertainty. Claims are assessed by means of docu- ments from Web sources. These documents are encoded using a set of features. We abstract from the specific nature of these features, but take into account that the trustworthiness of a source and the language quality of a document have a strong influence on the credi- bility of the claims. We capture these features as follows. A source s ∈ S is associated with a feature vector 〈f S 1 (s), . . . , f S mS (s)〉 of mS source features. In the same vein, 〈f D 1 (d), . . . , f D mD (d)〉 is a vector of mD document features, assigned to each document d ∈ D. Features of sources and documents interact with each other, and with the credibility of claims. A claim’s credibility depends on both, the trustworthiness of the source and the language quality of the document, which we call a direct relation. A claim is more likely to be credible, if it is posted by a trustworthy source using objective language. Yet, the intentions of a source, and thus its trustworthiness, may change over different contexts and hence documents. Therefore, we also reason about the credibility of claims via an indirect relation, exploiting that documents of different sources may refer to the same claim. For example, a source disagreeing with a considered credible by several sources shall be regarded as not trustworthy.6 _x0001__x0002_ _x0003__x0002_ _x0003__x0004_ _x0005__x0006__x0005__x0002_ _x0001__x0004_Sources Documents Claims _x0005__x0004_ _x0003__x0006_ direct relation indirect relation Figure 1: Relations in a probabilistic fact database. The Conditional Random Field model. To model these relations, and eventually derive the assignment of credibility probabilities, we rely on a Conditional Random Field (CRF) [25], see Fig. 1. We construct a CRF as an undirected graph of three sets of random variables, S, D, C for sources, documents, and claims. Here, S and D are sets of real-valued variables that represent trustworthiness of sources and language quality of documents, respectively, based on the aforementioned features. Set C is the set of binary variables introduced in §2.1, each variable representing a claim’s credibility. Direct relations are captured by relation factors in the CRF, also called cliques since they always involve three random variables (source, document, claim). Any random variable can be part in multiple cliques, reflecting the indirect relations. This implies a factorization of cliques to compute the joint probability distribution. In this model, S and D are observed variables. As an output vari- able, we consider a categorical variable C that represents credibility configurations of claims. A possible value o of C, called configura- tion, is an assignment o : C → {0, 1}, such that each variable c ∈ C is assigned the value o(c). Considering these variables, the model likelihood is expressed in the form of a conditional distribution, tailored from the generic form of a CRF [25]: P r(C = o | D, S; W ) = 1 Z ∏ π={c,d,s}∈Π φ(c = o(c), d, s; Wπ ) (1) where Π is the set of all cliques in the CRF; c, d, s are the claim, doc- ument, and source of a clique π, respectively; Z = ∑ c∈C ∏ π∈Π φ(c = o(c), d, s; Wπ ) is a normalisation constant to ensure that the probabilities over all configurations of C sum up to one; and W = ⋃ π∈Π Wπ is the set of model parameters controlling the effects of individual features. Using this model, we shall compute the conditional distribution of C, given the source and document features. This is realised by the log-linear model (aka logistic re- gression) that expresses the log of a potential function as a linear combination of features, instantiated from its generic form [25]: log φ(c = o(c), d, s; Wπ ) = wπ,o(c) + mD∑ t=1 wD π,t × f D t (d) + mS∑ t=1 wS π,t × f S t (s). (2) Hence, we have different weights for each configuration of C and Wπ = {wπ,0, wπ,1, wD π,t, wS π,t} is the set of all weights. 3 The above formulation is motivated by the CRF being a special case of log-linear models, which, extending logistic regression, are suitable for structured learning tasks [38, 25]. In our setting, the data has an internal structure via the relations between sources, doc- uments, and claims. Exploiting these relations, however, means that the inference of model parameters becomes complex. Hence, the po- tential function needs to be computationally efficient to enable user interactions in the validation process. A log-linear model enables efficient computation, while, at the same time, provides a compre- hensive model, in which the features of sources and documents are discriminative indicators for the credibility of the related claims. The weights enable tuning of feature importance, as features vary between applications and shall be learned from labelled data. Handling opposing stances. Documents may link the same claim with opposite stances—support or refute it [28]—and a source is considered trustworthy, if it refutes an incorrect claim. A model that only captures that a claim is part of a document would neglect this aspect. Yet, incorporating such information via a new type of random variable would mean that the number of variables is larger than or equal to the number of documents, which is much larger than the number of claims (see §8). We therefore introduce an opposing variable ¬c for each claim c. Then, model complexity increases only slightly: Configurations of C include opposing claims, W contains a doubled number of parameters, and any document connects only to the positive or negative variable of a claim. As c and ¬c cannot have the same credibility value, we enforce a non-equality constraint: P r(c, ¬c′) = { 0 if c = c′ P r(c, ¬c′|D, S; W ) otherwise. (3) 3.2 Incremental Inference with User Input Using the above formalisation, we further distinguish the set CL ⊆ C of validated, or labelled, claims. It contains all claims c for which, based on user input, we set P (c) = 1 in the probabilistic fact database. In the same vein, CU = C \ CL is the set of unlabelled claims. Based thereon, we define restricted variants of the categori- cal random variable C that represents credibility configurations of claims: CU and CL are variables for configurations involving solely the unlabelled claims of CU or the labelled claims of CL, respec- tively. Then, we need to solve the following optimisation problem to infer model parameters (as usual, P r(X) is the probability of one value of a categorical random variable X), derived from the principle of maximum likelihood [25]: W ∗ = arg max W log P r(CL | D, S; W ) (4) = arg max W log ∑ CU P r(CL, CU | D, S; W ). (5) The log-likelihood optimisation is convex, since the logarithm is monotonically increasing and the probability distribution is in expo- nential form. However, the problem becomes intractable due to the exponential number of configurations to consider for the random variable CU . Moreover, upon receiving new user input, CL and CU , and hence CL and CU change, so that re-computation is needed. Requirements for model inference. To be useful in our setting, an inference algorithm must meet two requirements. First, user input on correspondences should be a first class citizen. By propa- gating which claims have been validated, credibility probabilities can be computed for claims for no input has been sought so far. Second, each iteration of the validation process changes the credi- bility of claims only marginally. Hence, inference should proceed incrementally and avoid expensive re-computation of the credibility probabilities and model parameters in each iteration. Existing inference algorithms. Various inference algorithms have been proposed in the literature. Yet, none of them meets the afore- mentioned requirements. Traditional CRF models, such as [54], operate in a static manner, in which model parameters are inferred from a fixed set of labelled data by methods that incur high com- putational effort (e.g., gradient descent or trusted region methods). Hence, credibility probabilities and model parameters in our model would be computed from scratch every time new user input arrives. Moreover, the instantiation of a grounding based on this model re- quires another pass over the whole data. This makes it not suitable for interactive validation process considered in our work. iCRF algorithm. In the light of the above, we propose a novel incremental inference algorithm, iCRF , which adopts the view maintenance principle by maintaining a set of Gibbs samples over time. Estimation of credibility and model parameters exploits the results of the previous iteration of the validation process, thereby avoiding re-computation. As we will show experimentally, this does not only increase inference efficiency, but also yields a better approximation compared to random estimation. Our iCRF algorithm implements the third step of the validation process introduced in §2.3, i.e., the inference of the implications of user input on the probabilistic credibility model. In the z-th iteration of the validation process, reasoning is based on the prob- abilistic fact database of the previous iteration and the user input that has been received in the z-th iteration. That is, if c is the claim validated in the z-th iteration, we rely on the probabilistic fact database Qz−1 = 〈S, D, C, Pz−1〉, with CU z−1 and CL z−1 being the sets of unlabelled and labelled claims, respectively, as indicated by Pz−1. Then, these sets are updated, CU z = CU z−1 \ {c} and CL z = CL z−1 ∪ {c}, and inference returns a new probabilistic fact database Qz = 〈S, D, C, Pz 〉. In each iteration of the validation process, our iCRF algorithm adopts the Expectation-Maximization (EM) principle for inference. This choice is motivated by EM’s fast convergence, computation- ally efficiency, and particular usefulness when the likelihood is an exponential function (i.e., maximising log-likelihood becomes maximising a linear function). Specifically, we infer the values of the variables for unlabelled claims CU through a configuration of CU and learn the weight parameters W . By relying on an EM- based approach, we can further naturally integrate user input on the credibility of specific claims. This is a major advantage com- pared to approaches based on gradient-descent [47] that optimise model parameters, but do not enable the integration of user input and constraints (e.g., on opposing claims). Inference alternates between an Expectation (E-step) and a Maxi- mization (M-step), until convergence. EM-based inference is con- ducted in each iteration of the validation process, while each EM iteration updates the model parameters W . Hence, in the z-th it- eration of validation, we obtain sequences W 0 z , W 1 z , . . . , W l z and P 0 z , P 1 z , . . . , P l z of model parameters and credibility probabilities. E-step: We estimate the credibility probabilities from the current parameter values. The first E-step of the z-th iteration of the val- idation process is based on parameters W 0 z , given as input from the previous iteration of the validation process, i.e., W 0 z = W lz−1 z−1 , with lz−1 as the number of EM iterations in the z − 1-th iteration of the validation process. In the l-th E-step of the z-th step of the validation process, credibility probabilities are computed as follows: (1) A sequence of samples Ωl z is obtained by Gibbs sampling ac- cording to the conditional probability distribution: ql z (CU z ) = P r(CU z | CL z , D, S; W l z ) ∝ ∏ π={c,d,s}∈Π P rl−1 z (c) × φ(o(c), d, s; W l z ). (6) 4 We incorporate non-equality constraints (Eq. 3) into Gibbs sam- pling using an idea similar to [61], which, based on matrix factorisation, embeds constraints as factorised functions into the Markov chain Monte Carlo process. Note that Ωl z is a se- quence, as any configuration of CU can appear multiple times. We weight the influence of causal interactions (i.e., cliques) by the credibility of their contained claims, so that user input is propagated via mutual interactions between the cliques. (2) The probability for each claim c ∈ CU without user input is determined by the ratio of Gibbs samples in which c is credible: P rl z (c) = ∑ ω∈Ωt z ω(c) |Ωt z | . (7) For all other claims c ∈ CL, the probability is fixed by the user input: We set P rl z (c) = 1, if the user confirms a claim, and P rl z (c) = 0 otherwise. M-step: We compute the new parameter values by maximising the expectation of log-likelihoods as a weighted average of the probability distribution of current label estimates. That is, in the l-th M-step of the z-th step of the validation process, we have: W l+1 z = arg max W ′ ∑ CU ql z (CU z ) log P r(CL z , CU z |D, S; W ′) (8) This step is realised by a L2-regularized Trust Region Newton Method [45], suited for large-scale data, where critical informa- tion is often sparse (many zero-valued features). Proposition 1. iCRF runs in linear time in the size of the dataset. Proof. The E-step is implemented by Gibbs sampling, which takes linear time [19, 37] in the number of claims. The M-step is imple- mented by the Trust Region Newton Method, which also takes linear time in the dataset size [45] . 3.3 Instantiation of a Grounding Once the user input of the z-th iteration of the validation process has been incorporated, a grounding is instantiated. This corresponds to the fourth step of the validation process in §2.3, i.e., deciding which claims are deemed credible. Since claims are not independent, we take the configuration with maximal joint probability: gz (c) =    1 if (c ∈ CL z ) ∨ (o(c) = 1 ∧ o = arg max CU z P r(CU z | CL z , D, S; Wz )) 0 otherwise. (9) However, solving this equation is similar to solving a Boolean satisfiability problem. Thus, we simply leverage the most recent Gibbs sampling result Ω∗ z , obtained during EM, for instantiation. This is defined by a function decide as follows: gz (c) = decide(c, Ω∗ z ) =    1 if (c ∈ CL z ) ∨ (o(c) = 1 ∧ o = arg max CU z |{ω ∈ Ω∗ z | CU z = ω}|) 0 otherwise. (10) Consider a set of claims C = {c1, c2, c3} and assume that the last Gibbs sampling comprised three configurations, ω1 = [1, 1, 0], ω2 = [1, 0, 0], ω3 = [1, 1, 0], where the i-th vector element denotes the credibility of claim ci. Instantiation will return [1, 1, 0] as this configuration appears most often, so that its probability is maximal. 4. USER GUIDANCE Having discussed (i) inference based on user input and (ii) instan- tiation of a grounding, we turn to strategies to guide a user in the validation. This corresponds to the first step of the validation process presented in §2.3, i.e., the selection of a claim for validation. We first define a measure of uncertainty for a probabilistic fact database (§4.1). Then, two selection strategies are introduced (§4.2 and §4.3), before they are combined in a hybrid approach (§4.4). 4.1 Uncertainty Measurement The model of a probabilistic fact database, as constructed above, enables us to quantify the uncertainty related to credibility in- ference in order to guide a user in the validation process. Let Q = 〈S, D, C, P 〉 be a probabilistic fact database. Recall that P assigns to each claim c ∈ C the probability P (c) of it being credible, while C is the categorical random variable that captures credibility configurations over all claims. We quantify the overall uncertainty of the database by the Shannon entropy over a set of claims: HC (Q) = − ∑ C P r(C; W ) log P r(C; W ) (11) In our iCRF model, it can be computed exactly by [58, 57]: HC (Q) = Φ(W ) − EW [t(C)]T W (12) where Φ(W ) = ∑ C ∏ π φ(o, d, s; W ) is called the partition func- tion and EW [t(C)] = ∇Φ(W ). Since our model is an acyclic graph with no self statistics, the partition function is computed exactly using Ising methods [57], which run in polynomial time. We can further scale-up uncertainty computation by approximat- ing the entropy in linear time, as follows: HC (Q) = − ∑ c∈C [P r(c) log P r(c)+(1−P r(c)) log(1−P r(c))] (13) where the claim probabilities are obtained after each EM iteration (i.e., Eq. 7 for unlabelled claims, or directly by the user input for labelled claims). However, this approximation neglects the mutual dependencies between claims. 4.2 Information-driven User Guidance A first heuristic to guide the selection of claims for validation aims at the maximal reduction in uncertainty under the assumption of trustworthy sources. It exploits the benefit of validating a claim using the notion of information gain from information theory [59]. To capture the impact of user input on a claim c, we define a condi- tional variant of the entropy measure introduced earlier. It measures the expected entropy of the database under specific validation input: HC (Q | c) = P r(c) × HC (Q+) + (1 − P r(c)) × HC (Q−) (14) where Q+ = 〈S, D, C, P +〉 and Q− = 〈S, D, C, P −〉 are inferred from Q = 〈S, D, C, P 〉 by iCRF (§3.2), under input that confirms the claim, P +(c) = 1, or labels it as non-credible, P −(c) = 0. To take a decision on which claim to select, we assess the expected difference in uncertainty before and after incorporating input for a claim. The respective change in entropy is the information gain that quantifies the potential benefit of knowing the true value of an unknown variable [59], i.e., the credibility value in our case: IGC (c) = HC (Q) − HC (Q | c). (15) Using this notion, we chose the claim that is expected to maximally reduce the uncertainty of the probabilistic fact database. This yields a selection function for information-driven user guidance: selectC (C) = arg max c∈C IGC (c) (16) Note that we do not need to rank the opposing claim ¬c of a claim c, as their conditional entropies in Eq. 14 will be equivalent. 5 4.3 Source-driven User Guidance User guidance as introduced above assumes that sources are trustworthy—an assumption that is often violated in practice. To tackle this issue, we model source trustwor","Nguyen, T. T., Weidlich, M., Yin, H., Zheng, B., Nguyen, Q. V. H., & Stantic, B. (2019). User guidance for efficient fact checking (No. CONF)."
SSAFC_093,BRENDA: Browser Extension for Fake News Detection,"Misinformation such as fake news has drawn a lot of attention in recent years. It has serious consequences on society, politics and economy. This has lead to a rise of manually fact-checking websites such as Snopes and Politifact. However, the scale of misinformation limits their ability for verification. In this demonstration, we propose BRENDA a browser extension which can be used to automate the entire process of credibility assessments of false claims. Behind the scenes BRENDA uses a tested deep neural network architecture to automatically identify fact check worthy claims and classifies as well as presents the result along with evidence to the user. Since BRENDA is a browser extension, it facilities fast automated fact-checking for the end user without having to leave the Webpage.",Computer Science,Proceeding,2020,N,Y,155,"Online fake news has become a major societal challenge due to its consequences in real life. For example, there are instances of stock market disruptions 1, election meddling 2 and mob lynchings 3. To address this, several fact checking organizations such as Snopes, Politifact and FullFact have become popular. Typically they employ experts and journalists who perform a tedious task of manually selecting fact check worthy claims made in online news and social media debunking them. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR ’20, July 25–30, 2020, Virtual Event, China © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00 We propose BRENDA a proof of concept browser extension which anyone can install on desktop browsers to perform end-to-end fact checking. BRENDA automates following two tasks: (1) Selecting fact check worthy claims and (2) Verifying the truthfulness of claims based on the evidence found online. Existing demos (e.g, CredEye [ 6], FactMata4, [4] etc) are limiting to the users reading online news, since they have to first identify the claims within the articles, then switch to a different website for fact checking. There are also demos which either do only claim ranking [1 ] or just list the relevant websites [10 ]. Moreover, existing demos do not provide any explanation for the claim classifications. There are no existing demos which can jointly identify the claim and fact check them and provide evidence to the support the decision. To address these issues, BRENDA provides the following contributions: (1) BRENDA facilitates users to do fact checking without leaving the Website. If users are not sure on which claims to fact check, BRENDA can automatically filter fact check worthy claims. (2) BRENDA can automatically query online evidence via web search engines and verify claims. (3) BRENDA uses a proven pre-trained deep neural network model coined SADHAN which considers the latent-aspects of the claim to verify its truthfulness. (4) In addition to classifying the claims, BRENDA also provides evidence snippets highlighting the importance of both words and sentences relevant for classifying the claim using attention weights from the SADHAN deep neural network model [5]. 2 RELATED WORK Most fact-checking websites such as Snopes.com and Politifact.com perform manual fact check. Some automated fact-checking sys- tems such as CredEye [ 6 ] are available. However, since CredEye only uses word-level attention, it can only highlight which words were used for classifying a claim. BRENDA on the other hand can provide evidence at both-word level and sentence-level. Moreover, BRENDA can provide evidence w.r.t each aspect such as subject, author and domain of the claim. FactMata is a commercial tool for automated fact-checking, there is no description of the detection algorithm. Moreover, they do not provide any evidence snippets. Grover5[ 9] is another solution which focuses on detecting neural generated fake news. To the best of our knowledge none of these systems are provided as a browser extension which allows users to fact-check without leaving the article they are reading. There are some browser extensions such as The Factual6, Trusted Times7, and FakerFact8 which claim to support automated fact checking and they are listed in google chrome extension store. arXiv:2005.13270v1 [cs.IR] 27 May 2020 SIGIR ’20, July 25–30, 2020, Virtual Event, China Bjarte Botnevik, Eirik Sakariassen, and Vinay SettyAPI call claim Claim detection Search Engine Search results Pre-trained model Credibility Aggregation API response Figure 1: Block diagram of the server.Concat Losses Dsad Claim Word Embeddings Document Word Embeddings Subject Model Author Model Domain Model Softmax Figure 2: SADHAN Model However, there is no research paper or documentation explaining the model they use. Moreover, we could not find any system which can narrow down the claim within the article using fact-check worthiness detection and use that claim to detect fake news. 3 SYSTEM DESIGN BRENDA follows a client-server architecture and has a frontend and backend module. The frontend is a browser extension and the backend is a python Flask server. 3.1 Frontend: Browser Extension We develop a browser extension which works with the popular Google Chrome browser. When the user invokes the fact checking by clicking on the browser extension, JavaScript modules are used to retrieve information and details from the web pages and send the query to the server. When the results are returned back from the server, another JavaScript module is invoked to display the results. 3.2 Backend: Server The server provides a RESTful API for the browser extension. The browser extension sends the URL or claim text chosen by the user to the server. The server then analyzes the claim text first by retrieving relevant articles from the Web via search engines such as Google and analyzes them by applying machine learning models and gives a prediction for the credibility of the claim. A score indicating how credible the claim is based on the evidence found is sent back to the browser. In this section, we explain different parts of the server. The overall block diagram of the server can be seen in Figure 1. Querying the Web: Given a claim text, we use Google API to retrieve the top-10 relevant web pages. We use the claim text as the query Table 1: Comparison of SADHAN with DeClarE models for False claim detection on Snopes and PolitiFact datasets. Data Model True Acc. False Acc. Macro F1 AUC DeClarE 68.18 66.01 67.10 72.93 PolitiFact SADHAN 68.37 78.23 75.69 77.43 DeClarE 60.16 80.78 70.47 80.80 Snopes DHAN 79.47 84.26 80.09 85.65 without quotes. Before passing the text to the neural network for credibility prediction, we preprocess the text to tokenize, extract publication date, authors and summary etc using a python library Newspaper3k9. Since not all parts of the news article are important to classify the claim, we filter the articles with relevant snippets using cosine similarity (inspired by [7]). Then we select all the snippets above 0.75 similarity score for fact checking. SADHAN Model: In this demo, for the classification of fake news articles and false claims we use a deep neural network coined SADHAN [5]. SADHAN model uses hierarchical neural attention mechanism [ 8 ] for learning the representations for both claim text and the evidence news article both at word level and sentence level. As shown in Figure 2, SADHAN takes claim text and a evidence document embeddings as input. Optionally, SADHAN can also take latent aspects such as ‘author’, ‘topic’ and the ‘domain’ etc into account to guide the attention. The aspect attribute vector used in computation of attention at both the word and sentence level comes from latent aspect embeddings for which weights are trained jointly in the model using corresponding aspect attentions. As shown in Table 1, SADHAN outperforms powerful baselines BRENDA: Browser Extension for Fake News Detection SIGIR ’20, July 25–30, 2020, Virtual Event, ChinaExtension Button Analyze Text Analyze Article (a) PopupClaim Text Show Evidence Credibility Output Evidence Snippet User Feedback Usage Guide (b) ResultUser Feedback Supporting Evidence (c) User feedback for the resultTop k Sentences Claim Score Threshold Article Content Confidence Score User Feedback (d) Claim extraction Figure 3: Demonstration Snapshots which uses word-level attention such as DeClarE [ 7]. For more details and performance evaluation of SADHAN see [5]. Claim Detection: Since not all sentences in the articles are worthy of a fact check, we train a classifier and use it for detecting the claim check worthy sentences. We use ULMFiT, a language model fine-tuning technique [2] and use a model inspired by Averaged- SGD-LSTM [3 ] to train our classifier. The model is trained with a dataset with 9069 labeled sentences (4094 from a presidential debate dataset 10 and 4975 from the Politifact dataset 11. We combined these two datasets and together the dataset has 4666 with label “claim” and 4193 with label “non-claim”. We performed 5-fold cross validation and got a precision of 0.913, a recall of 0.937 and F1- score (micro) of 0.920. We use the softmax value of the model as a claim-check worthiness score for the given sentence. 10https://github.com/apepa/claim-rank/tree/master/data 11politifact.com 4 DEMONSTRATION The screen recording of the demonstration can be found here12. When the user invokes BRENDA, a popup is launched where the user can choose with what method they want to analyze the article. The user can then choose one of the two options, as shown in Figure 3(a). When the “Analyze marked text” is chosen the selected text is used as the claim and sent to the server, which then runs the series of web page extraction, NLP and classification explained in Section 3.2. The result from the SADHAN model is displayed in the same popup window (See Figure 3(b)). The user can also choose to see the evidence by clicking on the “evidence” button, which then extracts the evidence snippet according to the attention mechanism of SADHAN model. Users can also give a feedback if the model makes a mistake (Figure 3(c)) which in-turn could be potentially used to improve the classi- fier or evaluate the performance on the live data. When “Analyze 12https://www.youtube.com/watch?v=LjaqH5JogGo SIGIR ’20, July 25–30, 2020, Virtual Event, China Bjarte Botnevik, Eirik Sakariassen, and Vinay Setty Figure 4: Evidence visualization for the claim “Covid-19 can be cured by ingesting disinfectants” using the attention weights the whole article” is clicked, another popup shown in Figure 3(d) is launched. BRENDA automatically analyzes the whole article and fact checks the top scored claim using SADHAN model. The user can also explore other identified claims in the article by setting the claim score threshold and the top-k sentences. The user can also provide feedback on claim score prediction by our model. When the user clicks on the evidence button, the user can also see the highlighted sentences based on the attention mechanism in SADHAN model [ 5]. The sentence-level attention weights are aggregated using word-level attention weights. This provides an in- tuitive understanding of the text the model considered as important for the classification. For example, in Figure 4, for the claim “Covid- 19 can be cured by ingesting disinfectants” the evidence is shown with highlighted sentences with contrast of the color proportional to the normalized aggregated word-level attention weights. The Chrome browser extension along with the instructions on how to install it can be found here13. 5 CONCLUSION In this demonstration we proposed BRENDA which is a browser extension to tackle the challenge of misinformation. The user can use BRENDA to first identify fact check worthy claims in any news article online. Subsequently the user gets the credibility classifica- tion using a sophisticated deep neural network model. The users are also presented with the evidence from the model, and can achieve all this without leaving the Web page of the news article they are reading.","Botnevik, B., Sakariassen, E., & Setty, V. (2020, July). Brenda: Browser extension for fake news detection. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval (pp. 2117-2120)."
SSAFC_106,Two Stage Transformer Model for COVID-19 Fake News Detection and Fact Checking,"The rapid advancement of technology in online communication via social media platforms has led to a prolific rise in the spread of misinformation and fake news. Fake news is especially rampant in the current COVID-19 pandemic, leading to people believing in false and potentially harmful claims and stories. Detecting fake news quickly can alleviate the spread of panic, chaos and potential health hazards. We developed a two stage automated pipeline for COVID-19 fake news detection using state of the art machine learning models for natural language processing. The first model leverages a novel fact-checking algorithm that retrieves the most relevant facts concerning user queries about particular COVID-19 claims. The second model verifies the level of truth” in the queried claim by computing the textual entailment between the claim and the true facts retrieved from a manually curated COVID-19 dataset. The dataset is based on a publicly available knowledge source consisting of more than 5000 COVID-19 false claims and verified explanations, a subset of which was internally annotated and cross-validated to train and evaluate our models. We evaluate a series of models based on classical text-based features to more contextual Transformer based models and observe that a model pipeline based on BERT and ALBERT for the two stages respectively yields the best results.",Computer Science,Article,2020,N,Y,4,"Electronic means of communication have helped to eliminate time and distance barriers to sharing and broadcasting information. However, despite all its advantages, faster means of communication has also resulted in extensive spread of misinformation. The world is currently going through the deadly COVID- 19 pandemic and fake news regarding the disease, its cures, its prevention and causes have been broadcast widely to millions of people. The spread of fake news and misinformation during such precarious times can have grave consequences leading to widespread panic and amplification of the threat of the pandemic itself. It is therefore of paramount importance to limit the spread of fake news and ensure that accurate knowledge is disseminated to the public. In this work, we propose a robust, dynamic fake news detection system, that can not only estimate the correctness” of a claim but also provides users with pertinent information regarding the said claim. This is achieved using a knowledge base of verified information that can be constantly updated. Previous work on fake news detection has primarily focused on evaluating the relationship measured via a textual entailment task between a header and the body of the article. However, such a method is insufficient for identifying specific fake news claims without any knowledge of the facts relevant to the claim. This warrants the use of a new dataset specific to the COVID-19 pandemic. Developing a solution for such a task involves generating a database of factual explanations, which be- comes our knowledge base, that serves as ground truth for any given claim. We compute the entailment between any given claim and explanation to verify if the claim is true or not. Querying for claim, ex- planation pairs for each explanation in our knowledge base is computationally expensive and slow, so ∗ * These authors contributed equally. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: we propose generating a set of candidate explanations which are contextually similar to the claim. We achieve this by using a model trained with relevant and irrelevant claim explanation pairs, and using a similarity metric between the two to match them. Therefore, in the pipeline, firstly, for a given claim, a set of candidate explanations from the knowledge base have to be fetched in real-time. Then, the claim validated for truth using relevant candidate explanations. In this work, we have explored the use of Transformer (Vaswani et al., 2017) based models to both fetch relevant explanations as well as measure the entailment between a given claim and a factual explanation. We then evaluate our model on the basis of count of relevant explanation results fetched as well as the accuracy in verifying a given claim. We demonstrate the effectiveness of pre-trained multi-attention mod- els in terms of overall accuracy when compared with other natural language processing (NLP) baselines while maintaining near real-time performance. 2 Related Work The paper (Riedel et al., 2017) uses traditional approaches with a simple classifier model that makes use of Term Frequency(TF), Term Frequency and Inverse Document Frequency(TF-IDF), and cosine simi- larity between vectors as features to classify fake news. They have provided a baseline for fake news stance detection on Fake News Challenge (FNC-1) dataset1. We have implemented their approach on our dataset and results can be seen in Table 3. In (Nie et al., 2019), the authors present a connected system consisting of three homogeneous neural semantic matching models that perform document retrieval, sentence selection, and claim verification on the FEVER Dataset (Thorne et al., 2018) jointly for fact extraction and verification. Their Neural Semantic Matching Network (NSMN) is a modification of the Enhanced Sequential Inference Model (ESIM) (Chen et al., 2017), where they add skip connections from input to matching layer and change output layer to only max-pool plus one affine layer with ReLU activation. They use three stage pipeline in which given a claim, they first retrieve candidate documents from the corpus, followed by retriev- ing candidate sentences from the selected candidate documents and finally, the last stage classifies the sentence in to one of three classes. They used Bidirectional LSTM (BiLSTM) to encode the claim and sentences using GloVe (Pennington et al., 2014) and ELMO (Peters et al., 2018) embeddings. However, these tasks are concerned with static or slowly evolving domains, on topics that do not require domain expertise to annotate. Despite the partial success of the above methods, there were still certain shortcomings in terms of the accuracy of the results. Bidirectional encoder representations from Transformers (BERT) (Devlin et al., 2018) is a pre-trained language model trained on a large corpus comprising the Wikipedia and Toronto Book Corpus, and is shown to perform well on several natural language tasks like GLUE (Wang et al., 2018). Transformer based pretrained models achieved state of the art results in several NLP subtasks, their ease of fine-tuning makes them adaptable to newer tasks. In (Jwa et al., 2019), the authors propose a model based on the BERT architecture to detect fake news by analyzing the contextual relationship between the headline and the body text of news. They demonstrate that using Transformer based mod- els like BERT and fine-tuning them for the task of fake news detection gives better results than other models like stackLSTM (Hanselowski et al., 2018; Hermans and Schrauwen, 2013) and featMLP (Davis and Proctor, 2017). They further enhanced their model performance by pre-training with domain spe- cific news and articles, and countered the class imbalance for the final classification task by the use of a weighted cross entropy loss function. However, this approach can not be adapted for our task of fetching relevant explanations for each claim. (Naud ́e, 2020) and (Bullock et al., 2020) extensively studied the use of machine learning strategies to address various issues regarding COVID-19. The latter also describes fake news and misinformation as a major issue in the ongoing pandemic and further highlights the problem as solving an infodemic. In (Gallotti et al., 2020), the authors develop an Infodemic Risk Index (IRI) after analyzing Twitter posts Figure 1: Cross validated data examples across various languages and calculate the rate at which a particular user from a locality comes across unreliable posts from different classes of users like verified humans, unverified humans, verified bots, and unverified bots. In (Mejova et al., 2018), the authors examine Facebook advertisements across 64 countries and find that around 5% of advertisements contained possible errors or misinformation. But none of these mentioned works tackle the problem of misinformation by reasoning out the given fake claim with an explanation. 3 Dataset Using an existing misinformation dataset will not serve as a reliable knowledge base for training and evaluating the models due to the recent and uncommon nature i.e., the vocabulary used to describe the disease and the terms associated with the COVID-19 pandemic. It is important to generate real and timely datasets to ensure accurate and consistent evaluation of the methods. To overcome this drawback, we manually curated a dataset specific to COVID-19. Our proposed dataset consists of 5500 claim and explanation pairs. We describe the collection and annotation process in Section 3.1. 3.1 Covid-19 Claims Dataset There are multiple sources on the web that are regularly identifying and debunking fake news on COVID- 19. We scraped data from Poynter”2 , a fact checking website which collects fake news and debunks or fact-checks them with supporting articles from more than 70 countries, covering more than 40 languages. The Poynter website has a database exclusively for COVID-19 with over 7000 fact checks. Each fact check contains the corresponding claim that is being checked, the rating indicating the type of the claim, for example - ’False’, ’Mostly False’ or ’Misleading’, the name of the fact checker, the explanation given for the current claim, the location of origin of the claim and the date when the fact check was done. This data can be used to update our ”explanation” look-ups in a timely fashion so our database is constantly evolving as we learn more about the virus and the facts change. For each fact check, we collect only the ”claim” and the corresponding explanation” from this database which were rated as ’False’ or ’Misleading’. In this way, we collected about 5500 false-claim and explanation pairs. We further manually rephrase these false claims to generate true claims, as the ones that align with the explanation so as to create an equal proportion of true-claim and explanation pairs. We have taken claim-explanation pairs from the time period between January 1,2020 to May 2https://www.poynter.org/ifcn-covid-19-misinformation/ Dataset Number of sentence pairs False claim - Explanation pairs 5500 Cross validated False claim - True claim - Explanation pairs for train data 1000 Cross validated False claim - True claim - Explanation pairs for test data 200 Table 1: Dataset Information 15,2020 for our training data and from May 18,2020 to July 1,2020 for our test data. In this way, we evaluate the generalization of the model on completely new and unseen data. Our collected data follows the structure: [false claim, explanation] The subset of the data that we annotated and cross validated follows the structure: [false claim, true claim, explanation] Figure 1 shows some examples of the cross validated data. 3.2 Dataset Statistics The current proposed Covid-19 dataset contains cross-validated claim-explanation sentence pairs. Statis- tics about the distribution of labels are provided in Table 1. This is a dynamic dataset and we are contin- ually collecting and curating additional claim-explanation pairs. We plan to open source this dataset to facilitate more research in this domain. 4 Methodology The architecture consists of a two stage model, we will refer to the first model as Model A” and the second model as Model B”. The objective of Model A is to fetch the candidate true facts” or expla- nations for a given claim, which are then evaluated for entailment using the Model B. Next, we describe the training procedure as well as intended run time behaviour for both Model A and Model B. Figure 2: Block diagram of our two stage model pipeline 4.1 Model A First, to fetch relevant explanations, we train our Transformer model on a binary sentence entailment task, where the claims and explanations are the two sentences fed in as input separated by a [SEP] tag. We generate negative claim-explanation pairs through random sampling to ensure that equal proportions of positive and negative pairs are present. Training multi-attention network with our COVID-19 specific data enables the model to capture long-range correlations between the vector representations of claims and explanations of similar contexts. We train our models with a base encoder and a sequence classifica- tion head on top for binary classification of the labels. The model is trained to optimise the cross entropy loss. Through our experiments, we find that, on this trained model, if we generate embeddings for a single sen- tence (either claim or explanation individually) and compare matching [claim, explanation] embeddings using the cosine similarity metric, there is a distinction in the distribution of similarity scores between re- lated and unrelated [claim,explanation] pairs. Therefore, for faster near real-time performance, we cache the embeddings for all our explanations (knowledge base) beforehand, and compute the cosine similarity between the claim and the cached embeddings of the explanations. The vector of the [CLS](the start of sentence) token of the final layer works as a strong representation of the entire sentence, although we found that taking element-wise mean over all the token vectors leads to better performance. We fetch the top explanations for any given claim exceeding a certain threshold of sentence similarity as there could be several explanations relevant for a given claim. This threshold is determined on the basis of the summary statistics of the cosine similarity metric between the claim and relevant explanations in the validation set, as described in Section 5.3. These retrieved explanations serve as candidates for verifying the accuracy of the claim through Model B. 4.2 Model B The second part of the pipeline is to identify the veracity of a given claim. Model A fetches the candidate explanations while Model B is used to verify whether the given claim aligns with our set of candidate explanations or not. We can therefore treat this task as a textual entailment problem (Dagan et al., 2013; Adler et al., 2012). To train the Model B, we use a smaller subset of false claim” and explanation” pairs from our original dataset, and cross validate each sample with true claim” or in other words, claims that align with the factual explanation. However, this small annotated data is not sufficient to train the model effectively. Therefore, the parameters of the Model A, which was trained on a much larger dataset were used as initial parameters for Model B, and fine-tuned further using our cross validated dataset. We trained Model B in a similar fashion as Model A i.e. as a sequence classification problem with cross entropy loss. Once we have the candidate explanations for a given claim, we use Model B to estimate the probabilities of alignment of claim with each of the candidate explanations. We used the statistic of mean probability score and standard deviation of aligning and non-aligning claim and explanation pairs in the validation set to determine the thresholds for Model B classification. We trained and evaluated both Model A and Model B using several approaches based on classical NLP methods as well as more sophisticated pre-trained Transformer models. The flow of the Model A + Model B pipeline is shown in Figure 2. 5 Experiments 5.1 Baseline Models For baselining our model on classical NLP approaches, we use two simple bag-of-words (BOW) rep- resentations for the text inputs: term frequency (TF) and term frequency-inverse document frequency (TF-IDF). We followed the architecture proposed by (Riedel et al., 2017). The representations and fea- tures extracted from the claim and explanation pairs consist of the following: • The TF vector of the claim; • The TF vector of the explanation; • The cosine similarity between the 2-normalised TF-IDF vectors of the claim and explanation. For the TF vectors, we extract a vocabulary of the 5,000 most frequent words in the training set and exclude stop words (the NLTK (Bird et al., 2009) stop words for the English language). For the TF-IDF vectors, we use the same vocabulary and set of stop words. The TF vectors and the TF-IDF cosine sim- ilarity values are concatenated to form a feature vector with total dimension of 10,001 and is fed to the classifier. The classifier takes an input of 10,001 dimensional vector followed by a feed-forward network with 50, 20 and 2 dimensional dense layers respectively with each hidden layer having tanh activation and the last layer is a softmax layer. We trained in mini-batches over the entire training set using the Adam Model Model A Val Accuracy Model B Val Accuracy TF-IDF 0.832 0.799 GloVe 0.781 0.777 MobileBERT 0.921 0.877 ALBERT 0.927 0.956 BERT 0.944 0.927 Table 2: Model Performance on Validation Set optimiser (Kingma and Ba, 2014) with a learning rate of 0.001. The second approach involves use of word vectors for which we used 300-dimensional GloVe (Penning- ton et al., 2014) embeddings, pretrained on 2014-Wikipedia and Gigaword, and averaged over token embeddings to compute sentence vectors. So, for a given claim and explanation pair, we have a 300- dimension vector for claim as well as the explanation, both of which are concatenated to form a 600- dimensional vector that serves as input to our dense layer classifier. This model is a simple feed-forward neural network with 4 hidden layers having 200, 100, 50 and 2 hidden units respectively with the first three layers having ReLU activation while the last layer is a softmax layer. We trained in mini-batches of 32 over the entire training set with back-propagation using the Adam optimizer with a learning rate of 0.001. 5.2 Transformer Models We trained and evaluated three Transformer based pre-trained models for both Model A and Model B us- ing the training strategy described in Section 4. As our focus was to ensure that the proposed pipeline can be deployed effectively in a near real-time scenario, we restricted our experiments to models that could efficiently be deployed using inexpensive compute. We chose the following three models - BERT(base), ALBERT (Lan et al., 2019) and MobileBERT (Sun et al., 2020). The authors of MobileBERT demon- strated that using a teacher-student learning technique for progressive knowledge transfer from the BERT to MobileBERT model helps them achieve a task-agnostic model similar to BERT and can be deployed easily on resource limited devices due to faster inference speeds and lower memory consumption. The ALBERT model was proposed to increase the training and inference speed of BERT besides lowering the memory consumption. The authors demonstrate that the use of their parametric reduction techniques and a custom self supervised loss helps it to achieve results similar to BERT while having fewer parameters. Model A was trained on 5000 claim-explanation pairs on the sequence classification task to optimize the softmax cross entropy loss using a learning rate of 3e-5. This trained model was then validated on a test set comprising of 1000 unseen claim-explanation pairs. The training data structure here looks like: [claim, relevant explanation, 1] [claim, irrelevant explanation, 0] Model B was trained on a smaller subset of 800 cross validated [claim,explanation,label] data, on the same sequence classification task, where the label was assigned based on whether the claim aligned with the explanation - 1 or not - 0. This was validated on 200 unseen data-points. The loss function used was softmax cross-entropy with a uniform learning rate of 1e-5. The training data structure here looks like: [true claim, relevant explanation, 1] [false claim, relevant explanation, 0] 5.3 Evaluation Metrics For evaluating the performance of the overall pipeline model, we first evaluate the performance of Model A in its ability to retrieve relevant explanation. For this we use Mean Reciprocal Rank(MRR) (Craswell, 2009) and Mean Recall @10 (Malheiros et al., 2012), that is the proportion of claims for which the relevant explanation was present in the top 10 most contextual explanation by cosine similarity and their mean inverse rank. Equation 1 shows the MRR formula for our evaluation. M RR = 1 C C∑ i=1 1 ranki (1) where ranki is the position of the explanation that is relevant to a particular claim according to test data and C is the total number of claims. Equation 2 shows the Recall@10 formula for our evaluation. Recall@10 = 1 C C∑ i=1 f (top-10 explanations) (2) where f (x) = { 1 if true exp ∈ x 0 otherwise Here, true exp is the actual relevant explanation for a particular claim according to test data and C is the total number of claims. Once, Model A has retrieved relevant explanations, we evaluate the performance of Model B on comput- ing the veracity of the claim. Here, we only used explanations that exceed an empirically defined thresh- old in cosine similarity between the claim and the explanation. Through our experiments, we found that a threshold of mean - standard deviation of cosine similarity over the validation data worked well for picking relevant explanations. For evaluating the accuracy, we take a mean of the output probabilities for each claim, explanationi, defined by the Equation 3. ptruth = 1 n ∑ i modelB(claim, expi ) (3) where exp ∈ expA | cos(c vecA, exp vecA) > t Here, expA are the top-10 explanations returned by Model A. c vecA and exp vecA are the vec- tor representations generated by running claim and explanations individually through Model A and t refers to the threshold. 5.4 Results and Discussion The performance of several Transformer based models as well as classical NLP models were compared using the evaluation metrics described in Section 5.3. The results of the experiments on the test set are summarised in Table 3. Model MRR Recall@10 Accuracy TF-IDF 0.477 0.635 0.525 GloVe 0.182 0.410 0.579 MobileBERT 0.561 0.735 0.710 BERT 0.632 0.795 0.810 ALBERT 0.582 0.675 0.825 BERT+ALBERT 0.632 0.795 0.855 Table 3: Model Performance on test set The results in Table 3 clearly illustrate that Transformer based models are significantly better than classical NLP models. An interesting observation was that some models are better at retrieval of relevant Model Latency per claim (in seconds) Memory (in MB) TF-IDF 0.108 16 GloVe 0.003 990 MobileBERT 0.607 1200 ALBERT 2.376 942 BERT 3.106 1910 BERT + ALBERT 2.471 1398 Table 4: Model Compute performance and Memory usage explanations while others have a better classification performance. We find that a combination of the best performing Model A (BERT) and best performing Model B (ALBERT) yielded the highest MRR, Recall@10 and Accuracy on the test set for fact checking. We however do acknowledge that our models could still make errors of two kinds: firstly, Model A might not fetch a relevant explanation which au- tomatically means that the prediction provided by Model B is irrelevant, and secondly, Model A might have fetched the correct explanation(s) but Model B classifies it incorrectly. We show some of the errors our models made in Table 5. Table 4 shows the memory usage and latencies of the implemented models. The memory consumption and latency per claim in the classical NLP models was observed to be quite low in comparison to the Transformer based models. This is expected due to the lower parameter size of the TF-IDF and GloVe models. Among the Transformer based models, MobileBERT had the least latency per claim as ex- pected and explained in Section 5.2 while ALBERT consumed the least memory. The best performing BERT+ALBERT model utilized a memory of 1398MB and fetched relevant explanations of each claim in 2.471 seconds. The model latencies and memory usage were evaluated on an Intel Xeon - 2.3GHz Single core - 2 thread CPU. Claim: Cannabis could help prevent coronavirus infection . True Explanation: The study claiming that marijuana can cure coronavirus did not pass the peer review , it was conducted on artificial human tissues and not on real organisms . It is a classic preliminary research that may even fail . The authors themselves speak of the need for further studies and research . Top Fetched Explanation: The vaccine can provide stable immunity , while the presence of antibodies does not prevent reinfection . Remark: Model A fetched irrelevant explanation for this claim Claim: The vaccine is not the final solution against the novel coronavirus but antibodies are . Explanation: The vaccine can provide stable immunity , while the presence of antibodies does not prevent reinfection . Probability score: 0.340 Remark: Model B misclassified this claim-explanation pair as True Claim: WHO recommends wearing masks in public spaces to slow down the spread of coronavirus Explanation: The WHO changed its position about masks by now recommending community masks in areas with many infections . And it says that masks have to be used properly and alone can’t protect you from COVID-19 . Probability score: 0.686 Remark: Model B misclassified this claim-explanation pair as False Table 5: BERT+ALBERT Model Faulty predictions 6 Conclusions and Future work In this work, we have demonstrated the use and effectiveness of pre-trained Transformer based language models in retrieving and classifying fake news in a highly specialized domain of COVID-19. Our pro- posed two stage model performs significantly better than other baseline NLP approaches. Our knowledge base, that we prepare through collecting factual data from reliable sources from the web can be dynamic and change to a large extent, without having to retrain our models again for as long as the distribution is consistent. All of our proposed models can run in near real-time with moderately inexpensive compute. Our work is based on the assumption that our knowledge base is accurate and timely. This assumption might not always be true in a scenario such as COVID-19 where facts” are changing as we learn more about the virus and its effects. Therefore a more systematic approach is needed for retrieving and classi- fying claims using this dynamic knowledge base. Our future work consists of weighting our knowledge base on the basis of the duration of the claims and benchmarking each claim against novel sources of ground truth. Our model performance can be further boosted by better pre-training, through domain specific knowl- edge. In one of the more recent work by (Guo et al., 2020), the authors propose a novel semantic textual similarity dataset specific to COVID-19. Pre-training our models using such specific datasets could help in better understanding of the domain and ultimately better performance. Fake news and misinformation is an increasingly important and a difficult problem to solve, especially in an unforeseen situation like the COVID-19 pandemic. Leveraging state of the art machine learning and deep learning algorithms along with preparation and curation of novel datasets can help address the challenge of fake news related to COVID-19 and other public health crises.","Vijjali, R., Potluri, P., Kumar, S., & Teki, S. (2020). Two stage transformer model for COVID-19 fake news detection and fact checking. arXiv preprint arXiv:2011.13253."
SSAFC_109,FactCatch: Incremental Pay-as-You-Go Fact Checking with Minimal User Effort,"The open nature of the Web enables users to produce and propagate any content without authentication, which has been exploited to spread thousands of unverified claims via millions of online documents. Maintenance of credible knowledge bases thus has to rely on fact-checking that constructs a trusted set of facts through credibility assessment. Due to an inherent lack of ground truth information and language ambiguity, fact-checking cannot be done in a purely automated manner without compromising accuracy. However, state-of-the-art fact-checking services, rely mostly on human validation, which is costly, slow, and non-transparent. This paper presents FactCatch, a human-in-the-loop system to guide users in fact-checking that aims at minimisation of the invested effort. It supports incremental quality estimation, mistake mitigation, and pay-as-you-go instantiation of a high-quality fact database.",Computer Science,Article,2020,N,Y,0,"Modern society faces an unprecedented amount of unverified claims, which do harm to democracy, economics, and national security [13]. While up to three quarters of facts posted by Web sources are false [ 17 ], journalism and politics have been impacted by ‘false facts’ on a global scale [12 ]. Building an open, yet trustworthy Web requires fact checking, i.e., the assessment of the credibility of emerging claims towards a fact database [7, 10]. Credibility assessment can use automated classification meth- ods [ 15 ].While these methods scale to the volume of Web data, they are hampered by the inherent ambiguity of natural language, deliberate deception, and domain-specific semantics [ 16 ]. Hence, algorithms often fail to decipher complex contexts of claims [3 ]. Automatic methods further require large amounts of curated data, which is typically not available since such data quickly becomes outdated. Moreover, having algorithmic models judge the truth of claims raised ethical concerns on fairness and transparency [ 9 , 14 ]. Against this background, several state-of-the-art fact checking services such as Snopes, PolitiFact, and FactCheck, rely on human feedback to validate claims [ 1]. However, eliciting user input is challenging. User input is expensive, in terms of time and cost. Hence, a timely validation of controversial claims quickly becomes infeasible, even if one relies on a large number of users and ignores the overhead to achieve consensus among them. Also, claims pub- lished on the Web are typically not independent and any user-based Conference’17, July 2017, Washington, DC, USA 2020. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 assessment of their credibility shall be propagated between corre- lated claims. Finally, user input is commonly limited by some effort budget, which bounds the number of claims to be validated. This demo presents FactCatch,1 a system for inCremental pAy- as-you-go facT CHecking with minimal user effort. The system’s contributions are summarized as follows: (1) Incremental quality estimation: FactCatch features an effi- cient probabilistic model to reason on the credibility of claims. It exploits mutual reinforcing relations between Web sources and claims to assess the credibility of unchecked claims. (2) Effort minimisation: FactCatch guides a user in the fact check- ing process, while reducing the amount of validation effort needed to achieve a specific level of result precision. (3) Mistake mitigation: FactCatch helps to identify suspicious user input; claims that may have been validated by mistake. (4) Pay-as-you-go instantiation: FactCatch supports the separa- tion of credible and non-credible claims at any time, to serve downstream applications with a high-quality fact database. (5) Early termination: FactCatch includes means to stop fact checking to avoid to spend effort on marginal improvements of the quality of the fact database. FactCatch is one of the principal systems 2 for guided fact checking that combines the best of automatic and manual approaches: Users validate the results of algorithmic models, while they save efforts by validating solely the claims that are most-beneficial for credi- bility assessment. FactCatch further enhances the explainability of algorithmic models by showing how the credibility of a claim is derived from its relation to trustworthy sources. 2 USER INTERACTION 2.1 Requirements To incorporate users as a first-class citizen in fact checking, several requirements need to be met: (R1) Guided: Fact checking shall be guided rather than an ad-hoc process of collecting data sources and arguments. (R2) Incremental: Fact checking should exploit results of prior vali- dations for reasons of efficiency and collaboration: Validations of one user are reused by another one. (R3) Pay-as-you-go: Fact checking should continuously improve the credibility assessment, while enabling the instantiation of a fact database at any time. Users may then examine the database to decide whether to stop or resume validation. 2.2 The FactCatch Process FactCatch is the first system that aims to address the above require- ments, while minimizing user efforts. Fig. 1 illustrates the process behind FactCatch, which starts with a claim database that contains claims (candidate facts) extracted from the Web. The system adopts a human-in-the-loop interaction scheme, where the claim database is continuously updated in a pay-as-you-go manner, by: (1) selecting a claim for which user feedback shall be sought; (2) eliciting user input on the credibility of the selected claim, which either confirms it as credible or labels it as non-credible; (3) inferring the credibility of remaining claims upon user input by an algorithmic model; (4) instantiating the grounding that captures the facts that are assumed to be credible. At any time, a trusted set of facts can be used for downstream applications.Claim DB UserCATCH engine select elicit infer instantiate Figure 1: Overview of the FactCatch process In the above process, steps (1), (3), and (4) need to be instantiated with specific methods, which will be described in §3. 2.3 Validation Dashboard In FactCatch, user interaction is enabled through a rich validation dashboard. As shown in Fig. 2, it consists of multiple interactive views on top of a given claim database. Landscape View. This view gives an overview of claims in a two- dimensional canvas, in which rows are data sources and columns are claims. Each source has a list of claims that it provides. The canvas includes functionality to filter data sources by name. When a user hovers a row of the canvas, an info pane is offered to show the trustworthiness and the number of claims of the respective source. Clicking on a cell, the dashboard switches to the Claim View. It also offers a matrix layout where user can reorder columns and rows to identify co-clusters of sources and claims. Claim View. This view allows the user to investigate a particu- lar claim and decide on its credibility. The default meta-data tab explains the characteristics of a claim, such as origin, description, example Web document or social post, original publication date, and last-mention date. The second tab is a graph representation of the data sources mentioning this claim and their further claims. Such representation enables the user to investigate the relations between different claims to decide on their credibility. The view also includes a navigation pane to search and rank the claims. The ranking function minimises user efforts by guiding user to focus on the most beneficial claims first, which is explained in §3. Metric View. This view summarises the fact checking process by providing quality indicators such as uncertainty measures and the number of unverified claims. It also includes a chart panel that depicts the histogram of claim credibilities, which supports an assessment of the convergence of the validation process, see §3. 3 SYSTEM COMPONENTS 3.1 Functionality Quality estimation by a probabilistic graphical model. A key metric is to determine the current quality of the fact database, which we formulate as a joint probability computation over a network of claims, sources, and Web documents. Here, the network structure is derived from the fact that a claim can be involved in multiple documents, each being provided by a different source. To model these complex relations, and eventually derive the assignment of credibility probabilities, we rely on a Conditional Random Field (CRF). It is constructed as an undirected graph of three sets of random variables for sources, documents, and claims. Direct relations are captured by relation factors in the CRF, also called cliques since they always involve three random variables (source, document, claim). As a result, the infer function can be implemented by belief propagation over the CRF [10]. This model has several advantages for guided fact checking: (1) Each claim is assigned a correctness probability to model its credi- bility. This yields better interpretability than binary labels assigned by common classification models [ 10 ]. (2) The trustworthiness of each data source can be quantified as the probability of its respec- tive random variable, supporting validation decisions during the fact checking process. (3) All user inputs obtained up to a certain point are incorporated easily via belief propagation, whereas other models face non-convex optimisation of online learning. Effort minimisation by information gain theory. There is a trade-off between the precision of a knowledge base (the ratio of credible facts) and the amount of user input: The more claims are checked manually, the higher the precision. Since a validation of all claims is infeasible, FactCatch provides guidance by selecting and ranking claims for which user feedback should be sought based on the expected benefit. The latter is measured for a claim using an information theoretic model, which leverages the aforemen- tioned probability information. Then, the claim with the highest information gain is shown first for user feedback elicitation. More precisely, we define a conditional variant of the entropy over credibility values. It measures the expected entropy of the database under a specific validation input. The expected difference in uncertainty before and after incorporating input for a claim is the respective change in entropy (i.e. information gain). It quantifies the potential benefit of knowing the true value of a claim [10]. Mistake mitigation by indicative indexes. When validating claims, a user may make mistakes such as accidental confirma- tions of a (wrong) inferred credibility value of a claim. To mitigate such issues, we provide several indicative indexes: • Support information: A user is confronted with the current inferred credibility of the claim to validate, along with a trustworthiness assessment of related sources. Any decision to deviate from the current most likely credibility assignment is typically taken well-motivated. • Redo feature: A user can undo or change a previous validation. Belief propagation enabled by our probabilistic graphical model allows such anytime modification without recomput- ing the probabilities from scratch. Landscape View Claim View Metric ViewFigure 2: Multi-view Validation Dashboard • Confirmation check: We also developed a lightweight periodic check that is triggered after a fixed number of iterations of the validation process. For a random claim that has been validated, we compute a snapshot of the probability model while leaving out the validation of that claim. Then, the probability of the claim is compared with the respective user input. If there is a contradiction, then the claim is identified as a potential mistake and the user is notified. Pay-as-you-go approach by view maintenance. FactCatch en- ables the construction of a trusted set of facts from the claim data- base at any time. Technically, a grounding that decides which claims are deemed credible is instantiated every time the user input of an iteration of the validation process has been incorporated. This is done by taking the configuration with maximal joint probability on top of the probabilistic model. However, solving the maximal configuration is similar to solving a Boolean satisfiability problem. Thus, we leverage the most recent Gibbs sampling result of the expectation-maximization algorithm obtained during model infer- ence, for instantiation [10 ]. Note that our system also supports the instantiation of the most non-credible claims (i.e., fake news). Early termination by convergence indicators. FactCatch fur- ther reduces validation effort by observing the convergence of the probabilistic model. This is due to the property of diminishing returns of the information gain metric, where further user input leads to smaller improvements of the grounding quality. Our model enables the detection of such scenarios by several indicators. • Uncertainty reduction rate: The reduction of uncertainty of the claim database is monitored over time. • Grounding change: The number of claims for which the cred- ibility is switched after each user validation is counted. • Human-algorithmic consensus: measures the mismatch be- tween user input and model grounding by KL divergence. • Precision improvement rate: The precision of the model ground- ing is estimated based on k-fold cross validation. 3.2 Scalability Incremental inference. We developed a novel algorithm for in- cremental inference, iCRF, which adopts the view maintenance principle by keeping a set of Gibbs samples over time [10 ]. Estima- tion of credibility and model parameters then exploits the results of the previous iteration of the validation process. Batch validation. Users who validate claims face significant set- up costs, e.g., to familiarise with a particular domain. It therefore increases user convenience and efficiency if validation happens in batches of claims. We support such batching by selecting a subset of k claims with highest joint information gain. While the respective subset maximisation problem is NP-complete, we approximate it by greedy search based on its submodularity property [10]. Streaming fact checking. In practice, claims may arrive continu- ously from Web sources (e.g., social media). However, the model shall not be recomputed from scatch whenever a new claim arrives. Upon the arrival of new sources and claims, the model structure and its parameters need to be updated. We therefore develop an online expectation-maximization algorithm that reuses and updates the previous trained parameters, which accelerates convergence in the presence of new data. The core of this algorithm is the stochastic approximation for the expectation step and L2-regularised Trust Region Newton Method for the maximization step [10]. 3.3 Implementation To enable cross-platform deployment and potential collaboration between validating users, the system is implemented as a web application. The claim database is stored in PostgreSQL. The back- end is written using Flask framework. The front-end is an HTML5 website communicating with the server using Python. The system follows the Model-View-Controller pattern which includes a service layer to handle the main fact checking process. 4 DEMONSTRATION PLAN Datasets. The demo uses state-of-the-art datasets for fact checking systems: • Snopes stems from the by-far most reliable and largest plat- form for fact checking [ 13 ]. It covers domains such as news and social media, and includes 14549 sources and 1200 claims. • Wikipedia contains proven hoaxes and fictitious people from Wikipedia with 1955 sources and 157 claims. The dataset was derived by using curated claims from Wikipedia as queries for a search engine to collect related Web pages. Scenarios. A live demonstration will focus on three scenarios: • Single-user validation: A user performs fact checking from a fresh dataset. The user chooses a data file to import into the database via the dashboard and subsequently validates claims in the Claim View. During the validation, the user may consult the Metric View to decide whether to continue or stop the process. The user may also export a trusted set of facts for reporting purposes. • Multiple-user validation: A user resumes or continues an ex- isting validation task. Instead of importing a data file, the user starts with the dashboard directly as the previous vali- dation results are loaded from a PostgreSQL database. The Landscape View enables the user to see which claims have been validated and to understand the trustworthiness of data sources. Using the two-dimensional canvas, the user can then focus on a problematic cluster of sources and claims. • Reference validation: FactCatch also stores labelled data from the literature for demonstration purposes. This enables the user to compare their validation with labels of fact checking experts (e.g., from Snopes.com) to gain deeper insights. Demo Takeaways. FactCatch serves a diverse audience: It (i) helps media organisations to monitor their digital content and ensure information quality; (ii) helps governments to investigate social biases and national election interference; and (iii) helps the public to distinguish between legitimate news and deceptive content; The audience would enjoy highlighted benefits using our system: • Low waiting time: of system update upon each validation [5]. • Minimal effort: obtain a fact database of high quality with significantly less effort than baseline guiding techniques [ 10]. 5 RELATED WORK Extracting factual knowledge from Web data plays an important role in the construction of knowledge bases (KB) such as Free- base, YAGO and DBpedia, which store millions of facts about vari- ous domains [ 8 ].Some systems focus on extracting candidate facts (aka claims) [ 4], but leave the credibility assessment for expert ser- vices [ 1]. Other systems focus on static classification of claims, but neglect the dynamic and evolving nature of the Web [15, 16]. Going beyond the state-of-the-art, FactCatch provides a pay-as- you-go system [2 , 6, 11 ], which can be run autonomously or on top of existing claim extraction frameworks and expert services. It supports an incremental guided fact checking process with minimal effort and complete control over the quality and transparency. 6 CONCLUSION This paper presented FactCatch, a system to overcome the limita- tions of existing methods for automatic and manual fact checking. The system is not limited to experts, but enables any user to partic- ipate in incremental, pay-as-you-go fact checking in a transparent and guided manner. Highlights of FactCatch are: (i) Claims are not analysed individually but their complex network structure through documents and data sources is incorporated; (ii) claims are automat- ically ranked for validation to minimise user efforts; (iii) a single dashboard gives full control over the fact checking process; and (iv) a trusted set of facts may be instantiated at any time in the process. The system is further optimized for scalability through methods for early termination, batching validation, and online learning. In future work, we intend to extend FactCatch with crowdsourcing functionality. By relying on mass fact checking, controlled by a cost-profit model that prioritises highly contagious rumours, we strive for timely damage mitigation of emerging false claims.","Nguyen, T. T., Weidlich, M., Yin, H., Zheng, B., Nguyen, Q. H., & Nguyen, Q. V. H. (2020, July). Factcatch: Incremental pay-as-you-go fact checking with minimal user effort. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 2165-2168)."
SSAFC_118,E-BART: Jointly Predicting and Explaining Truthfulness,"Automated fact-checking (AFC) systems exist to combat disinformation, however their complexity makes them opaque to the end user, making it difficult to foster trust. In this paper, we introduce the E-BART model with the hope of making progress on this front. E-BART is able to provide a veracity prediction for a claim, and jointly generate a human-readable explanation for this decision. We show that E-BART is competitive with the state-of-theart on the e-FEVER and e-SNLI tasks. In addition, we validate the joint-prediction architecture by showing 1) that generating explanations does not significantly impede the model from performing well in its main task of veracity prediction, and 2) that predicted veracity and explanations are more internally coherent when generated jointly than separately. Finally, we also conduct human evaluations on the impact of generated explanations and observe that explanations increase human ability to spot misinformation and make people more skeptical about claims.",Computer Science,Article,2021,N,Y,35,"Automated fact-checking (AFC) makes use of nat- ural language processing (NLP) techniques to de- termine the veracity of a claim. The problem is defined in the following way: given a statement (claim) and some evidence, determine whether the statement is true with respect to the evidence (Stammbach and Ash, 2020). This is a challenging task for a human, let alone an autonomous sys- tem (Graves, 2018). However, AFC systems are able to approximate this process of evidence re- trieval and synthesis with some degree of success (Stammbach and Ash, 2020; Vlachos and Riedel, 2014). The benefits and applications of an AFC system are numerous. The problem of disinfor- mation is not new, however the rate of which it propagates has continued to increase, largely aided by the increasing popularity of social media plat- forms (Pennycook et al., 2021). AFC systems are starting to become a critical tool in combating the sheer quantity of claims that need to be verified. While accurate (Stammbach and Ash, 2020; Portelli et al., 2020), AFC systems have been un- able to supplement traditional fact-checkers due to a limitation in their design. A user may not ac- cept to believe in a statement without first under- standing the concepts and facts underpinning that statement. Such justifications are expected when reading journalistic fact-checking outcomes such as on Politifact; the fact-check outcome is accom- panied by an explanation informing the reader of how the decision was reached. Without provid- ing users with an explanation, the decision pro- vided by an automated system is far less likely to be trusted (Toreini et al., 2020), especially as it is not generated by humans. Automated systems have recently been devel- oped to this effect, and have demonstrated promis- ing initial results (Graves, 2018). While these ini- tial results are unquestionably impressive, critical evaluation of the work reveals that many of these systems use separate models for veracity predic- tion and explanation generation. We argue that systems such as these are not actually describ- ing their own actions and decision processes, and that the veracity prediction model is not made any more transparent. In this paper, we propose and experimentally evaluate a system that jointly makes a veracity prediction and provides an explanation within the same model. This is novel as compared to classic post-hoc explainability methods that are built on top of existing machine learning models. As such, the generated explanations more closely reflect the decisions made by the veracity prediction model. In addition to this, we show that large transformer models are flexible enough to multitask, and are thus able to explain their actions without detriment to the original task. This allows human end users to better interface with transformer models, foster- ing a more trustworthy relationship between hu- mans and deep learning models. We specifically address the following research questions: • RQ1: How can we design a deep learn- ing model to classify information truthfulness and, at the same time, generate a natural lan- guage explanation supporting its classifica- tion decision? • RQ2: Can such model result in both accurate classification decisions and high quality nat- ural language explanations? • RQ3: Are machine-generated explanations useful for humans to better assess informa- tion truthfulness? By creating an automated system that is capa- ble of both evaluating the truthfulness of a state- ment and simultaneously generating a human- interpretable explanation for this decision, it is hoped that automated fact-checking systems will become more widely adopted. 2 Related Work 2.1 Existing Explainable-AFC Models A number of techniques for generating explana- tions to accompany AFC decisions have been pro- posed. Saliency-based methods, such as those pro- posed by Shu et al. (2019) and Wu et al. (2020), use attention mechanisms to highlight the input that is most useful in determining the veracity pre- diction and present this information to the end user as a form of explanation. Logic-based approaches make use of graphs (Denaux and Gomez-Perez, 2020), rule mining, and probabilistic answer set programming (Ahmadi et al., 2019) to output a se- ries of logical rules that result in a veracity pre- diction. This set of rules constitutes an explana- tion. While these methods are highly transparent and logical, the resulting explanation is not always human-readable (Ahmadi et al., 2019). Summarisation techniques provide an expla- nation by summarising the retrieved evidence. The system proposed by Atanasova et al. (2020) utilises DistilBERT (Sanh et al., 2019) to pass contextual representations of the claim and evi- dence to two task-specific feed-forward networks which produce a classification and an extractive summary. Kotonya and Toni (2020) take a sim- ilar approach but tailor their model to the pub- lic health domain. The pipeline utilises Sentence- BERT (Reimers and Gurevych, 2019) to filter the evidence, a BERT-based veracity predictor, and a separate BERT-based summarisation model. The work by Kotonya and Toni (2020) differs from Atanasova et al. (2020) as it produces abstrac- tive explanations, which are generally more coher- ent and similar to the way a human would gener- ate a summary, rather than extractive explanations which take sentences verbatim from the evidence. The framework proposed by Stammbach and Ash (2020) also produces abstractive explana- tions, but places higher emphasis on the evidence retrieval process. The framework consists of two components: 1) an evidence retrieval and verac- ity prediction module, and 2) an explanation gen- eration module. The first component is an en- hanced version of the DOMLIN system (Stamm- bach and Neumann, 2019), which uses separate BERT-based models for evidence retrieval and veracity prediction. For explanation generation, GPT-3 (Brown et al., 2020), a large pertained multi-purpose NLP model based on the Trans- former, is used in ‘few-shots’ mode to generate a summary of the evidence with respect to the claim. The system we present in this paper differs to the existing literature as rather than using two sep- arate models for the veracity prediction and expla- nation generation, a single model is used to output both a veracity prediction and an abstractive sum- marisation. 2.2 BART Transformer Architecture BART (Lewis et al., 2020) is a transformer (Vaswani et al., 2017) model that aims to gener- alise the capabilities of both BERT (Devlin et al., 2019) and GPT-style models. It consists of a bi- directional encoder, similar to BERT, as well as an auto-regressive decoder, similar to GPT. BART is pre-trained on a de-noising task whereby input text is corrupted and the model aims to reconstruct the original document, minimising the reconstruc- tion loss. In contrast to existing de-noising mod- els, BART is more flexible in that it is not trained to rectify a specific type of input corruption, but rather any arbitrarily corrupted document. The pre-trained BART model can be fine-tuned to a number of downstream tasks. The authors19 noted that the model performs comparably to other models, such as RoBERTa (Liu et al., 2019b), on natural language inference tasks. They also note that BART outperforms current state-of-the-art models on natural language generation tasks, such as summarisation (Lewis et al., 2020; Shleifer and Rush, 2020). Its ability to perform well on these two contrasting tasks made it an attractive choice as the base model for a system that can jointly pre- dict the veracity of a claim, an inference task, and provide an explanation, a generative task. 3 A Model for Jointly Predicting and Explaining Truthfulness Many of the systems in the reviewed literature use separate Transformer models for veracity predic- tion and explanation generation. Outlined here is our proposed architecture, E-BART, that jointly outputs a veracity prediction, as well as a human- readable, abstractive explanation addressing RQ1. To adapt the BART-large encoder-decoder model to this downstream task, a ‘joint prediction’ head was developed. This head sits atop the BART model, and manipulates the transformer hidden states into the form of the desired output. Both the BART base model and the joint prediction head can be fine-tuned as a single unit to customise pre- trained BART weights to the joint prediction task. The joint prediction head is depicted in green in Figure 1. The head takes as input the final de- coder hidden state embeddings. It then passes all embeddings to a single feed-forward layer to pro- duce a series of logits which form the basis of the predicted explanation. To facilitate classification, the hidden state embeddings corresponding to the final sequence separator token (</s> in BART) are extracted and passed to a small feed-forward network to shape the output to the desired number of classes. The logits obtained from this are then passed to a final soft-max layer to produce proba- bilities for each class. Unlike in BERT which uses embeddings corresponding to the [cls] token which is pre-pended to the input to perform classi- fication, in BART the final sequence separator to- ken is used instead as the decoder can only attend to the left of the current token. This conditions the classification on the entire input sequence. It is instructive to consider the training and inference processes separately, as they differ slightly due to the auto-regressive nature of the BART decoder. During training, the encoder generates hidden Figure 1: E-BART Training configuration. Figure 2: E-BART Inference process. states from the tokenised input that are then in- jected into the decoder. The tokenised gold sum- mary is presented to both the input and sum- marization output of the decoder, with the input shifted right by one token. This conditions the decoder to predict the next token given the cur- rent token. Concurrently, the classification labels are presented to the classification output of the joint prediction head. The loss is calculated as the weighted sum (with parameters α and (1 − α)) of the Cross Entropy Loss computed between the summarisation logits and the gold summary, and the Cross Entropy Loss between the classification logits and the ground truth classification. Figure 2 shows the inference process. Run- ning inference on the model begins by running20 the encoder with the tokenised input to generate the encoder hidden states, as before. In contrast to the training process, the decoder is presented with the start sequence token (<s> in BART), and generates logits auto-regressively, guided by a beam search. The final phase of inference runs the decoder with the entire generated sequence pre- sented at its input. At this point, the joint predic- tion head extracts the embeddings corresponding to the token immediately before the final sequence separator token from the generated sequence. This is done to mirror the training process. These em- beddings are passed to the classification compo- nent of the joint prediction head, and then to a soft-max layer to produce the final classification. 4 Experimental Evaluation 4.1 Datasets To evaluate the proposed models we make use of different datasets. The FEVER dataset consists of 185,445 claims, associated evidence, and ve- racity labels. The claims were generated by ma- nipulating sentences taken from Wikipedia, and are labelled with either “Supports”, “Refutes”, or “Not enough info” based on whether the evidence entails the claim (Thorne et al., 2018). The e-FEVER dataset by Stammbach and Ash (2020) augments the original FEVER dataset (Thorne et al., 2018) with explanations generated by their framework. It consists of 50,000 exam- ples from the FEVER train set, and 17,687 from the development set. This provides a resource with claims, retrieved evidence, veracity labels, and ex- planations. The e-SNLI dataset (Camburu et al., 2018) ex- tends the SNLI dataset (Bowman et al., 2015) with human-generated explanations for each of the 570k examples. The SNLI task is to take two sentences and predict whether one entails, contra- dicts, or is neutral with respect to the other. e- SNLI adds complexity by also requiring a gener- ated explanation for the label. 4.2 Training Methodology To investigate RQ2 and evaluate the performance of the proposed model on the FEVER and ex- tended e-FEVER tasks, two different versions of the model were trained. In the e-FEVER dataset, if the GPT-3 component decided that the retrieved evidence was insufficient, it would produce a de- fault ’null’ explanation. Our first model, E- BARTSmall, was trained on the subset of the e- FEVER training set that did not include null ex- planations. This resulted in 40,702 examples. To process the data, the “+” character used to sepa- rate page titles from evidence was removed. The model inputs were tokenised and formatted as: “<s> claim </s> evidence </s>”. The verac- ity labels were made numerical and explanations were tokenised in a similar manner. The processed dataset was used to fine-tune the BART-large model with joint prediction head for 3 epochs. Our second model, E-BARTFull, was trained in ex- actly the same way as the first, however it was trained using the entire e-FEVER training set, in- cluding examples with null explanations. 4.3 Evaluation Methodology The development split of the e-FEVER dataset was prepared identically to the training split, pro- ducing e-FEVER Full and e-FEVER Small which do, and do not, include examples with null expla- nations, respectively. When evaluating the veracity prediction accu- racy of the models, it was noted that including the “Not enough info” class could under-represent the actual classification performance. Take the ex- ample in Table 1, which has a ground truth label of “Not enough info”. Manual inspection shows that the explanation and evidence indicate that the claim is indeed refuted, which was correctly pre- dicted by our model. Hence we report two sets of results, one with, and one without examples that have a e-FEVER label of “Not enough info”. 4.4 Evaluation Results on Original FEVER To compare with existing models, we report the classification performance of E-BART on the orig- inal FEVER development set. The DOMLIN sys- tem (Stammbach and Neumann, 2019) was used for evidence retrieval (discarding its veracity pre- dictions) to provide evidence for 17k out of the 20k examples in the development set. We use our E-BART models to generate veracity predictions for the 17k examples, and then label the remaining with ‘Not enough info,’ as specified in the DOM- LIN paper. Results are reported for the develop- ment set rather than the test set, as ground-truth labels were not published for the latter. On the FEVER dataset, E-BARTSmall and E- BARTFull achieved label accuracies of 75.0 and 75.1, respectively, outperforming state-of-the-art methods. For comparison, other published model21 Table 1: Ground truth label is “Not enough info” and predicted label is “Refutes”. Claim Evidence Generated Expla- nation Marnie was di- rected by some- one who was “The Master of Nothing”. Alfred Hitchcock Sir Alfred Joseph Hitchcock (13 August 1899-29 April 1980) was an English film director and pro- ducer, at times referred to as “The Master of Suspense”. Marnie (film) Marnie is a 1964 American psychological thriller film directed by Alfred Hitchcock. Marnie was di- rected by Alfred Hitchcock, who was “The Master of Suspense”. accuracies on this dataset include: BERT-BASED 74.6 (Soleimani et al., 2020), DOMLIN 72.1 (Stammbach and Neumann, 2019), UCL MR 69.7 (Yoneda et al., 2018), UNC 69.6 (Nie et al., 2019), and UKP-Athene 68.5 (Hanselowski et al., 2018). E-BART compares favourably to the existing lit- erature despite the e-FEVER training set having 95k less examples compared to FEVER, which the other models were trained on. It is hypothe- sised that the performance improvements are de- rived from using BART as a base model, and from requiring the model to further attend to the most relevant evidence in forming an explanation. The most noteworthy comparison is between E-BART and DOMLIN, which use identical evidence re- trieval mechanisms, thus isolating the contribution of E-BART over standard veracity predictors. 4.5 Evaluation Results on e-FEVER Table 2 shows the results obtained on the develop- ment e-FEVER dataset. To the best of our knowl- edge, there have been no other results reported on this recent dataset, hence we present a comprehen- sive snapshot of E-BART’s performance. Perhaps unsurprisingly, both our models per- formed better on e-FEVER Small, which con- tained less inconclusive examples. More sur- prising is the consistency of E-BART’s perfor- mance regardless of whether it was trained on e- FEVER Small or e-FEVER Full. This indicates that E-BART is robust to situations where evi- dence is sparse. Table 3, qualitatively shows that the model can even express the fact that it was not able to find relevant evidence. The ROUGE metrics evaluate the consistency between the generated and e-FEVER dataset ex- planations, but are not necessarily representative of explanation quality. For instance, the explana- tion generated by GPT-3 may include some addi- tional information compared to E-BART. Whether this additional information results in a better ex- planation compared to something more succinct is largely subjective and dependent on the system’s use case. In Tables 1, 3 and 4, we present exam- ples from the development set. 4.6 Evaluation Results on e-SNLI The e-SNLI task presents a similar challenge to e- FEVER, whereby the entailment between two sen- tences is predicted (similar to predicting veracity of a claim with respect to evidence), and an expla- nation is generated. A different version of the E-BART model was trained specifically on this dataset. The data was prepared by enumerating the labels, remov- ing noisy data, and tokenising the summaries. The first and second sentences were concatenated and tokenised in the same way as the claim and expla- nation for the e-FEVER evaluation. On the test e-SNLI dataset, E-BART achieved a label accuracy of 90.1 and a BLEU score of 32.70. The model proposed in conjunction with the e- SNLI dataset, e-INFERSENT, achieved an accu- racy of 84.0 and BLEU score of 22.4 (Camburu et al., 2018). In calculating the BLEU metric for the explainable models, the first two gold expla- nations were used as references as per (Camburu et al., 2018). As a further comparison, the follow- ing are the best performing models published in the literature1 which, however, do not provide ex- planations: CA-MTL 92.1 (Pilault et al., 2021), SemBERT 91.9 (Zhang et al., 2020), MT-DNN 91.6 (Liu et al., 2019a), SJRC 91.3 (Zhang et al., 2018), D-CRCo-AN 90.1 (Kim et al., 2019), and LMTransformer 89.9 (Radford et al., 2018). The performance of E-BART compares favourably with the state-of-the-art for this different task, corroborating the result from the FEVER task, and further confirming that E-BART 1https://nlp.stanford.edu/projects/ snli/22 Table 2: Effectiveness of the models on the e-FEVER dataset. Model Dataset Accuracy Accuracy ROUGE ROUGE ROUGE ROUGE no N.E.I full 1 2 L Sum E-BARTSmall eFEVER Small 87.2 78.2 73.581 64.365 71.434 71.585 E-BARTSmall eFEVER Full 85.4 77.1 59.447 50.177 57.697 57.782 E-BARTFull eFEVER Small 87.1 78.1 64.530 55.283 62.691 62.820 E-BARTFull eFEVER Full 85.2 77.2 65.511 57.598 64.071 64.144 Table 3: Ground truth label is “Supports” and predicted label is “Supports”. Claim Evidence Generated Explanation CBS is the network that aired The Millers The Millers The multi-camera series aired from October 3, 2013 to July 18, 2015 and ran 34 episodes over two seasons on CBS. CBS announced the cancellation of The Millers on November 14, 2014, four episodes into the show’s sec- ond season. The Millers aired on CBS, however it does not say whether it was the net- work that aired it. is flexible enough to provide explanations without compromising its performance on the main task. To compliment the BLEU metric, we provide some examples in Tables 5 and 6 for manual verification of explanation quality. 4.7 Validating the Use of Joint Models: Experiment 1 To further investigate RQ2 and test the ability of our joint models, we compare the performance of E-BART to a pipeline that produces a veracity pre- diction and generates an explanation using two in- dependent models. To ensure that the results be- tween the two methods are comparable, the archi- tecture and training methodology was kept as con- sistent as possible. The separate pipeline, which we term Separate-BART, uses a BART-based se- quence classifier, and a BART-based model for language generation. Both E-BART and Separate- BART were initialised with the same pre-trained weights, and were trained and evaluated on e- FEVER Small. However due to memory con- straints, the inputs were truncated to a maximum length of 256 tokens (which only truncated 4.56% of examples). In addition to this, a virtual batch size of 32 was used (batch size four, with eight gradient accumulation steps) to overcome conver- gence issues. When training the sequence gener- ator model, a batch size of two with two gradient accumulation steps was used, also due to memory restrictions on available hardware. In comparison, the joint model was trained with a batch size of four and no additional gradient accumulation. The results in Table 7 indicate that the predic- tion performance of both types of model is al- most identical, with Separate-BART being slightly more effective. Manual inspection of the gener- ated explanations revealed that both were of a sim- ilar quality in terms of expressiveness and cohe- siveness. This experimental result reinforces what was seen in the practical evaluations on e-FEVER and e-SNLI: that E-BART is able to jointly pro- vide an explanation without diminishing the per- formance on its main task. 4.8 Validating the Use of Joint Models: Experiment 2 This experiment aims to investigate whether the internal consistency between the predicted verac- ity and predicted explanation differs between the joint and separate models. We use the same E- BART and Separate-BART models from Experi- ment 1, but train an additional ’judge’ model to predict the veracity of a claim, given an explana- tion. The ground truth veracity labels and dataset explanations from e-FEVER Small were used to train the BART-based sequence classifier. As such, its weights are not conditioned on those of E- BART or Separate-BART, meaning that it is inde- pendent from both models. We run the experiment by taking the claims from the development set and the predicted ex- planations from E-BART. The claims and expla- nations are then passed to the ’judge’ model to23 Table 4: Ground truth label is “Refutes” and predicted label is “Refutes” (E-BART ̇Small Model). Claim Evidence Gold Expla- nation Generated Expla- nation Janet Leigh was incapable of writing. Janet Leigh Janet Leigh (born Jeanette Helen Morri- son; July 6, 1927-October 3, 2004) was an American actress, singer, dancer and author. Janet Leigh She also wrote four books between 1984 and 2002, includ- ing two novels. Janet Leigh was capable of writing. Janet Leigh wrote four books between 1984 and 2002, in- cluding two novels. Table 5: Ground truth label is “Entailment” and predicted label is “Entailment”. Sentence 1 Sentence 2 Generated Explanation One tan girl with a wool hat is running and leaning over an object, while another per- son in a wool hat is sitting on the ground. A tan girl runs leans over an object One tan girl with a wool hat is running and leaning over an object is a rephras- ing of a tan girl runs leans over an ob- ject. produce a veracity prediction. This ’judge’ verac- ity prediction is then compared against the verac- ity prediction from E-BART, and the accuracy is computed. The process was repeated for Separate- BART, and the results are presented in Table 8. The results show a higher accuracy for E-BART as determined by the ’judge’ model. This provides indication that the veracity prediction and expla- nation generated by E-BART are more consistent with each other than those generated by Separate- BART. Ultimately this means that joint models are one step closer to being truly interpretable com- pared to models that generate explanations sepa- rately in a post-hoc manner. While this is not con- clusive proof, it does provide some evidence that there are consistency gains to be made when using joint prediction and explanation models. 4.9 Testing the Impact of Explanations To address RQ3, we experimentally validated the benefit of explanations generated by our model with human annotators, performing the crowd- sourcing study detailed in the following. We col- lected the data using the Amazon MTurk crowd- sourcing platform. To test the impact of machine- generated explanations of truthfulness, we de- ployed two versions of the same human annota- tion task. In the first version (Task 1), we provided participants with a claim from the FEVER dataset and we asked them to provide a truthfulness as- sessment on the true/false binary scale along with a sentence justifying their assessment, as this has been shown to improve assessment quality (Kutlu et al., 2020). In the second version (Task 2), we provided participants with both the claim and the explanation generated by our E-BART system and ask for an assessment and justification. Thus, the only difference between Task 1 and 2 is the absence or presence of the E-BART explanation. Each worker has been asked to assess the truth- fulness of four claims, two labelled in the ground truth as “Supports”, and two labelled as “Refutes”. Each claim has been assessed by ten distinct hu- man participants. To avoid bias, we performed a randomisation process while generating the claim- participant assignments (i.e., in the MTurk HITs). For consistency, we kept the same assignments (i.e., same HITs) for the two versions of the task. Participants were only allowed to complete one version of the task. To ensure high quality of the collected data and to avoid adversarial behaviour, we required participants to spend at least 2 sec- onds on each task page. Figure 3 shows the external agreement between the ground truth and the crowd when considering both the individual participant judgments and the judgements aggregated over the ten participants assessing the same claim using majority vote. As we can see from the figure, the accuracy of the par- ticipants in Task 2 is higher than the ones of Task 1 (0.70 for Task 1 and 0.74 for Task 2 for the indi- vidual; 0.83 for Task 1 and 0.90 for Task 2 for the aggregated judgments). The differences are statis- tically significant according to the Mann-Whitney24 Table 6: Ground truth label is “Neutral” and predicted label is “Neutral”. Sentence 1 Sentence 2 Generated Explanation A young family enjoys feeling ocean waves lap at their feet. A young man and woman take their child to the beach for the first time. Just because a young family enjoys feeling ocean waves lap at their feet does not mean that they take their child to the beach for the first time. Table 7: Effectiveness of the joint and separate models. Model Accuracy Accuracy Rouge Rouge Rouge Rouge no N.E.I full 1 2 L Sum E-BART 87.2 78.2 73.581 64.365 71.434 71.585 Separate-BART 88.1 78.9 73.070 63.634 71.005 71.136 Table 8: Internal consistency of the joint and separate models. Model Accuracy Accuracy no N.E.I full E-BART 91.8 86.8 Separate-BART 90.4 85.8 U test at the p < 0.05 level for both the individual and the aggregated judgements. We can addition- ally observe that the display of explanations (i.e., Task 2) reduces the number of false positives (i.e., claims that are false but are erroneously perceived as being true by human subjects) from 122 to 93; Thus, it appears that the explanations automati- cally generated by our E-BART model have the ef- fect of making people more skeptical about claims (see also Table 3 for an example). Performing sim- ple aggregations and under condition of Task 2, we are able achieve 90% non-expert label accuracy, which is a promising step towards crowdsourced truthfulness annotations (Roitero et al., 2020). 5 Conclusions In this paper we explored the potential of AFC models jointly making a prediction and provid- ing a human-readable explanation for that predic- tion. To this end, we proposed the E-BART archi- tecture and evaluated its performance on the ex- tended FEVER and SNLI tasks. Experimentation revealed that E-BART could achieve results com- parable to the state-of-the-art and simultaneously generate coherent and relevant explanations. We argued that jointly predicting explanations makes AFC systems more transparent, and fosters greater Task 1 Task 2False True crowd RefutesSupports expert 138 122 34 226False True crowd RefutesSupports expert 167 93 47 213False True crowd RefutesSupports expert 17 9 0 26False True crowd RefutesSupports expert 21 5 0 26 Figure 3: External agreement between ground truth and crowd for raw (first row) and aggregated (second row) truthfulness assessments. Task 1 shows just the claim while Task 2 shows the claim and the natural language explanation generated by our E-BART model. trust in the system. Finally, human evaluation of the impact of generated explanations revealed that the explanations provided by E-BART gener- ally make people more accurate in detecting mis- information and more skeptical of a claim they en- counter online.","Brand, E., Roitero, K., Soprano, M., & Demartini, G. (2021, October). E-BART: Jointly Predicting and Explaining Truthfulness. In TTO (pp. 18-27)."
SSAFC_138,Synthetic Disinformation Attacks on Automated Fact Verification Systems,"Automated fact-checking is a needed technology to curtail the spread of online misinformation. One current framework for such solutions proposes to verify claims by retrieving supporting or refuting evidence from related textual sources. However, the realistic use cases for fact-checkers will require verifying claims against evidence sources that could be affected by the same misinformation. Furthermore, the development of modern NLP tools that can produce coherent, fabricated content would allow malicious actors to systematically generate adversarial disinformation for fact-checkers. In this work, we explore the sensitivity of automated factcheckers to synthetic adversarial evidence in two simulated settings: ADVERSARIAL ADDITION, where we fabricate documents and add them to the evidence repository available to the fact-checking system, and ADVERSARIAL MODIFICATION, where existing evidence source documents in the repository are automatically altered. Our study across multiple models on three benchmarks demonstrates that these systems suffer significant performance drops against these attacks. Finally, we discuss the growing threat of modern NLG systems as generators of disinformation in the context of the challenges they pose to automated fact-checkers.",Computer Science,Article,2022,N,Y,13,"From QAnon’s deep state to anti-vaccination campaigns, mis- information and disinformation have flourished in online ecosystems. As misinformation continues to induce harmful societal effects, factchecking of online content has become a critical need to ensure trust in the information found online. 1 However, systematic disinformation campaigns make manual efforts to filter misinformation unrealistic as claims must be verified promptly before they are allowed to spread and affect public opinion. 2 Consequently, new research in automated fact-checking explores designing systems that can rapidly validate political, medical, and other domain-specific claims made and shared online (Thorne and Vlachos 2018; Guo, Zhang, and Lu 2019). * Authors contributed equally Copyright © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Evidence Selection COVID-19 doesn't exist. 5G radio frequency communications have a damaging health impact and these are directly making people sick. COVID-19 is caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Recommended preventive measures include social distancing, wearing face masks in public, ventilation and air-filtering, hand washing, covering one's mouth... COVID-19 is caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The novel virus was identified in Wuhan, China, in December 2019. Severe illness is more likely in elderly patients, and those who have certain underlying medical conditions. They are more sensitive to radio waves from 5G communication. COVID-19 is caused by radiation from 5G Towers” AdvAdd AdvMod Claim: COVID-19 is caused by radiation from 5G Towers” Prediction: False True COVID-19 doesn't exist. 5G radio frequency communications have a damaging health impact and these are making people sick. The radiation depresses peoples’ immune systems and makes them more likely to suffer... Evidence Repository Claim Verification Figure 1: Outline of our two settings for adversarial injection of poisoned content into fact-checker evidence repositories. A popular emergent paradigm in automated fact-checking, Fact Extraction and Verification (F EVER ; Thorne et al. 2018), frames the problem as claim verification against a large repos- itory of evidence documents. As one of the first large-scale datasets designed in this framework, F EVER was released with 185k annotated claims that could be verified against Wikipedia articles. When checking a claim, systems de- signed in this framework search for related documents in the database, and retrieve relevant supporting or refuting evidence from these sources. Then, these systems evaluate whether the retrieved evidence sentences validate or contra- dict the claim, or whether there is not enough information to make a judgment. More recently, the S CI FACT (Wadden et al. 2020) and C OVID FACT (Saakyan, Chakrabarty, and Muresan 2021) benchmarks re-purposed this framework for the sci-PRELIMINARY PREPRINT VERSION: DO NOT CITE The AAAI Digital Library will contain the published version some time after the conference. entific domain by releasing datasets of medical claims to be verified against scientific content (Wang et al. 2020). While this framework has led to impressive advances in fact verifica- tion performance (Ye et al. 2020; Pradeep et al. 2021), current benchmarks assume that the available evidence database con- tains only valid, factual information. However, check-worthy claims are often made about new events that may not be verifiable against extensive catalogues, and that must be checked rapidly to avoid strategic disinfor- mation spread (Vosoughi, Roy, and Aral 2018). Consequently, deployed fact-checkers will need to operate in settings where their available evidence is sourced from contemporaneous re- porting that may be sensitive to the same misinformation they are designed to verify. Currently, malicious actors remain limited by the cost of running disinformation campaigns (DiResta et al. 2018) and the risks of operational discov- ery, 3 impeding the scale at which they can deploy these cam- paigns, and thus the balance of real and false content that fact-checkers must distinguish. However, the development of NLP tools capable of generating coherent disinformation (Zellers et al. 2019; Kreps, McCain, and Brundage 2020b; Buchanan et al. 2021) would allow malicious actors to over- load contemporaneous content with adversarial information and bias the evidence bases used by automated fact-checkers. Furthermore, even in settings where claims may be ver- ified against established and trusted knowledge, misinfor- mation can still find its way into repositories of documents used by fact-checking systems (Kumar, West, and Leskovec 2016). Wikipedia, for example, which underlies F EVER (and other benchmarks; Petroni et al. 2021), acknowledges that much of the content on the platform may be incorrect, and remain so for long periods of time. 4 For example, the Croa- tian Wikipedia was systematically contaminated by pro- nationalist bias over a period of at least 10 years. 5 Moreover, studies have uncovered articles on Wikipedia that were edited to provide favorable accounts on specific topics (e.g., work- ers at a medical device company edited articles to present an optimistic view toward treatments that used their product 6 ). Modern NLP tools would allow malicious users to scale up production of disinformation on these platforms, and increase the perception of false consensus or debate on these topics. In this paper, we evaluate whether automated disinforma- tion generators can effectively contaminate the evidence sets of fact verification systems, and demonstrate that synthetic disinformation drastically lowers the performance of these systems. We define adversarial attacks in two settings: A D - VERSARIAL A DDITION (A DVA DD ; §3), where synthetically- generated documents are added to the document base, and A DVERSARIAL M ODIFICATION (A DV M OD ; §4), where ad- ditional automatically-generated information is inserted into existing documents. In both settings, we curate a large collec- tion of adversarial disinformation documents that we inject into the pipelines of existing fact-checking systems developed for the F EVER , S CI FACT , and C OVID FACT shared tasks. 7 Our results demonstrate that these systems are significantly affected by the injection of poisoned content in their evidence bases, with large absolute performance drops on all models in both settings. Furthermore, our analysis demonstrates that these systems are sensitive to even small amounts of evidence contamination, and that synthetic disinformation is more in- fluential at deceiving fact verification systems compared to human-produced false content. Finally, we provide a discus- sion of our most important findings, and their importance in the context of continued advances in NLP systems. 2 Background In this section, we review the formulation of automated fact checking as fact extraction and verification, and recent ad- vances in automated generation of textual disinformation. Automated Fact-checking: Task Current research into automated fact-checking generally fol- lows the fact verification and extraction procedure of receiv- ing a textual claim expressed in natural language (e.g., Hy- pertension is a common comorbidity seen in COVID-19 pa- tients”), collecting supporting evidence from a repository of available documents (e.g., scientific manuscripts), and mak- ing a prediction as to the claim’s veracity based off the col- lected supporting evidence. Below, we define the two stages of this pipeline: evidence retrieval and claim verification. Evidence retrieval The evidence retrieval stage of fact verification systems is typically decomposed into two steps: document retrieval and sentence retrieval. During document retrieval, documents in the evidence repository that are rele- vant to the claim are selected. Existing methods typically use information retrieval methods to rank documents based on relevance (Thorne et al. 2018; Wadden et al. 2020) or use pub- lic APIs of commercial document indices (Hanselowski et al. 2019; Saakyan, Chakrabarty, and Muresan 2021) to crawl related documents. In the sentence retrieval stage, individual sentences from these retrieved documents are selected with respect to their relevance to the claim, often using textual entailment (Hanselowski et al. 2019), or sentence similar- ity (Thorne et al. 2018) methods. Typically, the number of retrieved sentences is capped for computational efficiency. Claim verification The claim verification stage of the pipeline evaluates the veracity of the claim with respect to the evidence sentences retrieved in the previous stage. De- pending on the content found in the supporting sentences, each claim can typically be classified as supported (SUP), refuted (REF), or not enough information (NEI, though some benchmarks omit this label). Systems must aggregate and weigh the evidence sentences to predict the most likely label. 7 We will release these documents under a Terms of Use to promote research in fact-checking systems in adversarial settings F EVER Claim Starrcade was an annual professional wrestling event that began in 1988. Original Starrcade (1988) was the sixth annual Starrcade professional wrestling pay-per-view (PPV) event produced under the National Wrestling Alliance (NWA) banner . G ROVER Starrcade was a monthly professional wrestling event for the decades between 1988 and 2003 that ran for the entirety of a weekend in Boston , Mass. Media Cloud Goldberg’s perfect 173-0 streak ended at Starrcade 1998 after Kevin Nash scored the fateful pinfall with the help of Scott Hall and his taser gun. S CI FACT Claim Taxation of sugar-sweetened beverages had no effect on the incidence rate of type II diabetes in India. Original The 20% SSB tax was anticipated to reduce overweight and obesity prevalence by 3.0% ... and type 2 diabetes incidence by 1.6% ... among various Indian subpopulations over the period 2014-2023 G ROVER ... analysis of a cone-by-one” kind of survey question in India reached out to -9 145 trillion , including 2,557 separate instances of type II diabetes (which is comparable to the prevalence rate in Pakistan ... Table 1: Sample A DVA DD document excerpts generated by G ROVER for the F EVER and S CI FACT datasets. Automated Fact-checking: Datasets We briefly describe the provenance and structure of our stud- ied datasets and refer the reader to the original works for in-depth descriptions of the construction of these resources. F EVER The F EVER testbed (Thorne et al. 2018) is a dataset of 185,445 claims (145,449 train, 19,998 dev, 19,998 test) with corresponding evidence to validate them drawn from articles in Wikipedia. Because of its scale and originality, the F EVER dataset is one of the most popular benchmarks for evaluating fact verification systems (Yoneda et al. 2018; Nie, Chen, and Bansal 2019; Zhou et al. 2019; Zhong et al. 2020; Subramanian and Lee 2020). S CI FACT The S CI FACT dataset (Wadden et al. 2020) con- tains 1,409 expert-annotated scientific claims and associated paper abstracts. S CI FACT presents the challenge of under- standing scientific writing as systems must retrieve relevant sentences from paper abstracts and identify if the sentences support or refute a presented scientific claim. It has emerged as a popular benchmark for evaluating scientific fact verifica- tion systems (Pradeep et al. 2021). C OVID FACT The C OVID FACT dataset (Saakyan, Chakrabarty, and Muresan 2021) contains 1,296 crowd- sourced claims crawled (and filtered) from the /r/COVID19 subreddit. The evidence is composed of documents provided with these claims when they were posted on the subreddit along with resources from Google Search queries for the claims. Refuted claims were automatically-generated by altering key words in the original claims. Synthetic Disinformation Generation Recent years have brought considerable improvements in the language generation capabilities of neural language models (Lewis et al. 2020; Ji et al. 2020; Brown et al. 2020; Holtzman et al. 2020), allowing users of these systems to pass off their generations as human-produced (Ippolito et al. 2020). These advances have raised dual-use concerns as to whether these tools could be used to generate text for malicious purposes (Radford et al. 2019; Bommasani et al. 2021), which humans would struggle to detect (Clark et al. 2021). Specific studies have focused on whether neural language models could be used to generate disinformation that influ- ences human readers (Kreps, McCain, and Brundage 2020a; Buchanan et al. 2021). One such study directly explored this possibility by training G ROVER , a large-scale, billion- parameter language model on a large dataset of newswire text with the goal of generating text that resembles news (Zellers et al. 2019). In human evaluations of the model’s generated text, the study found that human readers consid- ered the synthetically-generated news to be as trustworthy as human-generated content. While the authors found that neu- ral language models could identify fake, generated content when finetuned to detect distributional patterns in the gen- erated text, they hypothesized that future detection methods would need to rely on external knowledge (e.g., F EVER ). 3 A DVERSARIAL A DDITION : Evidence Repository Poisoning In this section, we simulate the potential vulnerability of fact- checking models to database pollution with misinformation documents by injecting synthetically-generated false docu- ments into the evidence sets of fact verification models, and assess the impact on the performance of these systems. Approach Our method, A DVERSARIAL A DDITION (A DVA DD ), uses G ROVER to produce synthetic documents for a proposed claim, and makes these fake documents available to the fact verification system when retrieving evidence. As G ROVER requires a proposed article title and publication venue (i.e., website link) as input to generate a fake article, we use each claim as a title and set the article venue to wikipedia.com. We generate 10 articles for each claim and split them into paragraphs (n.b., F EVER DB contains first paragraphs of Wikipedia articles and S CI FACT contains abstracts of scien- tific articles). Statistics for the number of documents gener- ated for each benchmark are reported in Table 3. Additional implementation details for the experimental setting of each benchmark can be found in the Appendix. Evidence CorefBERT Acc. KGAT Acc. MLA Acc. (Ye et al. 2020) (Liu et al. 2020b) (Kruengkrai et al. 2021) Total REF NEI Total REF NEI Total REF NEI Original 73.05 74.03 72.07 70.76 72.50 69.01 75.92 78.71 73.13 A DVA DD -min 34.80 47.22 22.38 34.08 48.63 19.52 60.93 73.04 48.81 A DVA DD -full 28.59 39.63 17.54 29.02 42.45 15.59 51.86 71.84 31.87 A DVA DD -oracle 21.18 27.09 15.26 23.43 31.47 15.38 29.05 29.76 28.33 Table 2: Effect of A DVA DD on F EVER claim verification. We bold the largest performance drop relative to the original evidence. Benchmark Evidence Source N F EVER F EVER DB 5,416,537 G ROVER Docs 995,829 MediaCloud Docs 74,273,342 S CI FACT Scientific Abstracts 5,183 G ROVER Docs 21,963 C OVID FACT Google Search Results 1,003 G ROVER Docs 2,709 Table 3: Corpus statistics of evidence repositories F EVER Study Setup For the F EVER benchmark, we select three high- ranking models from the leaderboard 8 with open-source im- plementations: KGAT (Liu et al. 2020b), CorefBERT (Ye et al. 2020), and MLA (Kruengkrai, Yamagishi, and Wang 2021). For document retrieval, all models use the rule-based method developed by Hanselowski et al. (2019), which uses the MediaWiki API to retrieve relevant articles based on named entity mentions in the claim. For each claim and poisoned document, we extract all keywords and retrieve associated Wikipedia pages. If we find overlaps between the associated Wikipedia pages of a claim and a poisoned docu- ment, then the poisoned document is matched with the claim for document retrieval. Once the retrieved documents are available, the KGAT and CorefBERT models use a BERT- based (Devlin et al. 2019) sentence retriever to rank evidence sentences based on relevance to the claim (trained using pair- wise loss). The MLA sentence retriever expands on this ap- proach with hard negative sampling from the same retrieved documents to more effectively discriminate context-relevant information that is irrelevant to the claim. Claim verifiers vary between models, but are generally based off pretrained language models (e.g., CorefBERT, MLA) or graph neural networks (e.g., KGAT). We use the REF and NEI claims from the F EVER development set to study how the preceding sys- tems are affected by the introduction of poisoned evidence.9 Impact of A DVA DD We report the overall (and claim- stratified) performance change of the tested models in Ta- ble 2. For all models, we see a significant performance drop 9 We discuss results related to SUP claims in the Appendix. Figure 2: CorefBERT’s predictions change from REF and NEI to SUP once A DVA DD poisons the evidence set. when G ROVER -generated paragraphs are introduced into the available evidence set (A DVA DD -full), indicating that fact verification models are sensitive to synthetically-generated information. This drop approaches the performance of an oracle (A DVA DD -oracle), where only G ROVER -generated documents are made available as evidence. As confirmation that these attacks work as expected, we depict in Figure 2 how model predictions change once the synthetic disinformation is added to the evidence set. A signif- icant number of claims that were originally predicted as REF or NEI are now predicted as SUP with the injected poisoned evidence. Consequently, we conclude that the poisoned evi- dence affects the model’s predictions in the intended way, and that cross-label changes for different pairings are rare. Fur- thermore, we also find that replacing the retrieved poisoned evidence with random retrieved evidence from F EVER DB does not cause the same performance drop (⇠ 7% vs. ⇠ 30%), indicating that these effects are caused by the injection of poisoned evidence, and not merely the removal of potentially relevant evidence (see Appendix B for further details). Effect of disinformation scale We also evaluate a setting where the attack is limited to retrieving only a single contam- inated evidence sentence (A DVA DD -min). The performance drops in the min setting are still considerable, suggesting that even limited amounts of injected disinformation can signifi- cantly affect downstream claim verification performance. Moreover, Figure 3 shows a histogram of the number of poisoned evidence sentences retrieved per claim and a strat- G ROVER Evidence (a) REF (b) NEI MediaCloud Evidence (c) REF (d) NEI Figure 3: Degree of evidence poisoning and resulting REF (a,c) and NEI (b,d) claim verification accuracy for A DVA DD - G ROVER (a,b) A DVA DD -MediaCloud (c,d) ified analysis of the final predictions. Figures 3 (a)(b) for A DVA DD -G ROVER (i.e., A DVA DD -full) show that accuracy steadily drops for claims with more corresponding evidence from poisoned documents; meanwhile, the likelihood that a prediction changes increases with more poisoned evidence. We note that claims labeled NEI are far more sensitive to the introduction of poisoned sentences than REF claims, even as the rate of contamination is approximately the same between both types of labels. While this result is promising because the model is more robust in the presence of even minimal refuting evidence, it also demonstrates that fact verification systems are more sensitive when no competing information is presented to a particular viewpoint (i.e., data voids; Boyd and Gołebiewski 2018). Quality of poisoned evidence We also evaluate whether poisoned evidence produced by A DVA DD is of sufficient quality to bypass potential human detectors. For 500 REF and 500 NEI claims from F EVER , we ran a study on Mechani- cal Turk where we presented three workers with five retrieved evidence examples (which could be from A DVA DD or from F EVER DB) and asked them to identify which examples were machine-generated. Our results show that humans underesti- mate the number of poisoned sentences (23.6% recall), and do not distinguish machine- from human-generated evidence (48.6% precision). While well-trained workers will improve at recognizing synthetic content, our results demonstrate the challenge of distinguishing these evidence sources for lay readers, pointing to the quality of the synthetic content, and the potential for such an attack to remain undetected. Source Evidence Retrieval Document Sentence G ROVER 87% 65% MediaCloud 99% 17% Source Claim Verification KGAT CorefBERT Original 70.76 73.05 + G ROVER 29.02 28.59 + MediaCloud 67.10 70.44 Table 4: Statistics and performance relative to the source of poisoned evidence: G ROVER or MediaCloud Comparison with human-compiled online evidence While we have shown that synthetic disinformation affects the performance of downstream claim verifiers when present in their evidence sets, the threat should be evaluated in com- parison to the threat of already existing online misinformation on the same topic. Consequently, we use the MediaCloud10 content analysis tool to crawl web content related to F EVER claims. We crawl English-language news since January 2018 that contains the keywords of a claim anywhere in their text and extract articles with a title that contains at least one key- word from the claim. Finally, we process these articles to have the same format as the original Wikipedia database, yielding 74M total available documents for retrieval (Table 3). In Table 4, we report the performance of an A DVA DD setting where only MediaCloud-crawled documents are avail- able to the retriever compared to our original setting where G ROVER -generated documents were available. We observe that evidence crawled from online content has less of an in- fluence on downstream fact verification performance (⇠ 3% vs. ⇠ 40% performance drop). While we are able to retrieve far more documents from MediaCloud due to the size of the database (99% of claims retrieve a document from Media- Cloud), the sentences from A DVA DD -G ROVER documents are selected more frequently in the sentence retrieval step. While this gap would likely be less pronounced with more contentious claims that yield competing viewpoints (Bush and Zaheer 2019), these results demonstrate that synthetic dis- information can be much more targeted to a particular claim of interest. Figure 3 supports this conclusion, where we ob- serve smaller performance drops for A DVA DD -MediaCloud (c,d) compared to A DVA DD -G ROVER (a,b) even when all retrieved sentences are sourced from the poisoned evidence. S CI FACT Study Setup For S CI FACT , we chose three systems for testing our attack: VeriSci (Wadden et al. 2020), ParagraphJoint (Li, Burns, and Peng 2021), and SciKGAT (Liu et al. 2020a). The VeriSci model was released by the creators of the S CI FACT benchmark and retrieves relevant abstracts to a claim using TF-IDF. The ParagraphJoint model, one of the top systems 10 mediacloud.org Model Evidence Set Sentence Sentence Abstract Abstract selection label label rationalized VeriSci Original 47.69 42.62 51.03 48.45 (Wadden et al. 2020) A DVA DD 27.05 23.50 25.57 24.33 SciKGAT Original 55.61 51.69 58.04 57.41 (Liu et al. 2020a) A DVA DD 39.44 36.97 37.46 36.98 ParagraphJoint Original 53.63 43.59 55.52 49.55 (Li, Burns, and Peng 2021) A DVA DD 37.68 32.60 41.31 37.12 Table 5: Effect of A DVA DD evidence on the S CI FACT benchmark. We bold performance drops relative to the original evidence. on the S CI FACT leaderboard, uses a word embedding-based method to retrieve abstracts. Both use a RoBERTa-based mod- ule for rationale selection and label prediction. The SciKGAT model uses the same evidence retrieval as VeriSci, but the KGAT model (Liu et al. 2020b) to verify claims. We use the 300 claims from the development set to evaluate our method. We generate G ROVER articles as with F EVER , but we set the venue URL to medicalnewstoday.com, which produces articles more likely to reflect scientific and medical content. Results In Table 5, we observe large performance drops across all metrics for all models. Furthermore, we note that our disinformation generator, G ROVER , is not trained on large quantities of scientific documents of the same format as the original evidence. Despite producing documents that are stylistically different, the disinformation is still retrieved as evidence, and affects the performance of the verifier. C OVID FACT Study Setup We run our analysis on the baseline system from Saakyan, Chakrabarty, and Muresan (2021). This model re- trieves evidence documents for claims using Google Search and then selects evidence sentences based off high cosine similarity between sentence embeddings of the claims and individual evidence sentences (Reimers and Gurevych 2019). A RoBERTa-based model predicts a veracity label. We gen- erate A DVA DD articles in the same manner as for S CI FACT , and run our analysis on the 271 REF claims from the test set. Results In both the Top-1 and Top-5 settings from Saakyan, Chakrabarty, and Muresan (2021), we observe a ⇠ 14.4% performance drop on REF claims (83.8% ! 69.4%). We note that C OVID FACT random and majority accuracy is only 67.6% due to a label imbalance. 4 A DVERSARIAL M ODIFICATION : Evidence Document Poisoning In Section 3, we investigated the effect of adding disinforma- tion documents to the evidence repositories of fact verifica- tion systems, simulating the setting where the dynamic pace of news might lead to fake information being used to verify real-time claims. However, even in settings where informa- tion has more time to settle and facts to crystallize, misinfor- mation can still find its way into repositories of documents used by fact-checking systems. Motivated by the possibility of malicious edits being made to crowdsourced information resources, we explore how NLP methods could be used to automatically edit existing articles with fake content at scale. Approach Our method, A DVERSARIAL M ODIFICATION (A DV M OD ), simulates this setting in a two-stage process. First, we use off- the-shelf NLP tools to generate modified versions of the claim presented to the fact verifier. Then, we append our modified claims to articles in the evidence base that are relevant to the original claim. We modify the original claims in two ways. In the paraphrase approach, we use a state-of-the-art para- phrasing model, PEGASUS (Zhang et al. 2019), to generate paraphrased versions of the original claim (see Table 7 for ex- amplea). These paraphrases generally retain the meaning of the claim, but often remove contextualizing information that would be found in the context of the article in which the new sentence is inserted. Because the paraphrase method attempts to produce synthetic evidence that is semantically equivalent to the original claim, we test its efficacy relative to a method that merely introduces irrelevant content to the evidence doc- ument. Motivated by Jia and Liang (2017), we alter a claim by applying heuristics such as number alteration, antonym sub- stitution, and entity replacement with close neighbors accord- ing to embedding similarity (Bojanowski et al. 2017). These modifications should not confuse humans, but would affect sensitive fact verification systems, providing a competitive baseline for assessing the strength of A DV M OD -paraphrase. Finally, our oracle reports the performance when the claim itself is appended to an evidence document. Results Our results in Table 6 demonstrate that injecting poisoned evidence sentences into existing documents is an effective method for fooling fact verification systems. Our A DV- M OD -paraphrase method causes a significant drop on all tested models for both REF and NEI labeled claims. Fur- thermore, we also note that A DV M OD -paraphrase achieves larger performance drops than the baseline method, A DV- M OD -KeyReplace, for most claim types (the KGAT model is slightly more sensitive to the baseline A DV M OD -KeyReplace for the NEI claims), indicating that injections of disinforma- tive content are more effective than non-targeted perturbances to the evidence (e.g., A DV M OD -KeyReplace). Evidence CorefBERT Acc. KGAT Acc. MLA Acc. (Ye et al. 2020) (Liu et al. 2020b) (Kruengkrai et al. 2021) Total REF NEI Total REF NEI Total REF NEI Original 73.05 74.03 72.07 70.76 72.50 69.01 75.92 78.71 73.13 A DV M OD -KeyReplace 53.83 66.50 41.15 42.82 68.90 16.74 60.93 81.83 40.02 A DV M OD -paraphrase 32.62 36.66 28.58 37.22 51.74 22.70 52.72 70.57 34.86 A DV M OD -oracle (claim) 4.78 7.94 1.61 11.90 23.78 0.02 25.17 45.96 4.37 Table 6: Effect of A DV M OD on F EVER claim verification. We bold the largest performance drop relative to the original evidence. Original Damon Albarn’s debut album was released in 2011. Paraphrase Albarn’s first album was released in 2011. His debut album was released in 2011. KeyReplace Matt Coldplay’s debut album was released in 202. Stefan Blur’s debut album was released in 1822. Table 7: Sample A DV M OD sentences 5 Discussion Adding synthetic content to the evidence bases of fact ver- ifiers significantly decreases their performance. Below, we discuss interesting findings and limitations of our study. Synthetic vs. human disinformation As mentioned in Section 3, the performance of our test systems is more sen- sitive to poisoned evidence generated from G ROVER than retrieved from MediaCloud, even as the number of documents retrieved from MediaCloud far exceeds the number generated from G ROVER . While F EVER claims may not generally be worth opposing online (leading to less directly adversarial content being retrieved from MediaCloud), we note that lan- guage models have no such limitations, and can generate large quantities of disinformation about any topic. Conse- quently, while misinformation already makes its way into retrieval search results (Bush and Zaheer 2019), language models could cheaply skew the distribution of content more drastically (Bommasani et al. 2021), particularly on topics that receive less mainstream coverage, but may be of import to a malicious actor (Starbird et al. 2018). Language models as a defense In the A DVA DD and A DV- M OD oracles settings, all tested systems performed better on claims labeled REF than for claims labeled NEI. This result implies that the G ROVER -generated evidence was less adver- sarial for these claims, or that the pretrained models which these systems use to encode the claim and evidence sentences were more robust against claims that should be refuted. Con- sequently, we conclude that language models encode priors about the veracity of claims, likely from the knowledge they learn about entities during pretraining (Petroni et al. 2019), a conclusion also supported by contemporaneous work in using standalone language models as fact-checkers (Lee et al. 2020, 2021). While this property can be an advantage in some settings (i.e., language models pretrained on reliable text repositories will be natural defenses against textual misin- formation), it will also be a liability when previously learned erroneous knowledge will counteract input evidence that con- tradicts it. Finally, we note that the presence of implicit knowl- edge in language models affecting the interpretation of input evidence implies that the training corpora of these LMs could be attacked to influence downstream fact verification. Prior work has explored poisoning task-specific training datasets (Wallace et al. 2021). As disinformation becomes more preva- lent online, the pretraining corpora of LMs will require more careful curation to avoid learning adversarial content. Limitations We identify three main limitations to our study. First, the F EVER document retrievers use the MediaWiki API to collect relevant Wikipedia articles based on entity men- tions in the claim. We assume our synthetic content could be included in the retrieved documents if it were titled with a mention of the named entities in the claim. For S CI FACT , this limitation is not present because synthetic abstracts are retrieved using statistical IR methods. Second, our method A DVA DD uses the actual claim to generate the synthetic article. In the absence of explicit coordination, synthetic poi- soned evidence would be generated without knowledge of the exact claim formulation, reducing the reali","Du, Y., Bosselut, A., & Manning, C. D. (2022). Synthetic Disinformation Attacks on Automated Fact Verification Systems. arXiv preprint arXiv:2202.09381."
SSAFC_141,The Case for Assessing Claim Difficulty in Automatic Fact-Checking,"Fact-checking is the process of evaluating the veracity of claims (i.e., purported facts). In this opinion piece, we raise an issue that has received little attention in prior work – that some claims are far more difficult to fact-check than others. We discuss the implications this has for both practical fact-checking and research on automated fact-checking, including task formulation and dataset design. We report a manual analysis undertaken to explore factors underlying varying claim difficulty and identify several distinct types of difficulty. We motivate this new claim difficulty prediction task as beneficial to both automated fact-checking and practical fact-checking organizations.",Computer Science,Article,2022,N,Y,0,"Misinformation (and closely related disinforma- tion, fake news, and propaganda) can cause seri- ous societal harm, such as influencing election out- comes, public health during a pandemic, or finan- cial markets (Reuters, 2021). Such concerns have motivated NLP work into automatic fact-checking (Thorne et al., 2018; Popat et al., 2018; Augenstein et al., 2019; Hanselowski et al., 2019; Wadden et al., 2020; Gupta and Srikumar, 2021; Saakyan et al., 2021). This task remains quite challenging; recent performance on key datasets for natural claims is less than 50% F1 (Atanasova et al., 2020; Augen- stein et al., 2019; Gupta and Srikumar, 2021). In this work, we analyze Politifact1 fact-checks and several NLP datasets based on them (Wang, 2017; Alhindi et al., 2018; Augenstein et al., 2019; Shu et al., 2018). Our analysis shows that some claims are far more difficult to fact-check than others, an issue that has received little attention. To illustrate this, Figure 1 shows several claims that were fact-checked by Politifact. For exam- ple, Claim 3 can be directly verified by a simple ∗ Both authors contributed equally. search leading to a major media news article. In contrast, Claim 5 appears to lack any direct sources (i.e. written evidence, online or otherwise) and his- torians were eventually consulted. These examples illustrate the vast range of difficulty across claims. Incorporating claim difficulty into fact-checking has conceptual, research, and practical implications. Conceptually, while prior work has modeled claim check-worthiness (to triage potential fact-checks) (Barrón-Cedeno et al., 2020; Nakov et al., 2021), a claim may be check-worthy yet extremely difficult or impossible to fact-check. For research, design- ing benchmarks to incorporate claims of varying difficulty could provide greater insights in bench- marking and assessing state-of-the-art capabilities. In practice, because the scale of new claims daily far exceeds the number of human fact-checkers, prioritizing which few claims to fact-check is es- sential. Claim difficulty prediction could help prior- itize claims that are not only check-worthy but also easy or fast to check, so that more claims could be checked. Easier claims might be delegated to less-skilled fact-checkers, or with human-in-the- loop architectures, to the automated model so that fact-checkers can focus on more challenging cases. We know of no prior work motivating this claim difficulty prediction task, or more fundamentally, investigating the underlying factors for why some claims are more difficult than others. In this opin- ion piece, we motivate this task and the opportu- nity it presents for NLP research to impact fact- checking in a novel and important way. The anal- ysis of claims we present leads us to identify five distinct factors contributing to claim difficulty. Modeling and predicting difficulty of different task instances is already recognized in other areas, such as machine translation (Mishra et al., 2013; Li and Nenkova, 2015), syntactic parsing (Garrette and Baldridge, 2013), or search engine switching behaviors (White and Dumais, 2009), among oth- ers. We believe that instance difficulty prediction arXiv:2109.09689v2 [cs.CL] 4 Feb 2022 [1: Mostly False] Midtown crime is up by 30 percent the last quarter. [Crime, Florida, Rick Baker] a. [therealdeal.com] Manhattan leasing strong in Q3; Midtown market stratifies ... b. [www.news-leader.com] Springfield police say a spike in stolen cars led to a 2 percent increase in crime in the third quarter of 2016 Unjudgeable. Claim is ambiguous. [2: Mostly True] There are more members of the U.S. Senate than the number of WI families who would benefit from GOP estate tax break. [Taxes, Wisconsin, Tammy Baldwin] a. [politicalticker.blogs.cnn.com] The candidates for Senate in Wisconsin stuck to their talking points in an animated debate Monday night... b. [www.dailypress.com] The Senate ... voted for a new tax break benefiting companies that shifted headquarters to foreign countries, but move back and invest in ... Hard. Poor source support. Lots of search results but none relevant. [3: Pants on Fire] Mark Pocan’s proposal to eliminate the U.S. Immigration and Customs Enforcement (ICE) would eliminate border enforcement. [Immigration, Legal issues, Crime, Regulation, Wisconsin, Leah vukmir] a. [www.vox.com] And it was split into three separate agencies: Legal immigration processing went to US Citizenship and Immigration Services, border enforcement went to Customs and Border Protection, and interior enforcement went to ICE. b. [news.yahoo.com] Haggman said his reply is ... that ICE is an internal enforcement agency and its absence wouldn’t affect border security. Easy. Well supported by trustworthy sources. Claim directly validated. [4: Mostly False] Premeditation, in murder cases like the Oscar Pistorius case, can be formed in the twinkling of an eye. [Criminal justice, Legal issues, Crime, Nancy grace] a. [Fox News Transcript] in law school they teach premeditation can be formed in an ininstant. people think you have to plan it out in advance. the reality is here this was an emotional crime. b. [User comment] In other words, the killer had to have formed the present intention of killing her while not in the heat of passion or rage, and carried out the killing with the specific intention of killing her. Nateyes is right that it can happen in a matter of minutes. at least under US law, but there has to be some time to reflect and the very specific state of mind of planning and intending to kill. c. [dutoitattorneys.com] In a recent High Court ruling in the matter of State v Raath, the full bench of the Cape High Court found that where a father forced his son to remove a firearm from the safe so that he could kill the son’s mother was not sufficient to constitute premeditated murder and it is with that in mind that we are of the view that there is no premeditation at this stage present in Ocar Pistorius’ matter. Hard. Well supported but all sources are not trustworthy. [5: Pants on Fire] When President Abraham Lincoln signed his Emancipation Proclamation, there were over 300,000 slaveholders who were fighting in the Union army.” [History, Chuck Baldwin] a. [American Studies Journal] Eventually 184,000 black soldiers fought for the Union army and contributed significantly to the victory of the North. Reports of atrocities among black Union soldiers had shocked Lincoln, and he subsequently issued the General Order No. 252 on July 30, 1863 as an Order of Retaliation that clearly reflected his stance towards the status of African American soldiers b. [International Studies Review] The number of actual slaveholders in the South of the Union does not amount to more than 300,000, a narrow oligarchy that is confronted with many millions of so-called poor whites... Unjudgeable. Well supported by trustworthy sources but claim cannot be validated.Figure 1: Examples illustrating various aspects of claim difficulty. Claims are in boldface, metadata is in [brackets] and snippets from Bing search results are italicized. 4[c] is cited by Politifact but not found by Bing. We submitted claims verbatim to Bing to find up to 50 search results with the constraint that search results predate Politifact’s publication to simulate a realtime search. We only show the most relevant search results for brevity. could yield similar benefits for fact-checking. 2 Claim Difficulty Analysis We analyze several NLP fact-checking datasets that use (real) Politifact data (Wang, 2017; Alhindi et al., 2018; Augenstein et al., 2019; Shu et al., 2018), not- ing that accuracy of fact-checking models tends to be lower on these fact-checking datasets vs. others, indicating the difficulty of these real-world claims. 1. Claim ambiguity. Fact-checking a claim re- quires understanding what it is about. Claim ambi- guity (under-specification) clearly undermines this. For example, in Figure 1’s first claim, the entity Midtown” in is ambiguous, e.g., search results for this claim text tend to retrieve information about Manhattan, NY. Given additional contextual meta- data (’Florida’), ambiguity still remains among Florida cities. Similarly, the relative temporal ex- pression last quarter” requires temporal grounding. For Claim 5, the phrase South of the Union” is ambiguous (states in the south but inside the Union vs. states to the south and outside of the union). Ambiguity arises in many forms, including am- biguous pronouns (Kocijan et al., 2020), ill-defined terms (Vlachos and Riedel, 2014), etc. Vlachos and Riedel (2014) simply side-step the problem by ex- cluding ambiguous claims, Thorne et al. (2018) resolve named entities, and other datasets pro- vide contextual metadata (Wang, 2017; Augenstein et al., 2019), yet targeted decontextualization (Choi et al., 2021) may be needed, as Claim 1 shows. Such ambiguity can complicate all stages of fact- checking: claim identification, de-duplication, and check-worthiness detection; evidence retrieval, and inference. Retrieval based on ambiguous claims can naturally yield poor search results, making ver- ification difficult or impossible. This does not ap- pear to have been considered in constructing most existing datasets (Wang, 2017; Alhindi et al., 2018; Popat et al., 2016; Shu et al., 2018; Augenstein et al., 2019). Even if evidence retrieval is perfect, ambiguity can also undermine inference. 2. Poor ranking. Evidence-based fact-checking assumes retrieval of sufficient, quality evidence. Whereas Vlachos and Riedel (2014) assume evi- dence manually retrieved by experts, automated fact-checking often relies on automated informa- tion retrieval (IR) search engines to obtain evidence. Fact-checking systems (Samarinas et al., 2021) and datasets (Popat et al., 2016; Baly et al., 2018; Au- genstein et al., 2019; Saakyan et al., 2021) have typically used only the top 5-30 search results, but while Web search engines may perform well on retrieval for easy claims (e.g., Claim 3 in Figure 1), difficult claims (Claim 2) are another story. Over- all, we find a vast gulf between sources of evidence consulted by professional fact-checkers vs. Web search engine results. Figure 2 (Appendix) shows that fewer than 10% of domains cited by Politifact on average appear in Bing’s top-50 search results. Domain % Politifact Rank Bing Rank www.politifact.com 7.2% 1 2 www.nytimes.com 2.5% 2 1 www.washingtonpost.com 2.5% 3 12 www.youtube.com 2.4% 4 11 docs.google.com 1.8% 5 16,752 twitter.com 1.6% 6 195 www.whitehouse.gov 1.3% 7 218 www.facebook.com 1.1% 8 189 www.jsonline.com 1.1% 9 267 www.tampabay.com 1.0% 10 1,167 www.politico.com 0.8% 11 31 www.cbo.gov 0.8% 12 1,196 alt.coxnewsweb.com 0.7% 13 N/A www.cnn.com 0.7% 14 4 www.census.gov 0.7% 15 329 www.senate.gov 0.6% 16 9,367 www.bls.gov 0.6% 17 475 www.cdc.gov 0.6% 18 269 Table 1: Top-18 evidence domains cited by Politifact (and their relative frequency) vs. Bing search results rankings. Only the top-2 domains are ranked similarly. Table 1 shows that many of the source domains2 used by Politifact (e.g., government pages, social media pages, google docs, etc.) get low rankings. This calls into question the current practice of simply pairing a verdict by Politifact (or other fact- checkers) with IR search results (Augenstein et al., 2019; Shu et al., 2018; Popat et al., 2016) unless we also triage claim difficulty based on retrieval results to filter out or tag difficult claims. 3. Limited access. Worse than poor ranking of evidence is when it cannot be accessed at all. For example, the deep web (Madhavan et al., 2008) contains far more information than the indexed Web yet is largely inaccessible to search engines. When information can be accessed, can it be read? Prior work has reported parsing difficulty even with HTML files (Augenstein et al., 2019; Vlachos and Riedel, 2014), whereas evidence domains used by Politifact often utilize many other formats. Table 2 shows our finding that roughly 40% of Politifact’s sources are difficult to parse due to issues such as dynamically-rendered javascript, links to pages without the main content (i.e., further link follow- ing needed), paywalls, media, etc. Many sources (7%) require tabular data extraction (Chen et al., 2019; Schlichtkrull et al., 2021). Relevant evidence is sometimes buried in a 500-page government re- port that is too long for existing systems to process well (Wan et al., 2019), or in non-textual formats (e.g., image or video). Such varying access and for- 2We use the term source domains” to refer to the websites that host the retrieved sources, e.g. www.nytimes.com. Reason # % Difficult 165 41.3% Senate Bills 15 3.8% Crawl Protections 3 0.8% Generic link 9 2.3% Indirect link 17 4.3% Javascript 9 2.3% Media 15 3.8% Misc 25 6.3% Not accessible 21 5.3% Paywall 11 2.8% PDFs 12 3.0% Table 28 7.0% OK 235 58.6% Table 2: Domain level statistics for Politifact’s expert sources. We sampled sources from Politifact’s top 400 most cited domains, organized here by parseability. mats across claims contributes to varying difficulty. 4. Unreliable evidence. Claim verification evi- dence must be reliable as well as relevant (Ginsca et al., 2015). We find that IR search results lack high quality sources (e.g., reputable news outlets and scientific venues) for many claims. Consider Figure 1’s Claim 4. The best two sources retrieved are comments from a transcript of a news show and a blog. The news transcript lands credence to the user comment but is insufficient alone because it does not specify its context is US law”, whereas the claim is about South Africa. In fact, this was a key reason for Politifact’s verdict as mostly false”. While such sources can be useful, gauging the degree of relative degree of trust to ascribe to them can be quite challenging. For example, Popat et al. (2016) found their per-domain strategy to be ex- tremely limiting. In general, such varying reliabil- ity of retrieved evidence, and difficulty estimating reliability for less-established sources, is another important contributor to varying claim difficulty. 5. Difficult Inference. Claims also vary greatly in the degree of complex or multi-hop (Jiang et al., 2020) inference required for verification. For (easy) Claim 3 of Figure 1, source b directly states that the absence of ICE would not affect border secu- rity, refuting the claim. In contrast, with (difficult) Claim 2, none of the sources jointly talk about WI families” and GOP estate tax break” together; a model would need to synthesize information from multiple sources. With Claim 5, one would first need to unpack the presuppositions within the claim (e.g., how many slaveholders on each side”, and whether the number is plausible). Politifact cross- referenced historical data, which requires sufficient grasp of pragmatics and common sense, challeng- ing current NLP limitations (Geva et al., 2021; Pan- dia et al., 2021). We also found that Politifact frequently cites documents like Senate bills that contain a variety of domain-specific vocabulary. 3 Where do we go from here? Improving benchmarks. Benchmarks define yardsticks by which empirical success and field progress are often measured. By designing fact- checking datasets to not only incorporate claims of varying difficulty, but to include different aspects of claim difficulty, evaluation could yield greater insights between alternative approaches, as well as better identify which aspects of modeling are working well vs. where future work is most needed. Our analysis of claim ambiguity (#1) calls for greater attention to decontextualize (Choi et al., 2021) ambiguous or underspecified language, in both the claim itself as well as evidence retrieved based on ambiguous claim text. This has compli- cate all stages of fact-checking: claim identifica- tion, de-duplication, and check-worthiness detec- tion; evidence retrieval, and inference. Moreover, if important metadata or additional context used by fact-checkers is omitted from a derived dataset, this can change not only the difficulty but the la- bel. Specifically, when veracity labels go beyond true/false determinations to also include indetermi- nate outcomes, such has not enough info” (Thorne et al., 2018), lack of sufficient context can actually change the correct label for a dataset from a con- crete determination to being unresolvable. How- ever, if annotations are taken directly from fact- checking websites (Popat et al., 2016; Wang, 2017; Alhindi et al., 2018; Baly et al., 2018; Augenstein et al., 2019; Atanasova et al., 2020), simply ingest- ing the official fact-check answer would not capture such a change in verdict from website to dataset. Whereas some fact-checking datasets have as- sumed access to the same evidence manually iden- tified by experts (Vlachos and Riedel, 2014; Al- hindi et al., 2018; Atanasova et al., 2020), enabling researchers to focus purely on the inference step, automated fact-checking in the wild typically as- sumes use of automated (noisy) IR (Popat et al., 2016; Baly et al., 2018; Shu et al., 2018; Augen- stein et al., 2019). A claim that might be easily checked from expert-found evidence may be diffi- cult or impossible to verify using IR search results. Benchmark evaluations might also include con- trolled testing of varying difficulty conditions, such as degree of claim ambiguity and expert-found vs. IR search results. Other evidence conditions might be varied as well, such as the portion of relevant vs. irrelevant evidence, reliable vs. unreliable ev- idence, whether or not sufficient evidence exists, and what formats must be parsed to obtain suffi- cient evidence. Direct IR evaluations could also benchmark evidence retrieval in realistic settings, such as searching the full Web on natural claims. Predicting claim difficulty. Predicting claim dif- ficulty – overall difficulty or different aspects of claim difficulty – promises many potential benefits. I. As discussed above, the ability to predict claim difficulty could yield better fact-checking datasets and benchmarks. II. Claim difficulty prediction could inform or complement model confidence es- timates. Various works (Guo et al., 2017; Dong et al., 2018) have found that posterior probabilities of deep models are not well-calibrated to predic- tive accuracy. Since model errors likely correlate with claim difficulty, difficulty prediction could pro- vide orthogonal information. III. An explainable difficulty prediction model could lead to a more flexible approach for fact-checking, for example, to develop model architectures with inductive bi- ases that exploit the specific aspects of the claim difficulty task that we have identified. Model ex- planations might also be tailored to claim difficulty, providing more thorough explanations for more difficult claims. For claims predicted to be too diffi- cult to automatically check, a robust system might change tactics to provide a different user experience (i.e., fail gracefully), providing the user with key evidence to consider instead of a full fact-check. IV. Claim difficulty prediction has real, practical value. Fact-checking organizations could prioritize claims that are not only check-worthy, but also easy or fast to check, improving throughput so human fact-checkers cover more claims in the same time. Fact-checking latency is also crucial: the impact of a fact-check on public opinion is far reduced if not released within an hour of a breaking claim (Jain, 2021). Moreover, easier claims might be automatically verified or delegated to citizen fact- checkers, improving scale, latency, and throughput of fact-checking organizations while enabling ex- pert fact-checkers to focus on more difficult claims. Difficulty-based pricing of claims might also help fact-checking organizations incentivize workers to undertake more difficult fact-checks (Hale, 2021).","Singh, P., Das, A., Li, J. J., & Lease, M. (2021). The Case for Claim Difficulty Assessment in Automatic Fact Checking. arXiv preprint arXiv:2109.09689."
