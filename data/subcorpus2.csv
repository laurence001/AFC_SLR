"Code","Title","Abstract","Field","Type","Year","Journalism","Use","Type","Function","Task","Citations","Text","APA"
"ML_DB_004","FactRank: Developing automated claim detection for Dutchlanguage factchecker","Fact-checking has always been a central task of journalism, but given the ever-growing amount and speed of news offline and online, as well as the growing amounts of misinformation and disinformation, it is becoming increasingly important to support human fact-checkers with (semi-)automated methods to make their work more efficient. Within fact-checking, the detection of check-worthy claims is a crucial initial step, since it limits the number of claims that require or deserve to be checked for their truthfulness. In this paper, we present FactRank, a novel claim detection tool for journalists specifically created for the Dutch language. To the best of our knowledge, this is the first and still the only such tool for Dutch. FactRank thus complements existing online claim detection tools for English and (a small number of) other languages. FactRank performs similarly to claim detection in ClaimBuster, the state-of-the-art fact-checking tool for English. Our comparisons with a human baseline also indicate that given how much even expert human fact-checkers disagree, there may be a natural upper bound” on the accuracy of check-worthiness detection by machine-learning methods. The specific quality of FactRank derives from the interdisciplinary and iterative process in which it was created, which includes not only a high-performance deep-learning neural network architecture, but also a principled approach to defining and operationalising the concept of check-worthiness via a detailed codebook. This codebook was created jointly by expert fact-checkers from the two countries that have Dutch as an official language (Belgium/Flanders and the Netherlands). We expect FactRank to be very useful exactly because of the way we defined check-worthiness, and because of how we have made this explicit and traceable.","Computer Science","Article","2021","Y","Y","Tool","Support","Claim detection","3","FactRank concentrates on the task of claim detection: sifting through large volumes of texts to find statements that are check-worthy: not only factual (and thus amenable, in principle, to a fact-check), but relevant to a broad public (and thus worth the effort of a fact-check). The FactRank website also performs the task of gathering potential claims, by continuously monitoring relevant sources. Monitoring and claim detection can be thought of as part of a pipeline of further tasks, in particular determining the veracity of statements (part of which may be matching statements to already fact-checked content) and building up a knowledge base. As highlighted above, claim detection is regarded by our journalistic experts as the most helpful candidate for automation in their work. FactRank’s classification algorithm was developed in a novel itera- tive process that rests on expert fact-checker input, a codebook to sup- port reliable human labelling, and an active-learning approach to combining machine learning and human expertise. Initial tests of an upvote/downvote functionality that is novel in the domain of fact- checking show promising results. Experiments on a dataset of 7037 human-labelled sentences and one involving an additional 1270 human- upvoted sentences show a classification accuracy of up to 74.6%, which is similar to state-of-the-art results for English as well as close to a human-annotators baseline of 75.5% that illustrates the inherent am- bivalences of the task and possible upper bounds for machine claim detection. 2. Related work Automated fact-checking projects generally focus on distinct parts of the fact-checking process, using a variety of approaches, such as NLP and machine learning. Several studies survey progress in the field for a wider audience, comprising journalists and fact-checkers [3,9,27]. Taken together, these present a number of promising, mostly small-scale projects, with few exceptions based on English-language materials. Although considerable progress has been achieved in the past decade, reliable end-to-end systems work, if at all, only for a very limited number of input categories. Automated verification is available for claims that have been fact-checked before, or for relatively simple statements, e.g. the current height of the national deficit, or the name of the president of Brazil. An area that has seen more success than others, according to Graves [[9], p. 3], is the first stage of the fact-checking process, claim detection. This entails source monitoring and identifying statements that are both factual and ‘check-worthy’, i.e. relevant input for fact-checkers (or systems) tasked with verification. An obvious addition to identifying check-worthy statements is ranking these according to their relevance. Babakar and Moy [[3], p. 14] break up claim spotting into four distinct tasks: 1 Monitoring claims that have been fact-checked before in new text. 2 Identifying new factual claims that have not been fact-checked before in new text. 3 Making editorial judgments about the priority of different claims. 4 Dealing with different phrasing for the same or similar claims. The present project, FactRank, deals with items 2 and 3: identifying and ranking new claims. Babakar and Moy [[3], pp. 27–31] list a number of projects in this category, most of which are not relevant for the present study since they cover a different range of sources (e.g., Vlachos’ Simple Numerical Fact Checker, meant to spot and check claims such as ‘Lesotho has a population of nearly 2 million’; [3], p. 28). The four projects that are closest to FactRank regarding scope and approach are ClaimBuster from the University of Texas ([14] and [15]; https://idir.uta.edu/claimbuster/), ContentCheck (a collaboration of academics and Le Monde, http://contentcheck.inria.fr/), Full Fact’s claim spotting module ([19]; https://fullfact.org/automated), and Chequeado’s Chequeabot ([9], p. 5). ClaimBuster aims to support all stages of the process (end-to-end fact-checking”), in which claim detection is one stage; ContentCheck focuses on the actual checking and looking up of facts, e.g. in Linked Open Data, rather than on claim detection. Full Fact deploys a claim detection system that leverages transfer learning and universal sentence representations, and it outperforms ClaimBuster and ClaimRank that use word-level representations. Konstantinovskyi et al. [19] also discuss an ontological approach to the labelling of claims (i.e. which categories of claims should be distinguished) and methods for obtaining these labels (especially crowdsourcing, see also ClaimBuster). These systems work on English-language texts. ClaimRank ([17] and https://claimrank.qcri.org/) uses a richer set of features than ClaimBuster and is, like Chequeado, one of the currently still limited number of claim detection systems that work on languages other than English (Chequeado: Spanish, ClaimRank: Arabic). An over- view also of earlier computational work in claim detection is given by Leblay, Manolescu, and Tannier [23]. The basic approach of applying supervised learning has remained the same, while a closer inspection of the classes used in early studies also shows the roots of the claim-detection task in sentiment mining: For example, the earliest cited article [31] aimed at separating facts from opinions and then focussed, like much of the work in sentiment mining, on a further analysis of the opinion sentences. It appears that the increasing interaction with pro- fessional fact-checkers over the years since then has brought the rele- vance of differentiating within the facts” class to the fore. Much work has been done on detecting specific signals in texts. Factmata (https://factmata.com), for example, detects signals of, for instance, hyperpartisanship, clickbait, deception, stance, claims vali- dation (whether a claim is supported or refuted by the evidence found”), subjectivity and arguments.2 We believe that these signals could be components of check-worthiness, but they are very specific and lack the overarching notion of relevance to a broad audience that we have identfied as well as circumscribed by features and questions, as central to check-worthiness.. The recent projects tend to go beyond machine learning and involve journalists throughout. However, based on the published papers, it is difficult or impossible to determine how concepts are defined, what procedures and materials have been established, and who contributes what at which stage. Based on our experience of collaboration, we are convinced that a principled approach is needed and that publicly available documentation is useful. In addition to studies and tools, datasets have been published. Through the CLEF CheckThat! Competition that has taken place annu- ally since 2018,3 claim detection and veracity detection algorithms have been tested on datasets in English and in Arabic. Datasets have comprised between 50 documents and 1500 tweets, and domains include web pages, social media, debates/speeches/press conferences. The authors of ClaimBuster have, in 2020, released a dataset4 of 23,533 sentences from all U.S. general election presidential debates (1960–2016) along with human-annotated check-worthiness labels. The dataset, example sentences for the label concepts, and the procedure are described by Arslan et al. [2]. The main novelty of FactRank, compared to these systems, is (a) its being the first system for claim detection for the Dutch language, (b) a principled and openly documented approach to defining the concept of check-worthiness, including a codebook, and (c) an iterative architec- ture that leverages the skills of both human annotators and machine learning. As part of our work, we have created (d) a Dutch-language 2 https://factmata.com/signals.html. 3 https://sites.google.com/view/clef2020-checkthat/datasets-tools, see also [6]. 4 https://figshare.com/articles/ClaimBuster_A_Benchmark_Dataset_of_Ch eck-worthy_Factual_Claims/11635293/1. B. Berendt et al. Online Social Networks and Media 22 (2021) 100113 3dataset of more than 8000 sentences with human annotations of check- worthiness. FactRank is a product of an interdisciplinary collaboration between professional fact-checkers, computer scientists and political scientists, and it has resulted in a live website that is being used by professional fact-checkers and journalists. An important part of the approach is the creation of a codebook that guides human coders in labelling examples as check-worthy or other- wise. The concept of check-worthiness (and in particular the notion of relevance that it involves) is notoriously difficult to define both in terms of its meaning and in terms of example datasets; it depends on time, place and context (e.g., [1]). We provide an English-language version of our codebook as part of the documentation of our approach, with the aim to help and encourage others build codebooks tailored to their materials’ times, places, and contexts. 3. Method FactRank is first of all meant to be a tool for fact-checkers. It should provide them with an instrument that can save them time, by auto- matically collecting claims that could be relevant to fact-check. Towards this end, FactRank aims to detect check-worthy claims in texts. We use the term ‘check-worthy’ to denote claims that are factual (meaning that it is possible to check whether they are accurate) and relevant (not every factual claim is relevant for fact-checkers to investigate). Of course, not only fact-checkers, but everyone interested in a critical reading of (on- line and offline) claims can profit from using FactRank. Within this large domain, FactRank focusses on political content and in particular on utterances of politicians. This influenced our choice of data sources as well as of labellers, and the design of our codebook. The process was bootstrapped and is being accompanied with professional fact checks and expert coders. The remainder of this section describes these choices in detail. The organisation of the section reflects the mixed-methods approach of the current paper. Since this yields inherently interleaved writing, we provide a reading recommendation for our two main audiences: (1) The reader used to computer-science documentation will recognise, after a flow-chart description of the processing pipeline (Section 3.1), a section on data (Section 3.4) and on the machine-learning models and training and test set-up (Section 3.5). For this reader, Sections 3.2 and 3.3 pro- vide background information on how the data were labelled, i.e. how the human ground-truth labels were defined and obtained, and what role the machine-learning models played in the selection of instances to be labelled. (2) The reader used to descriptions of human-subjects studies will recognise, after the description of the overall procedure (Section 3.1), the standard components of method: participants (Section 3.3), procedure (detailed actions of different participants: Section 3.3), and materials (data: Section 3.4). Since the concepts underlying the coding were themselves operationalised as part of the overall procedure (namely by the creation and use of the codebook), this conceptual description is given in Section 3.2. For this reader, Section 3.5, which describes the machine-learning model, provides background informa- tion on how the materials that the human participants saw were generated and on how the tool generates labels. 3.1. Procedure (1): the FactRank approach At the heart of the FactRank approach is an iterative procedure that combines human expertise and machine-learning capabilities to achieve a continuous improvement of the FactRank model’s automatic detection of check-worthy claims in incoming streams of text. Fig. 1 shows the basic procedure, split into nine phases. The archi- tecture rests on an interleaved sequence of training and labelling by humans and a machine-learning classifier model, with the size of the labelled datasets increasing across phases. Four of these phases involve the introduction of new, unlabelled data. In phase 1, human fact-checking experts labelled sentences concerning their check-worthiness”, and they created a codebook that described the reasoning behind their decisions. The purpose of the codebook was to serve as instructions to knowledgeable (not necessarily expert) human fact-checkers. Specifically, a group of student coders went through a training phase 2, in which they were given the codebook and asked to label the 367 sentences whose ground-truth label had already been established. They received feedback upon mislabelling (the ground-truth label and an explanation of why the experts had assigned that label). At the end of this phase, results were discussed with the experts, and the codebook refined. Equipped with the refined codebook and their knowledge gained in training, in phase 3 the student coders labelled a set of 2000 new sen- tences. These labelled sentences were used to train a first version of the machine-learning model in phase 4, in which the model was trained with the student labels as feedback. In phase 5, this model was used to generate a new dataset by selecting a further 5000 sentences that promised to be particularly relevant for learning the concept of check- worthiness, and these 5000 sentences were labelled by ‘the best’ of the student coders in phase 6. In phase 7, all human-labelled sentences accumulated so far were used to train the second version of the machine- learning model. This model is currently (phase 8) being applied ‘live’ on the FactRank website to label new sentences on an ongoing basis, ob- tained from daily crawls from the Flemish, Belgian and Dutch Parlia- ments, Twitter, FactCheck Flanders and VRT (Flemish public television) subtitles. Users of the website can give quality feedback by voting sen- tences up or down, according to their own perception of check- worthiness (phase 9). The usefulness of phase 9 for improving the model was investigated in a pilot study with student coders (different from the ones in phases 2–6). All coders were trained with the codebook and had access to it in all phases of labelling. Further iterations can be added to continue to improve the model. In the following sections, we provide more details on our notion of check-worthiness and how we operationalised it via a codebook (Sec- tion 3.2), the human actors in this pipeline (Section 3.3), the data (Section 3.4), the machine learning (Section 3.5), and the resulting quality of the automatic detection of check-worthiness, including how quality improved through the iterations (Section 4). 3.2. Concepts and their operationalisation: check-worthiness and the codebook The first step was to compose a codebook5 with guidelines on how to decide whether sentences contain check-worthy claims. The guiding principle for the coders was to look at sentences from the perspective of a fact-checker: does this sentence contain a claim that could give rise to a fact-check? Coders were therefore instructed as follows. Take the perspective of a fact-checker: Could this sentence be the start of a fact-checking? Thus, for a sentence to be check-worthy, it should be: 1 Factual. That means that a sentence should contain a claim that revolves around a fact that can be checked, in other words, that it can be deter- mined whether or not the claim is true. 2 Relevant. Not every factual claim is relevant for a fact-checker. Fact- checkers are only concerned with facts that matter to a broad audience. In other words, they are only concerned with claims that, if they turn out to be wrong, are reprehensibly false claims. The coding units were entire sentences. They were coded without further context. In other words, the previous and next sentences were not provided to the coders. The coders had to assign every sentence to one of the following 5 Available at https://people.cs.kuleuven.be/~bettina.berendt/FactRank/Co deboek_FactRank.pdf (in Dutch) and https://people.cs.kuleuven.be/~bettina. berendt/FactRank/Codebook_FactRank_EN.pdf (in English). B. Berendt et al. Online Social Networks and Media 22 (2021) 100113 4coding categories: 1 NF: Not factual 2 FR: Factual and relevant 3 FNR: Factual and non-relevant 4 Error: Not applicable. This code was used for incomprehensible sentences. These category names derive from ClaimBuster [14,15], and they operationalise journalistic concepts and practices (e.g. [21]). However, a previous study [22] had shown that these categories are far from trivial and require a more rigorous approach, involving explicit coding in- structions created by experts. This motivated us to create a detailed codebook that helped to explicate the meaning of the categories. The codebook contains a set of guidelines that should make the coding procedure reliable. The expert coders (see Section 3.3) applied the codebook to a set of sentences and discussed the results. These dis- cussions resulted in additions, adjustments, and refinements of the codebook. The discussions also gave rise to the Reference Dataset (see Section 3.4). The Reference Dataset was used to train six student coders. After a session in which the codebook was explained, the students coded six batches of 50 sentences (phase 2 in Fig. 1). Each batch was discussed with the researchers, and sometimes this resulted in adjustments or re- finements of the codebook. At first sight, the categories appear relatively clear and easy to spot: statements that claim something about facts (for example, numbers that are or are not correct), would be factual, and opinions would be non- factual (NF). However, relevance for a broad public is key, and the distinction between FR and FNR is often not straightforward. Also, claims are made in different (surface) forms, including as pre- suppositions, and journalists need to critically investigate all of these. Therefore, a codebook needs to provide more than concept definitions and example sentences: it needs to help coders understand why an example sentence would be considered interesting and relevant enough to be checkworthy (or not), and how to detect this. This will equip coders with the skills for analysing the topical and linguistic structure of their material. (In a prior step, coders must be selected who have a solid knowledge of the social and political context and a solid competency of the language.) The following five examples illustrate some of the complexities that a journalistic, fact-checking-based approach entails. Care was taken to describe signals as bases for heuristics that demand holistic judgement rather than mechanistic patterns. S1: Together with 122 other countries, we have requested, in the General Assembly of the United Nations, that a ceasefire be declared in Aleppo.” Category: FNR Explanation: You can check whether the request did indeed involve 122 countries, but for many people a few more or less will not make a difference. In addition, the fact that our country requests a ceasefire is neither controversial nor counterintuitive. S2: As regards Canada, it is even 90%.” Category: FR Explanation: You cannot know what this is about. However, since a number is being mentioned together with the signal word even”, it can be a relevant factual claim. [Other typical phenomena and signal words indicating FR are comparisons: increasingly”, growing”, etc.] S3: They do not need paternalism.” S4: This is therefore a good thing.” Category: NF Explanation: These sentences may not sound like opinions (they do not contain I think/I find”), but they are expressions of opinions. You could easily add I think/I find” to the sentence without changing the meaning: I think that they do not need paternalism” and Therefore I find this a good thing.” S5: I consider it undesirable that 80% of the migrants are unemployed.” Category: FR Explanation: The sentence begins with an opinion: I consider”. What follows is a factual claim because it needs to be checked whether 80% of the migrants are indeed unemployed. This is also something that many people are likely to find interesting. Fig. 1. Basic FactRank procedure. B. Berendt et al. Online Social Networks and Media 22 (2021) 100113 53.3. Participants and procedure (2): the human actors in the FactRank pipeline Fig. 2 gives a more detailed view of the general FactRank procedure, naming actors and datasets. The expert coders were three of the authors of the current paper: Jan Jagers, Peter Burger, and Alexander Pleijter, who work as fact-checkers. The student coders were six students from Leiden University’s Mas- ter’s program Journalism and New Media. They had previously taken a course on fact checking, in which they also conducted a fact-check themselves. All six students labelled in phase 3. In phase 2, these stu- dents had been ‘scored’ by their percent agreement with the ground- truth labels given by the expert coders. The five students with the highest agreement were considered to be the best student coders, and they labelled also in phase 5. Our starting data also contained claims collected from fact-checks done by other professional fact-checkers/journalists from relevant Flem- ish and Dutch media. Finally, student coders using the FactRank website (different from the other student coders) voted on sentences considered check-worthy by the model. The purpose of this phase 9 was two-fold: to serve as a first formative test of a projected functionality for the real-life website and its professional users, and to test the usefulness for model quality of voting as a form of getting human ground-truth assessments, a form that is more convenient than labelling from scratch. 3.4. Data Sources The datasets are derived from a broad range of sources from Flemish and Dutch politics and news sites: BE Parliament Plenary, BE Commis- sion, Interviews BE, Interviews NL, NL Parliament Plenary, politicians’ Twitter accounts, NL fact-check websites, and Factchecks Knack. The sources themselves remained the same over all phases. For phases 1–7 in Fig. 1, a static dataset covering the time between March 2017 and March 2019 was used, whereas phases 8 and 9 draw on a dynamic dataset that is continuously extended by daily collection from the sources. The reason why we focussed on politicians, and for instance not on clickbait, is twofold. First, politicians embody representative de- mocracy. They hold the parliamentary debate that, in theory at least, leads to, or is part of, the decisions and legislation made. Since that legislation affects all the people politicians speak for and their everyday lives, in a functioning democracy, politicians should use correct facts in their arguments. Here, correct” means facts that are as undisputed as possible. Political views differ regarding what to do. But the facts on the table – the building blocks of discussion – should ideally be agreed upon. We thus focus on politicians because their words are the beating heart of democracy. Second and related to that first argument, monitoring politicians’ spoken – and written(-down) - words is very time-consuming. Politicians produce large volumes of text and arguments every day as well in parliament, via their direct communication channels – i.e. social media – and in interviews in news outlets on paper, radio, and TV. Considering the first argument above, we consider the need for journalists to receive assistance, a priority. Our selection of sources considers the traditional arena of political actors (parliament) as well as mainstream media (from which interviews were taken) and Twitter as the most recent arena. Especially on Twitter, there is no control over what is written and no gate-keeping. This is one of the reasons why – in the Flemish/Dutch environment described here as well as elsewhere – the need for fact-checking has increased tremendously over the past years. The sources we used are, in descending order of importance, (1) transcripts of plenary debates held in the Belgian federal and in the Dutch parliament; (2) interviews with politicians from different political parties in Flemish and Dutch newspapers; and (3) Flemish and Dutch politicians’ writings on their microblog Twitter. These data were sup- plemented with claims that had already been fact-checked by Dutch and/or Flemish media. Those claims came from politicians, but also from other pundits such as academics or experts cited in media coverage, and also from that coverage itself – for instance, a newspaper headline. The parliamentary debates are complete, as is the list of Fig. 2. Human actors, machine-learning model versions, and data (figure legend: see Fig. 1). B. Berendt et al. Online Social Networks and Media 22 (2021) 100113 6Twitter accounts from the parliamentarians. Sources are representative in the sense that the dataset covers utterances in parliament from elected politicians that represent a wide range of different political/ideological viewpoints. Since both Belgium and the Netherlands have proportional electoral systems and low electoral thresholds, the number of parties represented in parliament is relatively high (7 Flemish parties in Belgium and 13 parties in the Netherlands). The selections of interviews and fact-checks were done based on subjective assessment of interest- ingness by the experts in our team. Data are scraped or obtained via the API (Twitter); no data cleaning issues have arisen. The distribution over sources is reported in Section 3.5 for the main evaluation dataset (D4, explained further below). Datasets The static dataset Dtotal consists of 410,000 sentences. All datasets to be labelled were selected from Dtotal, in ways that aimed at selecting interesting” sentences, as described below. First, 300 sentences were selected from this set to be labelled by our fact-checking experts in step 1. To avoid creating a useless dataset consisting mostly of uninteresting NF sentences, the 300 were not cho- sen randomly. Instead, the machine-learning model from the earlier study [22] was applied to all 410,000 sentences, for each of the cate- gories FR, FNR and NF, sentences were ranked by their score (most likely to be FR, as judged by the model” etc.), and the top-ranking sentences selected such that the distribution over (likely) FR, (likely) FNR, and (likely) NF was uniform. This resulted in 217 sentences for which our three expert labellers agreed (see Section 3.3). After initially labelling each sentence and explaining their decision individually, our three expert labellers also agreed on one ‘explanation’ of each of these sentences. A further 150 sentences from other professional fact-checkers were added to this. Regardless of whether these sentences were judged to be true, false, half-true, etc. by the other professional fact-checkers, the fact that they had been selected for this test indicated that the other pro- fessional fact-checkers deemed them check-worthy. Our experts agreed with these judgments. The combined dataset of 367 labelled sentences (the Reference Dataset, RD in Fig. 2) was used to train the student coders in phase 2. Our experts continued labelling further sentences. The 2000 sentences chosen for labelling in phase 3 (D1 in Fig. 2) were selected as follows: We trained the SVM model from Laperre et al. [22] on the 517 sentences that the experts had agreed on so far (110 FR, 224 FNR, 183 NF). The model was trained in a binary setting, i.e. FR against the rest, and applied to 10,000 sentences from Dtotal. From the result, 1000 sen- tences with >50% confidence of being FR were taken, and a further 1000 randomly chosen. This choice of instances reflected a utility metric [7] focused on precision in the early phases of model learning: we wanted to reduce the uncertainty of instances considered FR by our model via obtaining judgments from human labellers. The two-class setting was used only as a step in generating datasets to be labelled; the models were evaluated with respect to the three-class setting (see Section 4 below). Each sentence in D1 was labelled by all 6 student coders (i.e. each coder labelled 2000 sentences), and the majority labels were taken to be the ground-truth labels for these sentences in D2 . This resulted in 702 sentences for which there was majority agreement. The 5000 sentences chosen for labelling in phase 5 (D3 in Fig. 2) were selected as follows: We trained the SVM from [22] on the 2622 sentences that the experts (in their continuing labelling process), or the student labellers in phase 3, had agreed on so far (328 FR, 1227 FNR, 1067 NF). The model was again trained in a binary setting and applied to Dtotal. From the result, 5000 sentences close to the decision boundary of the SVM (i.e. around 50%) were chosen, stratified by source (1000 each from interviews BE, interviews NL, plenary/commission transcripts, 2nd chamber NL, and Twitter). This choice in a more advanced phase of model learning re- flected a more general utility metric, that of choosing instances that the model is uncertain about. Each sentence in D3 was labelled by 2 out of the 5 ‘best student coders’, such that each coder labelled 2000 sentences, and the agreed- upon labels were taken to be the ground-truth labels in D4 . Sentences from D3 on which the two coders disagreed were excluded from further consideration. In the next step, the results from all previous steps of human labelling were used as the ground-truth dataset, consisting of a total of 7037 sentences (D4 in Fig. 2). All sources originally crawled were represented in the dataset (see Table 3). 1100 sentences had been labelled by other professional fact-checkers, and 5937 by our student and expert coders. This dataset is available at https://github.com/lejafar/FactRank/tree/ master/factrank/data/training. Current deployment results (D5 in Fig. 2) originate from continuous source monitoring and labelling of the new data. They are stored for future iterations of the FactRank pipeline. We then created a dataset D6 by asking three student coders (from Antwerp University and not involved in the earlier rounds) to vote on outputs from the live FactRank website, i.e. on D5. The coders were trained by one of the experts in a similar way as the earlier Leiden coders. First, we explained the goal of FactRank and gave a detailed explanation of the codebook, and we discussed the examples used in the codebook. Next, we performed a small test with all three coders, giving them the same 20 statements. Since the FactRank website only allowed a binary classification, we discussed the results with a clear focus on distinguishing between check-worthy and non-check-worthy state- ments. Coders in this test run of step 9 in Fig. 1 were instructed as fol- lows: Go to factrank.org and look at all sentences from the Flemish Parliament, the [Belgian] Federal Parliament, and the Dutch Parliament. [Each student concentrated on one source.] For every sentence, do the following: If you think this sentence is indeed check-worthy, vote it up with the upvote button. If you think the sentence is not check-worthy, vote it down with the downvote button. If you are unsure, do not vote.” From these, we selected the upvoted sentences. These sentences can be used directly as FR statements. (Downvotes are either FNR or NF, and they can only be used when shifting to a binary FR vs. not FR classifier, which is left for future development and not considered in the present paper.) A further motivation was to boost the number and variety of positive examples (considered check-worthy by humans, similar to phase 3, inspired by classical strategies of relevance feedback in inter- active text retrieval). This resulted in 1270 sentences with an upvote. The model V2 was re-trained using this set D6 with the new upvotes. An overview of the numbers of sentences in the human-labelled datasets is given in Table 1. These are at the same time the class dis- tributions in the input datasets used for model training. The outputs of model training are summarised in Table 4. 3.5. The FactRank machine-learning model The machine-learning model V0 and V1 was inspired by the method for claim detection” of Claimbuster. It used, like Hassan et al. [14,15] did, a support vector machine (SVM). A linear kernel was used because it gave the best classification quality in preliminary tests. The features of the SVM included uni- and bi-grams, POS tags derived using pattern6 and sentiment analysis scores also derived using pattern. The process is Table 1 Class distributions in the ground-truth datasets labelled by human annotators. Dataset FR FNR NF Total RD 110 224 183 517 D2 328 1227 1067 2622 D4","Berendt, B., Burger, P., Hautekiet, R., Jagers, J., Pleijter, A., & Van Aelst, P. (2021). FactRank: Developing automated claim detection for Dutch-language fact-checkers. Online Social Networks and Media, 22, 100113."
"ML_DB_032","Automated fact-checking for assisting human factcheckers","The reporting and the analysis of current events around the globe has expanded from professional, editor-lead journalism all the way to citizen journalism. Nowadays, politicians and other key players enjoy direct access to their audiences through social media, bypassing the filters of official cables or traditional media. However, the multiple advantages of free speech and direct communication are dimmed by the misuse of media to spread inaccurate or misleading claims. These phenomena have led to the modern incarnation of the fact checker — a professional whose main aim is to ex amine claims using available evidence and to assess their veracity. As in other text forensics tasks, the amount of information available makes the work of the factchecker more difficult. With this in mind, starting from the perspective of the professional fact-checker, we survey the available intelligent technologies that can support the human expert in the different steps of her fact-checking endeavor. These include identifying claims worth fact-checking, detecting relevant previously fact checked claims, retrieving relevant evidence to factcheck a claim, and actually verifying a claim. In each case, we pay attention to the challenges in future work and the potential impact on real-world fact-checking.","Social Science","Article","2021","Y","Y","Process","Support","Multitasking","28","The spread of fake news, misinformation and disinformation on the web and in social media has become an urgent so- cial and political issue. Social media have been widely used not only for social good, but also to mislead entire commu- nities. To fight against such false or misleading informa- tion, several initiatives for manual fact-checking have been launched. Some notable fact-checking organizations include FactCheck.org,1 Snopes,2 PolitiFact,3 and FullFact.4 ∗Contact Author Such fact-checking organizations are also potential benefi- ciaries of and/or leaders in automated fact-checking research. As misinformation and disinformation have become major concerns globally, tech companies, as well as national and international agencies began work in this area. Recently, sev- eral international initiatives have also emerged such as the Credibility Coalition5 and EUfactcheck,6 and some tools have been made available such as Google Factcheck7 and Hoaxy.8 Moreover, fact-checking is a common task in settings that go beyond online misinformation, as the verification of content’s accuracy is a priority for many organizations [Karagiannis et al., 2020]. A large body of research has been devoted to develop- ing automatic systems for fact-checking [Li et al., 2016; Shu et al., 2017; Lazer et al., 2018; Vosoughi et al., 2018; Vo and Lee, 2018]. This includes datasets [Hassan et al., 2015; Augenstein et al., 2019], and evaluation campaigns [Thorne and Vlachos, 2018; Nakov et al., 2021a]. However, there are credibility issues with automated systems [Arnold, 2020], and thus a reasonable solution (i.e., human in the loop) is to facilitate human fact-checkers using automated systems. Yet, there has been limited work in this direction. Thus, to facilitate human fact-checkers, in this survey, we explore what fact-checkers want and what research has been done that can actually support them in their work. This is important because manual fact-checking is a time-consuming process, going through several manual steps. The study by Vlachos and Riedel [2014] describes the following typical se- quence of fact-checking steps: (i) extracting statements that are to be fact-checked, (ii) constructing appropriate questions, (iii) obtaining the pieces of evidence from relevant sources, and (iv) reaching a verdict using that evidence. In the current information ecosystem (including web and social media), there is a large volume of false claims not only in textual form, but also misleading or manipulated images and videos, including deepfakes,” and there has been a lot of recent work on fact-checking images and videos. However, here we limit our focus to automated fact-checking on text, as it remains the focus of most professional fact-checkers. arXiv:2103.07769v2 [cs.AI] 22 May 2021 There have been a number of surveys on fake news” [Shu et al., 2017; Lazer et al., 2018; Vosoughi et al., 2018; Alam et al., 2021], rumors [Zubiaga et al., 2018], fact- checking [Thorne and Vlachos, 2018; Kotonya and Toni, 2020], factuality [Li et al., 2016; Zannettou et al., 2019; Nakov et al., 2021b], and propaganda [Martino et al., 2020]. Unlike that work, here we study the desiderata of fact- checkers vs. the research attempts that aim to meet them. 2 What Fact-Checkers Want Recently, Full Fact carried out extensive interviews with pro- fessional fact-checkers from 24 organizations in 50 coun- tries [Arnold, 2020]. The report discussed key challenges they face where they believe technology can help. These include monitoring potentially harmful content, selecting claims to check, creating and distributing articles, and man- aging suggestions from readers (such as tip lines serving WhatsApp or Signal). The same report revealed that most fact-checkers do not be- lieve that tools to automate the verification of claims, i.e., the last step of a typical fact-checking pipeline [Vlachos and Riedel, 2014], will be used in the foreseeable future. Some believe that the required intuition and creativity can never be automated, even if some parts of their work can be supported. This sets up a twin challenge for Artificial Intelligence (AI) practitioners: first, to develop practical tools that solve the problems fact-checkers face, and second, to demonstrate their value to fact-checkers in their day-to-day work. In the mean- time, there is a recognised need for tools to help with finding claims, including previously fact-checked claims, and in pro- viding relevant evidence to help write fact-checking articles. 2.1 Finding Claims Worth Fact-Checking Choosing which claims to check is a complex process. Fact-checking is time-consuming and it often takes effort to determine whether a claim can even be checked, let alone whether it is misleading. Fact-checkers have to balance the potential harm that a misleading claim may cause (including risk to health, risk to democratic processes, and risk of ex- acerbating emergency situations) against the effort required to check a claim. Fact-checkers are also committed to being non-partisan, and thus it is important that such tools do not introduce any unfair bias. In many countries, governments choose not to publish reliable official statistics, thus making certain statistics-related claims virtually impossible to verify. While simple algorithms can often decide whether content is viral, it is much harder to estimate the checkworthiness” of a claim. For example, breaking news stories are often both popular and accurate. Given the limited resources of fact- checking organizations, many claims that are check-worthy nonetheless remain unchecked; thus, using historic lists of claims that were or were not checked is not a reliable indica- tion of whether similar claims are worth fact-checking. Claims may be found in many sources, including news websites, social media (text, audio, or video), and broadcast media. To monitor such a range of sources, fact-checkers of- ten use a variety of technologies, such as news alerts, auto- matic speech recognition and translation tools, all of which typically depend on underlying AI technologies. 2.2 Detecting Previously Fact-Checked Claims Misleading claims are often repeated in multiple channels, independently of any fact-checks or rebuttals.9 Once a claim has been established as misleading, the ongoing spread of re- peats or copies of the claim can be minimised by its rapid detection. In the simplest cases, these could be simple copy and paste” repeats that are relatively easy to detect, but more often they will be paraphrases of the original or endlessly evolving variations. Given the resources required to write fact-checking articles, it is preferable to respond to multiple repeats of a claim with a single fact-checking article. The number of fact-checking initiatives continues to grow. The Duke Reporters’ Lab lists 305 active fact-checking orga- nizations.10 While some of them have debunked just a couple of hundred claims, others such as PolitiFact, FactCheck.org, Snopes, and Full Fact have each fact-checked thousands or even tens of thousands of claims. Moreover, manual fact-checking often comes too late. It has been shown that fake news” can spread six times faster than real ones [Vosoughi et al., 2018], and that over half of the spread of some viral claims happens within the first ten minutes of their posting on social media [Zaman et al., 2014]. To counter this, quickly detecting that a new viral claim has already been fact-checked allows for a timely action that can limit the spread and the potential harmful impact. The prob- lem is made harder by the transient nature of many claims. For example, a claim about infection rates may be wrong to- day but correct next week, and thus re-using previous checks should be done carefully. For journalists, the ability to discover quickly whether a claim has been previously fact-checked could be revolution- izing as it would allow them to put politicians on the spot dur- ing live events. In such a scenario, automatic fact-checking would be of limited utility as, given the current state of tech- nology, it is not credible enough in the eyes of a journalist. Finally, false claims often originate in one language and then get translated to other languages. Tools that can spot repeated claims across languages would be useful to ad- dress this. More generally, multi-lingual tools can help fact- checkers around the world, even those with limited resources. 2.3 Evidence Retrieval Fact-checking is often limited by the time available: there are typically far more claims to verify than what is practically possible. Even if full automation remains out of reach (see the next section), tools that support fact-checkers in their manual verification process are to be welcomed. Tools that automatically retrieve relevant data from trusted sources may save fact-checkers a lot of time. This is espe- cially true if the evidence is hidden in large text documents, audio-visual recordings and streams, or is in a language that the fact-checker is not familiar with. Thus, combining au- tomatic transcription, summarization, translation, and search can make sources of evidence available to fact-checkers that would be impossible or impractical to access otherwise. 9President Donald Trump repeated one false claim over 80 times: Figure 1: A fact-checking pipeline. 2.4 Automated Verification On first consideration, the automated verification of claims seems like the ultimate application of AI to fact-checking. If such technologies can be developed and deployed, they would allow fact-checking organizations to be faster and to provide a more comprehensive coverage than manual fact-checking could ever achieve. However, many claims are not simply correct or incorrect, but may be partially correct, or correct but misleading without extra context, etc. One key role of professional fact-checkers is to help their audience gain full understanding of a claim, with all its nuances and complexity, rather than simply applying a binary classification. Fact-checkers can only have an impact if they are trusted by their readers. They therefore take great care to only publish fact-checks after meticulous research, and adhere to strict ed- itorial standards, as outlined, e.g., in the fact-checkers’ code of principles11 by the International Fact-Checking Network. This leads to a major hurdle before adopting fully automated verification methods: such methods will inevitably be im- perfect, and publishing incorrect fact-checks could seriously damage the reputation of the responsible fact-checking orga- nization. They may be more valuable as internal tools by pre- senting the evidence, reasoning and conclusion regarding a claim, before the (human) fact-checker writes and publishes their fact-checking article. 3 What Technology Currently Offers Fact-checking is not a straightforward or routine process. It requires a chain of steps that go from sensing media and spot- ting check-worthy claims all the way through to concluding whether the claim is true, partially-true, false, misleading, or perhaps impossible to judge. Figure 1 shows a typical fact- checking pipeline, partially derived from [Barr ́on-Cede ̃no et al., 2020]. Below, we discuss each step in this pipeline. 3.1 Finding Claims Worth Fact-Checking As fact-checkers are flooded with claims, they need to decide what is actually worth fact-checking. This has encouraged the development of AI solutions, e.g., as part of shared tasks such as the CLEF CheckThat! lab 2018-2021 [Nakov et al., 2018; Elsayed et al., 2019; Barr ́on-Cede ̃no et al., 2020; Nakov et al., 2021a], as well as inside dedicated fact-checking organi- zations such as Full Fact [Corney, 2019]. The problem is widely tackled as a ranking one, where the system has to produce a ranked list of claims coupled with check-worthiness scores. Such a score is important to in- crease the system’s transparency and to provide fact-checkers with the ability to prioritize or to filter claims. Fact-checkers can also provide feedback on how reflective this score is of the actual check-worthiness of a claim, which can be later used to tune the system. ClaimBuster [Hassan et al., 2017] is the first system for check-worthiness detection, and it was used by fact-checkers in the Duke Reporters’ Lab project.12 It was trained on a man- ually annotated dataset to distinguish between non-factual sentences, unimportant factual claims, and check-worthy fac- tual claims; it used features based on sentiment, named en- tities, part-of-speech tags, words, and claim length. Kon- stantinovskiy et al. [2021] developed a more detailed schema and dataset for check-worthiness annotation of TV shows. Gencheva et al. [2017] created a dataset of political debates, derived by observing which sentences were fact-checked by fact-checkers; they used a rich set of features modeling the sentence structure and the context of the claim. The dataset was used in the ClaimRank system [Jaradat et al., 2018], and was extended to multitask learning from nine fact-checking organizations [Vasileva et al., 2019]. Further extensions were used for the CLEF CheckThat! lab, where the partic- ipants developed models based on pre-trained transformers such as BERT and RoBERTa [Hasanain and Elsayed, 2020; Nikolov et al., 2020; Williams et al., 2020]. Finally, as observation-based annotations cannot give reliable negative examples, the task was also modeled using positive unlabeled learning [Wright and Augenstein, 2020]. During a recent general election, Full Fact used a fine- tuned BERT model to classify claims made by each political party, according to whether they were numerical claims, pre- dictions, personal beliefs, etc. [Corney, 2019] This allowed fact-checkers to rapidly identify the check-worthy claims, and thus to focus their efforts in the limited time available while voters are making their final decisions. Social media companies are also working on combating misinformation and disinformation on their platforms. Face- book described a proprietary tool to identify claims that should be fact-checked [Facebook, 2020]. They leverage flags by the users for a post indicating that it is potentially false, as well as features from the content of the replies, to predict whether the post contains false information. The model is updated using feedback from fact-checkers. 3.2 Detecting Previously Fact-Checked Claims Interestingly, despite the importance of detecting whether a claim has been fact-checked before, it has been explored only recently. Shaar et al. [2020] formulated the task, and re- leased two specialized datasets: (a) on tweets, which are to be compared to claims in Snopes, and (b) on political de- bates, to be matched to claims in PolitiFact. They further proposed a learning-to-rank approach based on a combination of BERT and traditional BM25, matching the input to the en- tire fact-checking article. Follow-up work explored the role of context for (b), including using neighboring sentences, co- reference resolution, and reasoning over the target text with Transformer-XH [Shaar et al., 2021]. The task was also fea- tured in the CLEF CheckThat! Lab [Barr ́on-Cede ̃no et al., 2020; Nakov et al., 2021a]. Vo and Lee [2020] explored a multi-modal setup, where tweets with claims about images were matched against the Fauxtography section of Snopes. Full Fact is currently trialling a similar tool internally. 12http://reporterslab.org/tech-and-check Recently, Google has released the Fact Check Explorer,7 which is an exploration tool that allows users to search a number of fact-checking websites, such that use ClaimReview from schema.org,13 for the mentions of a topic, a person, etc. However, the tool cannot handle complex claims, as it uses Google Search, which is not optimised for long queries. 3.3 Evidence Retrieval Evidence retrieval aims to find external evidence to help fact- checkers decide on the factuality of an input claim. When the input consists of a check-worthy claim and a (potentially closed) data collection, the process could finish in the pro- duction of a ranking of the relevant data —as in a standard retrieval scenario— or in the extraction of specific pieces of evidence, e.g., a text snippet or a recording. When dealing with a closed reference collection, the task can be addressed as a ranking problem, e.g., based on BM25 or on some kind of similarity over vectorial representations between the input claim and the documents in the collec- tion. Recent work has also combined document-level and sentence-level similarity to improve relevant document re- trieval [Akkalyoncu Yilmaz et al., 2019]. Once a relevant document has been found, it is possible to further extract relevant snippets representing arguments in favour or against the target claim, to be presented to the hu- man fact-checker [Alshomary et al., 2020]. It is also possible to further generate snippets to brief the fact-checkers with some relevant background knowledge about the target claim. Fan et al. [2020] achieved this by first generating and retrieving relevant passage briefs, then iden- tifying and retrieving documents based on entity briefs, and finally generating and answering question answering briefs decomposed from the claim. The CLEF-2013 INEX lab [Bellot et al., 2013] included a shared task that asked to retrieve evidence snippets from a pool of 50k books to confirm or to refute a claim. They found that entity matching was one of the most important features. The CLEF CheckThat! lab also featured tasks on claim evidence retrieval, at the document and also at the passage level, which was offered in Arabic [Elsayed et al., 2019; Barr ́on-Cede ̃no et al., 2020]. The Fact Extraction and Verification shared task (FEVER) focused on extracting an evidence sentence related to a claim from Wikipedia articles and determining whether it supports, refutes, or provides no enough information about the claim [Thorne et al., 2018]. As in INEX, named entities were among the key pieces of information, and they were often used to compose the queries to retrieve the most relevant articles [Malon, 2018; Hanselowski et al., 2018]. A typi- cal system to solve the task starts with document retrieval, e.g., using BM25, followed by sentence retrieval based on the similarity between the input claim and each sentence in the top-n retrieved documents, which can be measured using TF.IDF, Word Mover’s Distance, or BERT. Finally, it would use natural language inference to decide on the verdict. More recent work has used specialized neural semantic matching networks for each of these steps [Nie et al., 2019]. 13http://schema.org/ClaimReview Evidence retrieval might need to go beyond text, e.g., when verifying a claim about an image or a video. In such cases, reverse image search can help find other contexts where the multimedia content was used [Zlatkova et al., 2019]. This allows to check whether these contexts agree with the claim, and to detect out-of-context content, e.g., an image or a video from one event portrayed as being from a different event, as well as potentially manipulated images/videos. Popular tools for this include TinEye,14 Google Image Search, and Yandex Image Search. Relevant research tools are also being devel- oped in two EU projects: WeVerify15 and InVID.16 3.4 Automated Verification Automatic claim verification approaches can be divided into explainable and non-explainable. Explainable approaches, also known as reference-based approaches, are more relevant to assisting human fact- checkers. They verify the input claim against a trusted source such as tables [Chen et al., 2020] or a database [Ahmadi et al., 2019], or using inference over a knowledge graph, pos- sibly while also using Horn rules [Gad-Elrab et al., 2019]. This includes two approaches that we discussed above: find- ing previously fact-checked claims that can verify the in- put claim [Shaar et al., 2020], and fact-checking it against Wikipedia [Thorne et al., 2018; Nie et al., 2019]. Non-explainable approaches make a prediction based on the content of documents retrieved from the Web [Popat et al., 2016; Karadzhov et al., 2017; Augenstein et al., 2019], or on social media by modeling the message and its propagation, the users and their reactions over time, links to media sites, etc. [Castillo et al., 2011; Shu et al., 2017; Vosoughi et al., 2018; Nguyen et al., 2020]. This further includes analysis of the language used in the claims based on lexicons such as LIWC [Rashkin et al., 2017], or using perplexity analysis [Lee et al., 2021]. Fact-checking has also been done using masking in BERT-style transformers [Lee et al., 2020]. While automatic verification is hard, there are promising results for certain kinds of claims. For example, an explicit claim about a numerical value, such as In 2017, global elec- tricity demand grew by 3%.”, can be verified automatically using official statistics, even when this requires applying a complex formula [Karagiannis et al., 2020]. Success here depends on the availability of reliable data, presented in a consistent format, which varies widely between countries and fields. Similarly, simple claims can be verified with promis- ing accuracy when good evidence is available, e.g., for popu- lar entities on the Web [Augenstein et al., 2019]. While the accuracy and the scope of automated fact- checking algorithms keeps improving, two problems prevent their adoption in fact-checking organizations. First, even on the original datasets, their effectiveness is not high enough to allow automatic decisions. Second, most claims in the public realm are more complex, e.g., that COVID-19 vaccines have been developed too quickly and are still experimental.17 To verify such claims, fact-checkers might need to inter- view experts, to collaborate with other fact-checkers, to un- derstand the context and the framing of the claims, to track down and to verify multiple sources and pieces of evidence — all of which require human-levels intelligence. The gen- eral verification of arbitrary claims requires deep understand- ing of the real world that currently eludes AI. Indeed, most methods are designed to assist fact-checkers in their work with suggestions and assume that a human user will assess the verification output before assigning a true/false label. 3.5 Some Real-World Systems Below, we present a brief overview of some notable systems that cover multiple steps of the fact-checking pipeline, while also offering a suitable user interface. AFCNR: The system accepts a claim as an input, searches over news articles, retrieves potential evidence and presents to the user a judgment on the stance of each piece of evidence towards the claim and an overall rating of the claim’s veracity given the evidence [Miranda et al., 2019]. The system was extensively tested by eleven journalists from BBC. BRENDA: This is a browser extension, which allows users to fact-check claims directly while reading news articles [Bot- nevik et al., 2020]. It supports two types of input, either the full page opened in the browser, or a highlighted snippet in- side the page. In the first scenario, the system applies check- worthiness identification in order to decide which sentences in a page to fact-check. ClaimPortal: 18 After retrieving tweets in response to a query, the system [Majithia et al., 2019] scores them for check-worthiness using ClaimBuster and tries to verify each tweet using previously fact-checked claims from PolitiFact. Squash: The system is developed at the Duke Reporters’ lab, this system (i) listens to speech, debate and other events, (ii) transcribes them into text, (iii) identifies claims to check, and then (iv) fact-check them by finding matching claims al- ready fact-checked by humans [Adair, 2020]. Full Fact’s system is designed to support fact-checkers. It (i) follows news sites and social media, (ii) identifies and cat- egorizes claims in the stream, (iii) checks whether a claim has been already verified, and then (iv) enriches the claims with data to support the fact-checker. It is in daily use in the UK and several countries in Africa [Dudfield, 2020]. We believe that the prototypes presented above are good examples of the steps taken towards developing systems that cater to fact-checkers. More systems are now designed to efficiently identify claims originating from various types of sources (e.g., news articles, broadcast, and social media). Moreover, the fact-checker is now becoming a part of the system by providing feedback, rather than just being a con- sumer of its output. Finally, we see an increase in systems’ transparency by providing explainable decisions, thus mak- ing them more an assistive tool rather than a replacement for the fact-checker. However, there are several challenges left to tackle, as we present in the next sections. 18http://idir.uta.edu/claimportal/ 4 Lessons Learned The main lesson from our analysis is that there is a partial dis- connection between what fact-checkers want and what tech- nology has to offer. We provide more detail below. 1. Over time, many tools have been developed, either to au- tomatically fact-check claims or to provide facilities to the fact-checkers to support their manual fact-checking process. However, there are still limitations in both automated and manual processes: (i) credibility issue for automated systems, as they do not provide support- ing evidence, and (ii) scalability issue for manual fact- checking. 2. Automated fact-checking systems can help fact- checkers in different ways: (i) to find claims worth fact- checking, (ii) to find relevant previously fact-checked claims; (iii) to find supporting evidence (in the form of text, audio or video), translating (for multilingual con- tent) and summarising relevant posts, articles and docu- ments if needed, and (iv) to detect claims that are spread- ing faster to slow them down. 3. There is a lack of collaboration between researchers and practitioners in terms of defining tasks and devel- oping datasets to develop automated systems. In gen- eral, a human-in-the-loop can be an ideal setting for fact- checking, which is currently not fully explored. 5 Challenges and Future Forecasting Below we discuss some major challenges and we forecast some promising research directions: 5.1 Major Challenges • Leveraging multi-lingual resources: The same claim, with slightly different variants, often spreads over differ- ent regions of the world at almost the same or at differ- ent time periods. These may be international claims” such as medical claims about COVID-19, or stories that are presented as local, but with varied, false locations. Those claims might be fact-checked in one language, but not in others. Moreover, resources in English are abun- dant, but in low-resource languages, such as Arabic, they are clearly lacking. Aligning and coordinating the ver- ification resources and leveraging them across different languages to improve fact-checking is a challenge. • Ambiguity in the claims: Another reason why auto- matic fact-checking is challenging is related to the fact that often a claim has multiple interpretations. An ex- ample is The COVID death rate is rising.” Is this about mortality or about fatality rate? Does it refer to to- day/yesterday or to the last week/month? Does it re- fer to the entire world or to a specific area? In such cases, knowledge about the context is necessary in or- der to properly frame the claim and to filter out unlikely interpretations. After that, all remaining interpretations should be analyzed, which would further slow down the work of fact-checkers. One system that proposes a solu- tion to this problem is CoronaCheck.19 • System bias: The majority of existing systems are trained using datasets curated by a small group of peo- ple and often annotated by non-experts. This in turn results in systems biased towards how the system de- velopers perceive factuality and how the annotation task was described to the annotators. The dangers of bias in large language models is becoming increasingly obvi- ous [Bender et al., 2021], and should not be ignored just because the purpose of the system is benevolent. • Contextual information: The current state-of-the-art for automated fact-checking makes limited use of con- textual information, e.g., reader’s comments, linked sources of news articles, social network data for social media posts. Such information can provide useful sig- nals for enriching the current models. • Multimodality: Information is typically disseminated through multiple modalities such as text, image, speech, video, temporal, user profile, and network structure. Ad- dressing the problem based on a single modality can be a step towards failure. For example, it might be difficult to detect fake news pieces that are automatically generated using deep fakes and/or GPT-3-style text generation. To avoid such issues, multimodal approaches would be one way to go, if evidence can be gathered from multiple types of sources at the same time. This in turn requires multimodal datasets to develop suitable models. 5.2 Future Forecasting • Close collaboration between fact-checking platforms and researchers: We envision closer collaboration be- tween professionals from fact-checking platforms along- side researchers in the domain to discuss common inter- ests, existing solutions, and future directions, has been a challenge. • Integrated solutions: We also envision unified and open-source initiatives to develop resources for system development and benchmarking. • Usability: We further forecast more research on the sys- tem interface design, which would facilitate the adop- tion of AI by fact-checkers. It is important to de- velop systems that require minimal technical knowledge and reduce cognitive load. Such systems can help a larger number of fact-checkers and journalists in the fact-checking process. • Interpretability and explainability: Models should be designed in such a way that their outcomes are explain- able, unbiased, and more accountable to ethical consid- erations. • Efficient and real-time solutions: Finally, in order to tackle the velocity of the spread of fake news there is a need to develop systems that are efficient and scal- able for real-time solution. To be effective, such systems would need to be embedded within, or accessible by, so- cial networks and other big technology companies.","Nakov, P., Corney, D., Hasanain, M., Alam, F., Elsayed, T., Barrón-Cedeño, A., ... & Martino, G. D. S. (2021). Automated fact-checking for assisting human fact-checkers. arXiv preprint arXiv:2103.07769."
"ML_DB_054","Automated fact checking in the newsroom","Fact-checking is an essential task in journalism; its importance has been highlighted due to recently increased concerns and efforts in combating misinformation. In this paper, we present an automated fact-checking platform which given a claim, it retrieves relevant textual evidence from a document collection, predicts whether each piece of evidence supports or refutes the claim, and returns a final verdict. We describe the architecture of the system and the user interface, focusing on the choices made to improve its user friendliness and transparency. We conduct a user study of the fact-checking platform in a journalistic setting: we integrated it with a collection of news articles and provide an evaluation of the platform using feedback from journalists in their workflow. We found that the predictions of our platform were correct 58% of the time, and 59% of the returned evidence was relevant.","Computer Science","Proceeding","2019","Y","Y","Tool","Support","Information retrieval","13","Grounding information on reliable sources is a daunting experience, given the increasing amount of information circulating the web and other media platforms. Nevertheless, checking the veracity of claims is crucial for preserving trust in news sources. Manual verification of claims is a tedious task, that consumes a lot of time and effort from journalists and professional fact-checkers, and it typically requires searching for specific entities and content over large amounts of unstructured text [2, 4]. The rising interest in fact checking has led to the development of a number of approaches and tools automating the task or parts of it, with the motivation of facilitating the work of journalists, and interested readers more broadly [ 3, 9]. The most popular ef- fort, ClaimBuster [ 5], proposed a fact checking platform which detects factual claims that are worth checking and then uses APIs to query search engines and databases (Google and Wolfram Al- pha respectively). It also compares claims against previously fact checked ones in its database. The latter approach is also used by the system developed by Full Fact, Live, which is used to fact check repeated or paraphrased claims.1 Neither of these approaches is able to fact check previously unchecked claims, while the queries through existing commercial APIs are not tailored to fact checking, thus retrieving information that is not necessarily relevant. Other approaches rely instead on the detection of rumours based on the spread of readership over social media [7]. However, a rumorous claim is not necessarily false, and vice versa [16]. There is also work that checks claims against tables, such as those released by Claim: Tesla builds car factory in Shanghai. Evidence: Electric carmaker Tesla has signed an agreement with Chinese authorities to build a factory in Shanghai. We hope it will be completed very soon,” Tesla chief Elon Musk said. Figure 1: Example of claim and evidence. Extracted named entities in bold: organizations (purple), locations (blue) and people (green). the World Bank2, using trained classifiers to select the appropriate tuple [10 ]. However such approaches are typically restricted to fact checking numerical claims against tabular sources. Finally, some approaches aim at providing a pipeline of tools for information retrieval [ 14], but do not go as far as to provide an actual fact check mechanism. In this work, we propose an automated fact checking platform that checks claims by identifying sentences providing evidence in a large document collection. These sentences are classified as supporting, refuting or only related to the claim, and then com- bined into a final verdict using a state-of-the-art neural network- based approach [15 ]. The model is trained on the recently released FEVER dataset [ 11 ], a large scale fact checking dataset derived from Wikipedia comprising 185K claims. Unlike previous work, our model is able to check novel claims without relying on a database of fact checks. Also, the evidence retrieved and classified as sup- porting or refuting provides a justification of the verdict, which is likely to be relevant not only to assess the overall correctness of the platform, but also as part of the fact checking research conducted by journalists. Our retrieval model considers more information about a specific claim than generic search engines by leveraging information from words and named entities present in the dataset to obtain the best matching evidence for each claim (see Figure 1). We thoroughly evaluate the platform through user testing by journalists from the British Broadcasting Corporation (BBC) in the context of their workflow. Of the total 488 evidence passages retrieved by the system, the journalists reported that 58% were rele- vant and 59% were accurately classified as supports/refutes/other. Our platform can be applied to document collections beyond the ones used in this paper and our findings should help inform future research in automated fact checking and computational journalism more broadly. 2 FACT CHECKING SYSTEM Our fact-checking system comprises three main components: a document retrieval step, a sentence ranking, and a classification model, as shown in Figure 2. Initially we retrieve documents from a collection of news arti- cles, using a customized search engine based on inverse indexing and retrieval using the Okapi-BM25 algorithm [8 ]. Data is incre- mentally indexed from word and entity-level inverted indexes. The document retrieval component searches for documents whose fea- tures best match the claim. In the end of this step, we end up with approximately 10K documents related to the claim. w l lemmas e entities words heads features documentsd Document Retrieval Sentence Ranking Natural Language Inference (Hexa-F) ≈ 5k docs ≈ 25 evidence sentences Claim evidence 1 evidence 2 evidence 3 evidence 4 sentence l sentence p sentence n sentence m [title di , sentencel ] [title dk , sentencep ] [title dj , sentencen ] [title di , sentencem ] rank 1: rank 2: rank 3: rank 4: Claim Label: Figure 2: Fact-checking model comprised of three compo- nents: a) Document retrieval (top left), b) Sentence ranking (middle right), c) NLI prediction model (bottom). Following this, we rank the sentences in these documents accord- ing to predefined token and feature matching rules. We compute the cosine similarity of the claim with each best candidate sentence, using word embeddings trained on the One Billion Word Bench- mark corpus [1], and select those above a threshold tuned during development. This step aims at providing only sentences with high relevance to the claim, reducing the number of potential evidence that may be only marginally related with the claim. In the final component, we classify each of the extracted can- didate evidence in terms of whether they support, refute or are just related to the claim (other). We employ the natural language inference (NLI) model from the Hexa-F system [ 15 ] (one of the best performing systems in the FEVER shared task [12]) to classify the relation between the selected evidence sentences and the claim, one of supports/refutes/other, and a similar label which expresses whether the combined set of evidence sentences supports, refutes or is simply related to the claim. 2.1 Engineering Considerations In the first document retrieval component, we use inverted indexes to extract relevant documents, in Figure 3. We select all documents that match features in the claim, such as lemmas, words and ex- tracted named entities [6 ]. This document retrieval step takes about 50 ms to retrieve around 5k documents out of a dataset with about 445k documents. In the second component, we rank all the sentences from the 5k documents based on how well each sentence feature φ (si )j matches the claim φ (c)j , using a positional ranking approach. We use a ranking score based on ordered distances between N matching features in the sentence i: S1 (si , c) = ∑N j=1 exp (−di, j ), where di, j = pos(φ (si )j ) − pos(φ (si )j−1 ) represents the word distance between the position ( pos) of each j-th feature. This score is maximized if all words in the sentence match the claim exactly; it decreases exponentially with the word distance between matches. This part takes on average 336 ms. Next, we filter the sentences based on3580 the following rules: we keep only those sentences with length < 500 words; those that contain all the entities mentioned in the claim, as well as novel words that were not previously seen in previously selected sentences (less than 90% overlap with all words previously encountered). The novelty filter increases the diversity in the evidence passed to the entailment step. In the end of the second component, we re-rank the sentences by averaging the feature matching score of each sentence S1 and the cosine similarity between the claim and the sentence embeddings S2 = cos(si , c). We obtain these embeddings via a weighted average of all words in the sentence, weighted by term-frequency/inverse document frequency. To improve the performance of the sentence re-ranking component, we added the title of the document to each sentence and considered the weighted sum of all words in the title and sentence combined. This part takes about 652 ms, although it could be easily parallelizable. On average, 76 sentences are retrived per claim after these steps. We further removed all sentences with an averaged similarity score below a given threshold ((S1 + S2 ) /2<0.6), to ensure high quality evidence. In the end, about 25 sentences on average are selected. After re-ranking, all selected sentences are used as input in the NLI model (third component) which takes about 738 ms to predict labels for each of the sentences and the overall label for the claim. 3 USER INTERFACE The interface allows end-users to input a claim (black bar at the top of Figure 3), and receive a set of evidence sentences as output. The evidence is displayed in three columns: the top five ranking sentences that are in favour of the claim on the left, the top five that are against the claim in the middle, and the top five other sentences related to the claim on the right. In the bottom, a final overall label is presented to the user as either other, supporting or refuting the claim (label at the bottom). The interface allows users to directly evaluate it, providing feedback for evidence sentence w.r.t. the correctness of the label (correct label?”) and its relevance (relevant?”), as well as the correctness of the overall prediction. We provide a video with an example of a user interacting with the platform via the interface in. 4 USER EVALUATION DESIGN 11 BBC journalists provided feedback on the overall system and on the classification model. The journalists were asked to interact with the system by providing factual claims and evaluating the output of the model. For each claim, up to 15 evidence sentences were presented to them, 5 per category, each classified as supports, refutes or other. For each sentence, the journalists provided feedback on two aspects: relevance and correctness. To assess correctness, for each evidence sentence returned by the system, the journalist inputs the label which, according to his/her research on the subject, would be the correct one via the buttons in the correct label” box (see Figure 3). This measures primarily the accuracy of the entailment component of the system, assuming that the sentences returned are all related. For the final classification of the model (see Figure 3 (bottom)) they also assessed the overall prediction, to whether the claim was globally supported or refuted considering all the retrieved evidence. Precision Class supports refutes other all Relevant 71% 69% 49% 59 % Evidence Correctness 48% 27% 70% 58 % Global Correctness 56% 26% 31% 42 % Table 1: User evaluation on the full dataset. To assess relevance, the journalists were also asked to provide feedback as to whether they found each sentence returned relevant (see the buttons in the relevant ?” box in Figure 3). This part of the feedback aims to evaluate the quality of the retrieved evidence: whether it helps the journalists fact-check the input claim, regard- less of the classification label attributed by the system. It also serves as a proxy to measure the precision of the retrieval component, as all sentences shown to the journalists should be relevant (ideally). The journalists also had access to (i) a Question Answering (QA) [ 13 ] tool that could serve as an additional source of information, (ii) the full document’s text with annotated entities [6 ]. The QA implemented in the platform was used by the journalists, but was not evaluated in this paper. We provide an example of the addi- tional information, available for each extracted evidence sentence in Figure 4. 5 RESULTS Table 1 summarizes the results of the system is the user evaluation conducted. We show precision for each class (row:Relevant) mea- suring the proportion of the retrieved evidence sentences that were deemed relevant by the journalists, and precision for the predictions of the platform for the relation of each evidence sentence to the claim (row:Evidence) and the global prediction for the claim taking all the evidence into account (Overall result presented in Figure 3) (row:Global). The platform was evaluated on 67 claims in total, with a total of 488 evidence sentences retrieved: 30% supporting, 14% refuting and 56% related to the claim, as classified by our platform. In 71% of the claims checked by the journalists, it was reported that the evidence shown in the supports column was relevant, and so was for 69% of the evidence in the refutes columns. We consider these results to be very encouraging, given the difficulty of the task. Retrieving evidence that contradicts a given claim, is not usually as simple as retrieving related evidence by feature matching. Intro- ducing different information, e.g. entities, dates, actions, etc., that could refute the claim requires more complex language understand- ing methods. We further observe that the precision in predicting evidence as supports is 48% in the full dataset and increases to 67% in the subset of evidence deemed relevant (not shown in the table). The same trend is observed for the refutes label from 27% to 39%. This result suggests that the retrieval component still requires some improvement, especially for retrieving evidence refuting the claim. Strategies beyond feature matching are needed to improve the retrieval of relevant but opposing arguments. The precision of the classifier predicting the global label of the claim given the evidence also requires further improvement. Additionally, we received textual feedback from the journalists about the overall quality of the platform. They mentioned that it is helpful for fact checking, despite not being always accurate, both in the retrieval of relevant evidence and in their evaluation. An interesting remark mentioned possible improvements for handling3581 Figure 3: Example of the fact checking interface. Claim: Russia meddled with US elections” (top). Five maximum evidence sentences for each column: supports/refutes/other (middle). Example of final system decision and feedback buttons (bottom). Figure 4: Example of additional information for each evi- dence. Original document (right) with entities in bold. time-frames and dates. For instance, claims using the present tense should refer to current events, while those mentioned in the past tense together with dates should refer to that specific time-period only. Also they suggested that presenting the evolution of results over time would be very helpful to substantiate the claim. Han- dling time constraints in the retrieval process is a very interesting and challenging research direction. On the whole, the journalists reported that the system has a lot of potential to help their work. This user testing was extremely useful both for BBC journalists to experiment with state-of-the-art technology and for us to receive feedback to improve our platform in the future. 6 CONCLUSIONS AND FUTURE WORK This paper introduces a novel fact checking platform aimed to assist journalists in their investigative work-flow. Our platform can be used for search of supporting and refuting evidence regarding fac- tual claims. We evaluated using on a journalistic corpus with testing by eleven journalists, which found it to yield relevant results in 59% of the retrieved evidence. The performed user study provided very fruitful feedback to direct future work in automated fact checking. Suggested improvements such as handling temporal remarks, pose an interesting issue that we found very relevant to advance research in the field of information retrieval for fact checking.","Miranda, S., Nogueira, D., Mendes, A., Vlachos, A., Secker, A., Garrett, R., ... & Marinho, Z. (2019, May). Automated fact checking in the news room. In The World Wide Web Conference (pp. 3579-3583)."
"ML_DB_057","Social Computing for verifying social media content in breaking news","Social media is the place to go for both journalists and the general public when news events break, offering a real-time source of eyewitness images and videos through platforms like YouTube, Instagram, and Periscope. Yet, the value of such content as a means of documenting and disseminating breaking news is compromised by the increasing amount of content misuse and false claims in social media. To this end, cost-effective social-computing solutions for user-generated content verification are crucial for retaining the value and trust in social media for breaking news.","Social Computing","Article","2018","Y","Y","Tool","Support","Multimedia forensics","14","Social media is the place to go for both journalists and the general public when news events break, offering a real-time source of eyewitness images and videos through platforms like YouTube, Instagram, and Periscope. Yet, the value of such content as a means of documenting and disseminating breaking news is compromised by the increasing amount of content misuse and false claims in social media. To this end, cost-effective social-computing solutions for user-generated content verification are crucial for retaining the value and trust in social media for breaking news. Most people have a smartphone in their pocket today, so eyewitnesses experiencing an event like a terror attack will often post real-time claims, such as the numbers dead or injured in a location, to Twitter or Facebook. Eyewitness images and videos will also be uploaded to sites like YouTube and Instagram or even streamed live to sites like Periscope. For events such as the 2015 Paris shootings, 1 the first eyewitness videos of the various shootings were posted within 5 to 10 minutes of the event happening. This was followed about 20 to 30 minutes later with veri- fied news reports from sources such as Le Figaro, the BBC, and CNN. In other cases, verifying eyewitness or user-generated media and claims can take much longer, from hours to even days, as, for instance, in the case of Malaysia Airlines Flight 17, which was shot down on 17 July 2014. In many cases, as soon as a breaking news event starts trending on Twitter, it is accompanied by considerable numbers of false claims and content misuse.2 This involves the use of multimedia for misinforming the public and misrepresenting people, organizations, and events. Misuse prac- tices range from publishing content that has been digitally tampered using photo-editing software to falsely associating content with an unfolding event. The paper Detection and Resolution of Rumours in Social Media: A Survey”2 contains an extensive discussion on the problem of rumor detection in social media. Stuart E. Middleton University of Southampton Symeon Papadopoulos Centre for Research and Technology Hellas Yiannis Kompatsiaris Centre for Research and Technology Hellas 83 IEEE Internet Computing Published by the IEEE Computer Society 1089-7801/18/$33.00 USD ©2018 IEEEMarch/April 2018 Authorized licensed use limited to: Centre for Research and Technology (C.E.R.T.H.). Downloaded on March 13,2020 at 13:45:38 UTC from IEEE Xplore. Restrictions apply. IEEE INTERNET COMPUTING Given the grave societal and economic impact of having misused content and false claims fea- tured in mainstream news, it becomes extremely important for news organizations to be able to verify eyewitness media in very short time. To this end, journalists are turning to social-compu- ting approaches to automatically analyze and verify 3 user-generated content in real time. The eventual hope is that cost-effective social computing can reduce the time spent on verification to time scales nearer to real time. SOCIAL-MULTIMEDIA FORENSICS AND SUPERVISED VERIFICATION Methods from the field of digital forensics are often used for assessing the veracity of multime- dia items (images or videos) posted online. Some methods focus on the analysis of information encoded in the metadata of multimedia content, such as EXIF (Exchangeable Image File) infor- mation, which is often associated with JPEG and TIFF images. Several types of digital manipu- lation, such as the use of photo-editing software, leave traces in the form of metadata unless special care is taken to remove them, and analysis of these traces can detect manipulations. Un- fortunately, several of the most popular social media platforms, including Facebook and Twitter, automatically remove much of the metadata from posted content, rendering metadata-based methods useless for content obtained from these platforms. Other forensics-based methods aim at uncovering traces of manipulation in the visual content itself. In images, such methods4 can detect cases of splicing and copy–move operations—for ex- ample, inpainting of a part of one image into a second, or replication of a part of an image within the same image. Methods can leverage the uniqueness of noise patterns introduced by the captur- ing device in order to detect whether an image contains traces from another image captured by a different device. Other methods focus on patterns associated with the color filter arrays of mod- ern image-capturing equipment. Splice detection methods exploit traces left by the JPEG com- pression process, working on the basis that the splicing of two different images and the subsequent recompression will leave detectable traces in the final JPEG file. While all of the above methods yield satisfactory results when applied on well-controlled test samples, they have been found to exhibit poor performance in real cases.5 One of the reasons that state-of-the-art methods fail to detect manipulations in media content published on the web is that such content is often the result of numerous intermediate operations, including resizing, cropping, and recompression, which have an obfuscating effect on the traces of digital manipula- tion. Examples are Twitter and Facebook, both of which automatically resize and recompress all images uploaded to them.6 Recent work in the FP7 REVEAL project (see Figure 1) addresses the poor performance of indi- vidual tampering-detection methods7 by generating tampering probability heat maps based on a number of complementary forensic-analysis algorithms. The inclusion of multiple image-foren- sics algorithms and side-by-side comparisons gives a powerful means to journalists to under- stand where possible digital tampering has occurred. The problem of identifying digital manipulations in video content is even more challenging compared to the case of images, and it is further exacerbated in cases where such content is sourced from video-sharing and social-net- working platforms such as YouTube and Facebook. The Horizon 2020 progam’s InVID project is looking into resilient approaches for video forensics building upon the TUNGSTENE com- mercial forensics engine. 84March/April 2018 www.computer.org/internet Authorized licensed use limited to: Centre for Research and Technology (C.E.R.T.H.). Downloaded on March 13,2020 at 13:45:38 UTC from IEEE Xplore. Restrictions apply. SOCIAL COMPUTING Figure 1. A digital-forensics platform for image verification (screenshot taken from the Media Verification Assistant). Some cases of content misuse are not detectable by using forensics analysis—for example, when an image from a past event is reposted as being associated with an unfolding event. These re- quire other methods that attempt to detect misuse by analyzing contextual cues from social me- dia sources. 8 A typical approach adopted by such methods is to extract a variety of trust-oriented features from social media posts, and the accounts generating these posts, and to use them for training machine-learning models based on historically labeled cases of fake and real posts. Such methods have been shown to yield very high fake-post-detection accuracy.9 EYEWITNESS MEDIA AND FACT EXTRACTION The standard workflow for automated fact checking10 involves the monitoring of data sources, fact identification, fact extraction, and fact checking. The challenges for social computing mostly involve fact identification and extraction. Once a fact is extracted, it can be checked either manu- ally or automatically against databases from sites such as PolitiFact, FactCheck.org, Snopes, and Wikipedia. Factual claims come in many forms. The most important for social computing are factual asser- tions, contextual statements associated with a fact, and contextual statements associated with the trustworthiness of the fact. Factual assertions themselves can be true, false, half-truths, or exag- gerations. Contextual statements can allow a true representation of a fact or misrepresent it by suggestions of a false location, actor, or time stamp. Contextual text can also introduce ideologi- cal cues and loaded language to bias the interpretation of the fact. Finally, contextual statements can suggest trustworthiness, such as attribution to a trusted source or claims of previous verifica- tion that themselves might be subject to falsehoods or deliberate bias. Fact identification approaches, especially for news-related sources, try to classify sentences into nonfactual, unimportant factual, and check worthy” factual statements11 so that they can be fil- tered prior to fact extraction. Fact extraction is a type of information extraction problem that runs alongside information extraction techniques for concepts such as event, topic, location, and time. In the past, approaches such as argumentative zoning12 were applied successfully to extract fac- tual statements on well-structured and trustworthy scientific documents. However, the text in web and social media sources is often neither well structured nor trustworthy, so new approaches are being explored. Early work in this area focused on verb phrase patterns (e.g., was elected to”) to extract facts via systems such as OLLIE (Open Language Learning for Information Extraction).13 These used part-of-speech (POS) tagging, dependency parsing, and distant supervision coupled with seed 85March/April 2018 www.computer.org/internet Authorized licensed use limited to: Centre for Research and Technology (C.E.R.T.H.). Downloaded on March 13,2020 at 13:45:38 UTC from IEEE Xplore. Restrictions apply. IEEE INTERNET COMPUTING attributes and bootstrapping to provide unsupervised fact extraction. In particular, they were able to capture the long tail” of factual statements, which is very important for the contextual inter- pretation (e.g., Putin made a deal with the separatists”). Later advances,14 motivated by the need to answer queries in search engines, added noun phrase patterns (e.g. Obama’s wife”) very suc- cessfully. Typically, such approaches exploit large databases of attribute names and noun phrases such as Freebase and DBpedia. Automated fact checkers use either domain-specific databases (e.g., PolitiFact) or web-scale da- tasets (e.g., DBpedia). Recently, there has been a trend of real-time crowdsourcing of fact check- ing during events such as US political rallies, with the Trump–Clinton presidential debates being the latest example. Fake news sites have also been increasing in number and can easily mislead readers15 into trusting misinformation based on a credible but false source attribution. The iCheck system16 is a good example where domain-specific heuristics extract fact types that are visualized via a crowdsourcing interface for users to check claims and up- or down-vote them. Work from the REVEAL project17 has taken these ideas one step further to help journalists ver- ify breaking news. Automated fact extraction using semantic grammars, seeded with linguistic phrases originating from journalists, is used to extract evidence from social media content about news events such as incident reports, facts about damage, the numbers of the dead and injured, people, locations, and attributed sources. User-generated content from the scene of a breaking event, not yet syndicated via news organizations, is particularly important for journalists. Super- vised-learning algorithms are employed within REVEAL to identify and filter posts containing eyewitness images and videos. This type of social computing is coupled with real-time visualiza- tions (see Figure 2), allowing journalist to quickly find contextual content such as original men- tions of claims for subsequent verification. Figure 2. An interactive real-time visualization mapping extracted facts and eyewitness media in posts about the December 2016 Malta plane hijacking (screenshot taken from the Journalist Decision Support System). CONCLUSION We highlighted in this article the potential of employing social-computing approaches for speed- ing up the task of verifying user-contributed information and content sourced from social media platforms. The problem is complex and calls for a variety of approaches, each targeting different challenges stemming from the characteristics of user-generated content, including high volume, inconsistent quality, and a lack of provenance information. Multimedia forensics targets the ac- 86March/April 2018 www.computer.org/internet Authorized licensed use limited to: Centre for Research and Technology (C.E.R.T.H.). Downloaded on March 13,2020 at 13:45:38 UTC from IEEE Xplore. Restrictions apply. SOCIAL COMPUTING tual content of multimedia. Supervised verification is best suited to cases where contextual fea- tures can be extracted and labeled training sets of fake and real examples are available. Fact ex- traction and visualization approaches target text-based sources that contain references to different elements of an event, such as people, times, and locations. The REVEAL project is one of the first efforts to bring together those technologies under a sin- gle platform that could provide comprehensive verification support to professional users; details on the successful user evaluation of pilot prototypes can be found in D7.2 User Evaluation Re- port.” 18 However, there is still a long way to go before such tools are widely used by newsrooms and journalists day to day. Figure 3 provides an overview of the projects and datasets useful to researchers interested in automating verification tasks for social media and news-related content. Figure 3. The research and dataset landscape for researchers interested in verification of social media and news-related content. One key challenge involved in delivering such an integrated solution is the lack of an appropriate human–computer interaction (HCI) approach that would empower users (e.g., journalists) to make optimal use of the technologies described above. Given the extensive use of algorithms, an effective HCI approach would need to build the trust of users by providing intuitive control and a clear explanation of the results. Ultimately, users are in charge of the whole process and will 87March/April 2018 www.computer.org/internet Authorized licensed use limited to: Centre for Research and Technology (C.E.R.T.H.). Downloaded on March 13,2020 at 13:45:38 UTC from IEEE Xplore. Restrictions apply. IEEE INTERNET COMPUTING make the final decision with respect to whether a piece of user-generated content should be con- sidered authentic or not. Moreover, support for collaborative work among teams of journalists is another key social-computing challenge that is missing from existing news provider in-house so- lutions, which instead employ general-purpose communication and messaging platforms such as Slack and WhatsApp. In conclusion, the problem of real-time verification of user-generated content is expected to re- main unsolved in the near future, but marked improvements have already been achieved on indi- vidual parts of the verification process thanks to social-computing approaches incorporating intelligent information processing. In the future, we anticipate considerable progress on this problem by incorporating the latest advances from deep learning. One example would be em- ploying generative adversarial networks19 to build highly accurate and robust models for visually distinguishing between tampered-with and untampered-with regions in multimedia content. An- other example would be novel HCI approaches focusing on the explainability of automatically generated results and the collaborative aspects of the verification process.","Middleton, S. E., Papadopoulos, S., & Kompatsiaris, Y. (2018). Social computing for verifying social media content in breaking news. IEEE Internet Computing, 22(2), 83-89."
"ML_DB_071","Claimbuster: The first-ever end-to-end fact-checking system","Our society is struggling with an unprecedented amount of falsehoods, hyperboles, and half-truths. Politicians and organizations repeatedly make the same false claims. Fake news floods the cyberspace and even allegedly influenced the 2016 election. In fight- ing false information, the number of active fact-checking organizations has grown from 44 in 2014 to 114 in early 2017. 1 Fact- checkers vet claims by investigating relevant data and documents and publish their verdicts. For instance, PolitiFact.com, one of the earliest and most popular fact-checking projects, gives factual claims truthfulness ratings such as True, Mostly True, Half true, Mostly False, False, and even Pants on Fire”. In the U.S., the election year made fact-checking a part of household terminology. For example, during the first presidential debate on September 26, 2016, NPR.org’s live fact-checking website drew 7.4 million page views and delivered its biggest traffic day ever. The challenge is that the human fact-checkers cannot keep up with the amount of misinformation and the speed at which it spreads. One of the reasons for this is that fact-checking is an intellectually demanding, laborious, and time-consuming process.","Computer Science","Article","2017","Y","Y","Tool","Support","Multitasking","182","Our society is struggling with an unprecedented amount of false- hoods, hyperboles, and half-truths. Politicians and organizations repeatedly make the same false claims. Fake news floods the cy- berspace and even allegedly influenced the 2016 election. In fight- ing false information, the number of active fact-checking organi- zations has grown from 44 in 2014 to 114 in early 2017. 1 Fact- checkers vet claims by investigating relevant data and documents and publish their verdicts. For instance, PolitiFact.com, one of the earliest and most popular fact-checking projects, gives factual claims truthfulness ratings such as True, Mostly True, Half true, Mostly False, False, and even Pants on Fire”. In the U.S., the election year made fact-checking a part of household terminology. For example, during the first presidential debate on September 26, 2016, NPR.org’s live fact-checking website drew 7.4 million page views and delivered its biggest traffic day ever. The challenge is that the human fact-checkers cannot keep up with the amount of misinformation and the speed at which it spreads. One of the reasons for this is that fact-checking is an intellectu- ally demanding, laborious, and time-consuming process. This chal- lenge creates an opportunity for automated fact-checking systems. On the other hand, fact-checking technology is clearly falling be- hind, as there is simply no existing system that truly does auto- mated fact-checking. Today’s professional fact-checkers diligently perform their work as an art, following good practices in data and investigative journalism. A recent white paper [2] surveys existing tools that can be integrated. Although the relevant tools and tech- niques can assist fact-checking in various areas, a full-fledged, end- to-end solution does not exist. There have been some attempts,2 but those efforts did not lead to such fact-checking systems. ∗Work performed while at UT-Arlington. 2T. Wilner. Fail and move on: Lessons from automated fact-checking ex- periments. Poynter, September 7, 2016. This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Proceedings of the VLDB Endowment, Vol. 10, No. 12 Copyright 2017 VLDB Endowment 2150-8097/17/08. Starting in December 2014, we have been building ClaimBuster, an end-to-end system that uses machine learning, natural language processing, and database query techniques to aid in the process of fact-checking. It monitors live discourses (e.g., interviews, speeches and debates), social media, and news to identify factual claims, de- tect matches with a curated repository of fact-checks from profes- sionals, and deliver those matches instantly to the audience (e.g., by displaying a pop-up warning if a presidential candidate makes a false claim during a live debate). For various types of new claims not checked before, ClaimBuster automatically translates them into queries against knowledge databases and reports whether they check out. For claims where humans must be brought into the loop, it provides algorithmic and computational tools to assist lay persons and professionals in understanding and vetting the claims. Its use will be expanded to verify both political and non-political claims in many types of narratives, discourses, and documents such as sports news, legal documents, and financial reports. While the development of the full-fledged system is still on- going, several components of ClaimBuster are integrated and de- ployed in the real-world. One of its most mature components, the claim spotter, discovers factual claims that are worth checking. Given the plethora of discourses and narratives we are constantly exposed to, ClaimBuster gives each sentence a score that indicates how likely it contains an important factual claim that should be checked. This essentially provides a priority ranking that helps fact-checkers efficiently focus on the top-ranked sentences without painstakingly sifting through a large number of sentences. ClaimBuster was tested in real-time during the live coverage of all primary and general election debates throughout the 2016 U.S. election. Closed captions of the debates on live TV broadcasts, captured by a decoding device, were fed to ClaimBuster, which immediately scored each sentence spoken by the candidates and posted top-scored claims to the project’s website ClaimBuster) and Twitter account (@ClaimBusterTM). Post-hoc anal- ysis of the claims checked by professional fact-checkers at CNN, PolitiFact.com, and FactCheck.org reveals a highly positive corre- lation between ClaimBuster and journalism organizations in decid- ing which claims to check [5]. ClaimBuster has also been continu- ously monitoring Twitter and retweeting the check-worthy factual claims it finds in tweets from politicians and organizations (twitter. com/ClaimBusterTM). Recently it started to monitor Hansard” 3 – the transcripts of proceedings of the Australian parliament Claim Monitor Claim Spotter match found match not found Claim Checker claims keywords, queries ... social media debates Hansard Repository fact-checks claims PolitiFact.com CNN.com knowledge bases other sources other sources Website Twitter Slackbot API Fact-check Reporter Claim Matcher WebFigure 1: System architecture of ClaimBuster. ClaimBuster already produces true-or-false verdicts for certain types of factual claims. Given a factual claim which is scored highly by the claim spotter component, ClaimBuster may reach a verdict by two methods. One method is to translate the factual claim into questions and their accompanying answers. It then sends the questions to question-answering systems and compares the re- turned results with the aforementioned answers. It produces a ver- dict based on the presence/absence of a discrepancy between these two sets of answers. The other method is to search in a repository for similar or identical claims that have already been fact-checked by professionals and to use the verdicts from the professionals. In the case that ClaimBuster is not able to produce a verdict, it pro- vides processed search results from a general search engine to assist vetting the claim. The ClaimBuster project has received wide recognition in the fact-checking community and substantial media coverage. (See for a list of media outlets and the stories in which they cover ClaimBuster.) The aforementioned white paper calls ClaimBuster a tool with the most advanced generalised automatic claim spotting.” [2] Others considered it perhaps the biggest development to date” in ranking claims 4 and a pretty use- ful guide for journalists and those members of the public who wish to spend time using an algorithm to help find facts.” 5 ClaimBuster, upon completion, can benefit a large base of poten- tial users. It directly benefits citizens and consumers by improving information accuracy and transparency. It helps news organizations speed up their fact-checking and ensure the accuracy of their news stories. Businesses can use ClaimBuster to identify falsehoods in their competitors’ and their own reports and press releases. It can also assist professionals such as lawyers in verifying documents. 2. SYSTEM OVERVIEW The ClaimBuster system is hosted at and its features are being constantly expanded. Figure 1 depicts its system architecture. The claim monitor interfaces various data sources (social media, broadcasted TV programs, and websites) with ClaimBuster. The claim spotter identifies check-worthy fac- tual claims in verbose text from the data sources. The claim matcher 4K. Moreland and B. Doerrfeld. Automated Fact Checking: The Holy Grail of Political Communication. Nordic APIs, February 25, 2016. 5 P. Fray. Is that a fact? Checking politicians’ statements just got a whole lot easier. The Guardian, April 18, 2016. finds existing fact-checks that are closely-related or identical to the discovered claims. In this way, we fully leverage well-researched fact-checks from professional fact-checkers. This is particularly useful, because oftentimes the same false claims are repeated. 6 When a matching fact-check cannot be found, the claim checker queries external knowledge bases and the Web to vet the factual claims. The fact-check reporter compiles the evidence from the claim matcher and the claim checker, and presents fact-check re- ports to users through various channels, such as the project website, its Twitter account, a Slackbot, and a public API. Below we explain these components in more detail. Claim Monitor: This component continuously monitors and re- trieves texts from a variety of sources, upon which claim spotting is applied to discover important factual claims. At present, the system monitors the following sources. Broadcast Media: ClaimBuster uses a decoding device to extract closed captions in broadcasted TV programs. This was used for our live coverage of all twenty-one primary election debates and four general election debates of the 2016 U.S. presidential election. One challenge in delivering the live coverage of such events is the lack of speaker identity in the closed captions. ClaimBuster timely derives the speaker of a sentence using the Twitter Streaming API. 7 The idea is based on the premise that, during a popular live event, active Twitter users tend to mention the speaker while tweeting a statement the speaker made. Details of speaker identification in ClaimBuster can be found in [7]. Social Media: ClaimBuster has been continuously monitoring a list of 2220 Twitter accounts (U.S. politicians, news and media organizations) using the Twitter streaming API. It filters out non politics-related tweets using an SVM classifier [1]. Websites: ClaimBuster also gathers data from websites. For in- stance, as mentioned in Section 1, it monitors the transcripts of proceedings of the Australian parliament. Claim Spotter: Given a sentence, ClaimBuster gives it a score between 0.0 and 1.0. The higher the score, the more likely the sen- tence contains check-worthy factual claims. The lower the score, the more non-factual, subjective and opinionated the sentence is. ClaimBuster’s score is based on a classification and scoring model. The model was trained using tens of thousands of sentences from past general election debates that were labeled by human coders. 6A. D. Holan. All Politicians Lie. Some Lie More Than Others. The New York Times, December 11, 2015. Figure 2: The user interface of ClaimBuster when it is applied on a debate. Its features include the tokens in sentences and the tokens’ part- of-speech (POS) tags. The recall and precision in detecting check- worthy factual claim are 74% and 79%, respectively [3, 4]. The claim spotter has been applied on sentences from all the aforementioned sources, including the closed captions of the pres- idential debates, tweets, and Hansard. Post-hoc analysis of the claims from the primary debates for the 2016 U.S. presidential elec- tion checked by professional fact-checkers at CNN, PolitiFact.com and FactCheck.org reveals a highly positive correlation between ClaimBuster and journalism organizations in deciding which claims to check and the topics of the selected claims [5]. Although its scoring and ranking model was trained using a labeled dataset of presidential debates, we find that the model achieved strong results on politics-related tweets and Hansard as well. Claim Matcher: Given an important factual claim identified by the claim spotter, the claim matcher searches a fact-check reposi- tory and returns those fact-checks matching the claim. The reposi- tory was curated from various fact-checking websites. The system has two approaches to measuring the similarity between a claim and a fact-check. One is based on the similarity of tokens and the other is based on semantic similarity. An Elasticsearch 8 server is deployed for searching the repository based on token similarity, while a semantic similarity search toolkit, Semilar [8], is applied for the search based on semantic similarity. We combine the search results from both in finding fact-checks similar to the given claims. 8https://github.com/elastic/elasticsearch Claim Checker: Given a claim, the claim checker collects sup- porting or debunking evidence from knowledge bases and the Web. With regard to knowledge bases, it uses a question generation tool [6] to generate many questions based on the claim and select those good questions which are then sent to the question answering en- gine Wolfram Alpha via an API. 9 Then the answers from Wolfram Alpha are extracted. Simultaneously, it sends the aforementioned questions to Google via HTTP requests and extracts the answers from Google’s answer boxes in the HTML responses. If any clear discrepancies between the returned answers and the claim exist, then a verdict may be derived and presented to the user. Meanwhile, the factual claim itself is sent to Google as a general search query. The claim checker then parses the search result and downloads the web page for each top result. Within each such page, it finds sentences matching the claim. The matching sentences and a few of their surrounding sentences are then grouped together into a context. The contexts, answers returned from Wolfram Alpha and Google answer boxes, as well as any verdicts derived from those answers form the supporting or debunking evidence for the claim. The evidence is reported to the user, as follows. Fact-check Reporter: The fact-check reporter synthesizes a re- port by combining the aforementioned evidence and delivers it to users through the project website. Furthermore, ClaimBuster also delivers the claim spotter scores on claims through a variety of Figure 3: The homepage of ClaimBuster website. channels, including its website, Twitter account, API, and Slack- bot. Its Twitter account (@ClaimBusterTM) retweets the highly- scored tweets from politicians and organizations and posts highly- scored claims from live events such as the presidential debates. To this date, @ClaimBusterTM has retweeted and posted about 13K check-worthy factual claims. A Slackbot has been developed for users to supply their own text (i.e., directly as input or through a shared Dropbox folder) and receive the claim spotter score and fact- check report for that piece of text. The Slackbot has been published in the public Slack App directory and can also be installed by click- ing the ClaimBuster Slackbot” button in Figure 3. We also made available a public ClaimBuster API (note the button in Figure 3) to allow developers create their own fact-checking applications. 3. USER INTERFACE AND DEMONSTRA- TION PLAN We will demonstrate the user interface features of ClaimBuster’s website, Twitter account and Slackbot. Figure 3 is a screenshot of ClaimBuster’s homepage. It allows a user to apply ClaimBuster on their own text or view its results on the 2016 U.S. presidential debates and the Australian Hansard (cf. Section 2). The homepage also embeds tweets from the ClaimBuster Twitter account. Figure 2 is a screenshot of ClaimBuster applied on an archived presidential debate. The interface for the Australian Hansard is similar. Besides the basic information of the debate (e.g., title, date), the interface shows five panels. (1) The transcript panel displays the transcript of the debate. (2) The fact-check report panel displays supporting or debunking evidence collected by claim matcher and claim checker. (3) The social discussion panel allows users to discuss factual claims while collaboratively vetting them. (4) The video panel (omitted due to space limitations) has an em- bedded player which plays the debate video from YouTube. (5) The visualization panel shows a word cloud (omitted) and a claim spotter score chart for the sentences in the transcript. Sentences in the transcript panel are highlighted using different shades of blue proportional to their claim spotter scores. The web- site allows a user to sort the sentences by time or score and to use a slider to specify the minimum score for sentences to be highlighted. Every sentence can be annotated. An annotated sentence is under- lined in red. Users can discuss it, powered by the Genius platform , while collaboratively vetting it. When a user selects a sentence in the transcript panel, the fact- check report panel displays the supporting or debunking evidence for the selected sentence. Specifically, it shows three types of ev- idence. The leftmost column displays similar fact-checks (along with the verdicts) from the fact-check repository, if any. The middle column shows answers extracted from Wolfram Alpha and Google answer boxes. The rightmost column displays the related search re- sults from Google. The fact-check report is also directly available for any input sentence when the user clicks the button End-to-end Fact-checking” in Figure 3. The website further visualizes the content of a transcript using a word cloud and a claim spotter score chart. In the score chart, the data points represent the sentences and are color-coded by speak- ers. The x-axis of a point represents the corresponding sentence’s sequential position in the transcript, and the y-axis corresponds to the claim spotter score of the sentence. When a user adds the ClaimBuster Slackbot to their slack group, the user can ask the bot to fact-check a statement by using two commands. The /getscore command returns the claim spotter score of a sentence, e.g., Figure 4. The /factcheck command on a sentence returns its fact-check report. Figure 4: The ClaimBuster Slackbot","Hassan, N., Zhang, G., Arslan, F., Caraballo, J., Jimenez, D., Gawsane, S., ... & Tremayne, M. (2017). Claimbuster: The first-ever end-to-end fact-checking system. Proceedings of the VLDB Endowment, 10(12), 1945-1948."
"ML_DB_100","Invisible Friend or Foe? How Journalists Use and Perceive Algorithmic-Driven Tools in Their Research Process","The use of algorithmic tools by journalists for information-gathering has received particular attention in recent years. While it might facilitate the research process, there are also concerns about their impact on journalism. Based on reconstruction inter views with 27 journalists, we first answer the primary question to what extent journalists actually use algorithmic-driven tools for research purposes. Then, we analyze which folk theories journalists create during their use of algorithm-driven tools. Results show that algorithmic tools specifically designed for the journalistic research process are rarely or not at all used. Yet, more crucially, search engines and social media, that are driven by algorithms, play a major role when it comes to the search, selection, and verification of sources and information. However, journalists are not aware of this hidden research assistant facilitating their research process. When explicitly asked, they profess specific notions regarding the use of algorithmic-driven tools in the form of folk theories, which are predominantly negative regarding the influence of AI on journalism. At the same time, there is a still a strong feeling of a professional authority among journalists who feel they are able to work autonomously of any kind of influence, including algorithms.","Journalism Studies","Article","2022","Y","Y","Tool","Support","Multitasking","0","Invisible Friend or Foe? How Journalists Use and Perceive Algorithmic-driven Tools in Their Research Process The number of online methods for finding, selecting, and verifying sources available to journalists has surged during the last two decades. Through websites, search engines, and social media, journalists are able to consult a wide range of both elite and non-elite sources (Lecheler and Kruikemeier 2016; Van Leuven et al. 2018). Among online methods, the use of algorithmic tools by journalists for information-gathering has received particular attention in recent years (Von Nordheim, Boczek, and Koppers 2018). Research shows that these tools can be used for multiple purposes, such as selecting sources (Peterson-Salahuddin and Diakopoulos 2020; Thurman et al. 2016), detecting trends or finding story ideas (Diakopoulos 2020; Fletcher, Schifferes, and Thurman 2020), analyzing big data sets (Stray 2019), and for fact-checking purposes (Hansen et al. 2017). As a consequence, algorithms can be considered the journalist’s ‘research assistants’. While algorithmic-driven tools might facilitate the research process, there are also concerns about their impact on journalism in democracies. Most fundamentally, many algorithmic-driven tools are not specifically designed and developed with journalistic values and norms in mind (Diakopoulos 2019). Rather, algorithmic-driven tools struc- ture what information is easiest to find, and emphasize or de-emphasize specific sour- ces of information following certain rules pre-determined for them (Hamilton et al. 2014). This has raised concerns about algorithmic bias in, for instance, social-media tools for journalism (Thurman et al. 2016). This might mean that journalism no longer follows its own rules and norms in finding, selecting and evaluating information, but must consider its performance in relation with outside elite actors, such as tech companies. This has obvious consequences for how we conceptualize independence in journalism, and the fourth estate” role of journalists in democracies (Helberger et al. 2019). When looking at journalists themselves, this problem becomes even more pertinent. To journalists, algorithms are often black boxed, meaning that the technology and its workings are complex and untransparent that they are difficult to (fully) understand (Diakopoulos, Zhang, and Salway 2013). Making use of algorithms might not only influence or steer the research process and pose a threat to the independence of jour- nalism in democracies, journalists might not even know or understand the process of influence in the first place. This will also have negative consequences for the way jour- nalists gather, select, analyze, and verify information (Jamil 2020). For instance, when journalists go after breaking news” on social media, the information that is shown in the ‘timelines’ of journalists depends on various automated decisions resulting from various algorithms, possibly trapping them in an information feedback-loop they may not know how to escape (Gillespie 2014). So far, we have very limited knowledge about the extent to which journalists are aware of the use and influence of algorith- mic-driven tools or software and how this might guide their work (Fletcher, Schifferes, and Thurman 2020; Thurman et al. 2016). Therefore, in this study, we seek to under- stand to what extent journalists are aware of the algorithmic forces possibly influenc- ing their research activities. Based on reconstruction interviews with 27 journalists, we first answer the funda- mental question of the extent to which journalists actually use algorithmic-driven tools for research purposes. Then, we apply Peterson-Salahuddin and Diakopoulos (2020) concept of ‘algorithmic folk theories’ and test which folk theories journalists create during their use of algorithm-driven platforms. Because it is hard to understand how algorithms actually work, we expect that journalists create their own beliefs on what they exactly are and how they work. Subsequently, we try to understand how these folk theories influence the journalistic research process and which mitigating strategies journalists use to counter a possible unwarranted influence of algorithms on their research process. Together, this study is one of the first to use journalists’ own 2 Y. DE HAAN ET AL. perceptions in understanding the use and awareness of algorithmic-driven tools. We also argue that the use of algorithmic-driven tools at the start of the information-pro- duction process has normative implications in mediated communication. Using Algorithmic-Based Tools in Journalistic Research Before proceeding it is important to create more clarity on the concepts of AI and algorithms. While the concept of AI refers to technologies that outperform tasks of human intelligence, in practice, or at least in the media sector, we still talk of narrow AI. This is a bundle of technologies and techniques collection of ideas, technologies, and techniques that relate to a computer system’s capacity to perform tasks normally requiring human intelligence”. AI is driven by algorithms, a series of steps that is undertaken in order to solve a particular problem or to accomplish a defined out- come” (Diakopoulos 2015: 400). Algorithms can be automated to make autonomous decisions. This is the case when using search engines or social media or specific tools to search and analyze large amounts of data. In this article, we therefore use the term algorithmic-driven tools to indicate alle types of tools and software that make use of algorithms, including search engines. While a considerable number of studies have delved into the use of AI in the field of journalism, much of this research has focused on the production phase (2020; Dierickx 2020; Diakopoulos 2019; Jones and Jones 2019; Kunert 2020) and the distribu- tion phase (Bodo 2019; Nechushtai and Lewis 2019 ; Trielli and Diakopoulos 2019). The use of AI in the production phase focuses on how AI facilitates the editing, formatting and production of a journalistic story. This can also relate to translation software or to on robot journalism, where articles are automatically generated. Studies on the distri- bution phase of journalism relates to how AI can facilitate in the dissemination of the journalistic content to the public. While, in this digital era the research and production phase are sometimes intertwined as journalists can publish articles online while still doing research and adapting the article along the way (Diekerhof 2021), we still can distinguish the research phase in which information and sources are found, selected, analyzed, and verified. Studies about the use of algorithms in the research phase of journalism in democracies remains fragmented. Based on a current state-of-the-art, we identify four distinct ways how algorithms can assist in the research process. First, algorithmic-driven tools may play a role in the selection of sources and infor- mation (Peterson-Salahuddin and Diakopoulos 2020; Thurman et al. 2016). The use of search engines and social media for this purpose being a prevalent example. Previous work shows that the use of search engines is so ingrained in journalists’ day-to-day practices, that journalists might not consider this algorithmically guided information (Hornmoen et al. 2017). Tylor (2015) found that journalism students click on the first two search results as often as they do on all the subsequent results together, indicat- ing that the algorithms behind search engines may have an effect on the selection of sources. Thurman and colleagues (2017) argue that the academic world ‘has failed to recognize fully the changes to journalistic sourcing practices brought about by social media’ (Thurman, D€orr, and Kunert 2017: 838). Caplan and Boyd (2018) even state that algorithmic platforms, such as social media and search engines, have power over what DIGITAL JOURNALISM 3 is seen as newsworthy. This underscores the importance of finding out not only which tools journalists use, but also to what ends and how they do so. Second, moving to more specific tools, certain software can help suggest story ideas (Diakopoulos 2020; Fletcher, Schifferes, and Thurman 2020). Studies have focused on analyzing specific tools, such as SocialSensor, that helps detect news trends on social media (Thurman et al. 2016) and INJECT that suggested story angles for Norwegian newspaper journalists (Maiden et al. 2020). We do not know if specific software to sug- gest story ideas or angles is commonly used in daily practice. The third way algorithms are used in the research process revolves around the ana- lysis of datasets. Algorithms are found to be of possible value in preparation, by, for instance, collecting data from different documents (data extraction), suggesting pos- sible connections or language analysis (Gutierrez-Lopez et al. 2019). Particularly, for investigative journalists this is believed to be promising. Yet, research so far shows that algorithmic-driven tools are still not commonly used among investigative journal- ists as they face problems of data availability and do not readily provide unique sto- ries (Stray 2019). The fourth role that algorithms may play in the research phase, involves fact check- ing. Within journalism studies, most research of this application consists of case stud- ies, like those describing FactWatcher (Hassan et al. 2014) and Claimbuster (Hassan et al. 2017). Several authors mention difficulties that arise when attempting to adopt fact-checking tools, such as the lack of a database containing false information (Pathak and Srihari 2019). Often the existing fact-checking corpora are either too small in size, do not provide detailed annotations or are limited to a single domain” (Hanselowski et al. 2019, p. 1). Algorithmic-driven tools have the potential to facilitate journalists in their research purposes in multiple ways. At the same time, they also have the power to steer or influence the information flow of journalists, thereby threatening their independence. To understand this influence, we first need to know how journalists make use of algo- rithms in their daily working routine. Therefore, we pose the first research question: RQ1: How do journalists use algorithmic-driven tools in their research and selection process? Awareness of Algorithm-Driven Tools Using an algorithmic tool is one thing, but knowing that one makes use of it, is another. While many journalists use social media as a primary source for news (Lecheler and Kruikemeier 2016), an important question remains to what extent they are aware that these social media are driven by algorithms. There is much concern about the role of algorithms and filters in the use of news sources (Hamilton et al. 2014). As Diakopoulos states: Algorithms, driven by vast troves of data, are the new power brokers in society” (2014: 2). They present an outside influence to an independ- ent, self-confident and largely endogenous journalistic system honed in many democ- racies over decades (Schapals and Porlezza 2020). Therefore, it is of crucial importance to understand the extent to which journalists are aware of the use of algorithmic- driven tools as their virtual gatekeeping assistants. Do they know that their research process might be influenced by algorithms, and what do they do about it? 4 Y. DE HAAN ET AL. Studies among social media users in general show that users are often unaware that the selection of posts in news feeds or timelines is algorithmically generated (Eslami et al. 2015; Powers 2017). One might expect that journalists, who use these tools professionally and who are aware of the central role they play in public dis- course, have more knowledge about the way they work and their influence. The evi- dence here, however, is mixed. A survey of German journalists more than ten years ago, for example, found that many journalists were very critical of the content pre- sented by search engines, with little more than a quarter of them (26.8%) agreeing with the statement that the technology produced ‘neutral’ results (Machill and Beiler 2009). However, the same study found that they did not outperform the general pub- lic in search assignments, meaning that this critical attitude made little difference in practice. More recently, research has shown that journalists are quite critical when using social media as a source. However, this seems to be more due to the user gen- erated nature of the social media, than the algorithmic mechanisms behind these media (Brandtzaeg et al. 2016). Research among journalists who have more knowledge of computational journalism or algorithmic systems show for a need of more disclos- ure on algorithmic systems (Diakopoulos and Koliska 2017). Even if some journalists are unaware of the algorithmic nature of some of the tools they employ and the role AI might play in their work, this does not mean they do not have opinions about them. A poll conducted by Press Gazette, a British journalists’ trade magazine, found that 69% saw AI as a threat to independent journalism (Mayhew 2020). A global survey of ‘early adopter’ media professionals, who were inter- ested in the topic, found this group (perhaps unsurprisingly) to have a rosier outlook, but 24% still saw cultural resistance to AI in newsrooms as an obstacle to adoption (Beckett 2019). Research shows that these sentiments can have real consequences for newsroom choices. The perceived compatibility with journalistic ideals is an important factor driving the adaptation of technology in the newsroom (Milosavljevic and Vobic 2019). This means that to ascertain a full picture of the use of algorithmic-driven tools, one needs to also consider attitudes that exist towards the technology among journalists. Folk Theories on Algorithmic Driven Tools Our review of existing literature suggests a research gap concerning journalistic atti- tudes towards the use of algorithmic-driven tools. Studies of the general public (Thurman et al. 2019, Logg, Minson, and Moore 2019) used either questionnaires or experimental settings, giving test subjects a clear choice between human and algorith- mic input. In both approaches, respondents are explicitly made aware of the role algorithms and AI play. However, since journalists might not be aware of the algorithmic-driven nature of tools they employ, such an outspoken approach might be ill-suited to reveal implicit attitudes. To add to the problem, most software used in newsrooms is proprietary, meaning even expert users might be unable to deduce its inner workings. In the case of AI-systems this is further complicated by the fact that even its developers might be unsure of its actual functioning: the black-box problem (Castelvecchi 2016). DIGITAL JOURNALISM 5 This presents a dilemma. How can we study the use of untransparent technology by users who are (partially) unaware of their use? Other researchers have neatly circum- vented both these issues by looking only at the interaction between the user and the technology. Bucher (2017), for instance, explored users’ perception of Facebook’s news feed algorithm through interviews, proposing the ‘algorithmic imaginary’ as a guiding principle for interpreting the behavior of social-media users. In Bucher’s own words, the algorithmic imaginary is not to be understood as a false belief or fetish of sorts but, rather, as the way in which people imagine, perceive and experience algorithms” (p. 31). A concept that expands this idea further, looking at implications for behavior and learning, are ‘folk theories’. Gelman and Legare (2011) define these as implicit and imprecise but ... intuitive theories that have broad implications: they organize experi- ence, generate inferences, guide learning, and influence behavior and social inter- actions” (p. 379). The concept has been used successfully to study interaction between algorithmic systems and the general public. An open-answer survey carried out in Norway identified a number of folk theories of algorithms in general (Ytre-Arne and Moe 2020). Studies looking at the role folk theories played in user interaction with social-media algorithms found that they were key in driving user behavior. For instance, a study of user backlash against proposed algorithmic selection in Twitter timelines (DeVito, Gergle, and Birnholtz 2017). In depth-interviews of social media users suggest that users draw on diverse sources of information in developing folk theories and that they evolve over time (French and Hancock 2017). More recently, folk theories of social media have also been successfully identified in the newsroom (Peterson-Salahuddin and Diakopoulos 2020). Focusing on journalists’ thoughts about social-media platforms, researchers identified a common thread run- ning through all folk-theory understandings, finding their interviewees understood social media distribution algorithms as filters that did or did not allow audiences to be exposed to their content” (p. 30). Folk theories differed in the perceived effect pub- lisher attributes, engagement, and properties of specific social-media platforms had on this process. Interestingly, the editorial process involved a constant negotiation between folk- theory understanding of social-media algorithms and traditional journalistic values. (I’m not going to manufacture ... a story that isn’t really in line with [editorial practice] just because that’s what Facebook’s algorithm likes.” p. 34). This makes the folk-theory lens seem like a promising way to identify comparable strategies with regards to AI in a newsroom setting. Drawing on this concept, and recognizing the necessity to deter- mine awareness of tool use, we define our second research question as follows: RQ2: To what extent are journalists aware of their use of AI and what folk-theories exist surrounding it? Method The goal of this study is to understand if, and for which purposes, journalists make use of algorithm-driven tools in their research process and to detect a possible algo- rithmic influence in the information journalists use. We also aim to create insight into the way journalists perceive algorithmic tools, and what this means for how they use 6 Y. DE HAAN ET AL. them. In January 2020, prior to this study, we conducted ten interviews with journal- ists in the Netherlands and asked them which algorithmic tools they used. However, this approach turned out not to be successful, as many journalists could not spontan- eously mention any tools they used. When provided with a list of tools, many journal- ists were not acquainted with them. We realized that a lack of algorithmic awareness is the underlying problem here. Therefore, for this study we decided to conduct recon- struction interviews in which we discussed one or two specific journalistic stories by the interviewee. This way, we were able to distill their research steps in detail, without creating confusion about the terminology. This approach also enabled us to discuss steps that are taken largely subconsciously and helped create a complete picture of the tools the journalists use. Subsequently, semi-structured follow-up questions were asked to gain insight into the awareness of the possible influence of the tools used, and of the impact of AI and algorithms on the profession more generally. The reconstruction interviews took place with journalists in the Netherlands. As in many European countries, news organizations in the Netherlands are experimenting with AI applications, for research, production and distribution purposes (Mediaperspectives 2021a). A few newsrooms have been experimenting with auto- mated applications to generate stories, developing their own AI robots (Gieling, 2021). While the industry embraces innovative AI technologies, calls for responsible AI are growing. Several media companies, including Dutch public broadcaster NPO, commer- cial broadcasting companies RTL and Talpa and the two major publishers Mediahuis and DPG Media have signed a letter of intent to adhere to ethical guidelines for the use of AI (Mediaperspectives 2021b). In this study, we choose to look at ordinary journalists that spend much of their daily work finding, selecting and verifying information, and not technology experts or early adopters in the newsroom (Beckett 2019) or publishers or CEOs of media organi- zations (Newman 2021). We selected the interviewees based on their work-setting and frequency of publication. First, while large newsrooms might facilitate and encourage the use of algorithmic tools, freelancers or small newsrooms might be less aware or have less opportunities. Second, beat journalists might use other tools and for other purposes than investigative journalists (Stray 2019). In total, 27 journalists were inter- viewed, ranging from journalists working as freelancers, working in small newsrooms (up to 10 journalists) to large newsrooms (more than 100) and ranging from journalists who publish daily, or weekly, to journalists who work on large investigative produc- tions for a longer period of time (see Table 1). For reasons of diversity, we invited jour- nalists from a range of news organizations, including national and regional newspapers, broadcasters and online-only platforms. Prior to each interview, the interviewer selected one or two stories that the journal- ist had recently produced. By asking the journalist to draw a timeline showing every single step and discussing these, the interviewer tried to reconstruct the process in as much detail as possible. They specifically asked where information was found and selected, whether specific tools were used for selection and verification, and whether other tools were used to facilitate the research process, including those for data stor- age, translation, etc. If the interviewer detected the use of algorithm-driven tools, the DIGITAL JOURNALISM 7 journalist was asked about their knowledge of and viewpoints on such tools. The second part of the interview focused on the role of AI in journalism more generally, and the possibilities and challenges they might create for the journalistic research pro- cess. Each interview took approximately 1.5 hours and most were conducted using online teleconferencing software, due to the lockdown instituted in the wake of the COVID-19 pandemic. Two researchers conducted the interviews between January and April 2021. The first five interviews were done together in order to test the topic list. Afterwards, the interviews were conducted separately. All the interviews were transcribed by two research-assistants and analyzed by the two researchers using the qualitative soft- ware Atlas.ti. A grounded-theory approach was taken in which different rounds of coding was done (Strauss and Corbin 1990). The two researchers first coded, staying close to the material, after which in the following rounds of coding they rearranged the codes and formed thematic categories (Salda~na 2013). The final coding tree resulted from an iterative process of reflection between all the authors of this article. The coding tree consisted of five general codes including activities, explicit use of AI, job, tools, and knowledge of AI. These were then specified, resulting in 56 sub-codes, including different journalistic activities such as editing, image manipulation, polling, factchecking or generating story ideas. Knowledge of AI was sub-coded with the codes AI policy, mitigating strategies, sentiment towards AI and tool suggestion. In the last round, different reflection sessions were done to come to overarch- ing categories. Results To understand the possible influence of algorithms in the journalistic research process, we first need to know to what extent they are actually used. Our literature review shows that algorithmic tools can be used for multiple purposes, including selection of Table 1. 27 respondents to represent a cross-section of journalism in the Netherlands. Publication frequency High Medium Low Large newsrooms Online journalist Wire reporter Multimedia datajournalist Entertainment newspaper reporter Newspaper reporter Documentalist Newspaper reporter Investigative radio reporter Newspaper reporter Teamsize Medium newsrooms Online journalist Online journalist Online journalist TV- news producer Print magazine reporter TV- news producer Radio news producer Survey specialist Investigative TV journalist Investigative TV journalist Small newsrooms Radio news producer Investigative online reporter News magazine online editor Investigative online journalist Individuals (freelancers) Investigative online journalist Feature print reporter Foreign correspondent Feature print reporter Respondents were drawn from newsrooms of various sizes and selected according to publication frequency. This sample method ensured the full spectrum of Dutch journalism was represented on both measures. 8 Y. DE HAAN ET AL. sources and information, suggesting story ideas, analyzing datasets, fact checking, and generating stories. However, our study shows that the journalists we interviewed hardly make use of specific algorithmic tools. Nevertheless, every research process is algorithmic-driven: it heavily leans on search engines and social media tools, in par- ticular for the selection and verification of information. Search Engines and Social Media Tools Dominate in the Research Process Search engines, particular Google, are used in every step of the research process by every journalist we spoke to, except for one, who resorts to Duck Duck Go for privacy reasons. They are used for classic research purposes such as finding, selecting, and verifying sources and information. As a journalist of a current affairs magazine said: When I look for interviewees, I want people who are recognizable to the reader. So, basically, I ask around and Google a little. And one finding takes you to the next. Because you Google someone and you find a piece with another name, and then you Google that name again. And you continue like that. While newsbeat journalists are more likely to start with Google and switch to offline methods to contact sources for verification purposes, investigative journalists also fre- quently make use of niche search engines such as PubMed, Global Forest Watch and LexisNexis. They also are better acquainted with specific Google applications such as using Google Earth to compare images of a location over time. Moreover, investigative journalists make use of documentarians at the newsroom, specialized in thorough research of (historical) documents. Both investigative and newsbeat journalists frequently use social media, for instance to analyze how audiences discuss certain topics, to search for sources and get in contact with them, to discover what is happening at a certain place in real time and to find out what topics are discussed by other media and how. In addition, investigative journalists may also use social media to verify accounts, to check if a source’s statement is credible or to discover links between different social media accounts. As such, their searches on social media seem more thorough. An investigative journalist explained: Some information is online but might not typically be found through Google quickly. Then you really need to go to social media, through someone’s accounts and their activity online. And not necessarily only through official channels, but also just private ones. While verifying is done in the process of selecting information, none of our inter- viewees made use of automated fact-checking tools. Instead, fact checking among investigative journalists leans partially on cross-checking different search engines (e.g. niche search engines, Google Reverse Image Search, Google Earth), social media, self- collected data and official documents, but also strongly on (offline) contact with sour- ces and official institutions. Verification for newsbeat journalists often means checking what other media and official institutions report (often using search engines), as well as fair hearing of sources. The use of search engines and social media can hardly be ignored in the journal- istic research process. These algorithmic-driven tools are fully engrained in journal- ist’s research process. Besides for selection and verification purposes, specific tools such as feed readers (Inoreader, Feedly, Tweetdeck and CrowdTangle) are used to discover topics that are much debated on or often shared on social media, and, thereby, provide insight into the subjects and events that might be newsworthy. Such tools are used to help journalist find story ideas, particularly for beat journal- ists. Though investigative journalists could potentially use automated tools to help analyze datasets, the evidence for the use of such tools by our interviewees is merely anecdotal. In sum, while the debate on AI in journalism has increased exponentially the past few years, algorithmic tools specifically designed for the journalistic research process are rarely or not at all used by our interviewees. Yet, more crucially, search engines and social media, that are driven by algorithms, play a major role when it comes to the search, selection, and verification of sources and information. However, it remains unclear how journalists perceive these tools and to what extent they are aware of the algorithms driving them. Awareness and Knowledge Dominated by Folk-Theories While hardly any specific algorithmic tools are used by journalists, as we mentioned above, search engines and social media tools are embedded in the daily working rou- tine of journalists. In our interviews, we investigated whether journalists are aware of this, asking to what extent interviewees thought algorithms and AI had an effect on their work. In response, only a minority of journalists spontaneously named the algo- rithmic-driven tools they actually used. Those that did were mainly investigative jour- nalists or journalists that possessed a relatively sophisticated understanding of technology. For an investigative TV-reporter, working for an investigative news-pro- gram, the effect Google had on his work loomed large. Sometimes it can determine a lot. That’s what I had in this case. Some guy had written a critical piece and he came out on top. So I call him. Great source, great conversation. He’ll make it to the show ... .most likely the content is doing its work here ... Google thinks content is more important. That’s what earned him the top spot. Probably a lot of other people find this interesting, so this critical piece makes it to the top, since Google thinks that is important. The remaining responses mainly fell into two categories. The first did not know much about algorithms and AI. Particularly, they were not aware of how these tech- nologies are related to their own work at the newsroom. As one journalist said: I don’t know much about it. I’m really only involved with the journalistic process”. And similarly, another journalist said: I really don’t see the link with my work”. The second category of journalists did assume that algorithms and AI might have consequences for journalistic work, but just not their own. To them, the newsroom is involved with the core journalistic processes which does not involve the use of AI. Other depart- ments such as the data, sales or R&D development might make use of it. Others stated that at management level, there is much talk about AI, but not in the newsroom. A different picture emerged when the interviewees were prompted more generally about the effect the emerging presence of AI was having on journalism in general. As the journalists we interviewed were not aware or did not feel specifically involved with 10 Y. DE HAAN ET AL. algorithms in their daily work, we subsequently moved beyond the specific research process to understand how they perceive AI in general, trying to understand if they then might feel it can possibly influence their work. Here, the folk-theory concept proved a highly useful concept for categorizing notions held by journalists. As defined by Eslami et al. (2016), folk theories are a method for users to navigate the use of tech- nology that they cannot truly understand. Whether they are accurate or not is beside the point. They are a way in which users make sense of their own behavior. What interviewees’ conceptions of algorithms and in more general terms of AI had in common was that most defined it as a relatively autonomous force. The inter- viewees’ observations quickly fell apart into two main non-exclusive categories: seeing AI as dangerous or as a (potential) help. The former was far m","de Haan, Y., van den Berg, E., Goutier, N., Kruikemeier, S., & Lecheler, S. (2022). Invisible Friend or Foe? How Journalists Use and Perceive Algorithmic-Driven Tools in Their Research Process. Digital Journalism, 1-19."
"ML_DB_118","Challenges and opportunities for journalistic knowledge platforms","Journalism is under pressure from loss of advertisement and revenues, while experiencing an increase in digital consumption and user demands for quality journalism and trusted sources. Journalistic Knowledge Platforms (JKPs) are an emerging generation of platforms which combine state-of-the-art artificial intelligence (AI) techniques such as knowledge graphs, linked open data (LOD), and natural-language processing (NLP) for transforming newsrooms and leveraging information technologies to increase the quality and lower the cost of news production. In order to drive research and design better JKPs that allow journalists to get most benefits out of them, we need to understand what challenges and opportunities JKPs are facing. This paper presents an overview of the main challenges and opportunities involved in JKPs which have been manually extracted from literature with the support of natural language processing and understanding techniques. These challenges and opportunities are organised in: stakeholders, information, functionalities, components, techniques and other aspect","Computer Science","Article","2020","Y","Y","Tool","Support","Survey","5","Journalism is under pressure from loss of advertise- ment and revenues, in combination with competing online distribution channels that stream free content, while experiencing an increase in digital consump- tion and readers who demand quality journalism and trusted sources [1]. Information is no longer consumed from a single newspaper. Instead, readers have access to and can contrast fresh and first-hand information sources available on the internet and social media at any time. News organisations are constantly adapting their business models to digital media innovations, to improve information quality, competitiveness and growth [2]. Journalistic Knowledge Platforms (JKPs) are an emerging type of platform that combines state-of-the-art artificial intelligence (AI) techniques such as knowledge graphs and natural-language pro- cessing (NLP); and exploit news and social media information over the net in real-time, using linked open data (LOD), encyclopaedic sources and news archives to construct knowledge graphs and provide fresh and unexpected information to journalists, help- ing them to dive deeply into information, events and story-lines. JKPs are increasingly driving innovation Proceedings of the CIKM 2020 Workshops, October 19-20, Galway, Ireland. email: Marc.Gallofre@uib.no (M. Gallofré Ocaña); Andreas.Opdahl@uib.no (A.L. Opdahl) orcid: 0000-0001-7637-3303 (M. Gallofré Ocaña); 0000-0002-3141-1385 (A.L. Opdahl) © 2020 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).CEUR Workshop Proceedings ISSN 1613-0073 CEUR Workshop Proceedings (CEUR-WS.org) and transforming newsrooms, leveraging information technologies to increase the quality and lower the cost of news production. In order to drive research and design JKPs that allow journalists to get most bene- fits out of them and support newsrooms with better solutions, we need to understand the challenges and opportunities that JKPs present for both users and developers. To do so, we have reviewed the research literature in light of our own experience with devel- oping News Hunter [3, 4, 5], a series of JKP prototypes in collaboration with a developer of newsroom tools for the international market. This paper presents a synthesis of the challenges and opportunities for journalistic knowledge plat- forms that we have found in the literature, hopefully describing the most central factors that are driv- ing development of JKPs today. These factors have been grouped into six categories: stakeholders, in- formation, functionalities, components, techniques and other aspects. We conclude that JKPs offer many opportunities for effective production of high- quality journalism, real-time information, enriched background information, and multilingual and cross- platform solutions for monitoring worldwide mul- timedia output, by offering solutions to problems such as language independence, complex newsrooms workflows, and disperse information. Central chal- lenges include leveraging pre-news information from social media and multimedia sources, precise seman- tic lifting and enrichment of texts, scaling semantic technologies to big data, and detecting and reasoning over events. This paper is organised as follows: Section 2 sum- marises the methodology used for screening the challenges and opportunities. Section 3 briefly re- views the research literature. Section 4 explains the coding process. Sections 5 to 10 synthesise the main challenges and opportunities for each factor respec- tively — stakeholders, information, functionalities, components, techniques and other aspects. 2. Method Our research method consists of four steps: Firstly, we selected the most relevant research papers that we have identified in our previous studies on JKPs archi- tectures and news angles [4, 6, 7, 8, 9, 10, 11, 12]. From these selected papers we manually extracted claims, i.e., sentences that express potential challenges or op- portunities. Secondly, a purposive sampling was conducted in- dependently by two expert coders (the authors). The coders generated multiple codes for each extracted claim and the codes were cleaned with the support of NLP and NLU techniques (i.e., Damerau-Levenshtein distance [13], word2vec [14], and Wordnet [15])1. From the resulting cleaned codes, we selected the most representative ones as preliminary codes and divided them into categories. Thirdly, based on the preliminary codes, claims were independently coded once again by both au- thors. This time, the coders were allowed to code each claim with multiple codes for each category. The cod- ing agreement was estimated using Gwet’s AC1 [19] inter-rater reliability coefficient with nominal ratings. Because coders were allowed to not to code, to com- pute the Gwet’s AC1, empty codes were not treated as missing values, instead, they were treated as if they where coded as undefined”. Hence, to compute the contingency tables for multiple codes we applied the following rule: the agreement between coders A and B only happens between correctly matching codes (A∩B) and the other codes (A△B) were matched with missing values and treated as disagreements. Finally, when both coders agreed on the final codes for each claim, challenges and opportunities were ex- tracted from each claim following the assigned codes. 1Implemented in python with support of Scikit-learn [16], NLTK [17], SpaCy [18] and other libraries. 3. Reviewed papers After a broad survey of the literature, we selected eleven papers describing describing five research projects related to JKPs as the starting point of our review: NEWS [20, 21], EventRegistry [22], News- Reader [23, 24, 25], SUMMA [26, 27, 28, 29] and ASRAEL [30]. NEWS is a project, in collaboration with the Spanish Agencia EFE and the Italian ANSA news agencies, that makes use of semantic technologies to improve news agencies’ workflows, productiveness and revenues by focusing on the annotation, intelligent information re- trieval and user interface aspects [21]. EventRegistry is focused on collecting news articles, identifying and extracting information about events, and summaris- ing and visualising them [22]. NewsReader extracts information about what, who, where, when from mul- tilingual news articles and represents events in time using RDF in a knowledge graph, allowing users to find networks of actors along time [25]. SUMMA col- laborates with BBC Monitoring and Deutsche Welle to develop a multilingual and multimedia platform us- ing state-of-the-art NLP techniques to monitor inter- nal and external media work and provide data jour- nalism services [27]. ASRAEL aggregates news arti- cles and leverages the Wikidata knowledge base to de- scribe and cluster news events and provides informa- tion retrieval tools to interact with the resulting news representations [30]. 4. Coding process In the purposive sampling step, we extracted 322 claims from the related literature and marked them up using 406 codes. After cleaning and tidying up the initial codes, we identified six top-level categories which we divided into 62 sub-categories to be used for preliminary coding. The following six top-level categories were used: • Stakeholder: the agent that the challenge or op- portunity is for. The agent can be either a tech- nical agent or social agent. • Information: the information needed to meet the challenge or exploit the opportunity. • Functionality: the service or functionality that the platform should offer to meet a challenge or exploit an opportunity. • Component: the part of a platform that must be created or improved to meet the challenge or ex- ploit the opportunity. • Technique: the IT solution used to meet the challenge or exploit the opportunity. • Other aspects: another type of concern that the challenge or opportunity involves, such as customer heterogeneity, performance or maintenance. We computed the inter-rater agreement for the preliminary coding with the AC1 coefficient for each category: 0.77 for Stakeholders, 0.65 for Components, 0.71 for Techniques, 0.71 for Aspects, 0.72 for In- formation types and 0.57 for Functionalities. The average AC1 is 0.69 with a standard deviation of 0.063, which according to Landis-Koch and Altman’s benchmark scales, express an acceptable agreement among coders [19]. Finally, the assigned codes were discussed between and agreed on by the two coders. 5. Stakeholders Stakeholders are agents that represent the forces and interests that drive the future of JKPs. The identified sub-categories of stakeholders are: general user, news professional, fact checker, archivist, ICT professional, audience, customer, researcher, news agency, public organisation and technical agent. General users interact with services provided by the JKPs or newsrooms. These can be divided between the internal users that belong to newsrooms and the ex- ternals ones. The internal users are news professionals like journalists who use JKPs for creating histo- ries [20]; fact checkers who conduct an essential task in combating with fake news and misinformation [28]; archivists who maintain up-to-date the ontology and news archives [20]; and ICT professionals and knowl- edge engineers who represent those users involved in the development and maintenance of JKPs [21]. Whereas, the external users are the audience [22]; the customers to whom new agencies offer services; and researchers who investigate JKPs or analyse data, as in the SUMMA project where [political scientists want] to perform data analyses based on large amounts of news reports” [27, p. 2]. The organisations influencing the JKPs are: the news agencies, including newsrooms; the public organ- isations which are those governmental agencies that interact with or consume services from newsrooms’ JKPs, as in the SUMMA project which provides media monitoring and analysis services to the BBC own newsrooms, the British government, and other subscribers” [27, p. 1]; and the organisations that are responsible for controlling news media standards, vocabulary and ontologies (e.g., IPTC organisation2), which are indirectly influencing JKPs because the work of many news agencies and JKPs depends on those standards, as in the NEWS project where most of the NewsCodes defined by IPTC do not have al- ternative versions in different languages, only in English” [20, p. 9]. Finally, the technical agent, which is a stakeholder that represents the JKPs and any system or techni- cal infrastructure in newsrooms that support or inter- act with JKPs. A particular subtype of technical agent are the external systems that communicate with news- room systems, like the information systems of poten- tial customers [20]. 6. Information JKPs cover the whole information pipeline from gath- ering information and news creation to knowledge ex- ploitation and distribution. Our study identified the following sub-categories of information to be consid- ered in JKPs: news content, textual data, multimedia data, data format, metadata, LOD, events and infor- mation needs. News agencies produce both textual and multi- media news content which have to be managed and distributed to their customers and audience [21, 20]. As textual data we consider the raw text in form of news articles, documents, markup files, PDF, web pages, biographies, history and geopolitical data of countries, reports, social media feeds and social blogs. Whereas, as multimedia we consider live broad- cast, spoken content, photographs, audio and video. Besides, news agencies produce contents in different formats like plain text, Information Interchange Model (IIM), News Industry Text Format (NITF), NewsML and RDF [20]. Metadata is used to annotate and manage the pro- duced content. Metadata can describe e.g., author, language, creation timestamp, location, keywords, category, provenance, priority, urgency, status, up- dates, rights, interest, description or media type. JKPs use Linked Open Data (LOD) to annotate and enrich content using semantic vocabularies and leveraging knowledge bases, as in the ASRAEL project where they leverage the Wikidata knowledge base to pro- duce semantic annotations of news articles” [30, p. 1]. News agencies create stories describing events and deliver them to their customers and audience [21], making the events the central information need. De- spite that, social stakeholders have other information needs: General users are interested in knowing who, what, with whom, where and when events took place, networks of timeline actors implications, find the events of a certain type or in a certain place, obtain facts and retrieve evidence [24]. News professionals need access to news agencies’ archives and knowledge bases for documentation purposes, find connections from past events, follow histories and identify emerg- ing topics [20, 23, 27]. While customers have different information needs mainly depending on their busi- ness or interests, e.g., the press cabinet of a company is usually interested in news items talking about the company or its rivals, whereas a sports TV channel is interested mostly in news items describing sports events” [20]. 7. Functionalities JKPs provide different functionalities to their users. We identified twelve main sub-categories of function- ality: news creation, verification, source selection, monitoring, knowledge discovery, trends, alert, sum- marisation, clustering, personalisation, business support and content management. News professionals use the JKPs for the news cre- ation process. JKPs guide journalists in writing up their stories, support them with contextual back- ground knowledge for those stories [21], provide means for comparing current events with other simi- lar events [30], and facilitate access to previous work for creating similar content for a different audience, region or language [27]. JKPs also support news professions with verification tasks like fact checking, provenance [24], rights and authorship manage- ment [20, 21], which are typically time-consuming tasks for news professions as explained in manual verification of claims is a tedious task, that consumes a lot of time and effort from journalists and professional fact-checkers” [28, p. 1]. Source selection and monitoring functionalities are two common functionalities across the studied JKPs, which harvest and store content from internal and external sources and monitor them in real-time. By doing this, JKPs relieve journalists from these time- consuming tasks, as it was happening in the BBC where each of its ca. 300 journalist monitors up to four live broadcasts in parallel, plus several other information sources such as social media feeds” [27, p. 1]. Knowledge discovery is one of the most attrac- tive functionalities of JKPs. Knowledge discovery allows users to obtain news insights, analysis and relevant information, like in NewsReader where it increases the user understanding of the domain, facilitates the reconstruction of news story lines, and enables users to perform exploratory investigation of news hidden facts” [24, p. 1]. Other interesting functionalities among JKPs are: trends used to dis- cover emerging topics, long-term developments and changes in events over time [22, 25]; alerts to keep users up-to-date with the last incoming items [26]; summarisation of news histories and events to provide additional insights [22]; clustering of story-lines and events [27]; and personalisation of both the JKPs and its functionalities according to users’ preferences and profiles [21]. JKPs provide functionalities to news agencies and newsrooms organisation and workflows. JKPs are used as business support systems to manage internal newsrooms output; monitor what is being broadcast, produced and covered [27]; overcome limitations in newsrooms’ workflows; and improve productivity and revenues [20]. Another functionality provided by the JKPs is the content management which allows news agencies to produce, store, organise, manage, maintain and distribute the content and metadata produced every day [20]. 8. Components JKPs rely on different components to fulfil its function- alities and support users. We split JKP components into five sub-categories: input, processing, storage, in- teraction and output. As input, we consider the different sources of con- tent and information used in JKPs that are relevant for stakeholders. The textual and multimedia sources are sources of interest. However, not all analysed projects treat the information in the same way or use the same information types, like ASRAEL which only uses the title and first paragraph to represent the events [30]; and not all contents receive the same interest by news professionals, as in SUMMA which considers enter- tainment programming such as movies and sitcoms, commercial breaks, and repetitions of content (e.g., on 24/7 news channels) [...] of limited interest to moni- toring operations” [27, p. 1]. The processing components cover tasks from har- vesting and annotating input sources to processing and lifting them, following an ETL process (i.e., Ex- tract, Transform, Load). Input sources are harvested using different components, each with a specific pur- pose: harvesting, translating, filtering and transcribing. A common characteristic of the analysed projects is that source selection and monitoring functionalities are conducted in real-time by harvesting informa- tion sources [22, 23, 27]. The harvested content is then translated [27] and filtered according with the different stakeholders’ interests and needs. Spoken content is transcribed [27] and images are textually described [21]. JKPs use specific components to automatically an- notate the harvested content with metadata to support functionalities like business support, content manage- ment and personalisation [20]. The annotated content is typically processed by different components which are organised in an NLP pipeline. The NLP pipeline processes the content through state-of-the-art NLP and NLU modules to perform linguistic tasks [25, 24]. These tasks are focused on capturing and extracting the different information types described in section 6. Both the results of the NLP pipeline and the annotated content are disambiguated and represented semanti- cally using lifting components. The lifting component links the semantic representation of news items to a knowledge base, for examples an RDF-based knowledge graph [25], and enriches the semantic interpretations with facts from external knowledge bases, for example from the LOD cloud [24, 30]. The JKP storage infrastructure is normally composed of an archive, a knowledge base and an ontology. The archive stores news articles, biographies, reports [25] and other textual and multimedia items; the knowl- edge base is where the lifted semantic representations of news items are stored and enriched with external information [24]; and the ontology is used to represent the structure of the news items, leveraged information. metadata and vocabulary [20]. JKP users interact with the previous components mainly using three types of interaction components: front-ends, tools and query engines. JKPs provide front-end components [21] to allow stakeholders to access the system functionalities; tools which offer features to journalists when creating news articles or to general users when interacting with the system, like money converters or dictionaries [20]; and query engines that allow users to query, analyse or visualise the database through APIs [27]. News agencies use two types of distribution com- ponents for delivering content to their audience and customers [20]: push and pull. Push components of- fer interfaces where information consumers can select and subscribe to streams of news [20], whereas with pull components, news agencies offer interfaces to ac- cess, browse and query their repositories [20]. 9. Techniques Techniques used in JKPs can be grouped in eight sub-categories: semantic technology, fact extraction, conceptual model, reasoning, network analysis, event analysis, NLP and training. Semantic technology is used to support functional- ities like knowledge discovery, news creation, verifi- cation, clustering, trends, and content management. Semantic technologies support knowledge discov- ery by providing means for lifting news items, and disambiguating, enriching and leveraging them with information from external knowledge bases [21, 25] – processes carried by the lifting, ontology and knowl- edge base components; news creation, by providing systems and vocabulary to automatically annotate news in annotation components [21]; and verifica- tion, by combining semantic technologies with the lifting and knowledge base components and linking factual claims to its sources and external knowledge bases [24, 27]. Semantic technologies and semantic representation techniques facilitate clustering news items and events [30], and detecting trends and story lines [24]. Moreover, semantic technologies provide shared semantic resources and formats which are used to support content management and facilitate conceptual interoperability [25]. Fact extraction techniques extract facts from news items and link them to facts in external knowledge bases (e.g., Wikidata, Wikipedia). These techniques are used to provide functionalities like verification and knowledge discovery [27] and are common features of lifting, knowledge base and query components. Conceptual models provide vocabularies and ontolo- gies which are used in conjunction with semantic tech- nologies to support and standardise functionalities like content management and personalisation. Ontologies can be used for defining user interests and preferences based on the provided vocabulary or as shared mod- els [20]. Conceptual models are applied in distribu- tion, lifting, annotation, ontology, query, knowledge base and source components. Both conceptual models and semantic technologies facilitate the usage of other techniques like reasoning, network analysis and event analysis. These techniques support functionalities like knowledge discovery, clus- tering and trends, and are applied in the lifting, knowl- edge base, ontology and annotation components. Rea- soning techniques abstract and infer new knowledge from news items, events and temporal aspects [24, 25]. Network analysis is used to find networks of actors and organisation implications through different events and time [24]. Event analysis is applied to detect, identify and annotate the events described in news [21, 20]. The above techniques are supported by NLP tasks like named entity detection, role detection, topic de- tection, temporal expression normalisation, temporal relation detection, factual claims extraction, natural language understanding [25, 29, 27]. These NLP tasks, among others, are also used in JKPs’ functionalities such as knowledge discovery, content management, summarising, verification, trends, clustering, query, lifting and annotation. In order to obtain optimal re- sults from the NLP tasks, different training techniques have to be used over extensive news corpus [30]. 10. Other aspects Stakeholders, information, functionalities, compo- nents and techniques are influenced or affected by additional concerns of various types. We organised these other aspects into the following sub-categories: standards, proprietary, human factors, customers het- erogeneity, big data, multilingual, timeliness, quality, software architecture, performance, maintenance, and legacy. Before moving into JKPs, news agencies used their terms, categories and vocabularies to describe their items. Yet, the interoperability between news agencies and customers was difficult. The usage of standards like like IPTC news codes and media topics, semantic vocabularies, NAF and RDF improved the interoperability between news agencies and other stakeholders [20]. JKPs keep track of proprietary news information like authorship, copyrights and sources [21, 20] as a part of the content management functionalities. Property information is used as metadata in annotation compo- nents and provides provenance and reliability infor- mation [24, p. 4]. There are different human factors influencing JKPs and stakeholders. Before JKPs, news professionals were performing many processes by hand like news tagging, verification tasks, fact searching, finding related articles, and source monitoring. Performing these tasks manually is time-consuming, error-prone, consumes a lot of efforts, and reduces the amount and precision of the added metadata [21, 20, 28, 22]. Therefore, customers have to manually filter irrele- vant content received from news agencies, creating an information overload problem which is contrary to the information relevance that customers expect from news agencies [21, 20]. Moreover, because the difficulty of manually monitoring and finding related articles from other news providers, the audience, customers and news professions can get biased or incomplete information [22]. Customers are heterogeneous, they have different in- formation needs and use different systems to interact with news agencies [20]. According to our study, JKPs deal with big data requirements like volume, velocity, variety: The AS- RAEL project estimated that the number of collected articles ranges between 100.000 and 200.000 articles per day” and collected news articles from around 75.000 news sources” [22, p. 1]. NewsReader used an archive that contains billions of articles, biographies, and reports” [25, p. 1]. The SUMMA platform [was] able to ingest 400 TV streams simultaneously” [27, p. 6]. Other information aspects that JKPs deal with are the multilingual and timeliness data aspects. Infor- mation and news production are created in multiple languages (e.g., Catalan, Norwegian, Spanish, En- glish, Italian, French, Portuguese and Chinese) and need to be translated, transcribed and delivered to customers and audiences in their languages of prefer- ence [20, 27, 25, 30]. The timeliness aspect refers to the temporal aspect of events, thus news professionals, audience and customers want to receive the informa- tion as soon as it is generated [21] and reconstruct story-lines or histories over time [24, 27]. Quality of the results and outputs of JKPs are summarised in news agencies are required to pro- vide fresh, relevant, high-quality information to their customers” [21, p. 1] and ignoring these quality requirements can imply economic losses for cus- tomers [20]. Aspects concerning technical agents and their components include the software architecture, perfor- mance, maintenance and relation of JKPs with other systems. The software architecture of JKPs should consider scalability to deal with big data require- ments [21, 24, 27], distribution to run its components and systems over multiple machines [20, 26], com- ponents independence so they can be used for other purposes [26], interoperability between components and systems [20, 25], and performance for reducing the processing and distributing time of information and live feeds [21, 24]. Manual maintenance is a time-consuming and error-prone task [20] which is automated with JKPs to keep the JKP and on- tology up-to-date [26]. As JKPs communicate with customers systems, legacy components and other newsroom systems, JKPs need to be designed to fa- cilitate the integration with other technologies and systems [20, 26]. 11. Conclusion JKPs are a new type of platforms which offer many opportunities for newsrooms and journalists by com- bining AI techniques such as knowledge graphs, LOD and NLP to improve and facilitate the production of high-quality journalism. We collected challenges and opportunities that JKPs present and organised them into six categories that we assume are important for the evolution of JKPs (stakeholders, information, functionalities, components, techniques and other aspects). JKPs offer new opportunities for consuming and interacting with news by providing enriched content from external sources like Wikipedia or Wikidata to stakeholders seeking relevant information, such as news professionals and general audiences. News texts are enriched with additional information about, e.g., involved actors, places and organisations, the connections with other news and related events. In- formation and data sources in JKPs are no longer split along dispersed and disconnected repositories as it happens in traditional solutions. Instead, the infor- mation pieces are connected by the knowledge graph. JKPs enhance functionalities like news creation and content management. News creation is improved with background information providing journalists with better information for their stories. Automatic meta- data annotation and the usage of standards like IPTC relieve archivists from manually annotating news and improve the content management capabilities of JKPs and newsroom workflows. Knowledge graphs in JKPs bring new forms of representing news-related content and exploiting it. Techniques like network analysis, event analysis and reasoning improve the background information and knowledge discovery in JKPs while opening new research questions for researchers. JKPs can use standards such as RDF, IPTC’s media topics and semantic vocabularies which simplify the interop- erability and understanding between news agencies and stakeholders. The most highlighted opportunities that have been identified in the literature include event detection and analysis over time, real-time and up-to-date trustworthy information, access to en- riched background information for supporting news creation, multilingual and multimedia cross-platform solutions, and tools for monitoring worldwide media output and internal newsrooms production. On the other hand, providing one-size-fits-all JKP solutions for all possible stakeholders is challenging, because of their diversity and differing information needs. Newsworthy information comes from diverse news sources like pre-news information from so- cial media or multimedia sources such as TV news programs. Leveraging these information sources is a complex task which requires new techniques to distinguish potentially newsworthy information from non-relevant content and extract information from multimedia items like images or videos. Summaris- ing and presenting news-related information in JKPs like background information, events in time or actor networks to users with different information needs and skills is not a trivial task. JKPs consist of different components which interact together and with exter- nal components that need to be integrated in JKPs systems. Extracting precise semantic representations of and reasoning over relations and time remain open research questions. JKPs deal with big data, but some semantic technologies, reasoning and AI techniques are not yet ready for it. Among the reviewed JKPs, the most common challenges are problems such as language independence, multiple news channels, complex newsrooms workflows, dispersed and diverse information, lack of facts, and integration with legacy and customer systems. After reviewing the literature, we have realised that there is not a clear definition and agreement about what constitutes an event. The event concept is used in different ways in the literature, from a handshake between two actors to bigger events like the Spanish Civil War or events in between such as a trial process. In this study, we have only reviewed five JKP-related research projects, although they are the five most cen- tral ones we have found. Hence, we may have omitted important issues that were not represented or brought up in these projects. We are therefore planning to ex- tend the number of considered projects through a sys- tematic literature review and contrast and expand our findings with published works on data and digital jour- nalism. A logical continuation of this expanded study is the formal identification and modelling of goals, re- quirements and use cases for JKPs, which we did not find yet in the literature. Furthermore, we plan to for- malise a reference framework for JKPs and continue the development of our JKP identified to validate and integrate our findings.","Gallofré Ocaña, M., & Opdahl, A. L. (2020). Challenges and opportunities for journalistic knowledge platforms."
"ML_DB_126","A value-driven approach to addressing misinformation in social media","Misinformation in social media is an actual and contested policy problem given its outreach and the variety of stakeholders involved. In particular, increased social media use makes the spread of misinformation almost universal. Here we demonstrate a framework for evaluating tools for detecting misinformation using a preference elicitation approach, as well as an integrated decision analytic process for evaluating desirable features of systems for com batting misinformation. The framework was tested in three countries (Austria, Greece, and Sweden) with three groups of stakeholders (policymakers, journalists, and citizens). Multi criteria decision analysis was the methodological basis for the research. The results showed that participants prioritised information regarding the actors behind the distribution of mis information and tracing the life cycle of misinformative posts. Another important criterion was whether someone intended to delude others, which shows a preference for trust, accountability, and quality in, for instance, journalism. Also, how misinformation travels is important. However, all criteria that involved active contributions to dealing with mis information were ranked low in importance, which shows that participants may not have felt personally involved enough in the subject or situation. The results also show differences in preferences for tools that are influenced by cultural background and that might be considered in the further development of tools.","Social Science","Article","2021","Y","Y","Tool","Framework","Evaluation","3","Misinformation in social media is currently attracting a lot of attention. Misinformation is not a new phe- nomenon and has probably existed since the dawn of humanity. Structural evidence of scientific research on mis- information can be found Allport and Postman’s (1946) basic law of rumour, which demonstrates that the strength of a rumour is dependent on the importance of the subject and individual con- cerns regarding it, as well as the time and ambiguity of the evi- dence on the topic. New technical capabilities, such as social media, have naturally made these effects more universal. The 2000s witnessed rapid developments in social media and its increased outreach to everybody with Internet access. This has facilitated the spread of information, including misinformation and rumours, in virtually everything from local neighbourhoods to global concerns (Del Vicario et al., 2016). Until recently, there has been limited scientific evidence on how to deal with misinformation, but research on the topic has increased rapidly over the past few years. For instance, researchers have suggested various ways of dealing with citizen awareness, such as nudging, as a way of vaccinating social media users against misinformation (Piccolo et al., 2019). Other topics studied include nudging for accuracy in sharing on social media (Pennycook et al., 2020) and the limits of human cognition in dealing with and spreading misinforma- tion. Finally, researchers have examined a variety of approa- ches for making fact-checking more efficient, such as automatic detection of misinformation and correction of data, while at the same time pointing out the importance of human fact-checkers, as fully automated fact-checking methods are not yet strong enough. 1 This systemic problem requires stakeholder involvement at different levels, as misinformation is so widespread and con- stantly changing. Extensive stakeholder involvement is necessary for designing policies, methods, and tools. However, existing approaches to developing online tools tend to follow the tradi- tional path of dissemination of knowledge from science to sta- keholders while viewing technology users as passive consumers of finished products rather than active co-creators. This is particu- larly alarming today when available anti-misinformation products and tools are still new to the mass market and hence malleable, which is rare in the life cycle of a product (Smith and Medin, 1981; Svahn and Lange, 2009). Value-based software engineering (Boehm, 2003) is an emerging approach that aims to develop software tools (e.g., the tool by Aurum and Wohlin, 2007) based on the values and objectives of various stakeholder groups (Biffl et al., 2006), providing an economic categorisation of the value concept based on the monetary exchange between a customer and a provider. In this study, we investigate two major research questions: ● What are preferences for, perceptions of, and views of the features of tools for dealing with misinformation? ● How do these preferences depend on the cultural back- grounds of stakeholder groups and participants? Our goal is to study the preferences of various stakeholder groups for features of tools, to study the impact of cultural background on these preferences, and to develop recommenda- tions for considering these preferences in the further development of tools for dealing with misinformation. The next section provides a background of misinformation and discusses why we need automatic tools in a general setting. Sec- tion ‘Methodology' describes the integrated methodology used, and Section ‘Results' presents the results and a discussion. Finally, Section ‘Conclusions' concludes the article. Background A variety of definitions exist for misinformation, disinformation, fake news, rumours, and similar terms, and a large number of them emphasise the distinctions between misinformation and disinformation, as well as between disinformation and fake news. A review of the 2016 Presidential election in the United States, for instance, identified six different types of misinformation: authentic material used in the wrong context, imposter news sites designed to look like known brands, fake news sites, fake infor- mation, manipulated content, and parody content (Wardle, 2016). Wardle and Derakhshan (2017) suggested that mis- information refers to misleading information created without the intent to harm, whereas disinformation refers to information deliberately fabricated with the intent to impact social groups or societies. Burgoon et al. (2003) discussed misinformation in terms of deceptive language and false context. Farrel et al. (2018) dis- tinguished between disinformation and misinformation, con- sidering both subsets of misinformation: Disinformation largely involves the intent to deceive, whereas misinformation does not need to involve intentional deception. Giglietto et al. (2016) proposed a taxonomy based on perceptions of the source, the story, and the context and decisions of the audience and the propagator. In their taxonomy, there is pure disinformation” when both the original author and the propagator are aware of the false nature of information but nevertheless decide to share it. There is misinformation propagated through disinformation” when information is originally produced as true and then shared by a propagator who thinks it is false. There is also disin- formation propagated through misinformation” when informa- tion is devised as false by a creator but is perceived as true by a propagator. Irrespective of such distinctions, both misinformation and disinformation impact the public debate on issues such as health and science (e.g., the anti-vaccine movement), foreign policy (e.g., the wars in Iraq and Ukraine), migration, elections and so on. Recognising this, researchers from a variety of disciplines, including social sciences such as journalism (Ekström et al., 2019) and psychology (Ecker, 2017), have examined misinformation and disinformation. The problems of misinformation and disin- formation are usually called wicked problems” by design scien- tists, as no single comprehensive solution is capable of fully resolving them and attempts to mitigate them often can make them worse. Some examples of this include the backfire effect (Nyhan and Reifler, 2010), false misinformation warnings (Freeze et al., 2020), and the naiveté of social engineering in technology (Tromble and McGregor, 2019). Misinformation and disin- formation are also studied with regard to social psychology (e.g., people’s values, beliefs, information literacy, and motivations), regulatory and technical perspectives (social media, detection tools), and the practice of fact-checking. Given the large volume of published work we rely here on Vanenzuala et al. (2019), who conducted a meta-analysis of 650 articles on this topic to identify regulatory, technical, and normative aspects of misinformation. Cognitive psychologists have investigated the effectiveness of corrections and warnings of misinformation for a long time. Ecker et al. (2010) studied whether the continued influence of misinformation can be reduced by explicit warnings at the outset that people may be misled. They found that a specific warning with detailed information was more efficient than a general warning reminding people that facts are not always properly checked. However, a specific warning can reduce reliance on an outdated source of information but not eliminate it. Pennycook et al. (2018) investigated how fluency via prior exposure con- tributes to the believability of fake news. They found that tagging fake stories as disputed is not an effective solution because it ARTICLE HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS 2 HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS | (2021)8:33 simply attracts even more attention to the problem. They also found that repeating headlines increases perceptions of their accuracy. Schwarz et al. (2016) found that the myth-versus-fact article format is not efficient to deal with fake news because such articles subtly reinforce the myths through repetition and further increase the spread and acceptance of misinformation. Unfortu- nately, such articles make misinformation even more easily accessible by repeating it and illustrating it with pictures. This increases the probability that misinformation that the commu- nicator wanted to debunk will continue to be delivered. They found that it is better to simply provide correct information rather than try to correct wrong information. They also identified five criteria that people use to assess the accuracy of information: acceptance by others, amount of supporting evidence, compat- ibility with one’s own beliefs, general coherence of the statement, and credibility of the information source. Lerman (2016) stated that the interplay between humans’ cognitive limits and the social media network structure influences the spread of information. Finally, Chan et al. (2017) found that debunking messages for the correction of misinformation only increases the effects of the misinformation. Misinformation and tools for mitigating it. Several tools have been developed to counter misinformation, such as Botometer, Foller.me, TinEye, Insigna, Rbutr, Fakespot, NewsGuard, Greek Hoaxes Detector, DejaVu and Social Sensor. ● Botometer detects social bots and classifies online social media user accounts as either bots or human beings. This classification is based on various features of the user account profile, online social network structures, historical patterns of activity, and language and sentiments (Yang et al., 2019; Botometer tool, 2019). ● Foller.me analyses the profiles and tweets of social network users and shows various user characteristics, for example, general information such as name, location, language, join date, and time zone; statistics about tweets (number of tweets, followers, following); and tweet analysis (tweet replies, retweets, tweets with links). The main idea is to understand the detailed profiles of social media users to verify social media content (Sloan and Quan-Haase, 2016; Foller.Me tool, 2019). ● TinEye analyses user-generated content, like photos and videos, as well as detects whether an image, audio content, or video content is fake (Middleton, 2017; Tineye Tool, 2019). Members of the global community, in particular journalists, use this tool and others, such as FotoForensics and Google Reverse Image, to examine user-generated content. ● Rbutr is a machine-learning algorithm applied to community feedback to capture webpages with disputed, rebutted, or contradicted parts elsewhere on the Internet. This tool also provides sector-wise (e.g., health, education, immigrant, climate change) repositories of news and community rebuttal (Mensio and Alani, 2019) and provides warning messages (e.g., This is potentially malicious”) for particular news webpages with a bad reputation. ● Fakespot is a browser plugin that assesses the validity of online reviews based on their URL (Mensio and Alani, 2019; Fakespot Analyzer Tool, 2019). ● NewsGuard is another browser plugin that integrates the opinions of a large pool of journalists and informs users about the reliability of news websites and organisations. It uses nine journalist credibility and transparency criteria that are combined into labels (NewsGuard Tool, 2019). ● Greek Hoaxes Detector is a browser plugin that analyses news articles and assigns labels such as scam,” hoax” or fake” (Ellinika Hoaxes Tool, 2019). ● DejaVu is a system for detecting visual misinformation in the form of image manipulation aimed for use by journalists (Matatov et al., 2018). ● Social Sensor is a software that gathers social media data and analyses trends and what influences them (Schifferes et al., 2014). The aforementioned tools were designed for particular purposes and are limited in several respects, such as the following: 1. Requirements for participation: Some tools were developed based on stakeholder feedback. However, the developers did not involve end users in the process of developing the tools. They also did not collect end users’ preferences regarding these tools. When stakeholders were involved, it was frequently only one group of stakeholders or a very narrow circle of professionals who deal with misinformation. This has resulted in a narrow focus on professional intent instead of on how consumers of information can reduce their uptake of misinformation. 2. Technical issues: Almost all of these browser plugins support only Google Chrome. 3. Lack of integration of the views of fact-checkers: Fact- checkers are part of a growing community that plays an essential role in media policies. However, several of these tools were developed without any consideration from this community, which has led to unnecessarily incomplete detection mechanisms. Consequently, the full potential of fact-checking services has not been fully realised, and the lack of transparency in development and input parameters makes them unclear. This has led to decreased user trust, which is why it seems reasonable to evaluate the functionality of existing fact-checking tools to identify possible gaps. This is best done in a collaborative environment with a high degree of involvement by relevant stakeholders (Horne et al., 2019). A few studies have focussed on assessing the perceived needs of journalists navigating misinformation. In Schifferes et al. (2014), 22 journalists participated in an interview regarding the functionalities most relevant for tools countering online mis- information. According to this study, journalists emphasise the need to predict breaking news and verify content on social media as true or false. Brandtzaeg et al. (2018) conducted a study with 32 journalists and social media users on perceptions of fact- checking tools, concluding that users must be able to understand the limitations of tools and that tools need to be transparent on all ends, including in terms of funding. To the best of our knowledge, policymakers have not yet been included in such studies, although it is clear that policies desire the delivery of tools for dealing with misinformation. Participatory governance and value-based software engineer- ing. Several scientific works have discussed the need to under- stand the typology and features of misinformation (Rossi and Lenzini, 2020; Koulolias et al., 2018). The design and evaluation process we argue for in this article involves two components: (a) co-creation by users and elicitation of user preferences and (b) adequate aggregation and evaluation mechanisms. By co-crea- tion,” we mean a process that is aligned with Peters and Heraud (2015) and Gummesson et al. (2014) as an adaptive and inclusive approach to participatory governance based on the engagement and involvement of various stakeholder groups. Participatory governance, which is embodied in processes that empower citi- zens to participate in public decision making, has been gaining acceptance as an effective means of tackling democratic deficits and improving public accountability. Participatory governance and co-production processes require an understanding of human factors such as individual patterns of decision-making processes, as well as cognitive and behavioural biases; institutional structures; perceptions of the risks, benefits, and costs of various policy interventions; as well as a need for compromise-oriented solutions to honour diverse views and a variety of voices. Participatory governance also requires the involvement of various stakeholders. Stakeholder involvement in decision- making processes and in the development of tools and decision support systems is essential for meeting stakeholder requirements (cf. Komendantova et al., 2014). Furthermore, authors such as Kujala and Väänänem-Vainio-Mattila (2009) have shown that it is essential to consider stakeholders’ values regarding the functionalities and features of a tool when designing new software and that tools so designed are more likely to be used by the groups in question. To achieve this, a number of techniques may be used. Khari and Kumar (2013) tested common approaches experimentally with stakeholders, concluding that value-oriented prioritisation (VOP) met the demands and the environment of the stakeholders better than other techniques. VOP, a so-called preference-based approach that relies on techniques and models from the field of decision analysis, aims to elicit users’ values by studying their preferences (see Vetschera, 2006, for an introduction in the context of software engineering). Basic VOP is a scoring-based additive weighting approach in which a stakeholder or prospec- tive user ranks features (or requirements) according to his or her value-in-use (see Azar et al., 2007). If there is more than one user or stakeholder, the VOP process turns into a group decision problem (i.e., gathering preferential data from several stake- holders or prospective users to identify a selection of features that provides maximum value to users while respecting the resources of the development team). However, VOP in itself is not flexible enough to handle ranking statements and aggregate preferences from several stakeholders in an elaborated way. For this purpose, there exist the novel methods from the field of decision analysis described in the following section. Methodology The empirical data in this study were collected during a co- creation process with stakeholder groups that used workshops and interviews to extract design components from stakeholder dialogues and findings. The aim was to provide insights into expected requirements for anti-disinformation tools. A specially adapted multi-criteria decision framework (Danielson et al., 2020) was then used to understand the desirability of various system features of a tool for mitigating misinformation. Workshop setup and participants. The co-creation workshops consisted of stakeholders from three groups (journalists/fact- checkers, policymakers and citizens) in three countries (Austria, Greece and Sweden). The purpose of the workshops was to dis- cuss misinformation and, over several sessions, collect percep- tions of misinformation, test and discuss tools that address misinformation, as well as various features of these tools. Fur- thermore, we explored how information about particular online tools can be transferred to stimulate critical thinking and trust, as the latter is an important parameter in software adoption (Wu et al., 2011). We used the following sampling and invitation process. After thorough desktop research, a list of organisations was created that identified the most important stakeholders on the topic. A final contact list of various organisations representing our three stakeholder groups (policymakers, citizens, and journalists) was prepared. Hosting pilot team members were assigned to contact the organisations and to update the list accordingly. Subsequently, formal letters of invitation were issued to the target participants. The letter included a brief description of the Co- Inform program and the workshop objectives. It also included the workshop agenda (Appendix II). The team followed up with phone calls to the identified stakeholders and personally explained to them the goals of the project, the workshop methodology, and the importance of their participation. Two days before the event, a reminder e-mail was sent to the list of confirmed participants that provided them with more informa- tion about the location of the event. The formats of the workshops, as well as the sampling and invitation processes, were identical for all three countries to exclude the possibility that the results were influenced by differences in sampling process or format. The policymaker group consisted of government organisations (Ministry of Finance, Ministry of Education, Ministry of Health), nongovernmental organisations (Solidarity Now, Danish Refugee Council, UNHCR and others), grassroots organisations (domain expert organisations like Velos Youth Center), and municipality services organisations (organisations that provided aid to refugees, like Greek Refugee Council, could help us recruit refugees). The citizen group consisted of people from local communities, people from civil societies, refugees, migrants, as well as academics. The journalist group consisted of people from news agencies, radio, and television. The first co-creation workshop took place in September 2018 in Tokyo, Japan, and was organised by the International Council for Information Technology in Government Administration and the Organisation for Economic Co-operation and Development (OECD). The 103 participants at the first multi-stakeholder workshop included 11 government chief information officers, 65 high-ranking public officials, 8 journalists, 8 executives of international organisations, 9 executives from the private sector, and 2 policymakers. The purpose of the workshop was to assess the effects of misinformation in society and suggest mitigation strategies for the public sector. The second co-creation workshop was portioned among the three countries and took place in February–March 2019. The purpose of this workshop was to assess the initial needs of participants around misinformation, their level of trust in news sources, and their perceptions of misinformation and to collect their recommendations on possible interventions and policies. In Vienna, the Co-Inform workshop was organised in cooperation with the Ministry of Economy and Digitalization and included 21 policymaker, journalist, and citizen stakeholders, including representatives of the Austrian Chamber of Labour, the Housing Service of the Municipality of Vienna, and the Austrian Association of Cities and Towns. In Sweden, it included 16 participants, of whom four were journalists, five policymakers (mainly from the Social Democratic Party), and seven citizens (including from Anti-Rumour Sweden). It was hosted by the Botkyrka Multicultural Centre. In Greece, the workshop took place in the community of Serafeio with 31 participants (9 journalists, 9 policymakers, and 13 citizens), including represen- tatives of the Ministries of Finance, Digital Policy, Health, Immigration and Education. The third co-creation workshop took place in these same countries in November 2019. The major theme of the third Co- Inform workshop was Which features make people engage with misinformation-combatting tools, and why?” The theme was addressed over a series of five sessions: introduction to the overall workshop process, categorisation theory exercise, assessment of features of the interface of a potential tool, Multi-Criteria Decision Analysis (MCDA) sessions, and repertoire grid-nudging focus group sessions. Altogether 15 participants attended the third Co-Inform workshop in Sweden: 3 journalists, 1 policy- maker, and 11 citizens. In Greece, 19 people participated: six citizens, seven journalists, and six policymakers. In Austria, 16 stakeholders attended the workshop: five citizens, six journal- ists, and five policymakers. The only difference among the aforementioned workshops was that the participants belonged to three different cultures: ● Workshop 2 (as per our article): We recruited participants from all stakeholder groups (citizens, journalists, policy- makers) who were related to organisations that worked with migrants. ● Workshop 3 (as per our article): We recruited participants from all stakeholder groups (citizens, journalists, policy- makers) without focussing on any specific domain. The format, agenda, and master plan of the workshops were as follows. All pilot countries (Greece, Austria and Sweden) followed a common format that included an agenda, templates, survey forms, exercises, and workshop sessions based on a common master plan that was prepared by the responsible Co-Inform project partners in consultation with Co-Inform project technical partners. Each workshop followed the same master plan. In addition, discussions were held in all three Co-Inform pilot countries on common topics as per the master plans of the workshops. A main objective of the third workshop was to collect input on perceptions of functionalities, user experience features, and system features of tools that deal with misinformation in social media. Four sessions were conducted during each workshop. During the first session, the participants were presented with features. This was followed by a detailed discussion of each feature and the collection of feedback on what should be included or added. The following features were subject to evaluation by the participants: ● Feature 1 (Awareness): I am aware of existing misinformation online. ● Feature 2 (Why and when): I want to know why a claim has been flagged as misinformative. And I want to know who flagged it and when. ● Feature 3a (How it spreads and by whom): I come across something that I find misinformative. I would like to know how this information has spread online and who has shared it. ● Feature 3b (Life cycle [timeline]): I want to know the life cycle (timeline) of a misinformative post/article (e.g., when it was first published, how many fact-checkers have debunked it, and when it was shared again). ● Feature 4a (Sharing over time): I want to be able to quickly understand how much misinformation people have shared over time through an overall misinformation score. ● Feature 4b (How misinformative an item is): I want to be able to quickly understand how much misinformation a news item or tweet may contain through the provision of an overall misinformation score. ● Feature 5a (Instant feedback on arrival): When I encounter a tweet from someone else that contains misinformative content, I want to be informed that it is misinformative. ● Feature 5b (Inform on consistent accounts): I want the Co- Inform system to inform me of which accounts (within my network) consistently generate/share/create misinformative content. ● Feature 5c (Self-notification): I want the Co-Inform tools to notify me whenever I repeatedly share misinformation. ● Feature 6 (Credibility indicators): I want to see credibility indicators that I will immediately understand, and I want the credibility indicators to look very familiar, like other indicators online. ● Feature 8 (Post support or refute): I want to be able to post links to reputable articles and data that support or refute the story or claim. ● Feature 9 (Tag veracity): I want to be able to tag the veracity of an element (tweet, story, image, or sentence/claim) in the current tab I am seeing. ● Feature 10 (Platform feedback): I want to be able to receive feedback on what the platform is doing and has done with the tags and evidence I have submitted. The participants then ranked the features under three criteria, creating three different rankings of the features, one for each criterion. The three criteria were as follows: ● Trust: for trust in this tool ● Critical thinking: for making me think twice before I trust and/or share ● Transparency: for transparency in how the tool makes judgements Thereafter, they ranked the three criteria with respect to their relative importance based on question on the form: The top- ranked features under Trust, do they provide more or less value for you compared to the top-ranked features under Critical thinking?” If the answer was yes,” then trust was ranked above critical thinking (i.e., it was deemed to be of more relative importance, because the participant perceived greater value if the top-ranked features for trust were available compared to the top- ranked features for critical thinking). Elicitation and evaluation. Danielson et al.’s (2020) decision analytic framework was used as the rank-based elicitation and evaluation method. The method was implemented in DecideIT 3.1, which was also used in the workshops. Briefly, DecideIT is capable of operating with incomplete or numerically imprecise input data, such as rankings and interval value statements, in a combined model. To represent the ranking statements, we used a cardinal ranking approach (P-CAR). P-CAR is a calibrated method of creating feasible input in the form of surrogate imprecise value statements, which are derived from rankings provided by stakeholders. The feasible information is represented in the form of linear inequalities (greater than) in combination with interval bounds and a focal point that represents the most feasible surrogate value for a given element given its position in the ranking. This enables conventional multi-attribute value aggregation (Dyer and Sarin, 1979) so the results can be evaluated across multiple stakeholders and criteria. See Danielson and Ekenberg (2019) for details on P-CAR. The evaluation method originated from earlier work on evaluating decision situations involving numerically imprecise input. To avoid problems with aggregation when handling set membership functions and similar features, higher order distributions for better discrimination between the possible outcomes are introduced. To alleviate the problem of overlapping results, the methodology also contains a new evaluation method based on the resulting belief mass over the output intervals, without introducing further complicating aspects into the decision model. During the process, consideration is given to the entire range of output values, as well as how plausible it is that a specific feature will outrank the remaining ones, thus providing a robustness measure. In this way, DecideIT can evaluate the actual proportion of aggregated values for which a feature is considered more favourable than another, that is, whether there is a significantly larger amount of the feasible information (i.e., in the set of rankings provided by the participants where one feature is deemed to provide more value compared to another feature). This can be seen more concretely in Fig. 1, which shows the proportion of feasible information (e.g., Feature 2 is deemed to be more valuable than the rest, Features 3a and 3b are basically equal and more valuable than Feature 4 and the remaining features). See Danielson et al. (2020) for a detailed description of the tool and its underlying theory and Larsson et al. (2018) for details on aggregation across multiple stakeholders/participants. Results In line with OECD’s Recommendation on Digital Government Strategies, the findings from the first co-creation workshop in Tokyo emphasised the need for open, inclusive, accountable, and transparent processes by national governments and highlighted the fact that digital transformation in the public sector, as well as increasing accessibility of the Internet, has exacerbated various problems related to misinformation. Given the importance of factual information for combatting misinformation in the public arena, governments need to collaborate with stakeholders and invest in innovative ways of dealing with misinformation. A number of specific actions were proposed to deal with this societal challenge. Empowerment of citizens, encouraged engagement, education, moderate legislative action, as well as investment in new technologies are invaluable means of tackling misinforma- tion. For fragmented technological and innovative solutions to succeed in tackling misinformation on a broad scale, they need to be integrated and embedded into a co-creational system of poli- cies. More collaborative and effective management of mis- information needs to be supplemented with informed behaviours among citizens. Creating a trusted environment for citizens with adequate education is necessary as we enter an era in which big technological advances have the potential to disrupt even more than they already have. The subsequent workshops took place in three different loca- tions on two separate occasions and provided cross-cultural data for comparing the needs of various stakeholder groups related to decision support models. Data were collected, and the needs of citizens, policymakers, and journalists were identified. Table 1 shows that the need for collaboration and facilitation of tools was identified in all three case countries and by all three stakeholder groups. The need for tools to address education and awareness raising was also identified in all three countries and across all three groups of stakeholders. These tools are also required for sharing reliable information. However, an automatic correction Fig. 1 Results for all groups of stakeholders. Preferences. Table 1 Major preferences of various stakeholders regarding how to address misinformation in the three case countries (Swede","Komendantova, N., Ekenberg, L., Svahn, M., Larsson, A., Shah, S. I. H., Glinos, M., ... & Danielson, M. (2021). A value-driven approach to addressing misinformation in social media. Humanities and Social Sciences Communications, 8(1), 1-12."
"ML_DB_141","Factoring Fact-Checks: Structured Information Extraction from Fact-Checking Articles","Fact-checking, which investigates claims made in public to arrive at a verdict supported by evidence and logical reasoning, has long been a significant form of journalism to combat misinformation in the news ecosystem. Most of the fact-checks share common structured information (called factors) such as claim, claimant, and verdict. In recent years, the emergence of ClaimReview as the standard schema for annotating those factors within fact-checking articles has led to wide adoption of fact-checking features by online plat forms (e.g., Google, Bing). However, annotating fact-checks is a tedious process for fact-checkers and distracts them from their core job of investigating claims. As a result, less than half of the fact-checkers worldwide have adopted ClaimReview as of mid-2019. In this paper, we propose the task of factoring fact-checks for automatically extracting structured information from fact-checking articles. Exploring a public dataset of fact-checks, we empirically show that factoring fact-checks is a challenging task, especially for fact-checkers that are under-represented in the existing dataset. We then formulate the task as a sequence tagging problem and fine-tune the pre-trained BERT models with a modification made from our observations to approach the problem. Through extensive experiments, we demonstrate the performance of our models for well-known fact-checkers and promising initial results for under represented fact-checkers.","Information Science","Proceeding","2020","Y","Y","Prototype","Experimental","Information extraction","20","As a means to combat misinformation [1 , 17], journalists conduct research with evidence and logical reasoning to determine the ve- racity and correctness of factual claims made in public, and publish fact-checking articles (or fact-checks) on their news outlets [57]. Fact-checks play a significant role in the news ecosystem with a shared journalistic purpose of rebuting false claims, therefore they tend to share certain common structured information (called factors) in their journalistic practices [ 57 , 60 ]: a typical fact-check usually introduces the claim to be checked and the claimant who made the claim, and finally arrives at a verdict describing the veracity of the claim. A fact-check could also describe the context where the claim is made, provide evidence to support or attack the claim, etc. An example of a fact-check with three reported factors (claim, claimant and verdict) is shown in Figure 1. This paragraph is excerpted from a fact-check published by PolitiFact in May 2019, titled D.A.R.E. still thinks marijuana is a dangerous drug for kids”, which reached a verdict False” to the claim D.A.R.E removed cannabis from its list of gateway drugs.” made by Viral image” on Facebook. These three factors (claim, claimant and verdict) can summarize the main message of the fact-check for readers on a too long; didn’t read” agenda, and, more importantly, are structured information that can be understood by algorithms for various applications [7 WWW ’20, April 20–24, 2020, Taipei, Taiwan Jiang, et. al. 60 , 64, 65 ], e.g., Google and Bing display these factors as structured snippets for search results that correspond to a fact-check [3, 30]. However, the availability of such factors as structured informa- tion is limited. Traditionally, journalists use creative language to embellish their content and attract readers; therefore hiding these factors within the text content of their fact-checks. Structured infor- mation has only been recently made available with the global effort on computational journalism [5, 8 ]. In the context of fact-checking, a schema named ClaimReview [ 50] was developed to help anno- tating these structured information on web pages: A fact-checker can embed the ClaimReview markup in their HTML content of fact-checks, or submit these factors manually through an online ClaimReview markup tool [ 14 ]. This is a tedious process for fact- checkers and distracts them from their core job of investigating claims. As a result, Duke Reporters’ Lab reported that less than half of their recorded fact-checkers have adopted this schema as of July 2019, and only for a part of their published fact-checks [39 ]. Therefore, how to extract more structured information from the remaining fact-checks becomes an emerging research concern. In this paper, we propose the task of factoring fact-checks for automatically extracting structured information from fact-checking articles. Leveraging a public dataset of 6K fact-checks available on DataCommons [ 9], we first conduct an exploratory analysis of the task. We find that reported claimants and verdicts can be mostly found exactly in the text content of fact-checks, while claims are mostly paraphrased from one or more sentences. In addition, we find that these factors are heavily distributed in head and tail sentences of fact-checks, albeit differently between well-known and under-represented fact-checkers. In order to automatically extract these factors, we formulate this task as a sequence tagging problem and conduct several experiments by fine-tuning the state-of-the- art pre-trained BERT models [10 ]. Our experiments focus on the following research questions: • How well can models extract claims, claimants and verdicts? • Can model performance be improved with modifications? • Can models trained on well-known fact-checkers generalize? Our experiments demonstrate the performance of BERT models on well-known fact-checkers, especially under the modification made from our empirical observations. Although it is challenging for models to generalize to under-represented fact-checkers whose fact-checks are unseen during the training process, we demonstrate promising initial results by conducting additional experiments. As this task directly faces the misinformation problem and therefore requires extremely high accuracy to be fully automated, we dis- cuss several potential applications with the performance as is, e.g., pre-populating ClaimReview markups in a human-in-the-loop pro- cess [14 ], or supporting other downstream tasks of computational fact-checking [7, 60, 64, 65]. To summarize, we make the following contributions: • Defined the task of factoring fact-checks to assist fact-checkers. • Explored existing fact-checks and their reported factors to un- derstand the challenges involved in the task. • Modified and fine-tuned BERT models and conducted extensive experiments to approach the task. The rest of the paper is organized as follows: § 2 introduces back- ground and positions our work around related areas, § 3 explores (a) Google snippet. A fact-check from PolitiFact is displayed. (b) Bing snippet. A fact-check from Africa Check is displayed. Figure 2: Structured fact-check snippets. When searching a made-up explanation of the word newspaper”, fact-checking fea- tures are displayed by both Google and Bing. data, § 4 formulates the task and introduces the model, § 5 reports the results of our experiments, § 6 discusses potential applications, limitations and future work, and finally concludes. 2 BACKGROUND Fact-checks have been around since early 2000s and came to a broader public consciousness in 2016 [ 16 ], directly in response to the misinformation epidemic [1, 17 ]. In this section, we briefly introduce the background of misinformation and fact-checking, and situate our task in the broader scope of computational linguistics. 2.1 Misinformation and Fact-Checking There have been significant efforts on understanding misinforma- tion from both researchers [34] and practitioners [62]. Early work on misinformation discussed its psychological foundations [ 42 , 49, 61 ], economic incentives [1 , 13 ] and social impact [ 27 , 59 ]. Mean- while, studies from the computational community were mostly focused on detecting misinformation on the web [51 ]: these studies modeled the problem as a classification task and utilized various features (e.g., stylistic differences [ 12 , 46 , 58 ], public responses [ 27 – 29 ]) to access the trustworthiness of information. Although solid results were reported, these models heavily rely on the inflamma- tory and sensational language used by misinformation to instigate its readers, instead of verifying the information per se [45]. Fact-checking, as a complementary approach (arguably) orthog- onal to stylistic features and public opinions, rebuts misinformation by checking the veracity of individual factual claims [16 , 57 , 60 ]. Fact-checking is a time-consuming process done by journalists col- lecting evidence and writing articles (i.e., fact-checks). Although recent studies explored the potential of adding automation to sev- eral stages of this process (e.g., discovering check-worthy claims from large corpus [20– 22], checking existing knowledge graphs for evidence [7, 64, 65]), high quality fact-checks are still scarce. As the availability of fact-checks is limited, the utilization of fact- checks becomes of vital importance. Platforms have developed a variety of applications to utilize fact-checks, e.g., downranking [6 ], moderation [25, 26 ]. An application promulgated by search engines Factoring Fact-Checks WWW ’20, April 20–24, 2020, Taipei, Taiwan (i.e., Google [30 ] and Bing [ 3]) is enriching result pages with fact- checking features to maximize the exposure of high quality fact- checks by displaying structured snippets of their factors when searching a relevant query [60 ]. Figure 2 shows examples of such structured snippets from Google (Figure 2a) and Bing (Figure 2b) when searching a made-up explanation of the word newspaper”. Such application relies on structured factors reported by fact- checkers. However, less than half of fact-checkers reported factors for their fact-checks as of July 2019, according to Duke Reporters’ Lab [39 ]. Our task provides an upstream support for this process, as we expect more structured factors can be obtained with the help of factoring fact-checks. Specific use cases and other downstream applications that can benefit from our task are discussed in § 6. 2.2 Extracting, Mining and Verifying Claims There are several existing tasks in the broader scope of computa- tional linguistics that are related to our task. Claim extraction (or detection, identification) is a task of finding factual claims in an article. The target claim could be either context dependent [ 36 ] or independent [ 38]. In the fact-checking context, ClaimBuster is a popular system that ranks claims by checkwor- thiness” in news articles or political speeches [21 , 22 ]. In § 5, we apply this tool as a baseline method for the claim factor, and show that the most checkworthy” claim in a fact-check is often not the fact-checked one. Argument mining is a more general task of labeling arguments in an article, including both claim extraction and relationship (e.g., supporting or attacking, being premise or evidence) prediction between arguments [4]. This task is often formulated as a sentence classification or boundary detection problem, and its research has been applied to many forms of literature, including persuasive essays [11, 52 ], scientific publications [ 54 ], Wikipedia articles [ 2, 48 ], debate scripts [19, 43], etc. Our task can be approximately viewed as a specific case of relationship prediction for fact-checks, as we aim to extract the target claim and the verdict that supports or attacks the claim, except with different task formulation and context. In addition, we focus on only two factors instead of labeling the argument structure of the entire fact-check. Claim verification is a recently proposed task, that takes a claim and the corresponding context where claim is made as inputs, and outputs a binary verdict on whether the context supports or rejects the claim [ 55 , 56]. In our task, the claim is unknown and the verdict is free text: we take a fact-check as input, and output the claim and its verdict simultaneously, along with its claimant. 3 DATA EXPLORATION DataCommons hosts a dataset of URLs for fact-checks [9]. We use an internal Google tool to extract main article text from these URLs, filter out non-English fact-checks and keep the remaining 6,216 ones for our data exploration. These fact-checks are usually long articles, with 1,038 words and 24.4 paragraphs on average. Each fact-check is labeled with its three factors: claim, claimant and verdict, reported by its fact-checker. Claims are usually one or two sentences, with 22.2 words on average. Claimants are mostly names of people or organizations, with 2.2 words on average. Ver- dicts are mostly adjective phrases and 2.5 words on average.0 1000 2000 3000 4000 # of fact-checks Others africacheck.org factly.in washingtonpost.com factcheck.org politifact.com Fact-checker 348 220 250 627 856 3915 Well-known Under-represented Figure 3: Who are the fact-checkers? the number of fact-checks follows a power law distribution over fact-checkers, where well- known” (top 5) fact-checkers publish 94% of fact-checks. In this section, we explore several questions of the fact-check dataset to understand our task. 3.1 Who Are the Fact-Checkers? We first answer the question who the fact-checkers are. As shown in Figure 3: the number of fact-checks follows a power law distri- bution over fact-checkers, where top 19% (5/27) of fact-checkers publish 94% (5,868/6,216) of fact-checks and 40% (11/27) of fact- checkers have reported only a single fact-check. Notably, PolitiFact dominates this dataset with 63% (3,915/6,216) reported fact-checks. In the rest of the paper, we refer to the top five fact-checkers as well-known” ones, as they are reputed journalistic organizations that are heavily focused on fact-checking and have specialized fact-check” columns on their websites. Among them, three fact- checkers (PolitiFact, FactCheck.org and the Washington Post) are US-based and have been contributing to fact-checks for more than 10 years. The other two fact-checkers are also well-known in their respective markets: Factly.in is from India and Africa Check is from Africa, both founded in 2012. The rest of the 22 fact-checkers are under-represented” (i.e., with few samples) in this dataset. These fact-checkers could be a) newly established fact-checking teams from reputed journalistic organizations (e.g., Australian Associated Press), b) well-known news agencies doing occasional fact-checking (e.g., CNN, the Verge), or c) new coming agencies for localized fact- checking (e.g., factcheckNI for Northern Ireland). In § 5, we discuss how this split affects our experimental design. Overall, this dataset records more than one third of all fact- checkers that verified signatories of the International Fact-Checking Network (IFCN) code of principles [24 ], and contains all the well- known fact-checkers except Snopes, therefore it is a reasonably representative sample of the current fact-checking ecosystem. 3.2 Can Factors Be Found in the Fact-Check? Next, we answer the question if factors can be found in the fact- check article. In the introductory example shown in Figure 1, the claim D.A.R.E. removed cannabis from its list of gateway drugs.” and verdict False” can be both matched in the text content of the fact-check, while the claimant Viral image” cannot. Instead, the fact-check uses Facebook post” as the claimant. To answer this question for all fact-checks in the dataset, we start with exact string matching between factors and fact-checks. We find that most verdicts (76%, 4,743/6,216) and claimants (80%, WWW ’20, April 20–24, 2020, Taipei, Taiwan Jiang, et. al.0 1 2 3 4 5 6 7+ # of matched claims 0 500 1000 1500 2000 # of fact-checks 1335 2089 1681 667 233 108 46 57 (a) Can claims be found? 79% claims can be matched in fact-checks and 45% are matched more than once.0 1 2 3 4 5 6 7+ # of matched claimants 0 1000 2000 3000 # of fact-checks 1213 3509 805 305 144 82 51 107 (b) Can claimants be found? 80% claimants can be matched in fact-checks and 24% are matched more than once.0 1 2 3 4 5 6 7+ # of matched verdicts 0 1000 2000 3000 4000 # of fact-checks 1216 3975 749 158 74 22 8 14 (c) Can verdicts be found? 80% verdicts can be matched in fact-checks and 16% are matched more than once. Figure 4: Can factors be found in the fact-check? Around 80% of factors can be roughly matched in fact-checks based on a fuzzy matching rule, and most of them are matched only once. 4,988/6,216) can be matched in the fact-check, while claims are more difficult and match only 32% (2,000/6,216). This number is counter-intuitive for claims, as we expect the fact-checked claim should appear more frequently in the text content of fact-checks. After reading through several examples of claims and fact-checks, we find that although most claims are not exactly repeated in the text content of fact-checks, they are mostly paraphrased from one or more sentences, e.g., in Figure 2, the reported claim from PolitiFact Says the word newspaper stands for ‘north, east, west, south, past and present event report.’” is paraphrased to ‘newspaper’ is an acronym for ‘North, East, West, South, Past and Present Report.’” in the fact-check. To find these paraphrased factors, we develop a fuzzy matching rule: we first traverse each paragraph and keep the ones that contain at least a certain percentage of the reported factors, and then find the minimum window substring1 [35 ] of the overlap from the paragraph as the approximate match for the factor. To choose a reasonable threshold for above-mentioned percent- age, manually check 20 fact-checks and matched factors under a spectrum of thresholds. We find that setting the overlap thresh- old around two thirds gives the best match without introducing false positives (i.e., incorrectly matched factors). We apply the same threshold for all factors: For claims, this allows us to ignore certain paraphrase and conjugation (e.g., says/said/saying”); For claimants and verdicts, this threshold represents an exact match for short factors less than or equal to two words, e.g., False”, Mostly false”, John Doe”;2 and allows some flexibility for more than two words, e.g., the claimant John M Doe” can be matched without the middle name M”. After matching, we again manually check 100 random samples and find no false positives. Figure 4 shows the histograms of matched factors. Note that factors can be matched zero times or more than once. Under fuzzy matching, more claims (79%, 4,881/6,216) can be matched in fact- checks and 45% (2,792/6,216) are matched more than once (Fig- ure 4a). The number for matched claimants is also slightly increased, with 80% (5,003/6,216) matched and 24% (1,494/6,216) matched more than once (Figure 4b). Matched verdicts are roughly the same, 80% (5,000/6,216) verdicts are matched and 16% (1,025/6,216) are matched more than once (Figure 4c). In general, this observation suggests 1Comparison at word-level and case-insensitive. 286% (5,338/6,216) of claimants and 79% (4,915/6,216) of verdicts are of less than or equal to two words. that around 80% of factors can be roughly matched in fact-checks, the remaining ones are framed to the extend that exceeds our al- lowed threshold. 3.3 Where Are the Factors in the Fact-Check? Our final question is where the factors are in the fact-check if they are matched. To answer this, we normalize the locations of matched factors by the number of words in the fact-check. This results in a relative position measure ranging from 0 to 1 for each factor in each fact-check, where 0 represents the head and 1 represents the tail of the fact-check. Then, we estimate the probability density functions (PDFs) for the relative position measure of each factor, and plot them in Figure 5. Recall that well-known fact-checkers publish 94% of fact-checks, the PDFs of relative factor positions from all fact-checks would mostly reflect the distribution of well-known ones. Therefore, we estimate separate PDFs for well-known and under-represented fact-checkers in addition to the overall PDFs to compare their difference. As shown in Figure 5, there are two high-level observations that apply in general for all factors: a) factors distribute heavily on head (<0.15) and tail (>0.85) sentences of fact-checks, an observation that we later utilize for model design in § 4, and b) overall distributions (thick grey lines) are similar to the distribution of well-known fact-checkers (colored dashed lines) as expected. For claims, Figure 5a shows that claims are distributed differ- ently for fact-checks from well-known fact-checkers and under- represented ones (Mann-Whitney U = 2.1 × 108∗∗∗):3 Claims are found on both head (<0.15, 41%) and tail (>0.85, 28%) sentences from well-known fact-checkers, but mostly head (<0.15, 53%) sentences from under-represented fact-checkers. This is because well-known fact-checkers usually start their fact-check with an introductory paragraph and end with a concluding one, both of which likely de- scribes the checked claim. Under-represented fact-checkers usually do not write the latter part, and most fact-checks only introduce the claim in the beginning. Claimants are distributed similarly between well-known and under-represented fact-checkers. The distribution for well-known ones is slightly shifted to the left (U = 1.1 × 107∗∗∗), as shown in Figure 5b. All claimants are heavily distributed on head sentences 3 ∗p < 0.05; ∗∗p < 0.01; ∗∗∗p < 0.001 Factoring Fact-Checks WWW ’20, April 20–24, 2020, Taipei, Taiwan0.0 0.2 0.4 0.6 0.8 1.0 Relative position of matched claims 0 1 2 3 Density All fact-checkers Well-known Under-represented (a) Where are the claims? Claims from well- known fact-checkers are distributed on both head (<0.15, 41%) and tail (>0.85, 28%) sentences of fact- checks, but mostly heads (<0.15, 53%) from under- represented fact-checkers. The difference is sig- nificant (U = 2.1 × 108∗∗∗).0.0 0.2 0.4 0.6 0.8 1.0 Relative position of matched claimants 0 2 4 Density All fact-checkers Well-known Under-represented (b) Where are the claimants? Claimants from well-known and under-represented fact-checkers are distributed similarly, heavily on head sen- tences (<0.15, 62% for well-known ones and 66% for under-represented ones). The difference is sig- nificant (U = 1.1 × 107∗∗∗).0.0 0.2 0.4 0.6 0.8 1.0 Relative position of matched verdicts 0 2 4 6 8 Density All fact-checkers Well-known Under-represented (c) Where are the verdicts? Verdicts from well- known fact-checkers are distributed on the very end of the fact-check (>0.85, 73%), but mostly head (<0.15, 59%) sentences from under-represented fact-checkers. The difference is significant (U = 5.6 × 105∗∗∗). Figure 5: Where are the factors in the fact-check? Factors distribute heavily on head (<0.15) and tail (>0.85) sentences of fact-checks, and overall distributions are similar to the distribution of well-known fact-checkers. of the fact-checks (<0.15, 62% for well-known fact-checkers and 66% for under-represented ones). For verdicts, Figure 5c shows different distributions of relative po- sitions between well-known and under-represented fact-checkers (U = 5.6 × 105∗∗∗). For well-known fact-checkers, the verdicts are matched mostly on the very end of the fact-check (>0.85, 73%), usu- ally in the last several words. This reflect the journalistic style of well-known fact-checkers, especially PolitiFact and The Washing- ton Post, whose verdicts are based on their own rating systems and are reached in the very end. These verdicts usually act as comple- ments to linking the verb to be, e.g., We rate it Pants on Fire!” by PolitiFact and It earns Four Pinocchios.” by the Washington Post. On the contrary, under-represented fact-checkers tend to introduce their verdicts in the beginning of the fact-check (<0.15, 59%), and their verdicts are usually adjectives used to modify nouns, e.g., A false rumor claims...”. These fact-checks also have no concluding paragraphs in the end, therefore verdicts can only be found in the head sentences. 4 TASK AND MODELS Data exploration shows that most factors can be fuzzily matched in fact-checks. In this section, we formulate the task of factoring fact-checks as a computational linguistic task and introduce our models for the task. 4.1 Task Formulation In general, certain linguistic patterns can be found when a factor appears in the fact-check. These linguistic patterns can be roughly categorized to two types: the languages used by the factor per se and its surrounding context. 4.1.1 Linguistic patterns. Claims usually appear following certain verbs as context, e.g., (someone) claimed...”, (someone) said...”, (an image/post) shows...”, etc., although the beginning of the claim might not be directly adjacent to these verbs (as additional infor- mation can be inserted, e.g., claimed, at somewhere, that...”). In addition, the languages used by claims are mostly factual sentences, and sometimes contains quotation marks, numbers or statistics, entities, etc., e.g., A 1988 exorcism took place in the only house still standing after Hurricane Ike.” is a claim checked by PolitiFact which contains the year 1988” and the entity Hurricane Ike”. Claimants, opposite to claims, are usually followed by above- mentioned verbs as context (i.e., someone”, ”an image/post” in previous examples), and they are usually named entities such as persons (e.g., politicians, celebrities) or organizations (e.g., news agencies, social media platforms). Verdicts are mostly adjective words or phrases that describe ve- racity, e.g., true”, mostly true”, false”, or phrases from specialized ratings systems, e.g., pants on fire” by PolitiFact and the Pinoc- chio” system by the Washington Post. Occasionally, verdicts can also be descriptive text, e.g., out of context because...”, outdated as of...”. In terms of context, verdicts are often explicitly pointed out by a concluding sentence, e.g., we rate this claim true”, but it could also be embedded in sentences modifying nouns, e.g., a false rumor claims...”. 4.1.2 Sequence tagging. Combining these linguistic patterns to- gether, it is implausible to build and expensive to maintain a rule- based system that matches patterns to extract these factors. Instead, we formulate the extraction task as a computational linguistic task, the sequence tagging problem, that can be approached by probabilis- tic models. In general, the goal of the sequence tagging problem is to prob- abilistically estimate the distribution of labels to each word in a sequence. This is a common problem that are shared by a number of existing tasks: Part-of-speech tagging task assigns a part-of-speech label (e.g., noun, verb) to each word in a sentence [40, 47 ]; Named entity recognition task assigns a entity label (e.g., person, orga- nization, country) to each word in given text [33 , 41 ]; Extractive summarization task assigns a binary label representing if a word should belong to the summary [18, 63], etc. The expected input of the sequence tagging problem is a se- quence of words, and the output is an equal-length sequence of labels (i.e., a label for each word). In our task, there are three posi- tive labels (i.e., claim, claimant and rating) representing if a word should belong to a factor, and a negative label representing a word not belonging to any factors. WWW ’20, April 20–24, 2020, Taipei, Taiwan Jiang, et. al.actually 100% flatisearththethatclaimfalseamadeDoeMJohn actually 100% flat .isearththethatclaimfalseamadeDoeMJohn Claim: The earth is flat . Claimant: John Doe Verdict: False Fluent tagger: Concise tagger: . Figure 6: Fluent and concise tagger. The fluent tagger selects indexes of the entire span of matched factors, i.e., John M Doe” as the claimant and all words starting from the” as the claim; The concise tagger indexes of only the overlapping words, i.e., skipping non-overlapping words M” for the claimant and actually 100%” for the claim. 4.1.3 Generating ground-truth labels. Recall that fact-checks and its factors are available in our data and we developed a fuzzy match- ing rule in § 3. This information can be used to generate equal- length sequences as labels for the sequence tagging problem. For a fact-check, we first initialize an equal-length sequence with all negative labels, and then traverse through all matched factors and replace negative labels with positive ones at selected indexes. We propose two methods of selecting indexes: A fluent tagger that selects indexes of the entire span of matched factors and a concise tagger that selects indexes of only the overlapping words. As shown in Figure 6, the fact-check is John M Doe made a false claim that the earth is actually 100% flat.” and its three factors are The earth is flat” as the claim, John Doe” as the claimant and False” as the verdict.4 The fluent tagger labels John M Doe” as the claimant and all words starting from the” as the claim, while the concise tagger skips non-overlapping words M” for the claimant and actually 100%” for the claim. Intuitively, the fluent tagger is focused more on readability as it generates continues phrases as ground-truth, but it also inevitably includes unessential details in the sequence. On the contrary, the concise tagger is focused more on brevity as it only selects essential words of factors, but the results could be less readable if the matched factors miss several words. 4.2 Models Sequence tagging is a traditional computational linguistic problem that has been studied for decades from early statistical methods (e.g., Hidden Markov models [31], conditional random fields [ 32]) to current neural architectures (e.g., recurrent neural network [ 15 ]). To date, the most applicable models usually leverage transfer learning following the pre-training/fine-tuning paradigm. In short, during pre-training process, models are trained on unlabeled data with certain objectives, and during fine-tuning, models are initialized with pre-trained parameters and then re-train on labeled data over specific tasks. 4.2.1 BERT models. BERT (Bidirectional Encoder Representations from Transformers) is a recently developed model [ 10]. Its pre- training objective is to predict missing (i.e., masked) words in a sentence conditional on both left and right context, as well as to 4This one sentence fact-check is hypothetical to demonstrate how indexes are selected. In general, it is unlikely that all factors can be found in a single sentence.... ... ... ... If ..., then ... [CLS] A post says that D.A.R.E. this false . A post says that D.A.R.E. this false . ... ... ... If ..., then ... [TAIL] Claim: D.A.R.E. ... Claimant: Post Verdict: False Claim: D.A.R.E. ... Claimant: Post Verdict: False Figure 7: The framework for factoring fact-checks. First, the framework feeds the fact-check and its factors to a tagging pipeline; Then, the framework passes sequences and labels to the BERT model and uses cross entropy loss to fine-tune the model; Finally, the predicted labels is fed to a recovery pipeline to obtain factors. We feed our inputs one paragraph at a time and chunk paragraphs exceeding maximum length. lead token [CLS] is replaced with paragraph positions [HEAD], [BODY] or [TAIL]. predict the relationship between sentences (i.e., if a sentence is a next sentence of another). During fine-tuning, it has shown the abil- ity to achieve state-of-the-art performance on many computational linguistic tasks by simply replacing input and output layers. As BERT provides an easy access to the state-of-the-art without any specific neural architecture design, we experiment with BERT to explore the feasibility of our task. The specific framework of model is shown in Figure 7. First, our framework feeds the fact- check and its factors to a rule-based tagging pipeline described in § 4.1.3 to generate sequences and labels for the BERT model; Then, our framework passes these sequences and labels to the BERT model, obtains the activations of its last layer, and feeds them to an output layer for predictions. During the process, cross entropy loss is used to propagate errors and fine-tune the model [ 44 ]; Finally, the predicted labels are fed to a rule-based recovery pipeline to concatenate words and predict factors. Note that BERT is designed for short sequences with a default maximum sequence length of 512 while fact-checks are in general longer sequences.5 A common strategy dealing with long sequences is to truncate after the maximum length, because the head of the sequence usually captures its essence for a number of tasks, e.g., summarization, classification. However, truncation is not fit for our task as factors can be matched anywhere in the text content of a fact-check, as shown in Figure 5. Therefore, we run our framework on paragraph level and feed our inputs one paragraph at a time. If 51,038 words on average, and the length would increase with the wordpiece tokenizer from BERT. Factoring Fact-Checks WWW ’20, April 20–24, 2020, Taipei, Taiwan Table 1: Test on well-known fact-checkers. Fact-checks from this test set are published by the same set of fact-checkers in the train set. BERT models significantly outperforms baseline methods, and replacing [CLS] with paragraph positions help to improve the overall performance of models. Lead token Tagger Claim ROUGE-1 Claimant ROUGE-1 Verdict ROUGE-1 F1 Precision Recall F1 Precision Recall F1 Precision Recall Baseline .183 (.183) .300 (.300) .141 (.141) .237 (.237) .181 (.181) .352 (.352) .660 (.660) .638 (.638) .702 (.704) [CLS] Fluent .636 (.853) .669 (.897) .633 (.850) .769 (.894) .803 (.934) .759 (.883) .931 (.975) .934 (.979) .930 (.974) Concise .592 (.864) .615 (.897) .596 (.870) .784 (.907) .789 (.913) .783 (.906) .938 (.971) .940 (.973) .938 (.970) Paragraph position Fluent .638 (.854) .674 (.902) .637 (.853) .794 (.889) .821 (.919) .789 (.884) .940 (.978) .942 (.980) .939 (.978) Concise .646 (.866) .664 (.889) .652 (.873) .839 (.928) .852 (.943) .834 (.923) .941 (.975) .944 (.979) .940 (.974) a paragraph alone is longer than the maximum length, we chunk the paragraph to sub-paragraphs and feed them to BERT in order. 4.2.2 Modification on lead token. In the original BERT, the lead token of the input sequence is a special token","Jiang, S., Baumgartner, S., Ittycheriah, A., & Yu, C. (2020, April). Factoring fact-checks: Structured information extraction from fact-checking articles. In Proceedings of The Web Conference 2020 (pp. 1592-1603)."
"ML_DB_144","Real-time Claim Detection from News Articles and Retrieval of Semantically-Similar Factchecks","Factchecking has always been a part of the journalistic process. However with newsroom budgets shrinking it is coming under increasing pressure just as the amount of false information circulating is on the rise [MAGM18]. We therefore propose a method to increase the efficiency of the factchecking process, using the latest developments in natural language Processing (NLP). This method allows us to compare incoming claims to an existing corpus and return similar, factchecked, claims in a live system—allowing factcheckers to work simultaneously without duplicating their work.","Computer Science","Article","2019","Y","Y","Prototype","Experimental","Claim detection","8","In recent years, the spread of misinformation has be- come a growing concern for researchers and the pub- lic at large [MAGM18]. Researchers at MIT found that social media users are more likely to share false information than true information [VRA18]. Due to renewed focus on finding ways to foster healthy polit- ical conversation, the profile of factcheckers has been raised. Factcheckers positively influence public debate by publishing good quality information and asking politi- cians and journalists to retract misleading or false statements. By calling out lies and the blurring of Copyright c© 2019 for the individual papers by the papers’ au- thors. Copying permitted for private and academic purposes. This volume is published and copyrighted by its editors. In: A. Aker, D. Albakour, A. Barr ́on-Cede ̃no, S. Dori-Hacohen, M. Martinez, J. Stray, S. Tippmann (eds.): Proceedings of the NewsIR’19 Workshop at SIGIR, Paris, France, 25-July-2019, published at the truth, they make those in positions of power ac- countable. This is a result of labour intensive work that involves monitoring the news for spurious claims and carrying out rigorous research to judge credibility. So far, it has only been possible to scale their output upwards by hiring more personnel. This is problem- atic because newsrooms need significant resources to employ factcheckers. Publication budgets have been decreasing, resulting in a steady decline in the size of their workforce [Pew16]. Factchecking is not a directly profitable activity, which negatively affects the alloca- tion of resources towards it in for-profit organisations. It is often taken on by charities and philanthropists instead. To compensate for this shortfall, our strategy is to harness the latest developments in NLP to make factchecking more efficient and therefore less costly. To this end, the new field of automated factcheck- ing has captured the imagination of both non-profits and start-ups [Gra18, BM16, TV18]. It aims to speed up certain aspects of the factchecking process rather than create AI that can replace factchecking person- nel. This includes monitoring claims that are made in the news, aiding decisions about which statements are the most important to check and automatically re- trieving existing factchecks that are relevant to a new claim. The claim detection and claim clustering methods that we set out in this paper can be applied to each of these. We sought to devise a system that would auto- matically detect claims in articles and compare them to previously submitted claims. Storing the results to allow a factchecker’s work on one of these claims to be easily transferred to others in the same cluster. arXiv:1907.02030v1 [cs.CL] 3 Jul 2019 2 Claim Detection 2.1 Related Work It is important to decide what sentences are claims be- fore attempting to cluster them. The first such claim detection system to have been created is ClaimBuster [HNS+17], which scores sentences with an SVM to determine how likely they are to be politically per- tinent statements. Similarly, ClaimRank [JGBC+18] uses real claims checked by factchecking institutions as training data in order to surface sentences that are worthy of factchecking. These methods deal with the question of what is a politically interesting claim. In order to classify the objective qualities of what set apart different types of claims, the ClaimBuster team created PolitiTax [Car18], a taxonomy of claims, and factchecking organ- isation Full Fact [KPBZ18] developed their preferred annotation schema for statements in consultation with their own factcheckers. This research provides a more solid framework within which to construct claim de- tection classifiers. The above considers whether or not a sentence is a claim, but often claims are subsections of sen- tences and multiple claims might be found in one sentence. In order to accommodate this, [LGS+17] proposes extracting phrases called Context Dependent Claims (CDC) that are relevant to a certain ‘Topic’. Along these lines, [AJC+19] proposes new definitions for frames to be incorporated into FrameNet [BFL98] that are specific to facts, in particular those found in a political context. 2.2 Method It is much easier to build a dataset and reliably eval- uate a model if the starting definitions are clear and objective. Questions around what is an interesting or pertinent claim are inherently subjective. For exam- ple, it is obvious that a politician will judge their oppo- nents’ claims to be more important to factcheck than their own. Therefore, we built on the methodologies that dealt with the objective qualities of claims, which were the PolitiTax and Full Fact taxonomies. We annotated sentences from our own database of news articles based on a combination of these. We also used the Full Fact definition of a claim as a statement about the world that can be checked. Some examples of claims accord- ing to this definition are shown in Table 1. We decided the first statement was a claim since it declares the oc- currence of an event, while the second was considered not to be a claim as it is an expression of feeling. Full Fact’s approach centred around using sentence embeddings as a feature engineering step, followed by Table 1: Examples of claims taken from real articles. Sentence Claim? In its 2015 order, the NGT had banned Yes the plying of petrol vehicles older than 15 years and diesel vehicles older than 10 years in the National Capital Region (NCR). In my view, farmers should not just No rely on agriculture but also adopt dairy farming. a simple classifier such as logistic regression, which is what we used. They used Facebook’s sentence embed- dings, InferSent [CKS+17], which was a recent break- through at the time. Such is the speed of new devel- opment in the field that since then, several papers de- scribing textual embeddings have been published. Due to the fact that we had already evaluated embeddings for clustering, and therefore knew our system would rely on Google USE Large [CYK+18], we decided to use this instead. We compared this to TFIDF and Full Fact’s results as baselines. The results are displayed in Table 2. However, ClaimBuster and Full Fact focused on live factchecking of TV debates. Logically is a news ag- gregator and we analyse the bodies of published news stories. We found that in our corpus, the majority of sentences are claims and therefore our model needed to be as selective as possible. In practice, we choose to filter out sentences that are predictions since gener- ally the substance of the claim cannot be fully checked until after the event has occurred. Likewise, we try to remove claims based on personal experience or anec- dotal evidence as they are difficult to verify. Table 2: Claim Detection Results. Embedding Method P R F1 Google USE Large 0.90 0.89 0.89 [CYK+18] Full Fact (not on 0.88 0.80 0.83 the same data) [KPBZ18] TFIDF (Baseline) 0.84 0.84 0.84 [Jon72] 3 Claim Clustering 3.1 Related Work Traditional text clustering methods, using TFIDF and some clustering algorithm, are poorly suited to the problem of clustering and comparing short texts, as they can be semantically very similar but use dif- ferent words. This is a manifestation of the the data sparsity problem with Bag-of-Words (BoW) mod- els. [SR15]. Dimensionality reduction methods such as Latent Dirichlet Allocation (LDA) can help solve this problem by giving a dense approximation of this sparse representation [BNJ03]. More recently, efforts in this area have used text embedding-based systems in order to capture dense representation of the texts [WXX+15]. Much of this recent work has relied on the increase of focus in word and text embeddings. Text embeddings have been an increasingly popular tool in NLP since the introduction of Word2Vec [MCCD13], and since then the number of different embeddings has exploded. While many focus on giving a vector repre- sentation of a word, an increasing number now exist that will give a vector representation of a entire sen- tence or text. Following on from this work, we seek to devise a system that can run online, performing text clustering on the embeddings of texts one at a time 3.1.1 Text Embeddings Some considerations to bear in mind when deciding on an embedding scheme to use are: the size of the final vector, the complexity of the model itself and, if using a pretrained implementation, the data the model has been trained on and whether it is trained in a supervised or unsupervised manner. The size of the embedding can have numerous re- sults downstream. In our example we will be doing dis- tance calculations on the resultant vectors and there- fore any increase in length will increase the complex- ity of those distance calculations. We would therefore like as short a vector as possible, but we still wish to capture all salient information about the claim; longer vectors have more capacity to store information, both salient and non-salient. A similar effect is seen for the complexity of the model. A more complicated model, with more train- able parameters, may be able to capture finer details about the text, but it will require a larger corpus to achieve this, and will require more computational time to calculate the embeddings. We should therefore at- tempt to find the simplest embedding system that can accurately solve our problem. When attempting to use pretrained models to help in other areas, it is always important to ensure that the models you are using are trained on similar ma- terial, to increase the chance that their findings will generalise to the new problem. Many unsupervised text embeddings are trained on the CommonCrawl 1 dataset of approx. 840 billion tokens. This gives a huge amount of data across many domains, but re- quires a similarly huge amount of computing power to train on the entire dataset. Supervised datasets are 1CommonCrawl found at unlikely ever to approach such scale as they require human annotations which can be expensive to assem- ble. The SNLI entailment dataset is an example of a large open source dataset [BAPM15]. It features pairs of sentences along with labels specifying whether or not one entails the other. Google’s Universal Sen- tence Encoder (USE) [CYK+18] is a sentence embed- ding created with a hybrid supervised/unsupervised method, leveraging both the vast amounts of unsuper- vised training data and the extra detail that can be derived from a supervised method. The SNLI dataset and the related MultiNLI dataset are often used for this because textual entailment is seen as a good basis for general Natural Language Understanding (NLU) [WNB18]. 3.2 Choosing an embedding In order to choose an embedding, we sought a dataset to represent our problem. Although no perfect matches exist, we decided upon the Quora duplicate question dataset [SIC17] as the best match. To study the embeddings, we computed the euclidean distance between the two questions using various embeddings, to study the distance between semantically similar and dissimilar questions. Figure 1: Analysis of Different Embeddings on the Quora Question Answering Dataset Table 3: Comparing Sentence Embeddings for Clustering News Claims. Embedding Time Number Number Percentage of Percentage of method taken (s) of claims of clusters claims in claims in clusters clustered majority clusters of one story Elmo [PNI+18] 122.87 156 21 57.05% 3.84% Googe USE [CYK+18] 117.16 926 46 57.95% 4.21% Google USE Large [CYK+18] 95.06 726 63 60.74% 7.02% Infersent [CKS+17] 623.00 260 34 63.08% 10.0% TFIDF (Baseline) [Jon72] 25.97 533 58 62.85% 7.12% The graphs in figure 1 show the distances between duplicate and non-duplicate questions using different embedding systems. The X axis shows the euclidean distance between vectors and the Y axis frequency. A perfect result would be a blue peak to the left and an entirely disconnected orange spike to the right, show- ing that all non-duplicate questions have a greater eu- clidean distance than the least similar duplicate pair of questions. As can be clearly seen in the figure above, Elmo [PNI+18] and Infersent [CKS+17] show almost no separation and therefore cannot be considered good models for this problem. A much greater disparity is shown by the Google USE models [CYK+18], and even more for the Google USE Large model. In fact the Google USE Large achieved a F1 score of 0.71 for this task without any specific training, simply by choosing a threshold below which all sentence pairs are consid- ered duplicates. In order to test whether these results generalised to our domain, we devised a test that would make use of what little data we had to evaluate. We had no original data on whether sentences were semantically similar, but we did have a corpus of articles clustered into stories. Working on the assumption that similar claims would be more likely to be in the same story, we developed an equation to judge how well our corpus of sentences was clustered, rewarding clustering which matches the article clustering and the total number of claims clustered. The precise formula is given below, where Pos is the proportion of claims in clusters from one story cluster, Pcc is the proportion of claims in the correct claim cluster, where they are from the most common story cluster, and Nc is the number of claims placed in clusters. A,B and C are parameters to tune. ( A × Pos + B × Pcc ) × (C × Nc) Figure 2: Formula to assess the correctness of claim clusters based on article clusters This method is limited in how well it can represent the problem, but it can give indications as to a good or bad clustering method or embedding, and can act as a check that the findings we obtained from the Quora dataset will generalise to our domain. We ran code which vectorized 2,000 sentences and then used the DBScan clustering method [EKSX96] to cluster using a grid search to find the best  value, maximizing this formula. We used DBScan as it mirrored the cluster- ing method used to derive the original article clusters. The results for this experiment can be found in Ta- ble 3. We included TFIDF in the experiment as a baseline to judge other results. It is not suitable for our eventual purposes, but it the basis of the origi- nal keyword-based model used to build the clusters 2. That being said, TFIDF performs very well, with only Google USE Large and Infersent coming close in terms of ‘accuracy’. In the case of Infersent, this comes with the penalty of a much smaller number of claims in- cluded in the clusters. Google USE Large, however, clusters a greater number and for this reason we chose to use Google’s USE Large. 3 Since Google USE Large was the best-performing embedding in both the tests we devised, this was our chosen embedding to use for clustering. However as can be seen from the results shown above, this is not a perfect solution and the inaccuracy here will introduce inaccuracy further down the clustering pipeline. 3.3 Clustering Method We decided to follow a methodology upon the DBScan method of clustering [EKSX96]. DBScan considers all distances between pairs of points. If they are under  then those two are linked. Once the number of con- nected points exceeds a minimum size threshold, they are considered a cluster and all other points are consid- ered to be unclustered. This method is advantageous for our purposes because unlike other methods, such as K-Means, it does not require the number of clusters to be specified. To create a system that can build clus- ters dynamically, adding one point at a time, we set 2Described in the newslens paper [LH17] 3Google USE Large is the Transformer based model, found at, whereas Google USE uses a DAN architecture the minimum cluster size to one, meaning that every point is a member of a cluster. A potential disadvantage of this method is that be- cause points require only one connection to a cluster to join it, they may only be related to one point in the cluster, but be considered in the same cluster as all of them. In small examples this is not a problem as all points in the cluster should be very similar. How- ever as the number of points being considered grows, this behaviour raises the prospect of one or several borderline clustering decisions leading to massive clus- ters made from tenuous connections between genuine clusters. To mitigate this problem we used a method described in the Newslens paper [LH17] to solve a sim- ilar problem when clustering entire articles. We stored all of our claims in a graph with the connections be- tween them added when the distance between them was determined to be less than . To determine the final clusters we run a Louvain Community Detection [BGLL08] over this graph to split it into defined com- munities. This improved the compactness of a cluster. When clustering claims one by one, this algorithm can be performed on the connected subgraph featuring the new claim, to reduce the computation required. As this method involves distance calculations be- tween the claim being added and every existing claim, the time taken to add one claim will increase roughly linearly with respect to the number of previous claims. Through much optimization we have brought the com- putational time down to approximately 300ms per claim, which stays fairly static with respect to the number of previous claims. 4 Next Steps The clustering described above is heavily dependent on the embedding used. The rate of advances in this field has been rapid in recent years, but an embedding will always be an imperfect representation of an claim and therefore always an area of improvement. A do- main specific-embedding will likely offer a more accu- rate representation but creates problems with cluster- ing claims from different domains. They also require a huge amount of data to give a good model and that is not possible in all domains.","Adler, B., & Boscaini-Gilroy, G. (2019). Real-time claim detection from news articles and retrieval of semantically-similar factchecks. arXiv preprint arXiv:1907.02030."
"ML_DB_145","Toward Automated Factchecking: Developing an Annotation Schema and Benchmark for Consistent Automated Claim Detection","In an effort to assist factcheckers in the process of factchecking, we tackle the claim detection task, one of the necessary stages prior to determining the veracity of a claim. It consists of identifying the set of sentences, out of a long text, deemed capable of being factchecked. This article is a collaborative work between Full Fact, an independent factchecking charity, and academic partners. Leveraging the expertise of professional factcheckers, we develop an annotation schema and a benchmark for automated claim detection that is more consistent across time, topics, and annotators than are previous approaches. Our annotation schema has been used to crowdsource the annotation of a dataset with sentences from UK political TV shows. We introduce an approach based on universal sentence representations to perform the classification, achieving an F1 score of 0.83, with over 5% relative improvement over the state-of-the-art methods ClaimBuster and ClaimRank. The system was deployed in production and received positive user feedback.","Information Science","Article","2021","Y","Y","Tool","Support","Claim detection","70","Misinformation has recently become more central in public discourse [6, 37, 46]. As a consequence, interest has increased in the scientific community to further natural language processing (NLP) approaches that can help alleviate the burdensome and time-consuming human activity of factchecking [38, 40]. Factchecking is known as the task of producing an informed assessment of the veracity of a claim [13, 14]. The main mission of factcheckers is to give citizens information to make political choices, improve the quality of public political discourse, and to hold politicians accountable [14]. Misinformation and misperceptions can undermine this goal [9, 11]. However, there is a very small number of factchecking organisations in the world, about 160, compared to the volume of media items produced daily. The speed at which information flows online means there is less time to verify the claims made and myths spread further before being factchecked, if they are factchecked at all. Automating any parts of the factchecking process could cut down the time it takes to respond to a claim. It could also protect human factchecker's time to work on the more complicated checks that need careful human judgement. When considered inside a factchecking organisation, the factchecking process consists of a series of tasks, which Reference [3] defined as a process consisting of four tasks: (1) monitor media, i.e., capturing content such as articles, videos, images, (2) detect claims, i.e., spotting when an item contains a checkable claim, (3) check claims, i.e., doing the research to check and verify that claim, and (4) publish, i.e., creating a piece of content that encapsulates the results of the check. The vast majority of the scientific research has focused on the determination of veracity [20, 39, 45] or checking claims; despite often having been given different names in the scientific literature, such as fake news detection [26, 41, 44] or arguably rumour detection [2, 24, 27, 43]. The component checking the veracity of a claim cannot function in isolation, it needs a list of claims to check as an input. The majority of previous work on factchecking has started from a readily available list of claims and has omitted the previous steps of monitoring and spotting in the factchecking pipeline [20, 40]. This act of spotting claims within a corpus is known as claim detection, i.e., monitoring news sources and identifying if a sentence constitutes a claim that could be factchecked [18, 21]. Monitoring and spotting within a factchecking organisation is a time consuming semi-manual task, which is inevitably limited by the resources available. Automating the process of claim detection could mean that factcheckers can monitor a greater set of media, extract the claims made, and hopefully make smarter choices about what the most valuable items to be checking that day are. If deployed in a live factchecking situation, then it could also help separate out claims made in real-time during a ministerial speech, for example. This could help factcheckers quickly skim transcripts when time is limited. Despite the importance of this component in the factchecking pipeline, automation is still in its infancy and there is a dearth of scientific literature. Previous works on claim detection [12, 18] rely on definitions of claims that incorporate the concept of check- worthiness and/or importance. Our objective is instead to avoid these subjective concepts to come up with an objective way of determining what constitutes a claim that is checkable; rather than being important or worthy of checking, which is bound to subjective interpretation. This article describes the iterative process we followed together with factcheckers to come with up an annotation schema that would effectively capture claims and non- claims. This annotation schema avoids factors that can be affected by personal biases, such as importance, in the manual annotation to produce an objective outcome. Following this annotation schema through a crowdsourcing methodology, we generated a dataset of 5,571 sentences labelled as claims or non-claims. Further, we set out to present the development of the first stage in the automated factchecking pipeline. It constitutes the first automated claim detection system developed by an independent factchecking charity, Full Fact, along with academic partners. The main contributions of our work are as follows: 2 RELATED WORK Most work around automatically factchecking claims has focused on the later stages of determining the veracity of claims, usually by building knowledge graphs out of knowledge bases, such as Wikidata [5, 7, 35, 36, 42]. Less work has been documented on the preceding stage of claim detection. One of the best-known approaches to claim detection is ClaimBuster [18]. They collected a large annotated corpus of televised debates in the USA. Their model combines Term Frequency-Inverse Document Frequency (TF-IDF), part-of-speech (POS) tags, and named entity recognition (NER) features on an Support Vector Machines (SVM) classifier and produces a score of how important a claim is to factcheck. This has the caveat of then having to choose a cut-off score to determine the claims that will be considered worthy or important enough for factchecking. Our approach is instead to define an annotation schema that is binary, determining checkable claims rather than check-worthy claims, and is built on several types of claims. It is a simpler fit for the use case in this factchecking pipeline, i.e., in a live stream of subtitles, we are unable to know in advance which sentence will make it to the top ranking until the end of the entire programme. In Reference [15] annotations were collected using ClaimBuster-inspired annotation guidance from volunteers together with their age, gender and education. With possible labels being verifiable check-worthy (VCW), verifiable not check-worthy (VNCW), and not verifiable (NV), they obtained 2,100 labelled sentences (reduced to 264 high-quality labelled sentences). They find that annotators with a natural sciences background agreed internally about what constitutes a check-worthy claim, whereas those with humanities, medicine, and ontology backgrounds saw more internal disagreement on check-worthiness. Furthermore, using 35 labelled control claims to test annotator skill, they find that the age group 40–49 obtains a higher average score, and label more claims, than that of 30–39, which correspondingly scores higher than age group 20–29. Another recent approach to claim detection is ClaimRank [12]. They compiled a dataset by taking the outcome of factchecking a political debate, published by nine organisations simultaneously. Models were created to predict if the claim would be highlighted by at least one or by a specific organisation. The modelling is done with a large variety of features from both the individual sentence and the wider context around it. A subsequent version of the dataset [21, 29] includes a larger set of sentences in two languages, English and Arabic. These datasets are similar to ours in order of magnitude, however, use a different definition of claim, as is the case with others tackling the determination of check-worthiness of claims [4, 17, 31, 47]. We further elaborate on this in the next subsection. A slightly different approach to claim detection is that of context-dependent claim detection (CDCD) [25]. This study proposes identifying claims given a specific context. Articles relevant to a topic are used to detect claims on that topic. Another piece of work worth mentioning is the development of the FEVER (Fact Extraction and VERification) dataset [39]. Whilst this is primarily aimed at work regarding claim veracity, the mere presence of a vast quantity of claims in the dataset allow it to be extended for claim detection in the future. 2.1 Previous Attempts at Defining Claims 1 2 • We introduce the first annotation schema for claim detection, iteratively developed by experts at Full Fact, comprising seven different labels. • We describe a crowdsourcing methodology that enabled us to collect a dataset with 5,571 sentences labelled according to this schema. • We develop a claim detection system that leverages universal sentence representations, as opposed to previous work that was limited to word-level representations. Our experiments show that our claim detection system outperforms the state-of-the-art claim detection systems, ClaimBuster and ClaimRank. • With the annotation schema, crowdsourcing methodology and task definition, we set forth a benchmark methodology for further development of claim detection systems. Toward Automated Factchecking: Developing an Annotation Schema and Benchmark f... 1 sur 7 19-05-22, 13:11 There is a body of work on claim detection that has not formalised the definition of a claim, e.g., [12, 21]. Instead it directly relies on what has been identified by external organisations. The lack of a formal definition prevents others from replicating or extending their work. These studies used claims identified by nine organisations in a political speech as a proxy. The annotations were sourced from publicly available online articles. This is different from our approach, where we crowdsourced annotations following our definition of the task. The authors of Reference [12] acknowledge this limitation, which led to high number of false positives in their experiments. For example, an article consisting of a debate transcript with editorial comments will not highlight repeated instances of claims. This creates inconsistent annotation—during a TV debate, popular claims are discussed on repeated occasions. This had to be re-annotated by researchers in Reference [29]. Another caveat is that only three of the nine annotator organisations contributing to those online articles sign up to be neutral and transparent in their selection of claims as verified signatories of the International Factchecking Network's Code of Principles. ClaimBuster [18] provides a definition of a claim that revolves around the question: “Will the general public be interested in knowing whether this sentence is true or false?” Claims are considered to be those sentences for which the answer to this question is yes. Their aim was for anyone to be able to feed in a source, e.g., a political speech, and for the system to produce a list of claims ranked by importance, which could directly feed into the editorial process. This definition of a claim includes the judgement of “importance,” which we avoid in our work. We believe it is an editorial judgement best left to factcheckers. ClaimBuster annotators were journalists, students and professors. Annotations that agreed with the authors of that study were selected to ensure good agreement and shared understanding of the assumptions. Researchers from the ClaimBuster team also defined an annotation schema called PolitiTax, a taxonomy of political claims that we considered. However, the categories were not useful for the downstream task of checking the veracity of the claim by routing it to the right dataset or team at Full Fact, in part, due to the level of granularity in the taxonomy and, in part, because the team at Full Fact is split across topics. There was also a taxonomy defined by factcheckers during the HeroX factchecking challenge [10], which is less granular than PolitiTax. It has four claim types—numerical, political stance, quotes, objects. During this work, we discovered that the latter three categories are rare and intersect with others, so we did not use them in our schema. 3 DATASET 3.1 Our Claim Definition and Process Writing the annotation guidance was a long process. Full Fact's formal definition of a claim during the 2015 UK election, was “an assertion about the world that can be checked.” Media monitoring volunteers were encouraged to ask a factchecker if they had doubts on whether something was assessable. We worked on codifying some of this thinking in conversations with the factcheckers. However, as we captured more and more claims, this definition proved insufficient. We wanted to understand whether a claim could be better defined by breaking it down into sub-categories for better consistency across time, topics and annotators. We opted for defining a typology that would capture the different types of claims, which would be more comprehensive than the previous short definition. Asking annotators to identify the category that a sentence belongs to would encourage more critical thinking. Likewise, choosing the right category for a sentence would significantly reduce the personal bias with respect to judging whether it is a claim, which is bound to personal judgement. Annotators would choose the category pertaining to a sentence, and we would then simplify the schema by mapping those types to binary labels, claim or not a claim. We chose to decouple the importance of the claim from the claim itself. We felt that importance was heavily subjective, reliant on context and best left to factcheckers. Importance is a subtle, and forever changing feature. Even though the most “important” issues in the view of the UK public are often about the economy, immigration and health, their relative positions change. In some cases new issues become important, e.g., in the UK, the importance of claims about the EU increased significantly after the 2016 EU referendum. We also chose to decouple the topic from the definition. By making our definition descriptive of the claim and not, by proxy, the topic, we would have a more consistent final dataset. In some cases the selection of topics is an inherently political choice, e.g., it varies across the population whether “drugs” relate to the topic of “crime” or “health.” This kind of classification was avoided. To come up with the schema that would capture what constitutes a claim, we followed an iterative process. In the first step, factcheckers identified sentences that were definitely not a claim. They iterated on potential rules and found examples that broke them. They also identified some constraints, for example, a claim needs to be checkable with more readily available evidence, which means that a personal claim like “I woke up at 7 a.m. today” is not a claim capable of being checked. We were most concretely able to exclude claims based on an individual's personal experience, as more often than not they were un-checkable. This is similar to ‘verifiable experiential’ statements [30]. We went through several versions of the guidance with different taxonomies. They were trialled within Full Fact, and then two versions with external volunteers. The first version applied the 2015 thinking and was a binary accept/reject classification task, accompanied by a guidance. It listed several types of qualities of claims and non- claims. Claims, for example, may be explicit, implicit, or trivial. Non-claims in this version were formed of personal experience and opinion. We decided against these categories in the end as they sometimes involve explicit judgements from our annotators – these choices can sometimes be highly political. For example, in the case of “The EU is made up of 27 [instead of 28] countries” or “The NHS is there for everyone” some annotators could classify them as trivial while others might consider them explicit legal claims. The implicit/explicit categories were also removed, because whether the claim is implicit or explicit is not important for the next downstream task in the factchecking process after claim detection. For the second version, we looked at Full Fact's factchecks. They mostly covered statistical claims. We also identified claims around current laws or rules of operation and correlation/causation claims, e.g., “there's no clear correlation between prisons’ performance ratings and whether they're publicly-run or contracted out to the private sector.” This became the basis of our claim categories. Merging these categories and removing personal experience was deemed to be a good proxy for claims. There were many other types of claims that we identified, such as definitions, voting records, and expressions of support. We limited our categories to seven to make the task realistic for annotators. We also wanted to minimise the overlap between categories to make the task single- choice. 3.2 Annotation Guidance Our annotation schema is the first to be created with a factchecking organisation. It comprises seven categories, only one of which can be assigned to each sentence. Annotators were given definitions and examples of the seven categories, including the more detailed breakdown shown in Table 1: Table 1. Breakdown of the 4,080 Sentences with Majority Agreement Category Subcategory Counts Example Not a claim 54.8% “Give it all to them, I really don't mind.” Other Other other 10.4%* “Molly gives so much of who she is away throughout the film.” Support/policy 5.5%* “He has advocated for a junk food tax.” Quote 4.7%* “The Brexit secretary said he would guarantee free movement of bankers.” Trivial claim 1.6%* “It was a close call.” Voting record 0.7%* “She just lost a parliamentary vote.” Public opinion 0.4%* “A poll showed that most people who voted Brexit were concerned with immigration.” Definition 0.0%* “The unemployed are only those actively looking for work.” Quantity Current value 9.9% “1 in 4 people wait longer than 6 weeks to see a doctor.” Changing quantity Comparison Ranking Prediction Hypothetical statements 4.4% “The IFS says that school funding will have fallen by 5% by 2019.”Claims about the future Personal experience Uncheckable 3.0% “I can't save for a deposit.” Correlation/causation Correlation 2.6% “Tetanus vaccine causes infertility.” Causation Absence of a link Laws/rules of operation Public institutional procedures 1.9% “The UK allows a single adult to care for fewer children than other European countries.” Rules/rule changes *The proportions for “Other” sub-categories are taken from a random sample of 160 claims labelled as “Other.” 3 4 5 6 • Personal experience. Claims that are not capable of being checked using publicly available information, e.g., “I can't save for a deposit.” • Quantity in the past or present. Current value of something, e.g., “One in four wait longer than 6 weeks to be seen by a doctor.” Changing quantity, e.g., “The Coalition Government has created 1,000 jobs for every day it's been in office.” Comparison, e.g., “Free schools are outperforming state schools.” Ranking, e.g., “The UK's the largest importer from the Eurozone.” Toward Automated Factchecking: Developing an Annotation Schema and Benchmark f... 2 sur 7 19-05-22, 13:11 These categories have proven to broadly cover sentences from political TV shows that Full Fact has encountered over several years. Categories have different levels of occurrence (see Table 1). As previously found [15, 18, 21], “Not a claim” is the most popular category, amounting to about 55% of the annotations. “Other” is the second largest category with 952 instances, 23% of the whole. It can be broken down into claims that are less well-defined, with formal sub-categories being: “Definitions,” “Voting records,” “Public opinion,” “Trivial claim,” “Support,” “Quote,” “Other other.” We amalgamate these, because they are likely to overlap, and we wanted our annotators to only select one option. For example, “She said she voted to keep free school meals.” is both a quote and a voting record. Furthermore, high granularity of categories allows one to perpetually think of rarer categories and a high number of categories slows down annotation unnecessarily. To verify if our level of granularity was correct, we split a sample of 160 sentences in “Other” into sub-categories. The vast majority are in the “Other other” category (see Table 1), supporting our chosen level of granularity. 3.3 Crowdsourced Annotation The annotations were done by 80 volunteers recruited through Full Fact's newsletter—this meant that volunteers were keen on factchecking. 28,100 annotations were collected for a set of 6,304 sentences extracted from subtitles of four UK political TV shows, 14 episodes in total. TV subtitles were chosen, because 69% of the UK population get their news from TV. The software used for collecting annotations was Prodigy, a self-hosted annotation platform. It was customised to support multiple annotators with a login and password screen where each user would enter their credentials. Sentences were shown in random order. The preceding two sentences were also shown on the screen to provide context and assist with potential co-references. Once a sentence was annotated five times by different annotators, it was not shown again. Annotators were encouraged to contact us for any clarifications needed, with thoughtful questions such as: “Where it appears that a claim is dressed up as rhetorical question, should we classify it as a claim? For example, ‘Why should unelected officials in Brussels make rules to stop bananas being sold in bunches of more than 2 or 3?’” To answer this, questions are classified as the claims that they implicitly contain. 3.4 Agreement At the level of all seven granular categories the inter-annotator agreement is moderate, with a Krippendorff's alpha [23] of 0.46. However, we attain higher values of alpha of 0.70 and 0.53 when we do the mapping of annotations into the binary claim/non-claim annotation task, following either of the two methods shown in Table 3. Most of the disagreement was between “Not a claim” and “Other claim.” This showed that it is hard to define the boundary and explicitly list all kinds of claims, as we saw in the process of creating the annotation guidance. The disagreements across all sentence types can be seen in Table 2. Table 2. Annotation Disagreements Qu 12 Corr 10 10 Law 2 19 11 Pred 3 42 27 25 Other 50 102 129 87 87 Not 114 58 90 69 103 668 Pers Qu Corr Law Pred Other The most prominent disagreement is between “Other claim” and “Not a claim.” The labels are shortened versions of those in Figure 1 due to space limitations. (Qu: quantity, Corr: correlation and causation, Pred: predictions, Pers: personal experience.) For mapping the seven categories in the schema into the binary classification task distinguishing claims and non- claims, two different mappings were initially proposed (see Table 3). These two methods were proposed by first assuming the two extremes, i.e., Quantity (2) should be deemed a claim, whereas Not a claim (7) belong to non- claim. Subsequently, these two reformulations of the taxonomy were proposed after meeting and brainstorming with factcheckers. While the method in the first row would be reasonable for achieving higher inter-annotator agreement, it would lead to a classification performance prioritising high precision at the expense of a lower recall. The method in the second row was ultimately selected for further experimentation, as recall was deemed important by factcheckers and it is in turn more realistic for not omitting any of the seven initial categories. Hence, we moved on to evaluate our claim detection system on the labels originating from the second row of Table 3. Here, “Other type of claim” is not in the positive class for two reasons. First, there is a lot of disagreement between it and the “Not a claim” class. Second, the kinds of claim in the “Other” section—voting records, quotes, statements about public opinion polls, are less frequently written about by Full Fact. Table 3. From Seven Categories to Binary Claim vs. “Not a Claim” Classification Claim Non-claim Omitted N 2 3, 4, 6, 7 1, 5 0.70 6,095 2, 3, 4, 5 1, 6, 7 – 0.53 4,777 N = number of sentences annotated by majority. (1) Personal experience, (2) Quantity in the past or present, (3) Correlation or causation, (4) Current laws or rules of operation, (5) Prediction, (6) Other type of claim, (7) Not a claim. The agreement of 60% is still low—that is a lot of sentences to throw away if we were only to consider agreement among all the annotators. So, instead, we choose a majority vote where at least three annotators marked the sentence and more than half of them agree. Of the initial 6,304 sentences, this filter selects 4,777 sentences, 3,973 not claims, and 804 claims. This is in line with previous studies where the proportion of claims is 10–30% in political TV [12, 18]. As extra training data, we add 794 claims from the Full Fact database. Out of them 766 are annotated by us as positive, because they fall into our claim categories, for example “The courts have said that the so-called ‘bedroom tax’ is illegal.” The remaining 28 are in the “Other type of claim” category, for example, “The British economy is not only getting better, it is healing.” If we keep the seven categories in the dataset, instead of mapping them into the two classes, then the same method based on majority votes leads to a slightly smaller dataset with 4,080 sentences, i.e., due to the slightly lower agreement on the broader set of seven categories. 4 METHODS To capture the diversity of sentences observed during political TV shows, we propose to leverage universal sentence representations. We use InferSent [8] as a method to achieve sentence embeddings. These embeddings are different from averaging word embeddings, because they take word order into account using a recurrent neural network. The method provided by InferSent involves words being converted to their common crawl GloVe implementations before being passed through a bidirectional long-short-term memory (BiLSTM) network [19]. The sentence embeddings were pre-trained on a large dataset of Natural Language Inference tasks. Additionally, we also tried concatenating POS and NER information to the embeddings. For each sentence, the POS/NER feature vector was the count of each POS/NER tag in the corpus. We input our sentence representations to a range of supervised classifiers implemented using scikit-learn [32], with the classifiers set to their default parameters. The four classifiers we tested include Logistic Regression, Linear SVM, Gaussian Naïve Bayes and Random Forests, all of which use the default parameters provided with scikit-learn. We use a number of other features as baselines: • Correlation or causation, Correlation, e.g., “GCSEs are a better predictor than AS if a student will get a good degree.” Causation, e.g., “Tetanus vaccine causes infertility.” Absence of a link, e.g., “Grammar schools don't aid social mobility.” • Current laws or rules of operation, Declarative sentences, which generally have the word ”must” or legal terms, e.g., “The UK allows a single adult to care for fewer children than other European countries.” Procedures of public institutions, e.g., “Local decisions about commissioning services are now taken by organisations that are led by clinicians.” Rules and changes, e.g., “EU residents cannot claim Jobseeker's Allowance if they have been in the country for 6 months and have not been able to find work.” • Prediction, Hypothetical claims about the future, e.g., “Indeed, the IFS says that school funding will have fallen by 5% in real terms by 2019 as a result of government policies.” • Other type of claim, Voting records, e.g., “You voted to leave, didn't you?” Public Opinion e.g “Public satisfaction with the NHS in Wales is lower than it is in England.” Support, e.g., “The party promised free childcare” Definitions, e.g., “Illegal killing of people is what's known as murder.” Any other sentence that you think is a claim. • Not a claim, These are sentences that do not fall into any categories and are not claims, e.g., “What do you think?” “Questions to the Prime Minister!” 7 8 9 , 10 α 11 (1) A number of variants of the state-of-the-art claim detection system by ClaimBuster, using different combinations of TF-IDF, POS and NER features, as in Reference [18]. (2) Averaging pre-trained word embedding vectors for all words in a sentence. We evaluate: • Word2vec [28] via the Gensim implementation [34], using the GoogleNews embedding. • GloVe [33] trained on Common Crawl, as well as combining them with dimensionality reduction using principal component analysis (PCA). (3) TF-IDF representations of sentences with logistic regression. Numbers have a significant role in claims—the “Cardinal Number” part-of-speech tag is the second most discriminating feature in Toward Automated Factchecking: Developing an Annotation Schema and Benchmark f.. 3 sur 7 19-05-22, 13:11 For our implementation of the ClaimBuster system, we use the Watson Natural Language Understanding API, the updated version of the Alchemy API used by the original authors. All other features were implemented as outlined in Reference [18]. The ClaimRank [12] model was harder to re-implement. To maintain impartiality Full Fact cannot use the data on sentence speakers when selecting which claims to check. This special care is also encouraged in the computer science research for systems that are integrated into the infrastructure of society by the ACM code of ethics [1]. Additionally, our dataset did not have data on applause, laughing, or speaker crossover. Even though we unfortunately could not use one third of ClaimRank's features, we trained the FNN, SVM and logistic regression classifiers on the remaining features in our dataset [12]. 4.1 Experiment Settings The dataset consists of 5,571 sentences (4,777 from annotations and 794 from the Full Fact database of claims), of which 1,570 are claims and 4,001 are not claims, which gives a 30/70 class imbalance. We use stratified fivefold cross-validation to train and test our models. We use precision, recall and F -score measures to assess classifier performance. We show the best-performing classifier for any given feature set. We also show 95% confidence interval for the precision and recall using binomial distributions. This demonstrates possible overlap in results between different models. The interval is wide for recall due to the small number of positive examples. The next section will present the results of applying these methods. 5 CLAIM DETECTION Here, we present results for the binary classification, using the class grouping shown in the second row of Table 3. 5.1 Analysis of Results Table 4 shows the results of our experiments. Note that in this task recall is especially important as factcheckers do not want to miss out important claims, and hence our priority is to maximise recall while also keeping a good balance of precision and recall as measured by the F1 score. Interestingly, the simple approach of TF-IDF achieves high precision but low recall. We call our new model “CNC,” which stands for “Claim/No Claim.” It achieves a better balance of precision and recall; logistic regression classifier gives the highest overall F1 score of 0.83, outperforming all other techniques. In the interest of space and clarity, we do not include the results of the other classifiers (i.e., Linear SVM, Gaussian Naïve Bayes, and Random Forests). The use of POS and NER features in our model has no effect on the performance. GloVe embeddings achieve performance close to our method with F1 scores 2% lower and substantially lower recall scores. Despite the overlap in precision scores between GloVe and our method, the overlap is minimal in terms of recall. Our CNC model also clearly outperforms the state-of-the-art method by ClaimBuster at 0.79 F1; hence our method yields F1 scores that improve ClaimBuster by over 5% in relative terms. ClaimBuster performs similarly to CNC in terms of precision, albeit with substantially lower recall scores. ClaimRank has the best precision scores across the board, but with the lower recall scores. CNC achieves a 6% relative improvement in F1-score over ClaimRank. Table 4. Results for the Claim/No Claim Experiments Features Classifier P R F P-interval R-interval TF-IDF LogReg 0.90 0.59 0.70 0.89–0.91 0.56–0.61 TF-IDF+num. preproc. LogReg 0.91 0.59 0.70 0.90–0.92 0.56–0.61 Word2Vec SVM 0.85 0.75 0.78 0.84–0.86 0.73–0.77 GloVe LogReg 0.89 0.76 0.81 0.88–0.90 0.74–0.78 GloVe+PCA LogReg 0.89 0.75 0.81 0.88–0.90 0.73–0.77 CB LogReg 0.90 0.59 0.70 0.89–0.91 0.56–0.61 CB +POS LogReg 0.88 0.68 0.76 0.86–0.89 0.66–0.71 CB +NER LogReg 0.88 0.60 0.71 0.87–0.89 0.58–0.63 CB +POS+NER LogReg 0.87 0.71 0.78 0.86–0.88 0.68–0.73 CB SVM 0.84 0.70 0.76 0.83–0.85 0.69–0.73 CB +POS SVM 0.86 0.74 0.79 0.85–0.87 0.72–0.76 CB +NER SVM 0.84 0.71 0.77 0.83–0.85 0.69–0.73 CB +POS+NER SVM 0.86 0.75 0.79 0.85–0.87 0.73–0.77 ClaimRank LogReg 0.93 0.65 0.77 0.92–0.94 0.63–0.67 ClaimRank SVM 0.93 0.53 0.67 0.92–0.94 0.51–0.55 ClaimRank FNN 0.89 0.61 0.72 0.87–0.91 0.58–0.","Konstantinovskiy, L., Price, O., Babakar, M., & Zubiaga, A. (2021). Toward automated factchecking: Developing an annotation schema and benchmark for consistent automated claim detection. Digital threats: research and practice, 2(2), 1-16."
"SCOP_004","A Prototype Web Application to Support Human-Centered Audiovisual Content Authentication and Crowdsourcing","Media authentication relies on the detection of inconsistencies that may indicate malicious editing in audio and video files. Traditionally, authentication processes are performed by forensics professionals using dedicated tools. There is rich research on the automation of this procedure, but the results do not yet guarantee the feasibility of providing automated tools. In the current approach, a computer-supported toolbox is presented, providing online functionality for assisting technically inexperienced users (journalists or the public) to investigate visually the consistency of audio streams. Several algorithms based on previous research have been incorporated on the backend of the proposed system, including a novel CNN model that performs a Signal-to-Reverberation-Ratio (SRR) estimation with a mean square error of 2.9%. The user can access the web application online through a web browser. After providing an audio/video file or a YouTube link, the application returns as output a set of interactive visualizations that can allow the user to investigate the authenticity of the file. The visualizations are generated based on the outcomes of Digital Signal Processing and machine learning models. The files are stored in a database, along with their analysis results and annotation. Following a crowdsourcing methodology, users are allowed to contribute by annotating files from the dataset concerning their authenticity. The evaluation version of the web application is publicly available online.","Computer Science","Article","2022","Y","Y","Prototype","Experimental","Multimedia forensics","0","News authentication is considered a vital task for reliable informational services. The COVID-19 pandemic situation that we currently experience showcased the importance of fact-checking in fighting disinformation to protect our societies and democracies. The role of audiovisual recording is considered crucial in documenting news articles, thus convincing audiences about the truth of the underlying events [1– 3]. With the advancement of Information and Communication Technologies and the availability of easy-to-use editing and processing tools, one unwanted side-effect is the falsification of multimedia assets (i.e., images, audio, video) to alter the presented stories, making them more appealing (or intentionally doctored). In this context, unimodal solutions have been implemented to inspect each of the individual media entities, while multimodal forensic services are also deployed through online collaborative environments, plug-ins, serious games, and gamification components [1,2,4,5]. While the detection of manipulated photos/images and the evaluation of the associ- ated forgery attacks remain critical [ 6], audio and video content have become even more popular nowadays. In this context, audio offers some unique features, such as less demand- ing processing needs and the inherent time continuity, making tampering inconsistencies easier to reveal [ 7 ,8]. Semantic processing and machine learning technologies empowerFuture Internet 2022, 14, 75 2 of 17 today’s digital forensics tools. However, these new capabilities can also be exploited for counter-/anti-forensic means, requiring constant and continuous effort. 1.1. Related Work Content verification has always been a very important part of journalistic workflows and a crucial factor of journalistic ethics and deontology. In the context of Journalism 3.0, where new business models of low or no pay journalism, combined with news aggregation and republishing and the reuse of amateur user-generated content (UGC) [9], disinforma- tion has become a major problem for journalistic practice. As a result, several fact-checking organizations have appeared in the past decade, intending to find and debunk false claims that are spread throughout the Web and social media services [ 10]. Recent research of academics and organizations has been directed towards highlighting the best practices for content verification, through international cooperation networks [11]. In the modern media ecosystem, data variety is a very important parameter of big data volumes [12 ]. This means that fact-checkers need to manage content in many different modalities (e.g., text, audio, image, video). Different approaches and methodologies have to be defined for each case [ 13 ]. In disinformation, media assets may be used in a misleading context to support a false claim, or may be manipulated themselves. In the first case, an image/audio/video file is followed by an untrue description or conclusion, while in the latter, the media file has been maliciously edited. Such manipulations may include actions, such as copying and moving parts of the file to a different place and splicing in segments of a different file, aiming at affecting the semantic meaning of the file [14 ]. Common cases can be found in all file types, whether image, audio, or video. In the case of image tampering detection, spatial techniques can be used to locate suspicious regions and discontinuities within an image file. Media Verification Assistant is a project that allows users to upload images and applies several algorithms to provide forensics analysis [14 ,15 ]. In contrast to static images, audio and video files introduce the dimension of time. In video files, besides the spatial analysis of single image frames, the detection of temporal discontinuities can be crucial for the spatiotemporal location of malicious tampering [ 16 ]. Such techniques are expected to be computationally heavy. Audio is a very important modality present in the majority of video files. In this sense, audio can be used autonomously for the authentication of both audio and video assets. Audio information retrieval techniques are much less computationally complex. Audio forensics tools are not, however, as well-explored as those applied to visual information. Two important toolboxes on the market are the ARGO-FAAS [ 17] and the EdiTracker plugin [ 1]. They are, however, paid services, and not publicly available. Audio forensics techniques address the processes of audio enhancement, restoration, and authentication of an audio asset so that it can be considered as evidence in court [ 18, 19 ]. Authentication techniques aim at detecting artifacts within an audio file that can indicate malicious editing. Traceable edits can be found in the file container information or in the audio content [ 20 ,21]. Techniques that inspect file container inconsistencies investigate the metadata, descriptors, or the encoding structure. When the audio content is investigated, the aim is to use dedicated software to detect certain artifacts that may be inaudible by human subjects. Several different approaches can be found in the literature. Electronic Network Frequency (ENF) techniques make use of the phenomenon of the unintentional recording of an ENF through interference. Electronic networks provide alternating current with a nominal frequency of 50 or 60 Hz, depending on the region. However, the real frequency of the current fluctuates around this value. The electronic equipment that is used for recordings captures this frequency fluctuation, which can act as a timestamp of the recording. It is possible to isolate and track the ENF in recordings to check whether there is phase inconsistency in the fluctuation, or even to find the exact time of the recording from the log files of the electronic networks [22–25]. Other approaches investigate the acoustic environment of the recording, such as the Signal-to-Reverberation ratio of a room [ 26 , 27]. The specifications of a recording device Future Internet 2022, 14, 75 3 of 17 have also been proven to be traceable in research, providing an indicator of whether parts of an audio file were recorded with a different device [ 20 , 28– 30]. Dynamic Acoustic Envi- ronment Identification (AEI) may rely on statistical techniques that rely on reverberation and background noise variance in a recording [ 31 ]. Machine learning techniques are proven to be very useful for acoustic environment identification. Machine learning models do not rely on the definition of a set of rules for decision-making, but require a dataset of pre-annotated samples to train a classification model that can identify different classes, in this case, acoustic environments [31–33]. Another methodology for audio tampering detection investigates file encoding and compression characteristics. A huge number of highly configurable audio encodings are available that differ in terms of compression ratio, bitrate, use of low-pass filters, and more. A file that comes from audio splicing is very likely to contain segments encoded with different configurations, which can be traceable [ 34– 36 ]. Most encoding schemes depend on psychoacoustic models that apply algorithms to discard redundant, inaudible frequencies. The Modified Discrete Cosine Transform (MDCT) coefficients can be investigated using statistical or machine learning methods to detect outliers in specific segments of the file [37 ]. Even when the file is reencoded in another format, there are often traces of the effect of previous compression algorithms [38–41]. Media authentication can be supported during content production using container and watermarking techniques, such as hash-code generation and encryption, and MAC times- tamp embedding. Recovery of the inserted hash code that was generated by algorithms, such as SHA-512, enables the detection of tampered points within an audio stream [42 ]. Similarly, embedding timestamp information in files can allow the identification of an audio excerpt with a different MAC timestamp that has been maliciously inserted [20]. Whether the aim is training machine learning models or evaluating proposed analysis methods, one crucial part of every audio authentication project is the formation of a dataset. This is a very complex procedure due to the task’s peculiarities, and it often acts as a bottleneck for the robustness of such techniques. Not many datasets are available for experimentation. In [43 ], a dataset was recorded featuring different speakers, acoustic rooms, and recording devices. In [44 ] a dataset with different encodings was created through an automated process. In [ 45 ], existing recordings were edited to create a dataset. In [7], an automated process was proposed for the creation of a multi-purpose dataset using an initial set of source files provided by the user. 1.2. Project Motivation and Research Objectives It has been made clear that machine learning solutions for audio tampering detection require a dataset for the training of models. Since datasets with real cases of tampered files are not available, most works require the formulation of artificial datasets for model evalua- tion. Such datasets are often difficult to handcraft, so they follow automated procedures for dataset creation, simulating real-world scenarios. As a result, the implemented models are case- and dataset-specific. There is no evidence for the generalization of the models in mul- tiple scenarios and tampering techniques. For this reason, it is not yet feasible to integrate automated audio authentication into professional workflows without supervision, as they cannot be considered reliable for production and real-world applications. Furthermore, models that are pre-trained in known datasets and conditions may be more vulnerable to adversarial attacks [46]. On the other hand, traditional audio forensics techniques require expertise and fluency with audio analysis tools. In such an approach, human intelligence and experience play a crucial role in the process of authentication. While this is the most reliable solution and the preferable option in courtrooms, it cannot provide a viable alternative with massive appeal. There is an urgent need for tools that can help in the fight against disinformation. Such tools should be accessible to a broad audience of journalists, content creators, and simple users, to improve the overall quality of news reporting. Average users do not have the expertise to apply audio analysis techniques in the same way as professionals of audio forensics. Future Internet 2022, 14, 75 4 of 17 The motivation for the current research emerges from the hypothesis that it is feasible to strengthen a user’s ability to recognize tampered multimedia content using a toolbox of supervisory tools provided online through an easy-to-use interface. State-of-the-art approaches for audio analysis and tampering detection were integrated into a web ap- plication. The application is available publicly through a web browser. The results of the algorithms do not provide an automated decision-making scheme, but rather a set of visualizations that can assist the user in a semi-automated approach. This means that the framework does not include a model that performs binary classification of files as tampered, or not-tampered, but the final decision is the responsibility of the user, taking advantage of their perception and experience as well as the context of the media asset. Through the use of the application, crowdsourcing is promoted for the creation of a dataset with real-world tampered files for future use. The remaining of the paper is structured as follows. In Section 2, the proposed web framework is presented, in terms of the functionality, aims, and technical specifications. The integrated algorithms and their operating principles are listed without emphasizing technical details. In Section 3, the evaluation results from the reverberation estimation models and the initial implementation of the prototype web application are presented. In Section 4, the research results are summarized and discussed, and the future research goals of the project are defined. In Section 5, some of the limitations of the presented research are analyzed. 2. Materials and Methods As stated in the problem definition section, the proposed approach consists of a frame- work for the assistance of professional journalists and the public in detecting tampered audiovisual content. The core of the framework is a web application with a graphic user interface provided to the public for the submission and analysis of content. The application incorporates an ensemble of algorithms that provide the user with supervisory tools for semi-automatic decision-making. The analysis strategy is audio-driven, as it makes use of the audio channel. The integrated algorithms do not classify files as tampered or not, but rather support the users in decision-making. The application offers the necessary crowdsourcing functionality for dataset creation and user cooperation. The framework was designed and implemented as a component of the Media Authentication Education (MAthE) project, which aims at providing educational and gamification tools to battle misinformation [4]. 2.1. A Web Application for Audio Tampering Detection and Crowdsourcing The main goal of the web application is to combine the effectiveness of state-of-the-art signal processing, machine learning advances and human perception for computer-assisted audio authentication. The application: 1. Implements state-of-the-art analysis options. An ensemble of algorithms is incorpo- rated, addressing multiple audio tampering strategies. Such strategies may include encoding detection, recording conditions, background noise clustering, and others. 2. Follows a modular approach. The algorithms that are provided in the initial imple- mentation are available as individual modules. This allows the existing algorithms to be upgraded in the future, as well as the extension of the initially provided toolbox. 3. Supports human-centered decision-making. As was explained, it is within the ra- tionale of the MaThe solutions to promote computer-assisted decision making. The algorithmic implementations provide intuitive visualizations aiming at assisting the user in content authentication, taking also into consideration the user’s personal experience and perception, as well as the context of the asset under investigation. 4. Is publicly available. As was explained, the web framework aims to address a wide public. An important prerequisite for this is that it is freely available for anyone to use and contribute. Future Internet 2022, 14, 75 5 of 17 5. Requires no audio or technical expertise. The design principles prioritize ease-of-use, following a typical workflow. A more experienced user with a technical and signal processing background, may get better insight and understanding of the produced visualizations. However, the detection of outliers or suspicious points in a file time- line is self-explanatory and does not require a deep understanding of the algorithms and mechanisms. 6. Promotes crowdsourcing. Users and teams can become involved and contribute to the project in several ways to further advance the field of audio tampering detection. They can submit files, annotated as tampered or not tampered, with a brief justifica- tion. Users can also randomly browse files from the dataset, analyze them, and mark them as tampered or not tampered. Finally, as this is an open-source project follow- ing a modular architecture, researchers and teams are encouraged to contribute with code and extensions. The main functionality of the MAthE AudioVisual Authentication framework is shown in Figure 1. Users can submit files for analysis and investigation, or contribute by annotating existing files concerning their authenticity. Once a file is submitted, the appli- cation returns analysis results, and the user can decide if they want to submit the file to the database along with an annotation (tampered or not tampered), submit the file to the database without annotation, or not submit anything to the database. Contributing users can access submitted files, annotated or not, examine the analysis results, and provide annotation (tampered or not tampered), following a crowdsourcing methodology. Figure 1. The MAthE AudioVisual Authentication framework and functionality. 2.2. The Computer-Supported Human-Centered Approach The main concept of our approach depends on the idea that actors with no expertise in signal processing, machine learning, and computational methods can benefit from the visualization output of such techniques with little or no training. Computational methods in media authentication usually try to detectanomalies within the file under investigation. Such anomalies can be visually depicted (e.g., with a change in color). A non-expert user can perceive such depictions and interpret them accordingly, even without understanding or knowledge of the technical details that led to this visualization. After locating the sus- picious points within the file, the user can base their reaction based on contextual infor- mation and their own critical thought. For example, an object within an image that looks Figure 1. The MAthE AudioVisual Authentication framework and functionality. 2.2. The Computer-Supported Human-Centered Approach The main concept of our approach depends on the idea that actors with no expertise in signal processing, machine learning, and computational methods can benefit from the visualization output of such techniques with little or no training. Computational methods in media authentication usually try to detect anomalies within the file under investigation. Such anomalies can be visually depicted (e.g., with a change in color). A non-expert user can perceive such depictions and interpret them accordingly, even without understanding or knowledge of the technical details that led to this visualization. After locating the suspicious points within the file, the user can base their reaction based on contextual information and their own critical thought. For example, an object within an image that looks like an anomaly in the visualizations and also dramatically alters the Future Internet 2022, 14, 75 6 of 17 image’s semantic meaning probably indicates tampering. This is the main idea of the ReVeal project [ 14 ], which deals with tampered images and was a major inspiration of the present work. There is evidence from experiments that users with no technical knowledge were able to detect tampering of images with the support of such visualizations [ 6]. In this approach, a gamification approach was also tested that allowed users to ask for the help of such a visualization toolbox [5] in order to detect fake news and proceed in the game [ 4]. While such techniques are not robust in the automated detection of media content tampering, they can push the limits of human intellect and support users to make better decisions on fake content recognition. In this direction, in the present work, several visualizations based on anomalies that are detected by audio processing are proposed. Since the audio channel is commonly part of video files, this toolbox aims at supporting users with no technical expertise to make decisions on the authenticity of audio and video files. 2.3. An Ensemble of Methods for Audio Tampering Detection In the related work section, several approaches for tampering detection are presented, which may fall into specific categories. Such categories include relevant audible or inaudible artifacts that are produced during the malicious editing of audiovisual files. As a result, depending on the type of forgery and the technical flaws of such an action, one technique may be more or less suitable. Hence, the motivation of the project derives from the hypothesis that it is irrelevant to try to evaluate different approaches to choose the most efficient, since this cannot be applied universally to every case [8]. The MAthE AudioVisual Authentication approach proposes a superposition of meth- ods in a modular architecture that includes a dynamic group of algorithmic elements. Such techniques are either outcomes of previous research work within the project [7 ,8 , 47] or were found in the literature. The modular architecture allows for the modification of existing functions in the future, as well as its extension with new modules that come from new research, literature review, or contribution within the academic society. Another hypothesis that has played an important role in the MAthE architecture design is that the lack of real-world datasets, as well as the diversity of the characteristics of tampered files, sets a bottleneck to the maturity of automated decision-making schemes. Most models are trained with artificially created datasets that address a specific type of tampering (recording device, room acoustics, encoding, etc.). Moreover, disinformation is only relevant at certain time points of a file, where the editing alters the semantic meaning of the recording. This is something that a human subject may easily understand. For this reason, the proposed design incorporates signal processing tools and machine learning models in a semi-automated approach [ 48 ]. It is not within the project’s expectations to provide automated massive authentication of archive files, but rather to assist humans in analyzing and authenticating a specific file under investigation (FUI). The outcomes of the system require a human-in-the-loop [ 49 ] strategy. This is considered an effective combination of machine processing capabilities and human intelligence. The initial toolbox of signal processing algorithms that was included in the prototype version of the MAthE AudioVisual Authentication application is presented below. It is noted that the technical presentation and validation of every approach is not within the scope of the current paper. Instead, a short description of the main functional principles of every category of techniques is given, along with references to publications with the technical details of different algorithms. The toolbox is dynamic, and it will be supported by incorporating state-of-the-art feature-based [ 50, 51] and deep [52] (machine) learning approaches for audiovisual semantic analysis. It can also be deployed as a mobile applica- tion [ 53 ]. It is expected to further grow and evolve through the use of the application and the continuous dissemination of the MAthE project. Future Internet 2022, 14, 75 7 of 17 2.3.1. Common Audio Representations This family of tools includes typical audio representations, such as waveform, energy values, and spectrograms. Such tools are available in most typical audio editing applica- tions. Sound waveforms are the depiction of the amplitude of sound pressure of every audio sample, expressing the variation in the audio signal in time. For an audio signal with a common sampling frequency of 44,100 samples per second, waveforms may include a huge number of samples to be depicted, which can be computationally heavy to represent in an interactive graph running on a web browser. For this reason, in the proposed toolbox, time integration is performed, showing the root mean square (RMS) value for successive time windows as a bar diagram. This is used as a tradeoff to avoid the excess information redundancy of the waveform. Mel scale spectrograms provide a spatiotemporal represen- tation of audio signals, depicting the evolution of the spectral characteristics through a time interval [ 54]. Spectral information is given for specific frequency bands that apply to the Mel scale, which is inspired by the psychoacoustic characteristics of human auditory perception. They are included in the toolbox because they can be useful, and they enhance the MAthE framework’s all-in-one solution so that users do not have to make use of more than one piece of software for analysis and decision-making. 2.3.2. Different Encoding Recognition This family of techniques investigates the existence of small audio segments in the FUI that have different compression levels or encoding characteristics. This indicates that they may be segments of another file that were inserted in the original file. One common naïve approach that can be very effective in some cases is the calculation of the bandwidth, because most compression algorithms apply low-pass filtering to eliminate the higher frequencies that are of minor importance to the human auditory perception. Feature vectors are descriptors of several attributes of a signal. Different encoding and compression levels, even if they are often proven to be inaudible in listening tests with human subjects, can affect the features that describe an audio signal. In the Double Compression technique for audio tampering detection that was proposed in [8], the FUI was heavily compressed. Features are extracted from the FUI and the compressed signal. For every time frame, the feature vector difference is calculated between the two signals. Parts of the FUI that have different encoding are expected to have different feature vector distances. Moreover, the gradient of differences is calculated. This measure is expected to reach peak values when there is an alteration in the compression levels, indicating suspicious points. The double compression algorithm is summarized as follows [8]: 1. Heavy compression to the audio file under investigation (FUI), thus creating a double- compressed file (DCF). 2. A feature vector is extracted the FUI and the DCF, creating the (T × F) matrices Fi (t), where i = 1, 2, T is the number of time frames and F is the length of the feature vector. 3. For every time frame, the Euclidean distance D(t) of the two matrices is calculated 4. D’(t) = D(t) − D(t − 1) is calculated to show the differentiation between successive time frames. 5. D’(t) is expected to present local extrema in time frames that include a transition be- tween audio segments of different compression, indicating possible tampering points. For the feature selection, an audio feature vector was evaluated in [ 7 ]. Using a dedi- cated dataset creation script, a set of audio files were created, containing audio segments of different compression formats and bitrates. Specifically, segments of mp3-compressed audio in different bitrates were inserted randomly within an uncompressed file contain- ing speech. Subjective evaluation experiments with three experts in the field of media production indicated that human listeners failed completely to detect the inserted seg- ments for mp3 bitrates above 96 kbps, while they recognized approximately 10% of the inserted segments for mp3s of 64 kbps [ 7]. The dataset that was created in [ 7 ], along with the script for customized dataset generation, are documented and provided pub- Future Internet 2022, 14, 75 8 of 17 licly at http://m3c.web.auth.gr/research/datasets/audio-tampering-dataset/ (accessed on 26 January 2022). The selected feature set includes several frequency domain attributes, namely spectral brightness, with predefined threshold frequencies 500 Hz, 1000 Hz, 1500 Hz, 2000 Hz, 3000 Hz, 4000 Hz, and 8000 Hz, as well as rolloff frequencies, which are the upper boundary frequencies that contain energy ratios of 0.3, 0.5, 0.7, or 0.9 to the total signal energy, and spectral statistics (Spectral Centroid, Spread, Skewness, Kurtosis, Spectral Flatness, 13 Mel Frequency Cepstral Coefficients, Zero Crossing Rate, and RMS energy). The technical details of the aforementioned feature vectors are outside the scope of the current paper, but the methodology and feature evaluation process are presented thoroughly in [7]. 2.3.3. Reverberation Level Estimation Another indicator that several segments of a FUI may have been inserted from a different file is the effect of acoustic conditions on the recording. Every space has different reverberation levels that affect the recording. Especially since most newsworthy events are not recorded in ideal conditions of professional recording studios, a regression model based on a Convolutional Neural Network architecture [ 47 ] was trained using a big dataset of sim- ulated reverberation to provide a numerical estimations of the Signal-to-Reverberation ratio for every audio segment. Segments with outlier values are possibly related to malicious audio splicing. Convolutional Neural Networks (CNNs) are a type of deep learning architecture that have gained popularity in audio recognition and event detection tasks [55 ,56]. One main reason for their recent widespread is is that there is no need for a handcrafted feature vector. Instead, a visual representation of the audio information is fed to the networks as an image, and the input layers extract hierarchical features in an unsupervised manner during training. Different kinds of input have been evaluated for deep learning techniques, with spectrograms being the dominant approach [54]. Signal-to-Reverberation-Ratio (SRR) can be a useful attribute that can indicate audio slicing. SRR expresses the ratio of the energy of the direct acoustic field to the reverberation acoustic field. It is determined by the acoustic characteristics of the space of the recording, the positioning of the sound source and the recording device. The distance where the levels of the direct and the reverberation sound are equal (SRR = 1) is called the critical distance. At distances closer than the critical distance, we can assume SRR > 1, and at distances that are farther than the critical distance, SRR < 1. The critical distance itself depends on the room acoustic attributes. For recordings that take place under different conditions, the SRR is expected to differ. When segments from different recordings are pieced together, it is possible to detect the inconsistency in their SRR, even if it is not audible by human listeners. Calculating the SRR for different time windows can provide another criterion for audio tampering detection. In the proposed approach, a deep learning regression model is used for a data-driven estimation of the SRR, based on simulation data. A 3600-second-long audio file containing pink noise was created, using the Adobe Audition generator. Using the same software, reverberation was added to the file with different SRRs. Ten different SRRs were chosen, resulting in 11 audio files (including the original), producing a 39600-second-long dataset. The same source audio file was used for all SRRs, so that the model is trained to recognize the reverberation and not information related to the content of different audio streams. The selected SRRs are shown in Table 1. Table 1. The different Signal-to-Reverberation Ratios that were used for the model training. Signal (%) 100 90 80 70 60 50 40 30 20 10 0 Reverberation (%) 0 10 20 30 40 50 60 70 80 90 100 Future Internet 2022, 14, 75 9 of 17 The dataset was used for the training of a CNN regression model. The output of the model is a continuous value from 0 (no reverberation) to 1 (only reverberation). The model architecture is provided in Table 2. Table 2. The architecture and hyper-parameters of the CNN model for reverberation level estimation. Layer Type Configuration 1 Convolutional 2D Layer 16 filters Kernel size = (3,3) Strides = (1,1) 2 Max Pooling 2D Layer Pool size = (2,2) 3 Dropout Rate = 0.25 4 Convolutional 2D Layer 32 filters Kernel size = (3,3) Strides = (1,1) 5 Max Pooling 2D Layer Pool size = (2,2) 6 Dropout Rate = 0.25 7 Convolutional 2D Layer 64 filters Kernel size = (3,3) Strides = (1,1) 8 Dropout Rate = 0.25 9 Convolutional 2D Layer 128 filters Kernel size = (3,3) Strides = (1,1) 10 Convolutional 2D Layer 256 filters Kernel size = (3,3) Strides = (1,1) 11 Flatten Layer 12 Dense Neural Network Output weights = 64 Activation = ReLU L2 regularizer 13 Dense Neural Network Output weights = 64 Activation = ReLU 14 Dropout Rate = 0.25 15 Dense Neural Network Output weights = 24 Activation = Linear 2.3.4. Silent Period Clustering Besides room acoustics, the background noise also characterizes a recording. The environmental noise that is recorded in speech recordings is often inaudible, since it is mixed with a speech signal of a much higher level. However, in the small periods of silence that occur between words and syllables, the background noise signal is dominant. In the case of combining two or more recordings to create a tampered audio file, different background noise patterns may be distinguishable. Initial investigation has shown that by exporting a feature vector from small segments of silence (~25 ms) and p","Vryzas, N., Katsaounidou, A., Vrysis, L., Kotsakis, R., & Dimoulas, C. (2022). A Prototype Web Application to Support Human-Centered Audiovisual Content Authentication and Crowdsourcing. Future Internet, 14(3), 75."
"SCOP_049","Towards Understanding and Supporting Journalistic Practices Using Semi-Automated News Discovery Tools","Journalists are routinely challenged with monitoring vast information environments in order to identify what is newsworthy and of interest to report to a wider audience. In a process referred to as computational news discovery, alerts and leads based on data-driven algorithmic analysis can orient journalists’ attention to events, documents, or anomalous patterns in data that are more likely to be newsworthy. In this paper we prototype one such news discovery tool, Algorithm Tips, which we designed to help journalists find newsworthy leads about algorithmic decision-making systems used across all levels of U.S. government. The tool incorporates algorithmic, crowdsourced, and expert evaluations into an integrated interface designed to support users in making editorial decisions about which news leads to pursue. We then present an evaluation of our prototype based on an extended deployment with eight professional journalists. Our findings offer insights into journalistic practices that are enabled and transformed by such news discovery tools, and suggest opportunities for improving computational news discovery tool designs to better support those practices.","Computer Science","Proceeding","2021","Y","Y","Prototype","Support","Claim detection","0","A lot happens in the world every day—much of it is insignificant, but some of it warrants broader public attention. Journalism, and the news media more broadly, serves to help identify those things happening in the world that may be deserving of wider exposure, scrutiny, or debate in society [ 74]. This gatekeeping function—of influencing what gets published and publicized as part of the journalistic communication process—is a complex one driven by an array of forces both within and beyond journalism, and at various individual, organizational, social, and technical levels [ 78]. At the technical level, recent models of gatekeeping have begun to consider algorithmic influences in the context of broader sociotechnical gatekeeping practices [95 ], including at the information gathering stage of news production [ 25]. Conceived of as a way to potentially increase the efficiency and scale at which new news stories can be identified [ 42], such algorithmically informed approaches Authors’ addresses: Nicholas Diakopoulos, nad@northwestern.edu, School of Communication, Northwestern University, Evanston, Illinois; Daniel Trielli, dtrielli@u.northwestern.edu, School of Communication, Northwestern University, Evanston, Illinois; Grace Lee, gracelee@u.northwestern.edu, Medill School of Journalism, Northwestern University, Evanston, Illinois. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM. 2573-0142/2021/10-ART406 $15.00 Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021.406:2 Nicholas Diakopoulos, Daniel Trielli, and Grace Lee can help to orient journalists’ attention to events, documents, or anomalous patterns in data that are more likely to be newsworthy [ 15, 23 ], a process recently referred to as computational news discovery [25, 86]. This work focuses on a particular context of computational news discovery related to the identi- fication of algorithmic decision-making (ADM) systems in government. This is meant to support algorithmic accountability reporting, an emerging beat in journalism focused on investigating and scrutinizing algorithmic decision making across various domains of society [ 22]. For example, outlets such as ProPublica, Der Spiegel, and The Markup have spearheaded investigative journalism projects seeking to expose and hold accountable power wielded through ADMs.1 A few efforts have begun to catalogue a range of government use-cases for ADMs in the U.S., from services adminis- tration to regulatory enforcement [ 18], with one study finding that nearly half of the 142 surveyed agencies had used various AI and machine learning tools [31]. While some government initiatives have begun to develop responsible approaches to ADM deployment or even create registries of algorithms in use, such approaches are not widespread [35, 92]. The need for scrutiny of these systems is evident [32, 33] as is the need for additional research on the responsible design, develop- ment, use, and evaluation of ADMs deployed throughout the public sector [16, 36, 49, 61 , 73, 93]. In this work we focus on developing and evaluating a method and tool to help comprehensively monitor and track ADM usage across all levels of government to support journalism. An important underlying motivation for the current work is to find ways to reduce the effort needed to engage in the scrutiny of government algorithms. This research therefore undertakes the design and evaluation of a semi-automated computational news discovery tool called Algorithm Tips to enable such work. Algorithm Tips systematically and periodically monitors government websites (across U.S. government at all scales) for documents that may reveal new applications of ADMs. Through a series of automated evaluations, internal expert evaluations, and crowdsourced evaluations Algorithm Tips augments documents to produce news leads. These leads are presented in an online interface and sent to external professional journalists who ultimately decide whether to pursue the additional reporting needed to transform a lead into a publishable news story. We evaluated Algorithm Tips in an extended deployment with eight professional journalists who have experience reporting on algorithms in society for some of the largest most well-established news organizations in the U.S. Our findings articulate ways in which the tool meets the needs and practices of domain experts, and offers implications for the design and further research of computational news discovery tools more broadly. This research offers a couple of contributions, including (1) the design and development of a computational news discovery tool (Algorithm Tips) with design goals tailored to enable journalistic decision making around which leads to pursue in the domain of algorithmic decision making in government; and (2) an evaluation of that tool with eight professional journalists in an eight-week- long deployment which offers ecologically valid insights into how journalistic practices are enabled and transformed by such a news discovery tool. In particular we describe design goals related to supporting attention management, as well as verification and newsworthiness professional evaluations, and we tailor the information process and interface of Algorithm Tips to support those goals in the context of investigative and enterprise reporting. Our findings elaborate on how journalists made context-specific interestingness decisions using the evaluative information made available via automation, crowds, and experts; saw the tool as able to offer both a jumping-off 1See: Machine Bias: There’s Software Used Across the Country to Predict Future Criminals. And it’s Biased Against Blacks. ProPublica, May 2016. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal- sentencing; Blackbox Schufa. Der Spiegel. November, 2018. https://www.spiegel.de/wirtschaft/service/schufa-so- funktioniert-deutschlands-einflussreichste-auskunftei-a-1239214.html; and Swinging the Vote? The Markup. February, 2020. https://themarkup.org/google-the-giant/2020/02/26/wheres-my-email Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021. Towards Understanding and Supporting Journalistic Practices Using Semi-Automated News Discovery Tools 406:3 point for specific items and to provide background on the beat of government algorithms; and strove to manage their own effort and attention with respect to the tool. Overall our work provides insight into how to more effectively design and build computational tools to support the essential journalistic activities of news discovery and news gathering. 2 BACKGROUND In designing and developing Algorithm Tips we were informed by related work in (1) the domain of Computational Journalism as it relates to Human-Computer Interaction (HCI), including various tools that have addressed the topic of news discovery in different contexts; and (2) the topic of newsworthiness and the measurement of news values as it may support journalistic evaluations of potentially newsworthy information. 2.1 Computational Journalism Studies of journalism within the context of HCI [3 ] address a broad gamut of news-related activities, from information gathering [26, 29], production [56 , 64, 81], and sensemaking [14, 28], to distribution [ 80, 89] and audience consumption behavior [8, 34 , 63]. Oftentimes such research is oriented towards designing and developing new tools to augment journalists’ capabilities, or towards seeking to more broadly understand how journalists use computational tools in their sociotechnical practices [ 47, 88]. The current work contributes to this growing body of HCI literature on computational journalism, which itself is broadly oriented towards enabling information and knowledge production using algorithmic approaches that embrace journalistic values and advance journalistic practices [19, 23]. In particular we focus on the goal of supporting news discovery and gathering tasks [25, 70], which prior research has documented as challenging and time-consuming for journalists [47, 81]. In designing Algorithm Tips we were informed by prior work on computational news discov- ery tools. Early ideations about the role of computing in journalism suggested that algorithmic techniques might offer an information subsidy to the discovery and gathering phases of news production by allowing journalists to scan the environment for newsworthy events and happenings at increased scale, speed, and overall efficiency [ 42, 43]. Recently this has been framed as compu- tational news discovery (CND), defined as “the use of algorithms to orient editorial attention to potentially newsworthy events or information prior to publication” [25 ]. The premise for CND is supported by work in journalism studies showing that (non-computational) sources and infor- mation subsidies that are frequently used and routinely relied on do tend to save reporters’ time during news discovery [71]. Some of the first CND tools were developed in the context of social media monitoring to help detect newsworthy events, identify witnesses, or make sense of how people were responding to news events [ 26, 28, 76]. Given the scale and quality of social media data confronting journalists many of these tools support not only discovery tasks [ 30 ], but also curation and validation tasks [ 54, 62, 88, 100 ] which emphasize the importance of assessing and verifying sources and content [ 12]. CND tools have also been built to support a variety of use cases beyond social media listening, including the monitoring of numeric data streams or sets (e.g. crime, real estate, education) for trends, outliers, or anomalies [ 15, 55, 77], the monitoring of local court or business documents to surface newsworthy people, places, or companies [ 67], identification of local public meeting events that journalists may want to attend [97], and the ranking of claims in the media that may be worthy of fact checking [40, 46]. In this work we draw on this prior work to help elaborate design goals and inform key features for the design of the Algorithm Tips tool (See Section 3.1). We also distinguish the tool in its orientation towards monitoring and generating leads about the use of algorithmic decision making in government, a particular context that has not been addressed in prior work, but which is indicative Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021. 406:4 Nicholas Diakopoulos, Daniel Trielli, and Grace Lee of a larger class of investigative reporting efforts to monitor government documents that might be similarly supported technologically. Moreover, the evaluation of many of the tools developed in prior work has been based on case studies, or short-term scenario-based usage, rather than extended deployments (excepting [76]). In contrast, in this work, we report on a two-month-long deployment of Algorithm Tips with professional journalists in an effort to better study the use of such a tool as it relates to more ecologically valid journalistic workflows and practices [79]. 2.2 News Values and Newsworthiness Much of the research on tool development for journalists tends to emphasize the importance of aligning designs with journalistic values [ 23, 24, 47 , 51]. We address this observation in this section by presenting a closer examination of how CND tools might incorporate measurements of news values to support journalistic decisions of newsworthiness. Whether an event or occurrence in the world actually becomes a news story is the result of a sociotechnical gatekeeping process that is influenced by forces at various individual, organizational, social, normative, economic, and technical levels [ 78]. Yet research has shown that a number of factors, termed news values, have been repeatedly observed and are manifest in the types of stories that journalists tend to report and publish. For instance, journalists may consider news values such as timeliness, proximity, prominence, human interest, relevance, conflict / controversy, unexpectedness / surprise, reference to the power elite, audience fit, actuality, and consequence / impact, among others [ 5, 44, 45, 65, 75]. Whether a news lead is deemed newsworthy enough to become a news story arises out of a contextualized judgement process informed by these news values. In this paper we consider how to integrate measures of news values, or newsworthiness, into news discovery processes and tools in order to inform (but not supplant) professionals’ editorial judgements. In particular, we consider how different news values may be amenable to measurement using computational and crowd-based techniques (elaborated further in Sections 3.2.2 and 3.2.4), while acknowledging in our interface design that professionals should be enabled to make the final contextual judgment of whether and how a lead fits and is positioned at broader organizational and societal levels of interest [75]. All of the news discovery tools referenced above encode and reflect computational operational- izations of news values in some way, oftentimes in domain- and data-specific ways. For instance, several efforts have tried to capture the dimension of “unexpectedness / surprise” by detecting outliers or anomalies in numerical data streams or sets [27, 55, 77 ]. The Lead Locator system additionally measures “political relevance” in a domain-specific way, and includes a basic opera- tionalization of “magnitude” based on population sizes, which makes sense in the political reporting context [ 27]. The Vox Civitas system uses measures of “relevance” and “uniqueness” based on text analysis and cosine similarity to help filter social media posts in response to political speeches [ 29]. The Local News Engine captures the dimension of “reference to elites” by automatically extracting named entities of people, places, or companies from local documents [67]. The Reuters Tracer system explores several operationalizations of news values for evaluating detected events in social media, including “topical relevance”, “scale” (or “magnitude” of the event), “negative impact” including human, physical, and financial impacts, “location”, and “novelty” of the event [ 62]. In some cases news values are based on fairly straightforward statistical measures [27 , 55] while in others sophisticated machine learned models [ 62] or formal semantic models [ 60 ] are applied. There have also been a handful of prior efforts to measure news values in non-discovery contexts, such as for evaluating news headlines [68] or for predicting the sharing of news articles socially [91]. Given this prior work, and taking into account the specific context explored in the current work (i.e. focused on administrative documents related to the use of algorithms in government) we felt confident in computationally supporting four news values: timeliness, proximity, reference to the power elite, and topical relevance (See Table 1). Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021. Towards Understanding and Supporting Journalistic Practices Using Semi-Automated News Discovery Tools 406:5 In contrast to prior work on computational news discovery systems, this research also explores the use of crowdsourcing techniques to measure news values, and more importantly, evaluates these crowdsourced signals of newsworthiness with domain experts in a specific and extended deployment context. Crowdsourcing techniques have previously been used throughout journalistic processes for everything from checking documents and rating claims and content for factchecking or verification purposes, to identifying and verifying locations and context, co-developing topics for coverage, helping to moderate and route information in communities, gathering information, and even writing news articles [ 1 , 2 , 21, 50 , 52, 94, 101]. This work distinguishes itself from prior work by exploring the applicability of directly crowdsourced measures of news values in helping to augment professional judgements of newsworthiness. Crowdsourced rating approaches have been shown to be effective for the reliable measurement of a range of manifest and latent constructs in communications content [7 , 28, 53], which we extend here to apply to news values. We consider both quantitative evaluations of news values (i.e. numerical ratings on a 1-5 scale) and qualitative rationale for those ratings as an added dimension of context and information for end-users [57]. In particular we apply this approach to measuring news values that we think would benefit from the diverse social knowledge and independent evaluations that can be captured through crowdsourcing [84], including: bad news (which we frame more specifically as “negative societal impacts”), magnitude of impact, controversy/conflict, and unexpectedness/surprise (See Table 1). These measures are included in the Algorithm Tips user interface and are considered in our evaluation in terms of how journalists incorporate such crowdsourced measures of news values into their broader newsworthiness assessments. 3 ALGORITHM TIPS DESIGN Here we describe the design of Algorithm Tips both in terms of how information flows through the system and is augmented and evaluated, as well as in terms of how end-user journalists are able to view and interact with that information. The system was initially developed and piloted in early 2017 [ 90], and since then has gone through several design iterations to simplify the information architecture and design, reduce the amount of manual effort needed while taking advantage of automation where possible, and incorporate a novel crowdsourcing component. These iterations were spurred on by informal feedback from several professional journalists working at established news organizations who were shown early versions of the interactive prototype and methods either individually or in the context of sessions at practitioner conferences or workshops that were geared towards soliciting feedback. For instance, such feedback informed features to support the evaluation of newsworthiness including topical and date-based filtering and the highlighting of key entities in documents. We designed Algorithm Tips to support investigative or enterprise journalism [ 69] around the use of algorithms in government. Investigative or enterprise journalism reflects a typical orientation towards algorithmic accountability reporting [ 22, 23] and is characterized by extended time frames [ 42] with less of an emphasis on the speed or immediacy of the report with respect to events in the world [ 99 ]. Moreover, the specific reporting context of algorithms in society that we seek to support does not entail an exhaustive review of all leads. The goal is rather to signal when something is interesting and worthy of attention rather than demand the journalist exhaustively find everything of interest. We focus on supporting the initial news discovery phase (i.e. the initial contact or awareness a journalist receives about a potential lead) and on the transition into the news gathering process [ 70]. This design orientation has implications for how the tool is configured (e.g. to generate alerts on a weekly basis), what types of journalists might expect to find value from the tool (e.g. investigative or enterprise rather than daily or breaking), and the temporal affordances Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021. 406:6 Nicholas Diakopoulos, Daniel Trielli, and Grace Lee of the leads the tool might surface (e.g. oriented towards longer term issues and trends rather than events) [85]. After analyzing the extant computational news discovery systems and tools from the literature (See Section 2.1), and drawing heavily on [25 ], in the next section we articulate several more specific design goals for Algorithm Tips. The final flow of information through the system, from monitoring documents on the web, to suggesting news leads to professional journalists is depicted in Figure 1. The information process used to harvest and enrich documents is described in Section 3.2 and the interface used to present leads to end-user journalists is described in Section 3.3, both of which are described in terms of how they help support the overall design goals. 3.1 Design Goals • DG1-Attention: Be sensitive to available human effort and attention. Journalists are already overwhelmed with information from many different channels, sources, and information sub- sidies (e.g. press releases) [47, 81 ]. A lead discovery tool should be sensitive to the journalist’s attention economy by aligning suggested leads to their interests [ 25 ]. This can be supported through search and filtering that allows the information space to be interactively adapted, alerts that are configurable and schedulable to suit individual interests and timing needs, and support for marking of items so that users can come back to initially interesting leads when more attention is available. • DG2-Verification: Enable initial verification and quality assessment. In order for a lead to turn into a substantive piece of journalism it must be verified and confirmed as accurate [88]. This is an iterative process that unfolds as the lead is pursued and new information is collected, however, even at the initial phase a lead discovery tool should enable verification and information quality assessment activities [ 25]. This can be supported by including adequate context to evaluate or verify information and launch into follow-up research, such as by including a link to the original source document, provenance information on the search terms and search used to find the document, and date/time information to indicate the timeliness of the information [12]. • DG3-Newsworthiness: Enable newsworthiness evaluation. Journalists apply a range of context-specific criteria to a news lead in determining whether they think it is important enough to pursue and develop into a publishable news story [25]. These may include factors such as timeliness, proximity, significance, novelty, various dimensions of relevance, as well as whether it is likely to be of interest to their imagined audience. Journalistic decisions of newsworthiness can be supported by incorporating relevant metadata in the information design so that journalists can develop their own nuanced interpretations of these various factors. 3.2 From Documents to Leads In this section we describe the design of the first four stages of the Algorithm Tips information flow, beginning with the document monitoring apparatus. After documents are collected they are then evaluated, augmented, and filtered in several ways before they become leads that are delivered to journalists. In the following subsections we first describe the monitoring method, and then describe the details of different processes for automated, expert, and crowdsourced evaluation of the documents (See Table 1) which help support the design goals described in Section 3.1. 3.2.1 Document Monitoring. A method for targeted Google web searches and scraping was de- veloped in order to automatically monitor for potentially interesting instances of ADM systems Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021. Towards Understanding and Supporting Journalistic Practices Using Semi-Automated News Discovery Tools 406:7 Fig. 1. The Algorithm Tips information process and interface. used in government. In order to develop a list of search queries to target our concept of interest (i.e. ADMs) we considered how algorithms might be referred to across different government agencies (DG1-Attention). From our experience with public records requests for algorithms from state agen- cies we knew that the vocabulary used to describe algorithms internally could vary. For instance, an algorithm for predicting criminal recidivism might be referred to as a “risk assessment” by the relevant government agency. We created a list of algorithm-related terms such as “algorithm”, “predictive analytics”, “risk assessment” and so on. Initially, 61 terms were identified, based on brainstorming and the use of various thesauri. As described in [ 90] we then used term expansion and redundancy identification techniques to both widen the net and enhance the coverage or recall of the system while also striving for a non-redundant set of queries that would increase efficiency and be considerate of the demands the scraper put on external systems [9]. This process yielded a final set of 67 terms.2 Since our initial focus and area of interest is governmental algorithms we limit searches to U.S. government domains. This was done by appending the “site:.gov"" operator to the search. Initial results included a high degree of noise due to research papers hosted on government websites that described their methodologies using many of the search terms. A majority of those research papers were hosted by the National Institutes of Health (NIH). While informative, early assessments revealed that many of these references would not necessarily lead to algorithms being used by government agencies. As a pragmatic response, they were therefore filtered out of the searches by using another operator, ""-site:.nih.gov"".3 In the end, searches for data collection were constructed using the following format: “[search term]” site:.gov -site:.nih.gov. Searches are automatically executed by our system for all of the search terms tracked on a periodic basis (once per week in the current setup). Up to 100 results for each term are collected although a document (e.g. HTML, text, or PDF file) is excluded if its URL is a duplicate of a document URL found with a previously searched term. Also, if a document URL is found in subsequent rounds of data collection (e.g. in a future week) it is marked as a duplicate and only the first occurrence is retained. In an effort not to overload Google servers we only submit one search query every 1-2 minutes. 3.2.2 Automated Evaluation. Each document that is collected is automatically tagged with a few pieces of metadata to support the evaluation of several news values (DG3-Newsworthiness) (See Table 1). To support evaluations of the news value of “timeliness” we store a date-time stamp for every document, both for when it is initially found online, and for when it is finally published on Algorithm Tips. To augment documents with data about both the relevant jurisdiction (e.g. “City”, “State / Local Government”, or “Federal Agency — Executive”) and the government source that published information about the algorithm (i.e. an agency, organization, or municipality), we use the domain name of the URL where the document was found together with curated data provided by 2The final set of terms is available at REDACTED. 3Other government domains which might be expected to yield many research papers, such as nsf.gov, only accounted for a small minority of documents (∼1%) and were therefore not explicitly filtered out in order to maintain as wide a net as possible. Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021. 406:8 Nicholas Diakopoulos, Daniel Trielli, and Grace Lee the U.S. General Services Administration which links jurisdictions and sources to domain names.4 This information is meant to support the news value of “proximity” by conveying to end users the geographic scope of the lead. In order to support the news value of reference to “the power elite”, we automatically extract named entities for all people or organizations mentioned in each document.5 Based on frequency of occurrence in the document the top five people and the top five organizations are retained. In addition, we developed a custom model for assessing document relevance which scores each document on a scale from 0 (not relevant) to 1 (relevant) with respect to the topic of algorithmic decision making (DG3-Newsworthiness). This is a measure of topical relevance and is meant to align the editorial scope of Algorithm Tips’ leads with the presumed interest of external experts coming to Algorithm Tips for leads (DG1-Attention). To train the model we manually annotated a random subsample of documents collected via our scraping process. Data was collected across three separate dates in 12/2016, 3/2018, and 6/2019 with a goal of contributing to greater resilience of the model to temporal shifts in the data. Because text could not be extracted from all documents and due to link-rot between the collection and annotation, the final sample consisted of 223 relevant documents and 304 irrelevant ones. To ensure annotation reliability we developed guidelines in an annotator guidebook which includes clear definitions and criteria for relevance as well as positive and negative examples. We trained a second coder using the guidebook and on a random sample of 100 documents achieved an inter-rater reliability Cohen’s Kappa of 0.87 with one of the co-authors, indicating a reliable coding process. The second coder then annotated the remainder of the documents. The final set of 527 annotated documents was then used to train a model, with features including all term frequency inverse document frequency (TF-IDF) scores of unigrams, bigrams, and trigrams and excluding English stopwords. To establish the appropriate criteria for the model evaluation we first consider its intended use context. In particular, we assume that journalist end-users will have a limited amount of time available to review leads, and that they will be satisfied with finding some subset of reasonable leads without expecting to comprehensively find all leads (DG1-Attention). We similarly assume the internal expert evaluator has a fixed time budget but has the goal of increasing yield (i.e. finding as many relevant leads as possible in the time budget). A high precision will increase the yield. This use context therefore suggests that the classifier should be evaluated using the precision@k metric, which considers the precision of a ranked set of results of size k (where k controls the fixed time budget). We experimented with Bayes, Logistic Regression, Support Vector Machine, and Random Forest algorithms for learning the model and compared their performance with k = 10, 25, and 50. All models were trained using 5-fold cross validation by training on different subsets of 80% of the data to generate predicted scores for the 20% left out. We found that the Logistic Regression classifier outperformed the others, yielding a P@25=0.6 and P@50=0.6. So, for example, on a sample of 25 documents ranked with the model we would expect 15 of them (i.e. 60% ) to be relevant. In comparison, a random sample of 25 would be expected to yield about 10.58 relevant documents (i.e. 223 / 527 or ∼42% is the base rate of relevant documents in the sample). Given a fixed time budget that allows for the examination of, for instance, only 25 documents, the model is expected to boost efficiency by yielding about 42% more relevant documents in comparison to a random selection of documents (i.e. from ∼10.58 to 15). The model is therefore useful insofar as it should increase the yield of relevant documents identified by the internal expert given their fixed time budget. Alternatively, the model could be deployed to reduce 4https://github.com/GSA/data/tree/master/dotgov-domains 5We use the SpaCy large model for named entity recognition (https://spacy.io/usage/facts-figures) which has an accuracy very close to the state of the art at about 86% Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 406. Publication date: October 2021. Towards Understanding and Supporting Journalistic Practices Using Semi-Automated News Discovery Tools 406:9 the time budget but maintain a fixed yield, however we opted not to use the model this way as we want to tune the system to find as many relevant documents as possible. Admittedly there is still like","Diakopoulos, N., Trielli, D., & Lee, G. (2021). Towards understanding and supporting journalistic practices using semi-automated news discovery tools. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW2), 1-30."
"SCOP_070","Audiovisual Verification in the Evolution of Television Newsrooms: Al Jazeera and the Transition from Satellite to the Cloud","With the spread of the digital sphere and the proliferation of images from indirect sources that can be accessed by systems and users, verification routines have become essential to ensure media corporations’ credibility. The advances in artificial intelligence which allow automated fact-checking (AFC) initiatives to be created help detect falsehoods, but they do not eliminate the need for human intervention. On the contrary, information professionals are necessary, and their functions increasingly include procedures such as mediating in videos and images. This study analyses the evolution of verification routines in audiovisual journalism and how new techniques have influenced the perception of trust worthiness and the reorganization of the television newsroom by allowing content from outside the media’s own newsroom. To do so, it combines a method that examines the main literature on verification processes and compares it with the procedure used by Al Jazeera. In this regard, an exploration was conducted out based on participant observation in this international TV channel via interaction with journalists and heads of the corporation. The results indicate that advances in verification procedures make it possible to introduce visual material from the social media into the corporation’s common news topics contributing to the transition from the traditional newsroom to the cloud structure and the inclusion of new audiences. These changes affect journalistic routines in a profession which has no longer been in-person for some time, in which correspondents coexist with journalists working in virtual mobility, seeking and receiving images in and from the social media.","Journalism Studies","Article","2021","Y","Y","Process","Practices","Verification","0","Audiovisual verification in the evolution of television newsrooms Anàlisi 64, 2021 87 Checking information has been basic practice in journalism since its incep- tion. In just a few years, however, the spread of the social media has changed the dynamics of how the large television corporations work. The growing presence of images from the digital sphere has led the entire newsroom to undertake verification tasks that require some degree of technological training. Every minute, 500 hours of video made by users from all corners of the globe using the cameras on their mobile phones are uploaded to the Internet (Mohsin, 2020). This shift poses a challenge for newsrooms, which have to ensure the veracity of this material in order to preserve their credibility. The digital sphere poses a scenario in which the reputation of journalism has been brought into play, since not only have the traditional media lost sole sovereignty of the discourse but the social media themselves have become a source with no stable interlocution protocols; instead, there are only fortu- itous exchanges which lack solid knowledge or trust. Hence, there is a growing interest in finding new technological verification resources, and the appear- ance of professional, specialized fact-checkers are at the root of this new reporting style (Graves, 2012: 1). Indeed, over the last decade, fact-checking platforms have been set up in over 50 countries, 90% of them since 2010 (Fernández-García, 2017). Artificial intelligence and advances in creating automated fact-checking ini- tiatives (AFC) have not yet rendered communication professionals expendable in this process, although if there is any suspicion they help streamline the detec- tion of lies, online rumors and other kinds of disinformation (Graves, 2019). 2. Theoretical context: Disinformation and editing on the Internet In this convergence of media industries, the digital sphere and telecommuni- cations, information flows at a speed that would have been unimaginable only a few years ago (Thussu, 2007: 43). The Internet has turned newsrooms into permanently connected networks with uninterrupted information flows that have to be updated with little processing time given the demand for news production. This evolution has changed journalistic work (Castells, Van der Haak and Parks, 2012). Verification neutralizes disinformation only if it has not yet reached its peak circulation, because once it spreads the rebut- tal may not reach the audience (Rodriguez-Fernández, 2019). TV audiences’ migration to the Internet has turned them into not only information consumers but also content broadcasters, thus generating what is called networking journalism in which professionals and amateurs work together to get a story (Jarvis, 2006) through collaboration, with the journal- ists acting increasingly as moderators (Duffy, 2011: 9). In this news structure determined by digital platforms, there are converg- ing possibilities for connectivity and innovation with the proliferation and saturation of news flowing at hyperspeed (Beckett, 2020). 88 Anàlisi 64, 2021 Lola Bañon Castellón Any technological advance has a potential distorting effect, and there is no information that cannot be turned into disinformation (Virilio, 1995). The Internet has facilitated an intentional change in meanings, which is why not only the volume of information but also the veracity of the content must be considered (Hernon, 1995). In this scenario, the definition of fake news comes from the lack of corre- spondence between statements and facts, but if it is also done intentionally, it becomes what common sense identifies as propagandistic lies. Disinforma- tion, on the other hand, is more difficult to detect (Del-Fresno-García, 2019). It is a phenomenon that is known for its intentionality, with false sources or no identification of the sources (Rodriguez Andrés, 2018), and an abuse of power to gain some benefit (Van Dijk, 2006). Hypertextual media are multiplying their possibilities for becoming active subjects of disinformation in a scenario in which an unprecedented volume of images and content is being put into circulation on the Internet. Machines are capable of generating products that are journalistic in appearance for the purpose of disinformation, a polysemic phenomenon that mutates quickly (Manfredi and Ufarte, 2020: 65). 3. Method, research questions and hypotheses The method used is based on a review of the scientific literature on the evolu- tion of fact-checking, the strategies used to deal with disinformation (Roozen- beek and Van der Linden, 2019) and the preservation of the media’s ethical principles in this communication context (McBride and Rosenstiel, 2013). Consideration of verification is fundamental in today’s digital journalism (Adair, 2019), and websites devoted to this purpose have been proven to be instruments that restore credibility (López Pan and Rodriguez Rodriguez, 2020) and work towards democratic construction in a period of changes (Amazeen, 2017), while acknowledging the difficulties posed by the discur- sive ambiguities (Lim, 2018). We conducted the research in this context with a procedure based on sci- entific production via a systematic review that enabled us to provide a snap- shot of the matter (Codina, 2017). We also applied participant observation using the Mixed Methods Review perspective, as well as interactions with stakeholders (Grant and Booth, 2009), namely the professionals in the Al Jazeera corporation who were consulted directly. This work was carried out on visits to Doha, which came about because I was on the international jury at Al Jazeera’s Documentary Festival from 2012 to 2017. I have also had interactions seeking updates in 2020 through my job as a journalist with Ahmad Ashour, the senior content editor in Al Jazeera’s digital unit, and Taysir Alouni, one of the corporation’s most veteran jour- nalists and a witness to how the channel has evolved since it was founded. The contacts were initially in-person, but in recent months the pandemic has meant that the work has been done via videoconferencing. I have also sought Audiovisual verification in the evolution of television newsrooms Anàlisi 64, 2021 89 the professional opinion of Esraa Rabaya, the head of strategy and audience development at Al Jazeera. In this case, the communication and information exchange was conducted by email and telephone. The other professionals that contributed their points of view have preferred to maintain their ano- nymity. My aim is not to analyze specific cases of verification but to examine the little-studied matter of how audiovisual journalistic verification routines have evolved, as well as the new techniques used. By allowing content pro- duced outside the medium’s newsroom to be used, this content has influ- enced the perception of trustworthiness and the reorganization of the televi- sion newsroom. In keeping with the literature exploration and especially my participant observation of the verification process, specifically in the case of Al Jazeera, the following research questions were posed: Q1. Does the inclusion of material from digital sources in the media’s agenda lead to changes in organization and hierarchies in a large audiovisual newsroom? Q2. Do verification processes broaden the topics on news programs’ agendas? Q3. Are new forms and aesthetics being created for reporting with the participation of materials from social media users? Q4. By permitting materials from the social media to be on the agenda, is the new verification a key resource in locating new audiences? From these perspectives, the intention is to analyze the influence of verifi- cation processes on the journalistic routines of Al Jazeera’s professionals and their impact on the different platforms that make up the audiovisual media conglomerate. Hence, the following hypotheses were put forward: H1. The new verification processes are essential in enabling the social media to be included into a global cloud newsroom model. H2. The process of confirming the veracity of sources gives rise to new structures in which there is direct, continual mingling of communication professionals with engineers and IT experts within the newsroom, giving rise to new hierarchies in the decisions on the topics on the agenda. H3. The increased accessibility to such diverse, plural materials and sources made possible by the social media increases the flow of images that may be broadcast, which makes it essential to use human supervision and algorithmic technology from artificial intelligence. 4. Interaction and evolution of Al Jazeera’s verification routines The television corporation Al Jazeera was founded in 1996 with the aim of providing a different view of the news for Arabic peoples (Miles, 2005: 234), and it quickly gained a wide audience in the region (El-Nawawy and Iskander, 2002). Five years later, its website in Arabic was set up (Satti, 2020). Its broadcasting and website in English were launched in 2008, chal- 90 Anàlisi 64, 2021 Lola Bañon Castellón lenging the West’s media hegemony in the region and becoming a point of reference for modernization in audiovisuals and communication in Arabic (Zayani, 2005:1). It began at a time when the land-based model of television distribution, which could easily be subject to government intervention, was evolving towards satellite broadcasting with the potential for expansion. Al Jazeera reached its international standing following the 9/11 attacks in 2001, when it became the first network to provide images of Osama Bin Laden. That material was also distributed to the major global television cor- porations through agencies. The matter of whether the images of the Al Qaeda leader were real and how they had been obtained even became a sub- ject of debate in high-level international politics. At that time, without today’s digital technology, verification depended on human resources, which for Al Jazeera meant having a correspondent in Afghanistan, Taysir Alony, who was keenly aware of the political climate in the region, as well as image and sound technicians who could certify that the videos and audios had not been manipulated. At times, the process ended in a decision to delay the broadcast, while other times the materials were broadcast but with subtitles under the images with the phrases “Attributed to Al Qaeda” or “We have been unable to verify the authenticity”. Today, technology has evolved to make it possible to use Internet-based video editing and management systems in the newsroom, which allows meta- data to be imported, registered, organized, and synchronized so different departments have access to the cloud where all the information is stored. This entails a radical shift in the newsroom’s professional relationship with the material that it handles, going from a binary relationship (in which the journalist sends something via satellite) to a networked distribution in which audiovisual materials and data move around the Internet and are placed not in a specific repository but in a shared-access cloud. 4.1. New editorial structure: The digital unit In 2006, Al Jazeera set up its digital unit after noticing the changes in tech- nology and politics as well as in the attitude of a prosumer audience that was shifting from being passive observers to active collaborators. The unit conducted an experiment by handing out cameras to activists in different regions in the Arab world, and in 2008, during Mubarak’s rule, it was considering the idea of a report with material from Egyptian bloggers. In December 2010, the Tunisian revolution erupted and the authorities of the Ben Ali regime closed down the Al Jazeera head offices. Even so, the corporation had no problems providing images from the main conflict sites because social media and mobile phones were quite widespread by then. The same happened days later in Egypt: thanks to the installation of a satellite, Al Jazeera broadcast the signal from Cairo’s Tahir Square to the world. Audiovisual verification in the evolution of television newsrooms Anàlisi 64, 2021 91 Figure 1. Recording from social media made in Tunisia. A man celebrating the fall of Ben Ali in January 2011 (Al Jazeera) 4.2. Distortion of user-generated content When the Arab revolutions erupted in 2011, thousands of images were sent to Al Jazeera’s headquarters, making it a vital source of information for the rest of the world. At decisive moments, the traffic on Al Jazeera’s website rose by 1,000% and liveblogging by 2,000%. The traditional blog was overwhelmed, and Al Jazeera had to seek a platform capable of securely storing the traffic it was receiving from the social media. It hired a content management system (CMS), a framework for managing images and content that hosts users’ forums and blogs (Bañon, 2016). User-generated content (UGC) is defined as images and content uploaded to the Internet by authors unrelated to the media (Van Dijk, 2009; Cheong and Morrison, 2013; Bahtar and Mazzini, 2016). At that time, Al Jazeera was still seeking a model for guidelines to validate the images it received from activists, especially Egyptians and Tunisians, who were particularly active. It was a new experience for the entire world media 92 Anàlisi 64, 2021 Lola Bañon Castellón and mistakes were inevitably made in the midst of this historical acceleration. Nevertheless, a culture began to coalesce in which verification played an essential role, with the war in Syria marking a turning point. A specific unit with newsroom professionals was created, tasked primarily with verifying UGC, which became a news source virtually as important as agencies and the traditional newsroom (Marai, 2017: 22). As the war in Syria dragged on, Al Jazeera became strongly committed to covering it, which brought about a new phase in the network’s verification procedures. Training was provided to a hundred journalists to improve their skills dealing with UGC. Even so, the network was unable to avoid mistakes. Recognizing and explaining them is part of its newsroom code of conduct. One example occurred in December 2015, when several channels, including Al Jazeera, broadcast a video going around YouTube in which some Canadian children were supposedly singing the song Tala al Badr Aleina to welcome refugees. The song comes from the Islamic tradition and tells of the times when the prophet Muhammad had to seek refuge in the city of Medina. At a time when there was a sense of solidarity towards Syrian refugees, the video received thousands of displays of solidarity around the world. However, after the broadcast it was proven that it had nothing to do with them. The net- work apologized and prepared a second piece explaining how the misinfor- mation had occurred (Marai, 2007: 26). Figure 2. Concert falsely attributed as a welcome for Syrian refugees in 2015 (Al Jazeera) Source: Access to the video broadcast with false information about the performance: ; access to the news item in which Al Jazeera apologizes and recognizes the mistake. Watch | Facebook Another example of how the atmosphere can create the conditions for dis- information occurred following the terrorist attacks in Paris on 13 November 2015, which were committed simultaneously by gunmen and suicide bombers Audiovisual verification in the evolution of television newsrooms Anàlisi 64, 2021 93 in different places and caused 130 deaths. The Associated Press had recordings of the attack and identified a suicidal woman as Hasna Ait Boulahcen, with- out providing photos of her. But they began to circulate in the social media and Al Jazeera trusted one that several media had published. It turned out that the person in that photo was really Nabila Bakkatha. They located her in Morocco and interviewed her, admitting the mistake (El Katatney, 2017: 62). Figure 3. Al Jazeera Plus broadcasts a video with statements by Nabila Bakkatha, a Moroc- can woman, rectifying the news in which she had been identified as a suicide terrorist (Al Jazeera) Source: Access to the news item in which Al Jazeera recognizes the mistake: 4.3. The effects of verification on the newsroom The introduction of streamed images from the social media also entails a psy- chological leap in the concept of the newsroom, and in Al Jazeera there was resistance from some journalists (Khalifeh, 2017: 29). In the midst of this digital context, the verification procedure always begins with checking routines similar to the journalistic procedures before the social media. Hence, before conducting technical checks, journalists con- duct a prior exploration in an attempt to discover the origin of the source and ascertain whether it is reliable and therefore sent by someone they know or shared by a close colleague. 94 Anàlisi 64, 2021 Lola Bañon Castellón The level of falsehood in materials from UGC can appear in different forms: — Totally authentic. — True, but not recorded on the date it is broadcast. — True, but recorded in a different place from the one stated. — Manipulated, with distorted or added elements. — False and created with the explicit intention to deceive (Ghazayel, 2017: 69). Using this classification, the basic verification routine follows these basic steps: — Checking Twitter lists, enabling the followers to be classified. — Creating our own list to add accounts that interest us. — Searching via Google for lists created by other organizations and peo - ple, creating networks. — Using platforms like Tweetdeck that enable interaction with the plat- form. — Joining a relevant thematic WhatsApp and Telegram list. — Language check. Al Jazeera has a target audience in which Arabic is the majority language, with 420 million speakers and 30 dialects (Unesco, 2020). That implies consideration of the language, especia- lly to confirm geographical location. — And there is one basic precaution in the newsroom: downloading the video before beginning the verification process, since there is a risk it may be taken down. 4.4. Credibility tools and IT Every day, digital journalism has more new resources to improve the verifica- tion procedure (Pellicer, 2019). Here are some of the accessible tools: — Reverse image search, enabling images to be located if they are found elsewhere in the Internet. Google Image Search or Tineye are used with the key frames. The origin and context of the photo can then be ascertained. — Comparing with other servers. — Invid or Rev Eyecon can also conduct reverse searches for videos. — ImgOps enables images with similar content to be compared and alte- rations to be detected. — YouTube Data Viewer enables access to the main metadata. — Fingerprinting Organizations with Collected Archives (FOCA) and Metashield Clean Up detect metadata and information that is not visible in documents that can be found on websites. — Foller.me is a tool to find information about Twitter profiles. Audiovisual verification in the evolution of television newsrooms Anàlisi 64, 2021 95 — Pipl enables people’s profiles to be found via what they post on the Internet. — Anylizer analyses videos frame-by-frame, discovering irregularities if changes have been made. — Google Earth, Wikimapia, Open Streets Map and Google Maps can be used to verify places via geolocation. The list for detecting bots or fake profiles does not end there, nor do the techniques. For example, one can also look for the video’s URL or its ID code in Twitter to trace the first person who started circulating it. It is also important to be able to demonstrate that the stated location is correct (Younes and Mackintosh, 2017: 45-46). 4.5. The purpose of the Al Jazeera+ online platform By having professionally checked material, this study has been able to broad- en the topics on the agenda by introducing material from the digital sphere while preserving credibility – an essential factor for any medium, and even more so in a region in serious political upheaval. Hence, Al Jazeera’s digital platform is an Internet space that works: the YouTube channel has racked up an average of three million views, and it has over a million followers on Twitter. In this vein, it has successfully managed a transition involving turning a simple news website into an experimental informative online platform (Bañon, 2016). As a result of this evolution, in 2014 the corporation launched Al Jazeera Plus (AJ+), offering videos directly for Facebook, YouTube, Instagram, Twit- ter and Medium through mobile apps. It thus uses the social media to attract followers by encouraging them to share their stories, with formats based on very recent images or very short videos with little text. These changes and the entry into the digital world rendered it necessary to rethink internal proce- dures (Roettgers, 2013). The experience of AJ+ increased the importance of verification in journal- istic routines: the professionals are not always on the ground and the eyewit- ness reports come from indirect sources whose authorship is unknown at first glance, nor is there always information on the circumstances in which the images were recorded or the reasons for spreading them. One example of a verification process taken to the extreme of instantaneousness was the case of the Address Hotel fire in Dubai on New Year’s Eve, 2015. Minutes before the traditional media reported the news, AJ+ used the Datamindr service to which it subscribed in order to verify the location of the tweets. They thus detected that three eyewitnesses were transmitting the information with Peri- scope. 1 The verification was completed by checking the location and com- ments from eyewitnesses in the social media and by securing permission to 1. A Twitter app that enables live videos to be transmitted 96 Anàlisi 64, 2021 Lola Bañon Castellón use images from one of them. Thus, AJ+ created a story in a matter of min- utes (El Katatney, 2017). Figure 4. The video of the Address Hotel fire was viewed three million times and shared 40,000 times in just one day, with an impact calculated at twenty million people Source: Al Jazeera. The observation that new formats have been incorporated into news spac- es has thus been confirmed. One feature of AJ+’s posts is raw videos uploaded directly from mobile phones to the social media which are far from the qual- ity standards of traditional broadcasts. They are products created for the dig- ital sphere bearing the shared behavior and consumption of different social media in mind. This has enabled AJ+ to achieve audience growth, especially at times of humanitarian crises such as in Palestine or the exodus of refugees from the war in Syria. Such newsworthy situations have revealed that the most solid criteria to become viral are relevance, immediacy, and emotiveness (Rahimi, 2015). AJ+ averages half a million interactions a day, reaching 30 to 40 million people (El Katatney, 2017: 66). 5. Results: Examples of verification and impact in the newsroom Al Jazeera’s verification unit contributes 60 news items a day on average, which are added to the topics in the agenda after passing the pertinent checks. Here are examples in which it successfully detected fake news: 2 2. Data provided by Ahmed Mansour, Senior Editor for Digital Content at Al Jazeera. Audiovisual verification in the evolution of television newsrooms Anàlisi 64, 2021 97 1. Displacement of Syrian refugees after floods in northern Syria (20-01-21): The verification unit discovered that the recording was from a video from 2019. 2. Photo of floods in Sudan (6-09-2020): The verification unit discovered the photograph was from Haiti, dated 2017. 3. Attack on Hassan Nasrallah’s vehicle in the Lebanon (18-01-21): The verification unit discovered that although the date was correct, the image showed an explosion in the Syrian city of Idlib. It is clear that using materials from the digital sphere entails a transition from the traditional, highly hierarchical newsroom to another model more in line with a horizontal rationale. This shift means that journalists have to work with a content management system, which enables them to make productions using accessible software and coexist with a system hosted in the cloud. The nine members of Al Jazeera’s verification unit are all under the age of 35 and have previously worked in the newsroom, preferably in posts in the international and sports sections. The trend is towards a newsroom in which there is a shift from being pro- ducers of visual and news products to mediators and custodians of quality standards, with verification the top priority. This evolution will intensify as the subscription systems for distributing signals using broadband or IPTV connections improve,3 thus enabling streaming media4 with audiovisual con- tent broadcast via the Internet. Guiding protocols are also increasingly being adopted to improve the security of the Internet as a mechanism for convey- ing the production processes (Hunter, 2021). In the case of Al Jazeera, this transformation itself has given rise to new sources and a differentiated agenda of topics, although journalists specializing 3. Internet Protocol Television. 4. Streaming audio-visual content broadcast via the Internet. 98 Anàlisi 64, 2021 Lola Bañon Castellón in the tasks typical of each medium are still required even in newsrooms that have undergone integration processes (Salaverría and Negredo, 2008: 169). In the traditional newsroom, organizing a live broadcast required hiring a satellite, finding a time zone when the content can be uploaded to it, and broadcasting it, not to mention a very significant economic investment. With the Make TV platform, Al Jazeera can download dozens of videos via Internet simultaneously. Thus, journalists from anywhere in the world can send news to a central clearinghouse, which decides whether the image should be stored or broadcast live, depending on the circumstances. One- way organization is thus giving way to work dynamics in the cloud, where the stream of images and data have to be managed. Satellites are still neces- sary for distribution, but the mobile networks have been playing an increas- ing role in channeling, exchanging, moving and organizing information. They generate and lend coherence to this movement and organization pre- cisely in the cloud system. At the core of this process are the verification routines, which provide rec- ognition to the materials from the digital sphere. This intensifies the need for mediation from journalists, IT professionals and cross-disciplinary profession- als. They are also highly specialized tasks that require a collaborative culture with the outside world, in which important entities become associates. For example, Al Jazeera is a member of the First Draft Partner, an entity encom- passing the largest social media platforms and verification projects which are seeking to tackle these challenges together (Fernández-García, 2017). Verification tasks guarantee credibility, but their use in a newsroom in the cloud where streams circulate poses four significant challenges: overcom- ing ethical tensions, putting proven facts into context, generating relevant information and observing public interest, and raising awareness about cer- tain social issues. (Srisaracam, 2019) 6. Discussion and conclusions This study has confirmed that including information and images originating from the digital sphere has made it necessary to broaden the agenda of topics and the number of sources (Q2). Since there is material coming from the circuit outside the newsroom, the verification process is crucial and has led to changes in the organization of the newsroom (Q1), with the creation of a group of journalists and technicians specializing in this task. The qualitative characteristics of these materials coming from the audience’s mobile phones are far from the usual quality standards in the professional sphere, but they provide advantages such as immediacy and accessibility, which is leading to a new aesthetics of reporting (Q3) and contributing to the culture of TV view- ers’ participation. In this sense, management and verification makes it possi- ble to use material from the social media with quality assurances. These social media have helped redirect audiences to the screen and locate new potential audiences (Q4) for television broadcasts or the channel’s website. Audiovisual verification in the evolution of television newsrooms Anàlisi 64, 2021 99 In this study, therefore, Hypothesis 1 was confirmed and we can con- clude that the verification processes and their improvement are key features in enabling traditional newsrooms to transition to a cloud structure, since they allow the veracity of visual material from the social media to be checked, thereby preserving a fundamental feature of quality journalism. This model is also shifting from hierarchically organized professional relationships towards more horizontal models, in which technological training is essential. Hence, Hypothesis 2 is also confirmed in the sense that the new news- room has news professionals coexisting with experts in artificial intelligence and IT. The new nature of the newsroom is no longer strictly in-person; cor- respondents coexist with journalists via virtual mobility, seeking and receiving images streamed from the social media. The cloud needs to organize the traf- fic, and managing it is leading to the establishment of new horizontal hierar- chies within the newsroom. This should be the subject of subsequent studies, because the harmonization of the list of functions of the different information professionals should include excellence as a goal not only to find the best image or video but also to create the context to provide audiences with the best explanation via new formats generated in the social media. Technologies enable all of the material to be available to the group in the cloud newsroom, although verification protocols are the first step in this availability. Neverthe- less, this availability is subject to usage protocols, for example prioritizing broadcasting in the network’s news programs over others for strategic reasons. Finally, Hypothesis 3 is also confirmed, as the use of algorithmic technol- ogy is essential in verifying the volume and diversity of materials generated on the Internet. However, at the same time, the need for verification does not exclude traditional techniques. A newsroom with a hybrid coexistence of professionals in no way diminishes the importance of journalism but instead makes it necessary. There is increasing evidence of the need to lend meaning and context to the information, which means that journalists keep and pre- serve their topic specialization. Moreover, insofar as verification preserves the audience’s trust, it allows topics and sources in the agenda that have traditionally been excluded, including new coverage. It is therefore a relevant and inevitable way to incor- porate new audiences, thereby helping to grow audiences and rendering it possible to maintain big audiovisual corporations. The additional challenge posed for journalism by the new cloud news- rooms is how to boost creativity and create protocols for internal cohesion to motivate the professionals to organize the creation of quality content that serves society.","Castellón, L. B. (2021). Audiovisual verification in the evolution of television newsrooms: Al Jazeera and the transition from satellite to the cloud. Anàlisi, (64), 85-102."
"SCOP_103","Verification of Digital Sources in Swedish Newsrooms — A Technical Issue or a Question of Newsroom Culture?","This article analyses and discusses attitudes and practices concerning verification among Swedish journalists. The research results are based on a survey of more than 800 Swedish journalists about their attitudes towards verification (Journalist 2018) and a design project where a prototype for verification in newsrooms – the Fact Check Assistant (FCA) – was developed and evaluated. The results of the survey show a lack of routines when it comes to verifying content from social media and blogs and considerable uncertainty among journalists about whether this kind of verification is possible. The development of the prototype initially created reactions of interest and curiosity from the newsroom staff. Gradually, however, the degree of scepticism about its usability increased. A lack of time and a lack of knowledge were two of the obstacles to introducing new verification routines. It is not enough to introduce new digital tools, according to the journalists. Management must also allocate time for training. The paper’s ultimate conclusion is that changing journalists’ and editors’ attitudes towards verification in this digital age appears to be guided by newsroom culture rather than technical solutions.","Journalism Studies","Article","2021","Y","Y","Prototype","Practices","Evaluation","0","The discussion about fake news and disinformation has made fact-checking and verifica- tion a focus of journalism research. Fact-checking has developed into a new genre in news journalism, both within established news organisations and in separate entities, such as Politifact in the US and Faktiskt.no in Norway (Graves 2016). However, there is an important difference between the notions of fact-checking” and verification” (Wardle 2018; Allern 2019). Fact-checking is the process of assessing the val- idity of claims that have reached the public domain through some form of media. For example, journalists could use political fact-checking to assess the validity of statements by politicians and public figures during election campaigns. These statements may be broadcast or published in traditional media or posted online. The results of a fact- check are often published with a judgement on a graded scale from completely false” © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor  Francis Group This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial-NoDerivatives License , which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited, and is not altered, transformed, or built upon in any way to completely true”. This form of fact-checking requires an ability to cross-validate argu- ments based on earlier established facts. When it comes to verification in the newsrooms, this practice has been part of the working process of journalists since modern forms of journalism were developed. The ver- ification tools used in newsrooms have evolved over time to account for digitalization and proliferation of unverified multimedia content on social media. Verification is a basic norm in journalism and is traditionally carried out before or during the process of publishing (e.g., live reporting). Kovach and Rosenstiel define verification as the crucial divide between journalism and other kinds of media content, like entertainment, propaganda, and fiction – the essence of journalism is a discipline of verification” (2001, 71). In order to be properly applicable in daily work, many countries have codified the value of verification into formal ethical principles. In Sweden, Accurate News” is the title of the first paragraph in the professional ethical rules of journalists.1 Verification of digital content is often associated with assessing the validity of content such as images, videos and texts that are circulated on social media. This verification is extensively technical in nature and requires a special set of skills in the domain of open-source intelligence (OSINT). There is evidence that political fact-checking and digital content verification can occasionally overlap, particularly when politicians support their claims using digital multimedia content circulating on social media. This leads to a demand for technical digital content verification experts and traditional political fact-checkers to work more collaboratively (International Fact-Checking Network 2020). Furthermore, this professional commitment to verification is challenged by both exter- nal and internal factors in media development (Ekström 2020). There is a large flow of information from professional sources with sophisticated methods to news management. There are also sources that circulate news” on different websites and social media plat- forms, and these sources are often hard to evaluate. Internally in the newsrooms, journalists experience increased pressure in their daily work. Structural changes on the market have led to fewer journalists producing more content for multiple platforms, and the space for critical investigation and verification has been reduced (Witschge and Nygren 2009). In addition, accuracy ideals are challenged by the development of 24-hour rolling news practices and so-called liquid news, where verification becomes part of the publishing process rather than something that precedes it (Karlsson 2012; Widholm 2016). The purpose of this article is to go deeper into the field of verification in today’s digital environment. The article describes a joint project between academics and leading Swedish media companies that aimed to develop and test a digital tool for verification of digital content and evaluating sources – the Fact Check Assistant (FCA). The article also presents a recent survey among Swedish journalists concerning attitudes towards verification. This leads us to four research questions: 1. What are the attitudes of Swedish journalists towards verification? 2. Are there established routines for verification in the newsrooms? 3. How can a digital verification tool improve these routines, and what functions should be included in the tool? 4. What kinds of problems arise from the introduction of new tools and routines for asses- sing digital sources and verifying digital content? 2 M. PICHA EDWARDSSON ET AL. Background Verification in the Journalistic Process There is common agreement on verification as a professional norm in journalism. However, it is complicated to study the journalistic process to see how verification is done in practice. Verification is a fluid and contested practice, and studies show there is a lack of consensus in how to verify (Hermida 2015). Only a small part of the journalistic work is devoted to cross-verification, the most usual form of verification. Cross-verification entails having at least two independent sources for a claim, the juxtaposition of two news sources / ... / against each other with the express intention of ascertaining the infor- mation’s reliability” (Godler and Reich 2017, 567). A German study shows that only 5.5 per cent of the working time is devoted to cross-verification (Machill and Beiler 2009). In an Israeli study, about half of the articles had any cross-verification (Godler and Reich 2017). In local journalism, cross-verification seems to be even more rare; in a Swedish study of coverage of local authorities and politics, only one-third of the articles had at least two independent sources (Nygren 2003). Qualitative interviews with Canadian journalists show that verification is part of the whole working process. The need for cross-verification depends on how sensi- tive the issue is, and journalists use their experience when deciding on what to check or not (Shapiro et al. 2013). A common solution is to use well-known and authoritative sources in cases where verification is not possible. The degree of cross-verification is much lower if the source has been frequently used before (Godler and Reich 2017). The conflict between speed and the need for verification is as old as journalism itself. Digitalization of the media system has brought this tension to a new level, and there are tendencies of softer attitudes towards verification among journalists. In a compara- tive study with journalists in Sweden, Poland, and Russia, about one-third of the partici- pants agree that verification is not needed before publishing online; it can be done during the process. Forty-two percent believe that the online audience has lower demands on verification than print and broadcast audiences (Nygren and Widholm 2018). Increasingly, transparency has become important as a professional norm – not to replace verification but to open the process for the audience. In live reporting, verification becomes a collaborative effort for journalists and the audience; facts become less fixed and more fluid (Hermida 2015). However, the journalist is still regarded as a truth- seeker, but often in collaboration with the audience. Another important strategy is to put the responsibility for the accuracy on the source by means of only referring to what is being said (Ekström 2020). In this digital flow of information, social media platforms are important sources for journalists seeking information. Interviews with online journalists in Western Europe show great ambivalence towards these sources in terms of how to evaluate and verify facts and pictures from social media (Brantzaeg et al. 2016). Journalists often stick to trusted official online sources, and they often use traditional methods such as calling sources by telephone to verify the accuracy of what they find on social media. The level of knowledge about digital verification tools is low, and only a few journalists use JOURNALISM PRACTICE 3 them. What online journalists are asking for are tools that will help them conduct basic evaluations of accounts, geolocation, the time stamp, and the history of accounts, for example. Attitudes towards verification are an important part of the different journalistic cultures that exist today. These cultures differ depending on the existing media systems and between different media types and media organisations (Zelizer 2005). There are two main dimensions concerning epistemology in the journalistic cultures according to Hanitzsch (2007): objectivism and empiricism. The first of these dimensions is a philoso- phical question – is it possible to reach an objective truth” or are the statements the result of subjective judgements? The second dimension is about how the journalist can justify his or her claims – by empirical means as facts or are they based on opinions, ana- lyses, and values? The importance of verification among journalists depends on these questions. When it comes to media systems, a clear difference has been observed earlier between the fact-based Anglo-Saxon tradition and the more subjective and analytical continental European journalism (Chalaby 1996). However, this has been changing over time. Over the last decade, fact-based journalism has been challenged by more subjective and values-based journalism, as, for example, in Fox News. As a reaction to this development, we observe the rise of fact-checking as a new genre within news journalism (cf. Graves 2016). Methods Action Research and a Survey In her keynote at the Future of Journalism conference in 2017, Claire Wardle asked for more cooperation between academics and practitioners to research the information dis- order (Wardle 2018). One example is her own organisation, First Draft, which is based at Harvard. By carrying out experimental projects that bring together media companies and journalism educators, the organisation tests new forms of fact-checking and verification. The Fact-Check Assistant (FCA) is a similar project based in Sweden. In 2017, Swedish public service television (SVT) and Swedish public radio (SR), along with two large quality newspapers (Dagens Nyheter and Svenska Dagbladet), received a grant from the Swedish authority for research and innovation, Vinnova. The purpose of the grant was to develop a tool for verification of digital sources in the daily newsroom work – the FCA. Södertörn University was the academic partner responsible for development and coordination. The purpose of the project was to design a prototype in collaboration with journalists and tailor the tool to their specific needs to adopt a systematic and transparent method of verifying claims. The development part of the Fact-Check Assistant can be described as action research as originally described by Lewin (1946). Action research methodology is research on the conditions and effects of various forms of social action, and research leading to action” (Lewin 1946, 35). Galtung (2002) describes the methodology as a way to not only under- stand a problem but also to solve it with the people involved. Kemmis, McTaggart, and Nixon (2014) argue that all action research shares some common key features, for example by rejecting conventional research approaches where an external expert 4 M. PICHA EDWARDSSON ET AL. enters a setting to record and represent what is happening” (Kemmis, McTaggart, and Nixon 2014). Two features are apparent, according to Kemmis et al.: . the recognition of the capacity of people living and working in particular settings to participate actively in all aspects of the research process; and . the research conducted by participants is oriented to making improvements in prac- tices and their settings by the participants themselves (Kemmis, McTaggart, and Nixon 2014). The case study presented in the article displays methods that are common in inter- action design, more specifically goal-directed design as described by Cooper et al. (2014), which has earlier been used in a large number of media studies when implement- ing and testing new technical tools. In one such study, as described by Thurman, Dörr, and Kunert (2017), an automated news writing robot was introduced to British journalists. In another study using interaction design, as described by Stray (2019), AI is introduced to investigative reporters in the United States and Britain. In a third study, a creativity support tool was introduced to journalists in Norway (Maiden et al. 2018). In line with the methodology of interaction design, these studies have in common that they intro- duce, develop, test and implement new innovative digital tools for journalists and evalu- ate the result of the implementation. Our developmental part of the project consisted of workshops where the purpose and idea of a fact-checking tool were discussed with journalists and researchers. After the workshops, a prototype fact-checking tool was developed based on the initial feedback received during the workshops. Journalists and researchers tested and evaluated the pro- totype and then it was further developed in an iterative process that started in late 2018 and ended in the spring of 2020. (The iterative development process is described in more detail further on in this article as this is part of the case study results.) The method of goal-directed design (Cooper et al. 2014) entails that the users, in this case journalists and researchers, are engaged in the technical development of the tool. The main idea behind goal-directed design is to use the needs of the users as the starting point for developing the tool and then aim to build the tool in direct relation to the user needs. This development process usually takes place in a number of iterations with the users. Between every test or iteration, the prototype is developed further, and after a number of tests, the prototype is ready to be launched as a new product. The other part of our study consists of empirical material from the survey Journalist 2018” gathered by JMG, University of Gothenburg, Sweden. This is a broad survey on the ideals and daily work of Swedish journalists, and part of it included questions on ver- ification, the answers to which we found particularly relevant to this study and are pre- sented here for the first time. The survey was sent to a random sample of members of the Swedish Union of Journalists, which organises about 85–90 per cent of all journalists in the country. The response rate was 52 per cent, and the maximum number of respon- dents was 1,151. We did not address the questions on verification to freelancers, so the number of respondents to these particular questions was 876. We include the results from the survey in this article to analyse the point of departure for the FCA project, the existing attitudes among journalists towards verification of digital sources, and the situ- ation in the newsrooms with regard to standards and routines. JOURNALISM PRACTICE 5 This study is limited to one country, Sweden, with a media system described as demo- cratic-corporative” by comparative journalism research (Hallin and Mancini 2004). The system is characterized by strong journalistic professionalism and a low degree of political influence in media. This is important to keep in mind when comparing the results with other countries. Results A Need for Verification Routines Very few of the Swedish journalists work in newsrooms with clear routines for verification of content from social media and blogs according to the survey. Only 17 per cent agree that there are clear routines, and 43 per cent disagree. There is also a large group with no opinion that give no answer, probably a sign of uncertainty in this area. There are some differences between media types; a slightly larger share of the journalists working in public service radio and television are aware of routines for verification of content from social media. This difference is also visible in other questions in the survey concerning ver- ification. For example, journalists in public service media put more emphasis on equal ver- ification in publishing on all platforms: 57 per cent of journalists in public service agree compared to 49 per cent in newspapers Table 1. Is it possible to completely verify content on social media? Journalists demonstrated considerable uncertainty about this question. Less than one-third agree that it is poss- ible, and an equal share disagree. Many respondents neither have an opinion nor answer this question. There are also large differences between age groups regarding this question. Young journalists have greater confidence in verification of digital content, while their older col- leagues are more sceptical. Thirty-seven per cent of the young journalists agree that this type of verification is possible, and only 20 per cent disagree. For journalists over the age of 50, the figures are the opposite: over 40 per cent feel that this kind of verification is not possible Table 2. A deeper analysis shows a strong correlation between the answers to these questions (Pearson .983, significant at the 0.01-level). The journalists who answer that there are clear routines for verifying digital sources also believe that it is possible to do this type of ver- ification. And the opposite: a lack of routines also results in lower confidence in the pro- spect of verifying digital sources. To summarise, the survey shows a low level of routines Table 1 . In my newsroom, there are clear routines in verifying content from social media and blogs (per cent on a scale of 1–5, where 1 is agree completely) Newspapers Magazines Public service Other Total 1 – Agree completely 7 5 11 13 8 2 9 6 12 12 9 3 15 18 19 14 17 4 20 14 18 18 18 5 – Disagree completely 32 25 20 11 25 No opinion 11 22 14 18 15 No answer 6 10 6 14 8 Total 100 100 100 100 100 N= 383 169 211 97 878 Source: Journalist 2018, a survey from JMG, University of Gothenburg. 6 M. PICHA EDWARDSSON ET AL. when it comes to verification of digital sources and a diverse picture when it comes to the possibilities to perform this kind of verification in daily work. The results from the survey were also confirmed in the newsroom study assembled in the first step of the FCA project. One of the university researchers visited three news- rooms: local, regional, and national. With a combination of interviews and observations, he found that journalists were interested in digital verification and there was a need for tools to promote this. There was a general consensus for the idea that a digital assist- ant could function as a support mechanism to systematise the process and share knowl- edge with colleagues in the newsroom. At the same time, journalists were clear about the lack of time in their daily work, and some said a digital tool could be too much when they instead can use traditional methods like picking up the phone (Larssen 2020). He also found considerable resistance against verification as a general concept – one journalist was provoked and concluded, it might give the impression that regular journalism does not do fact-checking. But that is exactly what we do, it’s what we are good at and what journalism is all about” (Larssen 2020, 209). A Three-pillar Model As the iterative development process of this case study is part of the study’s results, a common structure in goal-directed design studies (Cooper et al. 2014), we will describe the process in this section. The theoretical starting point for the developmental process is a model for assessing news credibility based on previous research by Metzger et al. (2003). This shows that a three-pillar model can be applied by integrating a comprehensive evaluation of (1) the medium, (2) the source, and (3) the message (Metzger et al. 2003). This model, shown in Figure 1, served as our theoretical foundation for building a sys- tematic verification process to assess news credibility in a digital environment. To assess each of these three pillars, the user must answer a set of questions in a methodological and consistent manner as described below. Assessing the Medium A medium can be an app, a website or any other service through which content can be posted or shared. To evaluate a medium, there are a number of questions that the journal- ist can answer, such as Table 2. It is possible to completely verify news and other information found in social media and websites? (per cent by age group on a scale of 1–5) –29 30–39 40–49 50–59 60– Total 1 - Agree completely 11 15 9 12 10 11 2 26 22 19 13 9 18 3 25 18 22 17 13 19 4 11 20 19 21 19 19 5 – Disagree completely 9 6 8 20 26 13 No opinion 11 12 12 13 9 12 No answer 7 7 11 4 14 8 Total 100 100 100 100 100 100 N= 98 230 226 226 96 878 Source: Journalist 2018, a survey from JMG, University of Gothenburg. JOURNALISM PRACTICE 7 . Does the medium have a good reputation in the past (or is it blacklisted)? . Are there any technical security concerns (e.g., no SSL access)? . Does the WhoIs database entry support details provided publicly? . Is the medium known for not having any particular bias? Poor ranking, popularity, and reach? . Does the medium have liability terms (legally accountable)? Does it allow readers to flag/report content? To answer these questions, it is possible to utilise a number of digital tools that are often used for investigating and assessing media platforms and applications. Two examples of such tools are WhoIs/reverse DNS checks to verify the authenticity of web- sites and Google Maps/Earth for verification of geobase information. It is important to acknowledge that tools to scrutinise a particular medium may vary depending on the type of the medium and the skill level of the journalist. Assessing the Source The source of particular content corresponds to the individual(s) or entity(-ies) that pub- lished or shared the content being fact-checked. Among the questions that help assess a source are . Does the source have verifiable credentials? . Did the source ever get caught publishing dis/misinformation in the past? . Does the source have a history of quality content published by reputable media? . Can you check the source’s network of influence? . Can you check for any difficulty to authenticate information about the source? Figure 1. The three-pillar model for assessing news credibility (Metzger et al. 2003). 8 M. PICHA EDWARDSSON ET AL. Similar to media, journalists can scrutinise sources using various tools that are available either freely online or for a fee. Examples of tools that can provide insights into the iden- tity and background of a source are LinkedIn, Facebook and Pipl. Assessing the Message (Content) Finally, journalists can dig deeper into the actual message to verify the content that has been posted or published. When evaluating the validity of specific content, it is important to consider its format, because the format dictates which verification tools are most suit- able. For example, when verifying the authenticity of an image-based claim, this may be done through reverse image search tools, while it is more appropriate to assess the val- idity of a text-based statement by consulting a reliable online source. It would be necess- ary to investigate the validity of the content by answering a set of predefined questions based on whether the content is an image, video, or text or a combination of all three. For example, if the claim is based on an image or a video, questions that could be asked to assess the content’s validity would be . Was the image used in a misleading context? Has it been manipulated/doctored? . Does the image contain mismatching metadata? . Is there any audio manipulation in any way? . Has the image been fact-checked before? . Are dates, names, numbers, etc., correct? . Has the image used references that are verifiable? There are many free and commercial tools that are often used by reporters to do advanced OSINT to verify the authenticity and validity of particular content (Hayden 2019). Examples of tools that are used to reverse-search multimedia content are Google Images and TinEye for images, the InVid browser plug-in and YouTube Data- Viewer. There are also tools to investigate whether an image was doctored or manipu- lated, such as Forensically, which is used to magnify and analyse images and extract metadata information such as the place and time the image was captured. Google and archive.org are examples of services that provide cached copies of content that existed on the web but was later removed. Issuing a Verdict on a Claim’s Validity To document and collect results from the verification process, a model developed in the new genre of fact-checking was taken into the FCA project. Based on the answers to the questions about the medium, the source and the message, the journalist can make an overall assessment of the claim depending on the degree of confidence the journalist has in the final judgement. The scale available consists of the following scores: false, mostly false, mixed, mostly true, true and no decision. The journalist should justify the score by a thorough and evidence-based rationale. It is noteworthy that the verdict is meant as an interpretation by the journalist and not an objective fact in itself since not all fake news can be considered false. According to Molina et al. (2019), false news is but one of seven types of online content that could be labelled as fake news”. The other types of fake news” are polarized content, satire, misreporting, commentary, per- suasive information, and citizen journalism (Molina et al. 2019). While it cannot guarantee a conclusive verdict, this three-pillar approach allows jour- nalists to consistently and transparently show how the various questions were answered and where there are missing gaps in the investigation. It is important to document the journey from start to finish so it can be integrated into a sustainable knowledge base for future reference, potential distribution and sharing. Figure 2 summarizes how the verification process of FCA is followed. All testers were guided through the process and also offered the possibility of watching extensive tutor- ials on the topic. Figure 3 shows an example of how an image-based claim is reviewed. The questions on the left have resources that can be used to answer them. These resources can be added and updated continuously. The above is an example of an image-based claim just added and available for review. Workshops and a Prototype Upon developing the theoretical framework of the three-pillar model in our project, it was important to put it to the test by not only training a set of journalists in how to use it and observing their use but also getting their feedback as part of a goal-directed design process (cf. Cooper et al. 2014). A total of fifteen journalists and editors working at six Stockholm-based Swedish media corporations and three journalists from other parts of the country participated in two full-day workshops. Most of the workshop participants expressed interest in implementing this model and found it helpful to speed up the process and make it consistent and systematic. They made suggestions to simplify and structure the process so they could break the work of verification down into separate tasks. For example, one task would be to review a particular medium and another task would be to assess the validity of a picture-based claim. Participants recommended setting up a system that would allow them to work on certain verification tasks and then temporarily leave their desks. When they returned later, they wanted to be able to pick up where they had left off without losing the work they had already done. In other words, they requested having a memory-based system that would allow them to organise their time and work around their own pace. This request was attributed to the fact that they were rather busy with daily journalistic routines. Another strong request that emerged from the workshops was to have a library or toolbox of all the available digital tools that would help them answer the questions in the three-pillar model. According to some of the attendees, each tool should be easy to learn, perhaps through a tutorial and documentation. Furthermore, they suggested allowing the toolbox to be customised with additional tools that could be added as needed. The project team also arranged a number of field visits to present the prototype and get initial reactions from editorial managers in the media companies. While the general reactions were of interest and curiosity, there was an apparent degree of scepticism about the tool’s usability within newsrooms as well as towards the notion of sharing work with journalists in other media. In one particular meeting, the discussion about 10 M. PICHA EDWARDSSON ET AL. the prototype was highly critical, with the editorial managers taking the position that the prototype was not actually what was needed. Instead, they suggested creating a tool to discover disinformation that is poised to go viral. Since this was beyond the scope of the project, we did not pursue it. After this initial step, the developers created a prototype online application. The pro- totype was based on the assumption that for every single piece of online content, there was at least one digital medium where that content was published or shared, and there was at least one source that had actively put that content – as the message – on the Figure 2. The process that testers follow to add and review entries in FCA. JOURNALISM PRACTICE 11 medium. Hence, the role of the prototype was to guide journalists through the verification process by evaluating the reliability of the medium, trustworthiness of the source, and val- idity of the content. Through the prototype, journalists would be able to add specific claims to fact-check. Each claim would have a reference to the medium, where it was pub- lished along with the source that published it. Additionally, the prototype was meant to harness Web 2.0-based interactivity by allow- ing multiple journalists to verify and cross-validate the same claims. This approach, which relies on the wisdom of the crowd, has proven to be powerful in improving the reliability of the outcome, particularly if those involved in the process are vetted in advance (Kittur and Kraut 2008; Ullrich et al. 2008). We chose the scalable Ruby on Rails development framework to develop the pro- totype given that it is a proven efficient open-source web development framework with a structure that adheres to object-oriented programming. Due to its multi- layered functional structure with the Model, View, and Controller components, it was easily scalable and could serve as a base for integrating new services with ease (Bächle and Kirchberg 2007). We also chose it for its speedy development Figure 3. An example of how an image-based claim is verified in FCA. 12 M. PICHA EDWARDSSON ET AL. process and high server performance in addition to its wide community and thorough documentation. Feedback and Testing in the Newsrooms In April 2019, the first version of the prototype was completed and ready for testing. Initially, the testing included students at the university’s journalism department, and in May 2019 a workshop was held to give journalists from the Stockholm-based media a hands-on trial. During this workshop, the participants made important com- ments on various functions, and the developers subsequently revised the prototype in order to be more extensively tested. During the development stage, three trainees who participated in the earlier workshops volunteered to be test pilots” for the proto- type and provide feedback. Later, another five journalists with different backgrounds than the nine Stockholm-based media companies also tested and evaluated the prototype. The test sessions and evaluations were recorded, and the test persons’ reactions and comments were collected and analysed. Semi-structured interviews were undertaken with journalists in the newsrooms as a follow-up of the test sessions and evaluations. The analyses of the workshop material aimed to discern and organise emerging patterns and themes in the re","Edwardsson, M. P., Al-Saqaf, W., & Nygren, G. (2021). Verification of Digital Sources in Swedish Newsrooms—A Technical Issue or a Question of Newsroom Culture?. Journalism Practice, 1-18."
"SCOP_149","Fighting deepfakes: Media and internet giants’ converging and diverging strategies against hi-tech misinformation","Deepfakes, one of the most novel forms of misinformation, have become a real challenge in the communicative environment due to their spread through online news and social media spaces. Although fake news have existed for centuries, its circulation is now more harmful than ever before, thanks to the ease of its production and dissemination. At this juncture, technological development has led to the emergence of deepfakes, doctored videos, audios or photos that use artificial intelligence. Since its inception in 2017, the tools and algorithms that enable the modification of faces and sounds in audiovisual content have evolved to the point where there are mobile apps and web services that allow average users its manipulation. This research tries to show how three renowned media outlets—The Wall Street Journal, The Washington Post, and Reuters—and three of the biggest Internet-based companies—Google, Facebook, and Twitter—are dealing with the spread of this new form of fake news. Results show that identification of deepfakes is a common practice for both types of organizations. However, while the media is focused on training journalists for its detection, online platforms tended to fund research projects whose objective is to develop or improve media forensics tools.","Journalism Studies","Article","2021","Y","Y","Process","Practices","Survey","4","The implementation of artificial intelligence in techno- logically mediated communicative processes in the net- worked society poses new challenges for journalistic veri- fication. Simultaneously, it has enhanced different stages of news production systems. The effects of the technol- ogy that houses artificial intelligence are present both in the communicative flows and in a large part of the social- ization dynamics. Hence, the threat introduced by the emergence of deepfakes, doctored videos by using artifi- cial intelligence, arises as one of the most recent hazards for journalistic quality and news credibility. Although deepfakes are not only a concern for journalism, their existence has raised the uncertainty among users when trying to access news content. Likewise, the increasing sophistication of this form of fake news has put profes- sionals on alert (Vaccari  Chadwick, 2020). Misinformation has increased its relevance over the last few years, having now a major significance in the public agenda (Vargo, Guo,  Amazeen, 2018). In con- sequence, the number of projects and measures for counteracting this phenomenon has grown consider- ably. Example of this could be the Action Plan Against Disinformation developed by the European Commission (2018). Media and journalists are aware about how the Media and Communication, 2021, Volume 9, Issue 1, Pages 291–300 291 success of hoaxes undermines democracy and its relia- bility (Geham, 2017). Therefore, they try to react with actions that facilitate transparency and the fulfilment of their professional and ethical rules, like fact checking (Lowrey, 2017). This is an issue on which different lines of thinking have been opened. All of them try to counter- balance the result of misinformative political and social trends that became significative in 21st century societies (McNair, 2017) in a context where social media plays a central role as a space for the generation and dissemina- tion of fake news and the consequences that this entails (Nelson  Taneja, 2018). Techniques that guarantee the information verifica- tion’s efficiency—one of the core elements of journal- ism since its consolidation as a communicative technique in the modern age (Kovach  Rosenstiel, 2014)—are looking inside technological innovation for tools with the ability to support professionals in their daily tasks. It is true that the norms followed for producing accu- rate informative pieces are in some cases unclear and nuanced (Shapiro, Brin, Bédard-Brûlé,  Mychajlowycz, 2013). Nonetheless, journalism should not retain anti- quated verification techniques, but should rather update them to computational methods in order to evaluate dubious information (Ciampaglia et al., 2015). There are currently revamped verification systems with fact- checking techniques. Those may contribute to the elab- oration of news pieces that, after the application of a complex group of cultural, structural, and technological relations would show the legitimation of news in the dig- ital age (Carlson, 2017, p. 13). Although a high level of mistrust remains, some techniques used in these infor- mation verification services are able to build a bigger reliance by the users (Brandtzaeg  Følstad, 2017). Furthermore, this scenario has seen the emergence of new proposals for renewed professional practices and profiles (Palomo  Masip, 2020). This could be the case of constructive journalism, whose objective is regain- ing the lost trust of the media (McIntyre  Gyldensted, 2017). This is a journalistic movement that explores new paths. However, it will take time to see if these new approaches fit in the frame of emerging journalism in the Third Millennium, with a clear commitment to social ser- vice, transparency and accuracy. 2. Literature Review 2.1. Misinformation Through the Ages Falsehood, fantasy and fake news have walked along with the development of communication and journalism, initiating discussions about its practice and its role in soci- ety (McNair, 2017). Although there are evidence of misin- formation since the Roman Empire (Burkhardt, 2017), its major development took place with the invention of the print in the 15th century. The possibility of disseminat- ing written information in a faster and easier way made possible the circulation of falsehood too. Hence, the advent of new means of communication, increased the presence of deliberated false content, not always with harmful purposes. In this regard, one of the greatest examples of misinformation of our times was the radio broadcast of The War of the Worlds directed by Orson Welles on October 30th, 1938. That radio show was followed by thousands of listeners, and some of them believed that the Earth was under an alien attack, thanks to the narration of Welles’ cast (Gorbach, 2018). This radio show wanted to entertain the audience using an alteration of reality. However, manipulation of the truth has been used as a weapon in military conflicts over the centuries in order to ascribe malicious acts or characteristics to the enemy (Bloch, 1999, p. 182). A good example of this use of misinformation was the sequence of news published after the explosion in the boilers of the United States Navy ship USS Maine on February 15th, 1898. In the middle of the fight for being the most read against Joseph Pulitzer’s The New York World, William Randolph Hearst, editor of the New York Journal, sent a journalist to Cuba with the objective of telling the read- ers the details of a Spanish attack to this ship. Thus, when the correspondent arrived at the island reported that alleged attack did not exist. Nonetheless, the newspaper published a series of stories detailing the attack—even when they knew they were not accurate—causing a cli- mate of hate against Spain and acceptance of the coming war. Finally, The United States declared the war against Spain (Amorós, 2018, p. 34). After this conflict, misinfor- mation continued to be used against the enemy in war times. Thus, it is possible to identify strategies of its use in recent conflicts like the World War I and World War II, the Vietnam War or the Gulf War (Peters, 2018). 2.2. Fake News as a Threat to Journalism Falsity has cast a shadow over the discipline of commu- nication throughout history. One of the newest forms of misinformation is fake news, pieces that imitate the appearance of journalistic information, but deliberately altered (Rochlin, 2017). This form of deception has coex- isted with true news. However, the current communica- tive scenario, marked by the utilisation of high speed and low contrast means of communication—and among all social media—provides a fertile soil for the dissemina- tion of any form of misinformation (Lazer et al., 2018). Platforms like Facebook or Twitter are now among the primary news sources for Internet users (Bergström  Jervelycke-Belfrage, 2018). Fake news producers are aware of this fact. As a result, they have made the web the main channel for false content distribution, taking advantage of the possibility of communicating anony- mously provided by certain spaces (Vosoughi, Roy,  Aral, 2018). Furthermore, fake news producers have the chance of reaching as large audiences as consol- idated journalistic brands (Fletcher, Cornia, Graves,  Nielsen, 2018), which makes the verification of this fal- sities more difficult. Media and Communication, 2021, Volume 9, Issue 1, Pages 291–300 292 During the last few years, there have been differ- ent proposals for classifying fake news. Among them, the one developed by Tandoc, Lim, and Ling (2018) is perhaps the most exhaustive: news satire, a very common form of fake news with a large presence in magazines, websites and radio or TV shows; news par- ody, which shares some of the characteristics of news satire, but it is not based on topical issues. These pieces are fictional elements specifically produced for certain purposes; news fabrication, unfounded stories that try to imitate the structure of news published by legacy media. The promoters of these pieces try to deceive by blending them among the truthful ones; photo manipulation—alteration of images—and more recently videos—for building a different reality; adver- tising and public relations—dissemination of advertis- ing by masking it to look as journalistic reporting; and propaganda—stories from political organizations with the objective of influencing citizens’ opinion on them. Like some of the previous ones, they imitate the formal structure of news pieces. Regarding its formal structure, fake news try to imi- tate news items’ formal appearance. Thus, visual codes and elements like headlines, images, videos hypertext and texts conceived like journalistic pieces are common features of this misinformation strategies (Amorós, 2018, p. 65). Nonetheless, its major particularity is that fake news tries to attack the readers’ previous opinion, espe- cially on controversial issues related to racism, xeno- phobia, homophobia and other forms of hate (Bennett  Livingston, 2018; Waisbord, 2018). This connection makes possible the rapid replication of such content thanks to the ease of sharing through spaces like social media platforms. Thus, episodes like electoral processes (Lowrey, 2017), or more recently the Covid-19 pandemic (Salaverría et al., 2020; Shimizu, 2020), resulted in a deep growth of fake news circulation, at times using sim- ple methods but at times taking advance of the most advanced technology. 2.3. Deepfake: A Novel Form of Fake News Deepfakes, a combination of ‘deep learning’ and ‘fake’ (Westerlund, 2019), are “highly realistic and difficult-to- detect digital manipulations of audio or video” (Chesney  Citron, 2019). It can be defined as “a technique used to manipulate videos using computer code” (Fernandes et al., 2019, p. 1721), generally replacing the voice or the face of a person with the face of the voice of another per- son. Although the photo and video manipulation have existed for a long time, the use of artificial intelligence methods for these purposes has augmented the number of fakes and its quality. Some of these videos are humor- ous, but the majority of them are damaging (Maras  Alexandrou, 2019). Hence, this is a recent movement whose beginnings date back to 2017, starting then a rapid popularisation until now (Deeptrace Labs, 2018, pp. 2–4). This technique is the result of using Generative Adversarial Networks, algorithms designed to replace human faces or voices in thousands of images and videos in order to make them as realistic as possible (Li, Chang,  Lyu, 2018). The main advantage of these algorithms is that these systems are learning how to improve them- selves by creating deepfakes. Therefore, future creations will be improved thanks to past experiences. This fea- ture makes this misinformation procedure more danger- ous, especially due to the emergence of mobile apps and computer programmes that allow users without computer programming training to produce deepfakes (Nirkin, Keller,  Hassner, 2019; Schwartz, 2018). Farid et al. (2019, pp. 4–6) tried to label the different forms adopted by deepfakes in four categories: 1) face replacement or face swapping—this method involves changing one person’s face, the source, for another one, the target; 2) face re-enactment—manipulation of the features of the features of one person’s face like the movement of the mouth or the eyes, among others; 3) face generation—creation of a completely new face using all the potential provided by Generative Adversarial Networks; and 4) speech synthesis—alteration of some- one’s discourse in terms of cadence and intonation, or generation of a completely new one. As with other technologies, the same algorithms used for creating deepfakes could have a beneficial appli- cation in the field of psychology, building digital syn- thetic identities for voiceless users; or in robot sketches through advanced facial recognition for law enforce- ment, for example (Akhtar  Dasgupta, 2019; Zhu, Fang, Sui,  Li, 2020). Notwithstanding, its use seems to be more harmful than beneficial nowadays with examples of the use of these technologies in acts of fraud and crime (Stupp, 2019). Hence, one of the biggest challenges of deepfakes is to find out how to counteract them knowing that the debunking methods’ development is always late regard- ing the production of misinformation (Galston, 2020). However, a great deal of effort has been made—and is still made—to develop technology-based tools for detecting and correcting it, both from public and private organizations (Deeptrace Labs, 2018, p. 2). These tools will be helpful in almost all areas of communication, espe- cially for journalism. 2.4. Fact-Checking: Journalism’s Response to the Misinformation Wave In light of the above, verified information seems to be a necessity in our communicative context (Ekström, Lewis,  Westlund, 2020), especially because disruptive episodes like the coronavirus outbreak resulted in a clear increase of citizens’ informative consumption (Masip et al., 2020). Furthermore, political communication has shifted to a model in which political leaders share their messages online instead of doing it through traditional media (López-García  Pavía, 2019). Media and Communication, 2021, Volume 9, Issue 1, Pages 291–300 293 At this juncture, the media has increased the impor- tance of verification processes for correcting both inter- nal and external errors (Geham, 2017). Consequently, a new professional profile—the fact-checker—has emerged with the mission of debunking misinformation and prevent audiences of its consumption. These profes- sionals try to go to the origin of an information or a claim for gathering all the available data and contrasting it (Graves, 2016, p. 110). Fortunately, journalists have also benefited from the development of new technological tools designed for verifying images, videos or websites in an efficient manner (Brandtzaeg, Lüders, Spangenberg, Rath-Wiggins,  Følstad, 2016). Although verification has always been part of any journalistic process, the rapid growth of the fake news phenomenon over the past few years made this activity more important than ever. Thus, the census created by the University of Duke Reporters’ Lab counts now almost 300 fact-checkers in more than 60 countries by the by middle of 2020, a hundred more than on the same date in 2019 (Stencel  Luther, 2020). Regarding this, it is pos- sible to talk about fact-checking as a transnational move- ment (Graves, 2018) were both legacy and independent media organizations try to restore the trust lost by the media (Bennett  Livingston, 2018). 3. Method The starting point of this research will be the appli- cation of the Systematic Literature Review method (Kitchenham, 2004) as a method to set an approach on how deepfakes are being addressed and studied. Due to the novelty of this reality, this method will let us understand in an exhaustive way (Codina, 2017) what are researchers doing to assess this phenomenon and what efforts are being done to stop its spread. Hence, our method consisted in the following phases: 1) topic identification—‘deepfake’ and ‘deep fake’— and the period of analysis—all the available literature; 2) source selection—Web of Science’s SCI-Expanded, SSCI, CPCI-S, CPI-SS, CPCI-SSH, and Scopus; 3) search in databases—the selection of Web of Science and Scopus is justified by the importance of these two databases, which contain the most relevant contributions for the Social Sciences field in general and deepfakes specifi- cally; and 4) identification of the studied variables for each item—descriptive data (article title, date, jour- nal or conference, number of authors, and keywords), type of study, research techniques (observation, survey, interview, content analysis, case study, experimental or non-specified), principal contribution, DOI or URL, and institution and country. This search resulted in 54 different research items: 28 presented at international conferences and 26 pub- lished in academic journals—all of that after deleting duplicities and texts that did not fit the criteria, such as editorial articles, call for papers, or interviews, among others. These 54 examples comprise our sample that will be addressed in the next section in order to understand the path followed by researchers on this subject. Concerning the second stage of our study, it will analyse the approach taken by three renowned media outlets and news agencies—The Washington Post, The Wall Street Journal, and Reuters—and three of the most important Internet platforms—Google, Facebook and Twitter—in neutralizing the spread of deepfakes. Thus, case study of these six organizations will be applied in order to understand how they are managing to identify, label and notify deepfakes through differ- ent approaches—protocols, use of technology, collabora- tion with institutions, and funding of innovative projects. This will be done through the analysis of the available reports and statements of these six organizations. Consequently, the main goal of our study will be to identify the coincidences and disparities in the strat- egy of three major media outlets and three of the most important online platforms when trying to stop the diffu- sion of deepfakes. This will be relevant in order to under- stand if six of the main representatives from these two communicative fields are joining efforts and strategies in limiting or not its spread, and how these procedures could be improved. 4. Findings 4.1. Results of the Systematic Literature Review We will start by depicting the state of the research on deepfakes, especially the contributions indexed in the two main databases—Web of Science and Scopus. As shown in Table 1, research about this issue started in 2018 with four conference papers. However, it was quintupled in 2019, and during the first half of 2020 almost a half more of works on deepfakes than the previous year were published. Furthermore, the most salient element of this table is that this form of fake news used to have presence at conferences, but in 2020 they become a topic addressed in academic jour- nals too. Nonetheless, it is necessary to note that the situation resulting from the Covid-19 pandemic Table 1. Evolution of the studies on deepfake indexed in WoS and Scopus. Year Conference paper Journal article Total 2018 4 0 4 2019 16 5 21 2020 (1st half) 8 21 29 Media and Communication, 2021, Volume 9, Issue 1, Pages 291–300 294 has provoked the cancellation or postponement of many conferences. Regarding the authorship of this research, the most common approach is the participation of three authors. Thus, the arithmetic mean—3,13 authors—and the mode—14 articles have three authors—serve to confirm this. Also concerning the authorship, researchers from 24 countries were identified, most of them located in the United States and Asia—China, Japan, South Korea, India, or Taiwan. Finally, this review shows a large degree of unifor- mity concerning the type of studies published on deep- fakes. Almost all the reviewed articles and conference papers take a descriptive approach. This is because 32 of the items are the result of experimenting with new tools and algorithms to counteract it. Another important group of research is review articles on deepfake detec- tion and prevention or even about legal framework and legal concerns of this form of misinformation, something that was found 21 times. In sum, the novelty of deepfake implies a certain degree of youth for its research. At present, it is possi- ble to see two trends: Studies that present new forms to stop its spread, or studies that try to create context on its emergence and development. 4.2. Counteracting Deepfakes at The Wall Street Journal, The Washington Post and Reuters Recent advances in artificial intelligence and their democratisation have allowed average users to create deepfakes. This represents a major challenge for our soci- ety due to the potential harmful impact of these cre- ations, especially before electoral processes. Looking to the United States 2020 general election, The Wall Street Journal has created a division of 21 journalists whose unique objective is detecting, labelling and debunking misinformation, particularly deepfakes (Southern, 2019). This team is a joint effort of Standards  Ethics and RD departments, and this work is very linked to the use of technology with presence of journalist with video, photo, visuals, research and news experience that have been trained for deepfake detection (Marconi  Daldrup, 2018). Furthermore, The Wall Street Journal provides specialized training in fake news and deepfake identi- fication in partnership with different researchers. This has led to the development of a protocol to find exam- ples of this kind of misinformation with three stages: source examination (contact with the source, author- ship identification, and metadata check, among others), search for older versions of the footage available online, and footage examination with video and photo edit- ing programs. Meanwhile, The Washington Post has applied to deepfake detection very similar criteria to other fake news detection. Thus, the The Washington Post has added video experts to the tasks developed by the team led by Glenn Kessler—also known as ‘The Fact Checker’ (Kessler, 2019). The most important contribu- tion of this publication regarding this problem is the elab- oration of a taxonomy to classify and label deepfakes. The Washington Post was also pioneering in the use of scales to highlight the degree of truth and lie of any content. Regarding doctored videos, the newspaper sets out three categories of manipulation (Ajaka, Samuels,  Kessler, 2019): missing context (presentation of the video without context or with a context intentionally altered), deceptive editing (rearrangement and edition of the video in certain parts or details), and malicious transfor- mation (complete manipulation and transformation of the footage resulting in a completely new fabrication). A third approach to this reality could be the one adopted by the news agency Reuters. The news services provider reports its awareness and concern on the deep- fake spread (Crosse, n.d.). Hence, it has started a collab- oration with Facebook for detecting as much doctored user-generated content as possible among all the videos and photos that run on the platform (Patadia, 2020). In this regard, Reuters has started a blog whose objec- tive is verifying doctored materials in English and Spanish. All of that with the objective of debunking as much infor- mation as possible ahead of the 2020 United States elec- tion. This is a clear example of the emerging collabo- ration among technological platforms and the media, a joint effort in trying to stop the rapid growth of fake news and deepfakes in such significant moments like a presi- dential run-up. 4.3. Internet Giants’ Strategies Against Deepfakes The spread of falsehood through social media plat- forms and other Internet spaces is now a challenge for providers like Google, Facebook or Twitter. As a result, over the last few months they have started different initiatives whose unique objective is finding efficient ways to detect and stop the misinformation and, more recently, deepfakes. Regarding this, these three companies show differ- ent approaches against this matter. Google, for instance, has made available to the research community a large set of manipulated and non-manipulated videos (Dufour  Gully, 2019). With this initiative, they want to help in the development of identification techniques by taking advantage of the great amount of information saved in their files. In addition, they collaborate with the Defense Advanced Research Projects Agency to fund different researchers that are developing media forensic tools. On the other hand, Facebook is financing differ- ent research projects within its ‘Deepfake Detection Challenge.’ This initiative, boosted by companies like Facebook, Microsoft and Amazon Web Services and research units from various universities across the United States, tries to assist researchers that are work- ing on the development of artificial intelligence-based deepfake detection tools. Thus, a corpus of more than 100,000 videos was available to these researchers that Media and Communication, 2021, Volume 9, Issue 1, Pages 291–300 295 fight for presenting useful mechanism in order to win dif- ferent awards. Furthermore, Mark Zuckerberg’s social network tries to counteract this form of misinformation by deleting doctored videos or photos, or labelling it as fake news with the help of fact-checking media outlets (Bickert, 2020). This is particularly important for those related to the 2020 United States run-up due to the influence that fake news could have in this process. Finally, Twitter shows a simpler approach towards this problem. They summarize their strategy in the fol- lowing four rules (Harvey, 2019): Identification through a notice of Tweets with manipulated content, warning of its manipulated condition before sharing it, inclusion of a link to news articles or other verified sources in which users can find out why and how the content has been doc- tored, and elimination of all that manipulated content potentially harmful or threatening to anyone’s safety. These diverging strategies on behalf of the major online platforms are in part the product of self-regulated methods for fighting deepfakes, as there is still incipient intervention on behalf of the states in regulating con- tent on social media and other outlets. The question to be asked here is whether it is the online platforms’ sole responsibility to tackle misinformation or if there are any social interests in this situation for which other public entities should allocate resources to. The European Commission already pointed out in 2018 the need for governments to invest in research and detection of misinformation, while also prompt- ing these to hold social media companies account- able (European Commission, 2018). So far, in the last two years the EU has launched a series of initiatives to tackle the issue: a code of practice against dis- information, the creation of the Social Observatory for Disinformation and Social Media Analysis and the set-up of the Rapid Alert System, among other RD projects such as PROVENANCE, SocialTruth, EUNOMIA or WeVerify (European Commission, 2020). Despite a lot of efforts being made to avoid the spread of mis- information in the EU, deepfakes are still not as much on the agenda as other academics are asking for, while also describing their worry for seemingly understaffed programs (Bressnan, 2019). Measures taken by coun- tries to prompt social media companies in acting against fake news contain different levels of intervention and are mainly dedicated to counteracting disinformation in political advertisement. France and Germany, for exam- ple, require online platforms to establish an easily acces- sible and visible way for users to flag false information, while Australia requires all paid electoral advertising, including advertisements on social media, to be autho- rized and to contain an authorization statement (Levush, 2019). In the United States, some states have already taken specific measures to counter deepfakes, although these are still merely reactive and not preventive, such as Texas passing a law that criminalizes publishing and distributing deepfake videos with the intention to harm a candidate during the electoral process; or California, where a law was passed last October making it illegal for anyone to intentionally distribute deepfakes for deceiv- ing voters or perjure a candidate (Castro, 2020). The implications of these incipient interactions between governments and social media companies might have relevant governance questions in the forth- coming years, all the while these companies are also starting to take new approaches to their governance structures, such is the case of the Facebook, who set up the Independent Oversight Board, which “aims to make Facebook more accountable and improve the decision- making process,” in the words of Nick Clegg, currently Facebook’s VP of Global Affairs and Communications and former Deputy Prime Minister of the United Kingdom (Moltzau, 2020). 5. Discussion As shown in the previous section, the media and Internet platforms have initiated different strategies to fight mis- information and, more particularly, the spread of deep- fakes. In this regard, there are some similarities and dif- ferences among the strategies of these two communica- tive sectors. First of all, it seems clear that the collaboration among platforms and media outlets increases over time. Example of this could be the agreements among Reuters and Facebook whose objective is to detect fake news and share its correction. Furthermore, other fact-checking organizations collaborate with this social network in labelling false content and warn users about this. Another coincidence is the use of technology as a weapon in the battle against deepfakes. Both news media and digital platforms have understood that high technology and the use of algorithms as powerful as those used for creating fakes is the only chance to coun- teract them. Thus, media outlets are increasingly train- ing journalists and interdisciplinary teams in the use of these mechanisms that allow them to identify this form of misinformation. The third match could be the growing synergies between the academic and communicative sides. Thus, media outlets and platforms try to collaborate with researchers and institutions specialized in fake news detection, both in training and to apply their methods. Regarding the divergences when dealing deepfakes, online platforms are able to fund research projects whose objective is developing artificial intelligence-tools for identifying this form of fake news. The media, how- ever, does not have such possibilities due to the expen- diture of these activities. Another difference in dealing with this issue could be that the media use to correct misinformation instead deleting it. As shown before, some of the social media platforms have the elimination of doctored content among their strategies. This presents a clear challenge. Although deleting manipulated videos or photos ends Media and Communication, 2021, Volume 9, Issue 1, Pages 291–300 296 with the problem for future or potential users, does not for those users that have seen them. In contrast, labelling these materials as false or manipulated—the approach followed by verification media outlets—could be helpful for future users. 6. Conclusions Deepfakes have become a reality in our communicative system. Media outlets and Internet services providers try to counteract it with different outlooks. However, the development of the techniques for producing misinfor- mation seems to advance faster than those for debunk- ing it. Regarding this, the available research is mainly focused on two aspects: On one hand detection tools, and on the other hand, the implications of this form of fake news for democracy and national security. The fact that so far only big technological giants are capable of introducing hi-tech expensive solutions for fighting deep- fakes motivates that the available mass of research on the subject is fundamentally dedicated to address the questions raised by these corporations which are mainly technological. On the other hand, journalism focused media, which are not able to invest large amounts of money on deepfake detection are therefore unable to push their concerns into the research agenda. For this reason, producing research on the implications of deep- fakes for journalism and under journalistic premises presents itself as elemental, as well as further investiga- tion on how the media trains its professionals for detect- ing advanced misinformation. The novelty of this deceiving technique provokes its understudied situation, but the constant growth of works on this matter show that it will be an important field for researchers on misinformation and media foren- sics in the following years. However, the study is able to show to some extent that the media and digital plat- forms’ have notable similarities and differences when it comes to their strategies. This could be due to the dif- ferent nature of their business models, but neverthe- less sometimes it seems to be a matter of investment. Digital platforms have joined efforts with technological, academic or entrepreneurial partners, spending large amounts of money in this field, which is something that many medi","Vizoso, Á., Vaz-Álvarez, M., & López-García, X. (2021). Fighting deepfakes: Media and internet giants’ converging and diverging strategies against Hi-Tech misinformation. Media and Communication, 9(1), 291-300."
"SCOP_181","Computational News Discovery: Towards Design Considerations for Editorial Orientation Algorithms in Journalism","Computational news discovery (CND) is a particular application area within computational journalism related to the use of algorithms to orient editorial attention to potentially newsworthy events or information prior to publication. Previous work in this area has been concentrated on prototyping CND tools, which can, for instance, send alerts and leads to journalists about social media events, documents of interest, or salient patterns in streams of data. This article describes a qualitative interview study of journalists as they incorporate CND tools into their practices. Findings provide insights into how CND tools interact with the internal attention economy and sociotechnical gatekeeping processes of the newsroom and how future CND tools might better align with necessary journalistic evaluations of newsworthiness and quality, while ensuring configurability, human agency, and flexible applicability to a wide range of use cases. These findings begin to outline a conceptual framework that can help guide the effective design of future CND tools.","Journalism Studies","Article","2020","Y","Y","Tool","Practices","Evaluation","9","There is perhaps no aspect of the news production pipeline that isn’t increasingly impacted by the use of algorithms. Computational approaches are now broadly applied in journalistic work including in information gathering (Thurman et al. 2016), providing signals to assess the veracity of content or sources (Fletcher, Schifferes, Thurman 2017), automatically generating written articles (Graefe 2016), creating new interactive bot experiences (Lokot and Diakopoulos 2016; Ford and Hutchinson 2019; Jones and Jones 2019), and optimizing or otherwise influencing the distribution of content on homepages, apps, or platforms (Bucher 2016). Computational journalism considers how computing—defined as the systematic study of algorithmic processes that describe and transform information” (Denning 2005)—is applied to support journalistic tasks and embrace journalistic values (Diakopoulos 2019a). Here computational journalism” is emphasized rather than related terms such as algorithmic journalism”, automated journalism”, or data journalism” (Thurman 2019; CONTACT Nicholas Diakopoulos nad@northwestern.edu This article has been corrected with minor changes. These changes do not impact the academic content of the article. ß 2020 Informa UK Limited, trading as Taylor  Francis Grou Zamith 2019; Coddington 2015) as it broadly captures the idea of using algorithms to transform information and data for journalistic purposes. At the same time, computa- tional approaches often entail the close collaboration of algorithms and people in hybrid systems that take advantage of the capabilities of algorithms for scale and speed, but also leverage the complementary capabilities of people (Brynjolfsson and McAfee 2014; Diakopoulos, 2019a). In particular this research examines this hybridiza- tion of algorithmic and human effort in the context of a specific application of compu- tational journalism in news production: computational news discovery. News discovery is described in some of the earliest ideation documents about com- putational journalism. Hamilton and Turner (2009) posit that, a reporter could be alerted when a trend appears, an anomaly arises, or when a specific individual or entity or location is referred to in the data stream”. The premise is that with growing volumes of information, computing can offer a subsidy to public interest journalism, lowering costs and increasing the efficiency and scale at which new news stories can be identified (Hamilton 2016). The intervening years have brought forth a variety of system prototypes that have reified the use of computing for discovering news, which has recently been referred to as computational news discovery” in a review of research on computational journalism (Thurman 2019). Monitoring systems can send alerts and craft leads that orient journalists’ attention to social media events, docu- ments of interest, or anomalous patterns in streams of data that may reveal important news stories. For instance, in computational fact spotting scenarios algorithms sift through claims to identify those that can be checked (Graves 2018). In one particular implementation, a machine learned classifier is used to identify fact checkable state- ments in CNN transcripts, which are then sent to newsrooms as daily tipsheets that fact checkers may refer to in making coverage decisions (Hassan et al 2017; Adair et al 2019). With this prior work in mind, here I define computational news discovery (CND) as: the use of algorithms to orient editorial attention to potentially newsworthy events or information prior to publication. Given the recent technical developments and demonstrations of CND systems, some research has begun to investigate how they are integrated into journalism practice, are applicable (or not) to different journalistic use-cases, and could be designed to enable more effective journalistic use (Diakopoulos 2019; Stray 2019). In furthering the technological lens to the study of computational journalism with a focus on the hybrid nature of newsroom technologies (Anderson 2013), here I adopt a human-centered and sociotechnical frame which considers how CND systems inter- act with and influence the experiences of journalists in undertaking their newswork. The focus is on individual perspectives on usage, including utility as well as social and normative expectations, rather than any wider organizational factors that may, of course, also play a role in adoption. In particular this work sets out to ask: What are the human-centered needs of journalists with respect to the effective use of computational news discovery systems in their sociotechnical gatekeeping practices of selecting and developing news items for publication? This research addresses this question by undertaking interviews with 18 stakeholders who have created, used, or both created and used computational news discovery systems in the context of journalism. 946 N. DIAKOPOULOS The results of this study offer insight into the experiences of practitioners with respect to computational news discovery systems. Based on these results this article contributes an initial elaboration of a conceptual design framework that can help inform and guide the future development of computational news discovery systems that are consistent with user needs and which begins to unpack the use of algorithms as part of sociotechnical pre-publication gatekeeping processes. In particular, the find- ings highlight the essential role of human effort and attention in developing leads into news items of publishable quality, elaborate factors related to the newsroom attention economy and how these tools fit within it, and suggest design opportunities for CND systems to better align with the various newsworthiness and quality evalua- tions journalists need to make across a wide range of journalistic scenarios. Related Work Here I consider two areas of related work which inform the conceptual approach taken in this research: (1) computational news discovery as an application area of computa- tional journalism which speaks to the use of algorithms in sociotechnical gatekeeping processes, and (2) design-oriented and human-centered approaches to studying jour- nalistic work enabled by computational tools that have the goal of identifying design implications. Gatekeeping and Computational News Discovery The concept of gatekeeping captures the idea that information can be variously impeded or passed onward in the process of communication. Not all news information is published and made widely available. There is a matrix of forces at play which impact gatekeeping decisions, including individual cognitive differences or biases, work routines for news production, organizational characteristics, external social insti- tutional actors such as advertisers or governments, and social systems such as culture or ideology (Shoemaker and Vos 2009). While some of the earliest work on gatekeep- ing focused heavily on the role of the individual (White 1950), contemporary models of gatekeeping consider not only human actors such as journalists, strategic professio- nals, and individual amateurs, but also technical actants such as algorithms and their role and interplay with other actors as part of broader sociotechnical gatekeeping practices (Wallace 2017; Thorson and Wells 2016; Lewis and Westlund 2015). Oftentimes when algorithms are considered in gatekeeping processes the focus is on their role in distributing news to the public via feeds (DeVito 2017), aggregators (Nechushtai and Lewis 2019) and apps (Bandy and Diakopoulos 2020), including by examining how editorial values are embedded into the code of such curators (Weber and Kosterich 2018). A less closely studied aspect of algorithmic gatekeeping relates to the role that algorithms can play not only in distribution but also in news produc- tion (Heinderyckx and Vos 2016). In other words, algorithms used in the input and throughput stages of gatekeeping (Wallace 2017) to inform a sociotechnical process prior to wider publication. While some recent work has considered how algorithms used at these stages can introduce biases into the information suggested to journalists DIGITAL JOURNALISM 947 in a sociotechnical gatekeeping process (Thurman et al 2016; Diakopoulos 2019), the current work aims to contribute more broadly to understanding how internally used algorithmic curators (i.e. CND systems) interact with human gatekeepers (i.e. journal- ists) in a sociotechnical news selection process. CND systems can contribute to gatekeeping by allowing users to monitor the vast and overwhelming scale of content produced and published on social media plat- forms. They can help to detect newsworthy events, aggregate responses, and identify, track, and suggest useful sources and witnesses during breaking news or other types of scheduled events like speeches (Diakopoulos, DeChoudhury, and Naaman 2012). For instance, The City Beat tool was developed to detect and alert journalists to local events in New York City, and was deployed to several newsrooms on a trial basis (Schwartz et al 2015). More recently, the Tracer system was developed to monitor mil- lions of daily tweets, cluster posts in order to detect events, and present those events to journalists in a sortable and searchable interface that has proven itself able to accel- erate Reuters’ news alerts in many cases (Liu et al. 2017; Nourbakhsh et al. 2017). CND systems can also help monitor data sources such as numeric data streams or textual documents and identify items of interest to be brought to the attention of journalists. For instance, the BBC’s Data Stringer prototype was developed to monitor data streams and trigger alerts when rules relating to trends or outliers were matched (Shearer, Simon, and Geiger 2014). The Marple system used statistical methods in order to send alerts to local journalists about anomalies, outliers, or trends in munici- pal data sets (Magnusson, Finnas, and Wallentin 2016). The Local News Engine scans data from courts, housing developments, and business licenses to detect the names of newsworthy people, places, or companies, which are sent to local media (Perrin 2017). The Tadam system ingests a variety of different documents from the Web, press releases, or document dumps in order to send alerts to reporters when a document shows up that matches their preset filters (Plattner, Orel, and Steiner 2016). The Washington Post’s Lead Locator system mines a national voter file dataset in order to help national politics reporters identify interesting locations for their reporting based on demographic patterns and political relevance (Diakopoulos et al 2020). Interactive data-driven expert systems have been deployed to help journalists identify what might be meaningful and newsworthy patterns warranting story development (Broussard 2015). News discovery approaches have also been applied in fact checking workflows to help spot fact checkable claims mentioned in the media (Graves 2018; Hassan et al 2017; Adair et al 2019). Journalism practice is also utilizing approaches from machine learning and data mining to expand the scope of individual investigations by filtering for known patterns that orient attention to entities or documents that suggest new lines of inquiry (Diakopoulos 2019a; Stray 2019). The current research considers both these bespoke approaches as well as more formalized systems (i.e. CND approaches and systems), with an emphasis on discovery from non-social media sources. These various approaches and tools represent a new journalistic source, offering access to informa- tion at the input stage of the gatekeeping process (Wallace 2017) by utilizing algo- rithms to orient editorial attention to items that journalists might not be aware of otherwise. In particular, this research seeks to understand the sociotechnical 948 N. DIAKOPOULOS gatekeeping processes around CND approaches and systems in order to inform the design of future CND tools. Design-oriented Studies of Newswork Human-centered design can be used to help develop new journalistic products, serv- ices, and experiences that de-center the role of technological affordances and instead focus on how to harness technology to meet user needs (Chaplin 2016). More broadly, design methods can facilitate a deeper understanding of a sociotechnical context and offer insights that can guide the creation of new technologies that support and align with journalistic goals. Recently, there have been calls for journalism studies to engage more deeply with human-computer interaction (HCI) research in order to better under- stand how journalists interact with algorithms and automation in news production and how journalistic values can come to be embedded into technologies (Aitamurto et al. 2019; Diakopoulos 2019). These in turn build on earlier observations of the need for scholarly attention towards the imagined values and engineering design ... of journalistic artifacts” (Anderson 2013), and the importance of how technological actants are inscribed and instructed by humans” (Lewis and Westlund 2015) so that such inscription is deliberate with respect to the goals of stakeholders and the intended contexts of use. This work addresses this conceptual space by examining the uses and practices surrounding current CND tools with an eye towards how the next generation of such tools could be designed to more effectively support journalistic contexts, work, and values. In particular, our findings are oriented towards identifying design implications that articulate various constraints, affordances, and social or nor- mative expectations of the possible design space for future CND tools (Sas et al. 2014). A design orientation aligns with the conceptualization of digital journalism studies conveyed by Eldridge et al (2019) by enabling empirically informed opportuni- ties for deliberate sociotechnical reconfiguration of journalistic practices. A growing corpus of research looks at software or tools to support journalistic activ- ity with an eye towards gaining insights that inform future designs and elaborate jour- nalistic concepts. Recent studies in this vein have examined the uses and limitations of automated writing software (Thurman, D€orr, and Kunert 2017), and the utility of social surveillance tools (Thurman 2017) which suggest opportunities for how future computational tools could better support journalistic work. Another line of research has studied visual analytic tools for investigative journalists, revealing different news discovery use-cases depending on whether users already had a hypothesis to verify (Brehmer, Ingram, and Stray 2014) or were more interested in hypothesis generation (Felix et al. 2015). Systems have also been built to help sort social media information such as Tweets or online comments, and user evaluations have underscored the desire of journalists to interactively configure information filters according to domain or scen- ario specific criteria, to receive adequate context to evaluate or verify information, and to use such tools as a starting point to identify angles of interest for subsequent inquiry (Park et al 2016; Diakopoulos, DeChoudhury, and Naaman 2012). Specific design requirements for software to support the use of user-generated content in newswork have also been developed, including requirements for flagging or marking DIGITAL JOURNALISM 949 leads for later, reflecting update frequencies, and supporting the ongoing nature of verification work (Tolmie et al 2017). The INJECT system was designed to support cre- ativity amongst journalists by providing cues about people, background, or conse- quences of a news story that could trigger ideas for novel stories (Maiden et al 2018). Taken together, these human-centered and design-oriented studies begin to suggest a rich and deeply contextual design space for CND systems, which the current research seeks to empirically elaborate and refine. Study Methods In order to better understand the use of CND systems and approaches from a user- centered perspective a qualitative study consisting of 18 semi-structured interviews was undertaken. The goal of the interviews was to elicit the perspectives and experi- ences of practitioners, with a particular eye towards how CND systems and approaches might be designed to more effectively integrate into journalism practice. Participants Participants for this study were selected using a purposive sampling strategy in order to deliberately reflect a range of systems and perspectives on those systems. Various editorial orientation systems and projects were identified by reviewing the literature as well as online articles and blogs. Systems were then selected to straddle both data- driven tools and products as well as one-off projects that utilized algorithms for directing editorial attention for the purposes of story finding, alerting, or document investigation. Specific systems studied include RADAR, 1 Newsworthy, 2 Klaxon,3 several computational fact spotting tools including those of the Tech  Check Cooperative (Adair et al 2019), FullFact, 4 and Chequeado,5 as well as a range of internal projects from outlets ranging from regional and national newspapers, to online news publica- tions, and interest-specific digital outlets. Individuals associated with the identified sys- tems were contacted via email for interview. Snowball sampling was then used to expand the sample by asking initial interviewees for referrals to other relevant poten- tial participants. These recruiting processes yielded 18 individuals that were interviewed. Most of the participants were initially interviewed for the purposes of the author’s journalistic endeavor to report on the media industry’s use of automation and algo- rithms in news production. Post hoc IRB approval was obtained to re-analyze the data collected in these interviews for the purposes of this research. Light disguise is used in the reporting of results in order to protect the privacy and confidentiality of inter- viewees (Bruckman 2006). This means that, while precautions have been taken to pre- serve anonymity and to not identify participants directly, active members of the communities where such projects are discussed may be able to guess identities. Any sensitive details are therefore omitted, as are the identities of the organizations where internal projects were selected for study since participants might be more easily re- identified if those outlets were named. Participants include individuals with a diverse range of perspectives on computational news discovery systems such as creators 950 N. DIAKOPOULOS (N ¼ 4, labeled C1 ... C4), users (N ¼ 7; U1 ... U7), or both creators and users (N ¼ 7; CU1 ... CU7). Interview Materials and Procedure An interview guide was developed iteratively as interviews were undertaken and ana- lyzed, allowing for theoretical sampling of concepts as they emerged from the data (Glaser and Strauss 2009). In its final form the guide included 20 questions including follow-up prompts, addressing topics typically covered in the following ordering: how the system’s leads are used in newswork, the overall utility of the system’s leads, the information interface of the system including how leads are presented, the news- worthiness of the leads provided, the volume of leads received and developed into news reports including time spent on leads, how the system fits into workflow, whether the leads were trustworthy, and any ethical reflections or other thoughts on the wider use of such tools in journalism (See Appendix A for more details). The semi- structured interview procedure allowed some latitude to focus each interview and additionally tailor questions and prompts as well as their ordering according to an interviewee’s particular expertise, experience, and perspective. Interviews were conducted over a two-year span, from early 2017 to late 2018. The median interview lasted 50.5 min (min ¼ 27; max ¼ 64) and was conducted via audio connection (e.g. phone or Skype) in English. All interviews were audio recorded with consent and were later transcribed. No monetary incentive was provided to participants. Analysis Interviews were fully transcribed and then analyzed using an iterative qualitative method involving open coding of key excerpts, constant comparison, typologizing, and memoing (Glaser and Strauss 2009; Lofland and Lofland 1994). This process was ongoing as interviews were undertaken, which helped inform follow-up questions in latter interviews based on the analytic results from earlier interviews. Analysis of inter- view materials was further augmented and grounded using document analysis of related materials from the various systems studied, including any extant blog posts, product descriptions, and video presentations where a system’s functionality or design were discussed. Findings From the iterative analysis of the transcribed interviews several factors emerged with respect to the use of CND systems and approaches. Chief amongst these factors is the role that humans must still play in developing and evaluating the leads produced by these systems. Several sociotechnical factors that moderate the attention environment with respect to CND systems are elaborated, including the willingness of reporters to pursue leads in different contexts, external factors related to news cycles, the scope of monitoring offered by tools, and the user interface used to convey leads. The findings DIGITAL JOURNALISM 951 further expand on how systems can serve to modulate important editorial evaluations related to newsworthiness and quality assessment that journalists undertake in their lead development work. The Human Role: Evaluation Participants consistently reinforced the idea that people should be involved in evaluat- ing the leads produced by CND systems. This held for the simplest of leads, such as alerts signaling a change to a web page, and for more complex leads where an algo- rithm might itself embed evaluative criteria to rank or draw attention to a subset of information more likely to pan out. While an algorithm permits a scale of monitoring that would otherwise be impossible, having people evaluate leads imbues the overall system with a degree of flexibility to suit different use-cases. Despite the varying degrees of evaluation an algorithm might itself encode, human evaluation of leads was seen as essential in at least three areas: (1) lead development, (2) newsworthiness assessment, and (3) quality assurance. These are briefly describe next and further elabo- rated in subsequent subsections. Additional editorial effort was often seen as needed in order to develop leads towards publication. In some cases, such as for the leads produced by RADAR, the leads could be published as-is, largely because they had already undergone substantial human editorial development before being distributed as leads. Publishers were able to directly excerpt and use snippets of the text from some of the Newsworthy leads as well. In such cases the editorial effort is mainly that of curation. But for most CND tools studied there was more substantive editorial attention needed, typically involv- ing additional reporting to gather and assess related information as the lead became publication-worthy. These contrasts highlight the spectrum of human effort and atten- tion that might be invested in leads. Newsworthiness assessment was another important role that people were seen to play. Participants articulated the entire gamut of news values in their evaluation of leads, but the role of people was seen as particularly essential in evaluating news- worthiness dimensions such as audience fit and actuality (i.e. relevance to the current moment). For instance, FullFact intentionally built their claim spotting system to separ- ate checkability” (which they thought an algorithm was suited to recognize) from checkworthiness” (which they thought should be left for a person to evaluate). We thought that importance was actually something that is an editorial decision and will change over time. Claims, for example, about the EU ... two years ago wouldn’t have been as important as they are now” (C3). By deferring checkworthiness judgements to people the goal was to keep the sociotechnical system more responsive and flexible to a dynamic world where the importance of statements might change over time. The third area of human evaluation relates to the desire to assure the journalistic quality of leads that are developed and eventually published. Several journalists acknowledged that CND leads could only be a starting point: I only ever viewed it as a preliminary screen that needed a lot more reporting ... I would be very very ner- vous about reporting just from algorithmic output” (CU7). Another participant con- curred, Reporters are ultimately going to want to vet everything themselves by hand 952 N. DIAKOPOULOS to ensure that it’s correct, to ensure that they understand it” (CU5). Having people check the leads supplied by an algorithm was seen as the most reliable way to ensure the highest quality standards were met. Lead Development Effort and Attention The editorial effort and attention needed for a lead to mature into a publishable story varies a great deal across use cases. In some cases a lead might entail a few hours of reporter effort to make calls or find illustrative local examples: Typically the people quoted in the RADAR stories are national people of limited relevance to our readers. So for us as a local paper it’s better to get reaction from the people involved locally” (U5). But in other cases it could be a whole day’s work to do an interview, get a photo, and work up the details of the lead. RADAR leads are already highly refined, but for less fully formed leads there might be even more work to do. One participant estimated it could take a day or two of effort to prepare a fact check for publication. Another participant remarked that to develop a Newsworthy lead at their radio sta- tion, We have to put like one or two people to work with it for one week” (U1). Statistical aberrations and trends may demand substantial work to assess how interest- ing they are and begin to explain them: If we see in some areas that the prices of housing are going up very fast ... Why is that? Then we can ask questions in that region: What’s happening here and try to find cases and do journalism from that” (U1). While it was accepted that CND leads would demand effort and attention to develop into meaningful journalistic contributions, some participants recognized there may be opportunities to utilize lower-skill labor to initially assess leads before passing them along to more seasoned journalists for further investigation. To determine if there’s a real story being suggested, one participant remarked, I’m hoping that basic- ally we can make it so that the leads are self-explanatory enough that people can basically sit their interns down and sit slightly less experienced reporters down” (CU2). Lower skill workers would thus act as an initial screen of leads produced by the sys- tem. On the other hand, some leads may not get taken up if there’s not someone with enough skill to interpret and evaluate them. As one tool creator explained, Some newsrooms that are maybe a little more data savvy, they tend to use these stories more whereas others that are not they don’t use them as much” (C1). So while there is a spectrum of human effort needed, there is also a spectrum of skill that is needed to pursue certain leads. The availability of human attention is a key factor in determining whether any given news lead develops into a full story. One participant described a situation where automatically generated leads were sent to collaborators but took months before they were looked at and yielded a story. Also, sending too many leads might overwhelm the available attention of a newsroom and users may simply tune them out: We couldn’t take care of everything ... there was too much information for us to handle because we are like 20 people working here in our newsroom” (U1). A lead may look interesting but there may not be enough human capacity to further develop it to watch, with the goal of not triggering alerts on irrelevant site changes that might end up overwhelming or distracting the user. A user of Newsworthy leads explained that he didn’t want every lead the system produced, but rather only the ones he was topically interested in. The Chequeado system is deliberately set up to monitor media that include at least one outlet from each administrative unit of the country. The scope of monitoring is thus an editorial decision that dictates how wide a net the sys- tem fishes with. This in turn impacts the volume, precision, and relevance of leads pro- duced, and presumably increases the likelihood that leads match with interests and receive attention. The scope of monitoring can also be algorithmically widened based on an initial query, augmenting human ability by obviating the need to know exactly what to monitor. If the monitoring scope of a system is set too wide, either through user configuration or algorithmic expansion, control can be maintained by supporting the filtering of leads according to various relevance criteria. This came up in the con- text of a fact spotting system in which several users indicated they were not inter- ested in checking the claims of pundits or other journalists and so wanted to filter those out to reduce distraction, while also recognizing that other outlets might still be interested in those leads. This underscores the variability in interests between different journalists and outlets and how monitoring scope and post-filtering can enable editor- ial control and support diverse uses. User Interface Design A final factor that impacts the flow of attention towards leads is the nature of the user interface (UI) and how it frames information for users. Some systems, such as Newsworthy or Tech  Check, send discrete chunks of information via email. By expli- citly marking individual items or sets of items (e.g. a set of claims for fact checking) for attention this may, however, create expectations around the relevance or import- ance of those items. Other systems, such as RADAR or Chequeado, provide an inter- face that presents a ranked list of items that can be browsed. A ranking UI has the advantage of communicating some degree of relevance that corresponds to the order- ing of leads, without definitively marking some subset as worthy of attention. A mix- ture of the two approaches involves sending discrete leads via email with a link to an interactive UI containing more details and a full ranking. Date-time order (i.e. most recent at top) is a common default for rankings, though leads can be sorted according to other criteria. Some interfaces provide keyword search functionality allowing users to find leads based on specific interests. Finding the right presentation of the information from the lead was seen as a chal- lenge: I’m fairly optimistic at this moment about us being able to generate useful leads. But I’m fairly pessimistic about us being able to communicate that well enough to the reporters who are supposed to then make sense of them” (CU2). At Chequeado they’ve carefully crafted the information shown in the UI. Each lead shows the claim, the media outlet where it was published, a link to the claim (e.g. article where it was detected), and then shows the paragraph where the claim was found as additional context. Initially users see the last 3 days of checkable claims, but can continue scroll- ing down until they eventually start seeing claims that are less fact checkable. The rationale for including some of the not fact checkable claims in the interface was to DIGITAL JOURNALISM 955 be able to collect feedback to further train the machine learning (i.e. by including negative cases). Newsworthy leads, on the other hand, consist of a chart that visualizes the trend or statistical anomaly behind the lead, a few sentences of generated text describing what it’s about, and a link to the original spreadsheet with the data back- ing the lead. Linking to the data was found to be important: Most or almost all reporters that get these leads and do something with them actually look at the data themselves” (C1). This approach was also taken with the Tech  Check leads: a link from each lead provides quick access to the source transcript which allows reporters to assess the context of a statement before further pursuing. Importantly, lead presen- tations included vit","Diakopoulos, N. (2020). Computational news discovery: Towards design considerations for editorial orientation algorithms in journalism. Digital Journalism, 8(7), 945-967."
"SCOP_339","Detection and visualization of misleading content on Twitter","The problems of online misinformation and fake news have gained increasing prominence in an age where user-generated content and social media platforms are key forces in the shaping and diffusion of news stories. Unreliable information and misleading content are often posted and widely disseminated through popular social media platforms such as Twitter and Facebook. As a result, journalists and editors are in need of new tools that can help them speed up the verification process for content that is sourced from social media. Motivated by this need, in this paper, we present a system that supports the automatic classification of multimedia Twitter posts into credible or misleading. The system leverages credibility-oriented features extracted from the tweet and the user who published it, and trains a two-step classification model based on a novel semi-supervised learning scheme. The latter uses the agreement between two independent pretrained models on new posts as guiding signals for retraining the classification model. We analyze a large labeled dataset of tweets that shared debunked fake and confirmed real images and videos, and show that integrating the newly proposed features, and making use of bagging in the initial classifiers and of the semi-supervised learning scheme, significantly improves classification accuracy. Moreover, we present a Web-based application for visualizing and communicating the classification results to end users.","Information Science","Article","2018","Y","Y","Prototype","Experimental","Claim detection","42","Popular social media platforms such as Twitter and Face- book are nowadays an integral part of the journalistic and This work has been supported by the REVEAL and InVID projects, under Contract Nos. 610928 and 687786, respectively, funded by the European Commission. B Symeon Papadopoulos papadop@iti.gr Christina Boididou christina.mpoid@gmail.com Markos Zampoglou markzampoglou@iti.gr Lazaros Apostolidis laaposto@iti.gr Olga Papadopoulou olgapapa@iti.gr Yiannis Kompatsiaris ikom@iti.gr 1 Urban Big Data Centre, Glasgow, UK 2 CERTH-ITI, Thessaloniki, Greece news diffusion process. This is not only due to the fact that these platforms have lowered the barrier for citizens to contribute to news generation and documentation with their own content, but also due to the possibilities they offer for rapidly disseminating news to one’s network of contacts and to broader communities. These new capabilities with respect to publishing and sharing content have led to the uncontrolled propagation of large volumes of news content over social net- works. It is now possible for a news story published by an individual to reach huge numbers of readers in very short time. This is especially true for cases where multimedia con- tent (images, videos) is involved. Those often undergo faster and wider sharing (and sometimes become viral) due to the fact that multimedia is easy to consume and is often used as evidence for a story. The high volume and dissemination speed of news- relevant social media content creates big challenges for the journalistic process of verification. On the one hand, news organizations are constantly looking for original user- generated content to enrich their news stories. On the other hand, having very little time at their disposal to check the veracity of such content, they risk publishing content that 72 International Journal of Multimedia Information Retrieval (2018) 7:71–86 is misleading or utterly fake, which would be detrimental to their credibility. For instance, in the case of a breaking story (e.g., natural disaster, terrorist attack), there is a mas- sive influx of reports and claims, many of which originate from social media. It is exactly this setting where the risk of falsely accepting misleading content as credible is the high- est. As misleading (or, for the sake of brevity, fake), we con- sider any post that shares multimedia content that does not faithfully represent the event that it refers to. This could, for instance, include (a) content from a past event that is reposted as being captured in the context of a currently unfolding similar event, (b) content that is deliberately manipulated (also known as tampering, doctoring or photoshopping), or (c) multimedia content that is published together with a false claim about the depicted event. Figure 1 illustrates a famous” example of a fake photograph that is often recy- cled after major hurricanes and supposedly depicts a shark swimming in a flooded freeway. It is noteworthy that despite this being a well-known case, there are numerous people who still fall for it (as attested by the number of retweets in each case). In contrast, as real, we define posts that share content that faithfully represents the event in question, and can there- fore be used in the context of news reporting. There are also in-between cases, such as for instance, posts that debunk fake content or refer to it with a sense of humor. Since those posts are quite obvious for human investigators, but rather hard for automatic classification systems, we consider them to be out of the scope of this work. The impact of fake content being widely disseminated can be severe. For example, after the Malaysia Airlines flight MH370 disappeared on March 2014, numerous fake images that became viral on social media raised false alarms that the plane was detected.1 This deeply affected and caused emotional distress to people directly involved in the inci- dent, such as the passengers’ families. In another case, on April 2013, a fake tweet was posted by the Associated Press account, which had been hacked for that purpose, stating that the White House had been hit by two explosions and that Barack Obama was injured.2 This caused the SP 500 index to decline by 0.9%, which was enough to wipe out $ 130 billion in stock value in a matter of seconds. Examples such as the above point to the need for methods that can identify misleading social media content. One of the first such attempts [9] used a supervised learning approach, in which a set of news-related tweets were annotated with respect to their credibility and then used to train a model to distinguish between the two classes; experiments were con- ducted on a dataset collected around trending news stories 1 snopes.com/photos/airplane/malaysia.asp. 2 www.theguardian.com/business/2013/apr/23/ap-tweet-hack-wall- street-freefall. and annotated with the help of crowd workers, leading to an accuracy of approximately 86%. However, this level of performance was achieved by performing feature selection on the whole dataset (i.e., both training and test) and by a cross-validation approach that did not ensure full indepen- dence between the events included in the training and test sets, respectively. Furthermore, some of the employed credi- bility features, such as the retweet tree of a tweet, are hardly applicable in a real-time setting. Follow-up research on the problem [10] suffered from similar issues, i.e., the leaking” of information from the training set into the test set, thus giving an optimistic sense of the achievable classification accuracy. In this paper, which offers an extended presentation and more thorough treatment of our previous work [7], we present an approach that moves beyond the supervised learning paradigm for classifying social media content into credible (real) or misleading (fake). The proposed approach uses a variety of content-based and contextual features for the social media post in question and builds two classification models that are used to produce two independent first-level predic- tions regarding the credibility of the post. At a second step, a top-level classifier leverages these first-level predictions on unseen” content for retraining the best of the first-level models, following a semisupervised learning paradigm. In that way, the resulting model is well tuned to the special characteristics of the unseen content and produces more con- fident predictions. Experiments on a public annotated corpus of multimedia tweets demonstrate the effectiveness of the proposed approach. Additionally, we propose a Web-based user interface for visualizing and communicating the result of automatic analysis to end users. The contributions of this work include the following: (1) the use of a feature set for the representation of users and tweets, extending the ones used by previous studies [1,9,10]; (2) the application of an agreement-based retraining scheme, previously proposed in [36] for the task of polarity classifi- cation, which allows the model to adapt to new, unknown datasets; (3) an extensive experimental study on a large annotated corpus of tweets investigating the impact of the proposed novelties and comparing with state-of-the-art meth- ods; (4) a Web-based application that allows users to test our approach for verification, and to further investigate the role of different features on the verification result. 2 Related work The presented work focuses on the problem of misleading social media content detection, and more specifically on Twitter posts (tweets) that are accompanied by multimedia content. More precisely, given a single tweet that claims to provide information on an event and contains an image or International Journal of Multimedia Information Retrieval (2018) 7:71–86 73 Fig. 1 Examples of fake shark image that was posted several times after major hurricanes in the USA (depicted posts refer to Sandy, Matthew, Harvy and Irma) video to support the claim, our task is to return an estimate of its credibility. Furthermore, given that news professionals are generally reluctant to trust black box” systems, a second objective is to be able to communicate the system’s output by illustrating which features matter most toward the final estimate. Finally, for the system to be applicable in the real world, it is important to ensure generalization across differ- ent events, i.e., to make sure that the system can adapt to new content. Given the above definition, the examined problem is related but distinct to several other problems. Hoax detection [17] is the problem of debunking entire stories posted on the Web. Thus, it deals with larger amounts of text than a single social media post, and it is typically not backed by multime- dia evidence. A similar problem is rumor detection. A rumor is an unverified piece of information at the time of its publica- tion. Typically, rumors do not directly correspond to a single piece of text or a social media post, but rather to a collection of items that disseminate it. Zubiaga et al. [43] present a survey of approaches for rumor detection, including veracity classi- fication and the collection and annotation of rumor-focused datasets from social media. Finally, a related problem is auto- mated fact-checking, which pertains to the classification of sentences into non-factual, unimportant factual, and check- worthy factual statements [12]. Fact-checking methods rely on structured knowledge from databases, such as FreeBase and DBpedia, which contain entities, events, and their rela- tions. The above problems are distinct from the one examined in this paper. For instance, hoax detection and fact-checking typically operate on different types of inputs than social media posts and commonly concern claims that can be verified via a combination of database cross-checking and reasoning. On the other hand, rumor detection operates on social media content, but considers collections of posts. In contrast, the focus in this paper is on the problem of veri- fying individual social media posts, typically posted in the context of an unfolding newsworthy event. This is an impor- tant differentiating factor, especially in the context of the first moments after a claim (expressed by an individual post) circulates in social media, when there is little or no contex- 74 International Journal of Multimedia Information Retrieval (2018) 7:71–86 tual information available (e.g., comments responding to the claim, networks of retweets). The particular problem studied in this paper was the focus of the Verifying Multimedia Use” benchmarking task, which was organized in the context of MediaEval 2015 [2] and 2016 [4]. According to the official task definition, given a tweet and the accompanying multimedia item (image or video) from an event that has the profile to be of interest in the international news, return a binary decision representing ver- ification of whether the multimedia item reflects the reality of the event in the way purported by the tweet”. In a comparative study that we recently conducted [6], we present a detailed comparison among three high-performing approaches on the problem, among which is the approach presented here. The typical methodology for detecting a misleading social media post is to extract a number of features from it, and clas- sify it using a machine learning algorithm. Typical features can be text-based, such as linguistic patterns or the presence of capital letters and punctuation, user-based, i.e., infor- mation extracted from the profile of the user account who made the post such as age or number of followers/friends, or interaction-based, such as the number of responses to the post. As mentioned in the introduction, the work by Castillo et al. [9] is one of the earliest attempts on the problem. The approach attempted to assess credibility at the event/topic level, i.e., produce a credibility score for an entire set of tweets discussing one event. The extracted features included text-based (e.g., tweet length, fraction of capital letters), user-based (e.g., account age, number of followers), topic- based (number of tweets, number of hashtags in the topic), and propagation-based, i.e., features describing a tree cre- ated from the retweets of a message. Besides the critique that the training and test cases were not entirely independent during the training/cross-validation process, the fact that the approach operates on the event level instead of the tweet level means it is not flexible enough for our task. However, many of the features are directly applicable to our task as well. Similarly, Vosoughi et al. [38] use text-, user-, and propagation-based features for rumor verification on Twit- ter. In a work that is directly comparable to the one presented here, Gupta et al. [10] train a system on a set of features in order to classify between tweets sharing fake images and tweets sharing real images on a dataset of tweets from Hurri- cane Sandy. In that way, tweet classification is used as a first step toward verifying the associated images. However, as mentioned in the introduction, the separation between train- ing and test cases was not adequate for reliably assessing the generalization ability of the method. In a similar work, O’Donovan et al. [22] performed an analysis of the distri- bution of various features within different contexts to assess their potential use for credibility estimation. However, their analysis remains preliminary in the sense that they only ana- lyze feature distributions and not their effectiveness on the classification task. In our work, we move one step further by directly analyzing the performance of different configu- rations and variations of our approach. More recently, Wu et al. [39] presented a classifier trained on posts from the Chi- nese microblogging platform Sina Weibo. Besides typical features, the paper presents a propagation tree” that models the activity following a post (reposts, replies). This, however, is only applicable long time after a post is published, once a sufficiently large propagation tree is formed. Another recent approach is that of Volkova et al. [37], where Twitter posts are classified into suspicious” versus trusted” using word embeddings and a set of linguistic fea- tures. However, the separation between the two classes is made based on the source, i.e., by contrasting a number of trusted accounts to various biased, satirical, or propaganda accounts. This approach likely ends up classifying the writ- ing styles of the two distinct types of account, while in our case no distinction between trusted and non-trusted accounts was made during model building. Similarly, Rubin et al. [29] use satirical cues to detect fakes, which only applies to a spe- cific subset of cases. Another category of methods attempt to include image features in the classification, under the assumption that the image accompanying a post may carry distinct visual characteristics that differ between fake and real posts [14,34]. While this assumption may hold true when contrasting verified posts by news agencies to fake posts by unverified sources, it certainly cannot assist us when compar- ing user-generated fake and real posts. One typical example is fake posts that falsely share a real image from a past event and claim that it was taken from a current one. In this case, the image itself is real and may even originate from a news site, but the post as a whole is fake. Since we are dealing with multimedia tweets, one seem- ingly reasonable approach would be to directly analyze the image or video for traces of digital manipulation. To this end, the field of multimedia forensics has produced a large num- ber of methods for tampering detection in images [23,31,42] and videos [24] in the recent years. These include looking for (often invisible) patterns or discontinuities that result from operations such as splicing [42], detecting self-similarities that suggest copy–move/cloning attacks [31], or using near- duplicate search to build a history of the various alterations that an image may have undergone in its past (image phylo- genies”) [23]. However, such methods are not well suited for Web and social media images, for a number of rea- sons: – Splicing detection algorithms are often not effective with social media images, as these typically undergo numer- ous transformations (resaves, crops, rescales), which eliminate the tampering traces. International Journal of Multimedia Information Retrieval (2018) 7:71–86 75 – Building an image phylogeny requires automatically crawling the Web for all instances of an image, which is an extremely costly task. – It is highly likely that an image may convey false infor- mation without being tampered. Such is the case, e.g., of posting an image from a past event as breaking news, or of misrepresenting the context of an authentic image. Therefore, an image disseminating false information in social media may no longer contain any detectable traces of tam- pering, or it may even be untampered in the first place. For that reason, we turn to the analysis of tweet- and user-based features for verification. Finally, an important aspect of the problem is not only to be able to correctly classify tweets, but also to present verification results to end users in a manner which is under- standable and can be trusted by end users. Currently, there exist a few online services aiming to assist professionals and citizens with verification. The Truthy system [26] is a Web service that tracks political memes and misinformation on Twitter, aiming to detect political astroturfing, i.e., organized posting of propaganda disguised as grassroots user contri- butions. Truthy collects tweets, detects emerging memes, and provides annotation on their truthfulness based on user manual annotation. RumorLens [28] is a semiautomatic plat- form combining human effort with computation to detect new rumors in Twitter. TwitterTrails [20] tracks rumor propaga- tion on Twitter. There also exist some fully automatic tools, such as TweetCred [11] which returns credibility scores for a set of tweets, and Hoaxy [30], a platform for detecting and analyzing online misinformation. Finally, with respect to analyzing multimedia content, there are two notable tools: (a) the REVEAL Image Verification Assistant [41], which exposes a number of state-of-the-art image splicing detection algorithms via a Web-user interface, and (b) the Video News Debunker [35], which was released by the InVID project as a Chrome plug-in, to assist investigators in verifying user- generated news videos. 3 Misleading social media content detection Figure 2 depicts the main components of the proposed frame- work. It relies on two independent classification models built on the training data using two different sets of features: tweet- based (TB) and user-based (UB). Model bagging is used to produce more reliable predictions based on classifiers from each feature set. At prediction time, an agreement-based retraining strategy is employed, which combines the outputs of the two bags of models in a semisupervised learning man- ner. The verification result is then visualized to end users. The training of classification models and a set of feature dis- tributions that are used by the visualization component are based on an annotated set of tweets, the so-called Verification Corpus, which is further described in Sect. 4. The implemen- tation of the framework and the corpus are publicly available on GitHub. 3,4 3.1 Feature extraction and processing The design of features used in our framework was carried out following a study of the way in which news professionals, such as journalists, verify content on the Web. Based on rel- evant journalistic studies, such as the study of Martin et al. [19], and the Verification Handbook [32], as well as on pre- vious similar approaches [9,10], we defined a set of features that are important for verification. These are not limited to the content itself, but also pertain to its source (Twitter account that made the post) and to the location where it was posted. We decided to avoid multimedia forensics features follow- ing the conclusion of our recent study [40] that the automatic processing of embedded multimedia on Twitter remove the bulk of forensics-relevant traces from the content. This was also confirmed by our recent MediaEval participation [3,5], where the use of forensics features did not lead to noticeable improvement. The feature extraction process produces a set of TB and UB features for each tweet, which are presented in Table 1. Tweet-based features (TB): we consider four types of feature related to tweets: (a) text-based, (b) language- specific, (c) Twitter-specific, and (d) link-based. (a) Text-based These are extracted from the text of the tweet, and include simple characteristics (length of text, number of words), stylistic attributes (number of ques- tion and exclamation marks, uppercase characters), and binary features indicating the existence or not of emoti- cons, special words (please”) and punctuation (colon). (b) Language-specific These are extracted for a prede- fined set of languages (English, Spanish, German), which are detected using a language detection library. 5 They include the number of positive and negative sentiment words in the text using publicly available sentiment lex- icons: For English, we use the list by Jeffrey Breen, 6 for Spanish the adaptation of ANEW [27], and for Ger- man the Leipzig Affective Norms [15]. Additional binary features indicate whether the text contains personal pro- nouns (in the supported languages), and the number of detected slang words. The latter is extracted using lists 3 github.com/MKLab-ITI/computational-verification. 4 github.com/MKLab-ITI/image-verification-corpus. 5 code.google.com/p/language-detection/. 6 github.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107. 123 76 International Journal of Multimedia Information Retrieval (2018) 7:71–86 Fig. 2 Overview of the proposed framework. MV stands for majority voting Table 1 Overview of verification features Tweet-based features (TB) User-based features (UB) Text-based User-specific #words* has please” #friends* has location length of text* has colon #followers* has existing location #question marks* contains happy emoticon* follower–friend ratio* has bio description #exclamation marks* contains sad emoticon* #tweets tweet ratio contains question mark* #uppercase chars* #media content account age contains exclamation mark* has profile image is verified* has header image #times listed* has a URL* Language-specific Link-based (common for TB and UB) #pos senti words* contains 1st pers.pron.* WOT score alexa country rank #neg senti words* contains 2nd pers.pron.* in-degree centrality alexa delta rank #slangs contains 3rd pers.pron.* harmonic centrality alexa popularity #nouns readability alexa reach rank Twitter-specific #retweets* #mentions* #hashtags* #URLs* has external link Link-based features are extracted in the TB case for external links that tweets may share, and in the UB case for the URL included in the account profile. Features with an asterisk were proposed in [1,10] and will be referred to as Baseline Features (BF), while the full feature set (union of BF and newly proposed ones) will be referred to as Total Features (TF) of slang words in English7 and Spanish. 8 For German, no available list was found and hence no such feature is computed. Moreover, the number of nouns in the text was also added as feature, and computed based on the Stanford parser only for English [16]. Finally, we use the 7 onlineslangdictionary.com/word-list/0-a/. 8 www.languagerealm.com/spanish/spanishslang.php. Flesch Reading Ease method9 to compute a readability score in the range [0: hard to read, 100: easy to read]. For tweets written in languages where the above features cannot be extracted, we consider their values missing. (c) Twitter-specific These are features related to the Twitter platform, including the number of retweets, hash- 9 github.com/ipeirotis/ReadabilityMetrics. International Journal of Multimedia Information Retrieval (2018) 7:71–86 77 tags, mentions, URLs and a binary feature expressing whether any of the URLs points to external (non-Twitter) resources. (d) Link-based These include features that provide infor- mation about the links that are shared through the tweet. This set of features is common in both the TB and UB sets, but in the latter it is defined in a different way (see link-based category in UB features). For TB, depending on the existence of an external URL in the tweet, its reli- ability is quantified based on a set of Web metrics: (i) the WOT score, 10 which is a way to assess the trust on a website using crowdsourced reputation ratings, (ii) the in-degree and harmonic centralities,11 computed based on the links of the Web graph, and (iii) four Alexa met- rics (rank, popularity, delta rank and reach rank) based on the rankings API.12 User-based features (UB): These are related to the Twitter account posting the tweet. We divide them into (a) user- specific and (b) link-based features. (a) User-specific These include the user’s number of friends and followers, the account age, the follower– friend ratio, the number of tweets by the user, the tweet ratio (number of tweets/day divided by account age) and several binary features: whether the user is verified by Twitter, whether there is a biography in his/her profile, whether the user declares his/her location using a free text field, and whether the location text can be parsed into an actual location, 13 whether the user has a header or profile image, and whether a link is included in the profile. (b) Link-based In this case, depending on the exis- tence of a URL in the Twitter profile description, we apply the same Web metrics as the ones used in the link-based TB features. If there is no link in the pro- file, the values of these features are considered to be missing. After feature extraction, the next steps include prepro- cessing, cleaning, and transformation. To handle the issue of missing values on some of the features, we use linear regres- sion for estimating their values: We consider the attribute with the missing value as a dependent (class) variable and apply linear regression for numeric features. The method cannot support the prediction of Boolean values, and hence 10 www.mywot.com/. 11 wwwranking.webdatacommons.org/more.html. 12 data.alexa.com/data?cli=10url=google.com. 13 Using: github.com/socialsensor/geo-util. those are left missing. Only feature values from the train- ing set are used in this process. Data normalization is also performed to scale the numeric feature values to the range [− 1, 1]. 3.2 Building the classification models We use the TB and UB features to build two independent clas- sifiers (CL1 , CL2 , respectively), each based on the respective set of features. To further increase classification accuracy, we make use of bagging: We create m different subsets of tweets from the training set, including equal number of sam- ples for each class (some samples may appear in multiple subsets), leading to the creation of m instances of CL1 and CL2 (m = 9 in our experiments). These are denoted as CL11 , CL12, . . . CL1m and CL21, CL22, . . . CL2m , respectively, in Fig. 2. The final prediction for each of the test samples is cal- culated using the average of the m predictions. Concerning the classification algorithm, we tried both logistic regression (LR) and Random forests (RF) of 100 trees. 3.3 Agreement-based retraining A key contribution of the proposed framework is the intro- duction of an agreement-based retraining step (the fusion block in Fig. 2) as a second-level classification model for improving the generalization ability of the framework to new content. The agreement-based retraining step was motivated by recent work on social media sentiment analysis that was demonstrated to effectively address the problem of out-of- domain polarity classification [36]. In our implementation, we combine the outputs of clas- sifiers CL1, CL2 as follows: For each sample of the test set, we compare their outputs, and depending on their agreement, we divide the test set in the agreed and disagreed subsets. The elements of the agreed set are assigned the agreed label (fake/real) assuming that it is correct with high likelihood, and they are then used for retraining the best performing of the two first-level models (CL1, CL2)14 to reclassify the disagreed elements. Two retraining techniques are investi- gated: The first is to use just the agreed samples to train the CL classifier (denoted as CLag ), while the second is to use the entire (total) set of initial training samples extending it with the set of agreed samples (denoted as CLtot ). The goal of retraining is to create a new model that is tuned to the specific data characteristics of the new content. The result- ing model is expected to predict more accurately the values of the samples for which CL1, CL2 did not initially agree. In the experimental section, we test both of the above retraining variants. 14 The selection is based on their performance on the training set during cross-validation. 78 International Journal of Multimedia Information Retrieval (2018) 7:71–86 Fig. 3 Snapshot of the Tweet Verification Assistant interface. Given a tweet, a user can explore the verification result, including the extracted feature values and their distribution on the Verification Corpus 3.4 Verification result visualization The main idea behind the visualization of the produced veri- fication output is to present it along with the list of credibility features that were extracted from the input tweet and the user account that posted it, and to give to end users the option to select any of these features and inspect its value in relation to the distribution that this feature has for real versus fake tweets, as computed with respect to the verification corpus (Sect. 4). Figure 3 depicts an annotated screenshot of this applica- tion, which is publicly available. 15 In terms of usage, the investigator first provides the URL or id of a tweet of inter- est, and then the application presents the extracted tweet- and user-based features and the verification result (fake/real) for the tweet in the form of a color-coded frame (red/green, respectively) and a bar. It also offers the possibility of inspect- ing the feature values in the central column. By selecting a feature, its value distribution appears at the right column, separately for fake and real tweets (side by side). Moreover, a textual description informs the user about the percentage of tweets of this class (fake or real) that have the same value for this feature. In that way, the investigator may better under- stand how the verification result is justified based on the individual values of the features in relation to the typical” values that these features have for fake versus real tweets. 15 reveal-mklab.iti.gr/reveal/fake/. 4 Verification corpus Our fake detection models are based on a publicly avail- able verification corpus (V C) of fake and real tweets that we initially collected for the needs of organizing the MediaEval 2015 Verifying Multimedia Use (VMU) task [2]. 16 This con- sists of tweets related to 17 events (or hoaxes) that comprise in total 193 cases of real images, 218 cases of misused (fake) images and two cases of misused videos, and are associated with 6,225 real and 9,404 fake tweets posted by 5,895 and 9,025 unique users, respectively. The list of events and some basic statistics of the collection are presented in Table 2. Several of the events, e.g., Columbian Chemicals, Passport Hoax and Rock Elephant, were actually hoaxes, and hence all content associated with them is fake. Also, for several real events (e.g., MA flight 370), no real images (and hence no real tweets) were included in the dataset, since none came up as a result of the data collection. Figure 4 illustrates four example cases that are characteristic of the types of fake in the corpus. These include reposting of past images in the context of a new event, computer generated imagery, images accompanied by false claims, and digitally tampered images. The set of tweets T of the corpus was collected with the help of a set of keywords K per event. The ground truth labels (fake/real) of these tweets were based on a set of online arti- cles that reported on the particular images and videos. Only 16 The V C was since then expanded with new data that was used as part of the VMU 2016 task. However","Boididou, C., Papadopoulos, S., Zampoglou, M., Apostolidis, L., Papadopoulou, O., & Kompatsiaris, Y. (2018). Detection and visualization of misleading content on Twitter. International Journal of Multimedia Information Retrieval, 7(1), 71-86."
"SSAFC_041","The Fake News Challenge: Stance Detection using Traditional machine learning Approaches","Fake news has caused sensation lately, and this term is the Collins Dictionary Word of the Year 2017. As the news are disseminated very fast in the era of social networks, an automated fact-checking tool becomes a requirement. However, a fully automated tool that judges a claim to be true or false is always limited in functionality, accuracy and understandability. Thus, an alternative suggestion is to collaborate a number of analysis tools in one platform which help human fact checkers and normal users produce better judging based on many aspects. A stance detection tool is a first stage of an online challenge that aims to detect fake news. The goal is to determine the relative perspective of a news article towards its title. In this paper, we tackle the challenge of stance detection by utilizing traditional machine learning algorithms along with problem specific feature engineering. Our results show that these models outperform the best outcomes of the participating solutions which mainly use deep learning models.","Computer Science","Proceeding","2018","Y","Y","Prototype","Experimental","Stance detection","0","Fake news is one of the controversially discussed is- sues lately. New York Times defines it as ”a made- up story with an intention to deceive”1. Moreover, propaganda, conspiracy theories and other false sto- ries have always been used in the media for a second gain like monetizing, political goals and opinion ma- nipulation. Online services such as factcheck.org and PolitiFact.com perform manual fact checking to filter fake news. The current online environments like social media create powerful tools to spread false stories extensi- vely. As a result, journalists and fact checkers with their current strategies cannot label fake stories in real time before they are out of control. Automating those strategies is one solution to speed up the procedure. This kind of issues is considered to fit a machine lear- ning task (Markowitz and Hancock, 2014; Hardalov et al., 2016; Jin et al., 2017). Until lately, the work on fighting fake news is handled in many separate projects and studies. Howe- ver, organizations like FullFact.org suggests to open collaborations between these projects to build a plat- form that provides a collection of tools to handle the different aspects of fact checking routines2. Similarly Fake News Challenge (FNC-1), which is an on-line competition, also suggests a solution for fake news detection to be composed by a collection of automa- ted tools to support human fact checkers and speed up their processes. Stance detection is among the col- lection of these tools 3. Stance detection has been proven to be useful in disinformation detection. (Jin et al., 2016) applied the stance to analyze the credibility propagation for news verification through building connections bet- ween micro-blogs (tweets) as supporting or denying each others’ viewpoints. (Qazvinian et al., 2011) use the stance observed in tweets in a Belief Classifica- tion to classify false and true rumors, even though rumors checking is found to be different from news checking. Stance detection for fact checking in the emerging news has mostly been investigated in micro- blogs. The stance detection task presented by FNC-1 is about predicting the stance of one piece of text (news body) towards another (news headline). Particularly, it should predict whether the news body has the stan- ces Unrelated, Discuss, Agree or Disagree to a news headline. Most of the teams participated in the FNC- Masood R. and Aker A. The Fake News Challenge: Stance Detection using Traditional Machine Learning Approaches. DOI: 10.5220/0006898801280135 In Proceedings of the 10th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (KMIS 2018), pages 128-135 ISBN: 978-989-758-330-8 Copyright c© 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved including the winner team used deep learning appro- aches to solve the task. Although deep learning is a powerful technique and it has shown a great success in various tasks, the deep architectures is said to be short on providing more understandable results in terms of features extracted by the deep architecture and their performances. This drawback is the motivation in our work. Thus, instead of deep learning we use traditi- onal machine learning approaches along with the ap- propriate feature selection/engineering and show that we can beat the deep learning approaches in an at- tempt to provide useful information about engaged fe- atures and their performances. Thus our contributions are as follows: • We provide a solution for the FNC-1 task using traditional machine learning algorithms. • We extract a range of different features that are useful for the stance detection task. • We perform feature analysis and discuss different experimental training and testing settings which were important to obtain state-of-the-art results. • We achieve a score of 82.1%4 which is currently the best score achieved for the fake news stance detection task. We first discuss related work, and then in Section 3 we describe the data and the scoring system. In Section 4 we introduce our method including the fe- atures and the machine learning approaches used to learn the models. Then, in Section 5, we discuss ex- perimental settings followed by the results discussion in Section 6 and conclusion in Section 7. 2 RELATED WORK The problem that is introduced in this paper was pu- blished first as a pure stance detection tool within a plan to employ it in a wider fake news detection plat- form. Many researches proposed methods which em- ploy stance in disinformation detection. The veracity of claims were also predicted using the stance of arti- cles and the reliability of their sources (Popat et al., 2017). Stance features were also employed in de- tecting the credibility of rumours, which are also de- fined to be unverified claims (Enayet and El-Beltagy, 2017). Moreover, Using Tweets publishing time and stances as the only features to model the veracity of tweets using Hidden Markov Models achieved high accuracy (Dungs et al., 2018). Some other cases that 4This score is calculated using the FNC-1 scoring sy- stem targeted rumours used also stance features to identify their veracity (Zubiaga et al., 2018). Detecting stance of news articles is the most rela- ted work to our task. On this line the work of Fer- reira  Vlachos addresses rumor debunking based on stance. The aim is to estimate the stance of a news he- adline towards its paired claim as Observing, For or Against. Linguistic features are extracted from each claim and headline pair (Ferreira and Vlachos, 2016). The FNC-1 task extends the work of Ferreira  Vlachos to predict the stance of a complete news arti- cle (body of an article) towards a title or headline pai- red with that article. For this task results of first three top systems have been announced. The first ranked team5 approach is based on a 50/50 weighted average ensemble combining a gradient-boosted decision tree model fed with text based features from the headline and the body pair, and a deep learning model based on one dimensional Convolutional Neural Network (CNN) with Google News pre-trained vectors. Unlike most of the approaches used by the parti- cipating teams described above we employ traditional machine learning techniques along with feature en- gineering. We investigate several features and deter- mine their contribution towards the task. We also ex- periment with different training settings. Overall we show that our approach leads to slightly better results than those reported by deep learning strategies. 3 THE FAKE NEWS CHALLENGE The fake news challenge (FNC-1) is a machine lear- ning task which is a contribution between AI commu- nity, journalists and fact-checkers. It forms a basis for fighting fake news and aims to develop tools towards fake news detection. One of the tools is a stance de- tection tool which is the first interest of the challenge. The challenge is about predicting the stance of a news article (body of the article) towards its paired title or headline. In the following sections we describe the data and the scoring mechanism of FNC-1. 3.1 Data The data used in the competition was extracted from Craig Silverman’s Emergent dataset 6 which is part of a research project that employs rumor tracking in de- tecting misinformation. The dataset consists of 2595 articles that relates to 300 claims (headline) so that for each claim there are between 5 to 20 articles. These articles are labeled manually by journalists as agree, disagree or discuss the claims they are paired with. The FNC-1 organizers mixed and matched the ar- ticle bodies and their headlines, and used the labels relative to the claims. They got 75,119 labeled pairs as the following: • Unrelated: The topic of the headline is different from the topic of the article body. • Discuss: The body observes the headline’s claim neutrally without taking a position. • Agree: The body confirms the headline’s claim. • Disagree: The body refutes the headline’s claim. The resulted pairs were divided by FNC-1 orga- nizers into 49,972 pairs as training data and 25,147 pairs for testing. The training dataset was a match be- tween 1648 unique headlines and 1683 unique article bodies, whereas the test dataset is a match between 880 unique headlines and 904 unique article bodies with no overlaps between the splits. In addition, the test data used to finally evaluate the competitors was supplied with an additional 266 pairs that the organi- zers derived and labeled using Google News articles. The headline’s length ranged between 1 - 40 words with an average of 11 words. While the article body length ranged between 3 - 4800 words with an average of 350 words. The training dataset is highly unbalan- ced with class distribution as the following: 73.13% unrelated, 17.83% discuss, 7.36% agree and 1.68% disagree. 3.2 FNC-1 Scoring System and Baseline Classifier FNC-1 scoring system adds 0.25% score for each pair classified correctly as Unrelated. The score is incre- ased by 0.25% if the pair was related and was classi- fied as any of Discuss, Agree or Disagree classes. If the pair was correctly classified as Discuss, Agree or Disagree, the score is increased to 0.75%. We consi- der the approach that won the FNC-1 as our baseline system. This system scored 82.02% according to the FNC-1 scoring system. 4 METHOD In our methodology we apply traditional machine le- arning approaches, specifically, L1-Regularized Lo- gistic Regression provided by LibLINEAR (Fan et al., 2008) using WEKA (Hall et al., 2009) and Random Forest classifier from the same WEKA toolkit. Both approaches rely on feature engineering. Our feature engineering focuses on the article content and tries to find parts of it that would best describe the stance the article has towards the headline. The data provided for the FNC-1 stance detection task is limited to articles’ text with no reference to sources, writers or any explicit meta data. Given this, the features we relied on are only linguistic features. In the following sections we describe our features in detail. 4.1 Headline Features • Headline Length (H-Len). This is equal to the number of words in the headline. • Headline Contains Question Mark (H-Q). A feature indicating whether a headline contains a question mark or not (0 or 1). 4.2 Article Content Features We split each article content into a heading, middle, and tail parts based on the sentences7. The motiva- tion behind this splitting is that most news articles are written in a specific style in which the article begin- ning (heading) introduces the main argument(s) that the entire article wants to convey to the users, the body part (middle) provides more detailed informa- tion about the argument(s) made earlier and a conclu- sion towards the end (tail) summarizing what is de- tailed in the body. We have experimented with diffe- rent splitting strategies however, dividing the entire article into first 5 sentences (heading), 4 sentences from the tail and 10% of the middle sentences (min. 2 sentences)8 gave us best performance. In the fol- lowing we explain the features extracted from these parts. • Bag of Words (BoW): We extract uni-grams and bi-grams from the heading and tail parts of the ar- ticle. However, we retain only the 500 most occur- ring n-grams and delete all the remaining ones. • Root Distance (Root-Dist): This feature is calcu- lated similar to the study of (Ferreira and Vlachos, 2016). However, for our case we compute 3 dif- ferent features (feature vectors), i.e. one for the heading part, one for the middle and one for the tail part. For each sentence in each part we parse it using Standard CoreNLP parser and compute its root distance to pre-collected words list obtained from related work (Discuss or Refute words). 7Sentence splitting has been performed using The Stan- ford CoreNLP tools (Manning et al., 2014) 8We start taking from the median, then left and right of it until we have reached our threshold.KMIS 2018 • Sentiments: For each sentence in each article part we compute its sentiment score. The tool used is Stanford Sentiment (Socher et al., 2013) which gi- ves each sentence a score between 0 (high positi- vity) to 4 (high negativity). • Sentence Length (Sentence-Len): This feature indicates the maximum and the average length of the sentences in the respective article parts. • Punctuation Count (Punct): We use several punctuation such as dot, comma, etc. and for each of them we compute how many times it appears in the entire article (not only in the three parts). • Lemma Count: We remove all stop words from the headline and lemmatize the remaining words. For each sentence in each article part we count the occurrences of each lemma that also appears in the headline and take the sum of all lemma occur- rence counts as a lemma count feature. We also do this for the entire article regardless of the men- tioned article split boundaries. • Character Grams (Ch-Grams): We build sets of character sequences of lengths 4, 8 and 16 from the headline. For each character sequence set we count how many times the sequences appear in each sentence of each article part. Each sentence is assigned three count values each indicating how many times the sentence includes any sequence from the respective length category. We use lem- matized text before building the character sequen- ces and also remove all the stop words. • Word2Vec Similarity (W2Vec-Sim): For this fe- ature, a vector space representation of both the headline and each article part is computed using word embedding (Mikolov et al., 2013). For the embedding, we used Google’s Word2Vec pre- trained words and phrases from Google News 9. Once the embedding vectors are obtained we compute the cosine similarity between the given vectors. • Word Grams (N-Grams): This is similar to the Ch-Grams feature however, instead we take word sequences of lengths 2, 4, 8 and 16. • Hypernyms Similarity (Hyp-Sim): We use WordNet 3.1 (Miller, 1995) and collect hyper- nyms from the first synset of nouns and verbs. The nouns and verbs are taken from the headline, article heading and article tail. For the collected hypernyms we build word embedding vectors and compute similarities between title-article heading and title-article tail using cosine. 9https://code.google.com/archive/p/word2vec/ • Cosine Similarity (Cos-Sim): This feature com- putes the cosine similarity of the headline to each sentence in each article part. The vector values are word counts. Before computing we take lemmas of the words and remove stop words. • Paraphrase Alignment (ppdb): This feature captures an alignment score calculated between two texts depending on the Paraphrase Database (Pavlick et al., 2015) and the Kuhn-Munkres al- gorithm (Kuhn, 1955; Munkres, 1957). It is com- puted between words from the headline and words from a sentence in each article part. This feature is calculated similar to (Ferreira and Vlachos, 2016). • Subject, Verb and Object Triples Entailment (SVO): This feature indicates the entailment re- lations between the subject, verb and object tri- ples of the headline and each sentence in each ar- ticle part. The entailment is again found using the paraphrase database (Pavlick et al., 2015). This feature is computed as in (Ferreira and Vlachos, 2016) work but instead of indicating the entail- ment with 0 or 1 we count how many sentence in each article part have this entailment relationship. • Negation (Neg): We use the Hungarian algorithm (Kuhn, 1955) to align words between the headline and words from each sentence from the article. Then we check for each aligned word pairs whet- her one of them is the negation of the other accor- ding to (Ferreira and Vlachos, 2016). Each sen- tence is assigned a counter indicating how many negated pairs it contains. We compute this feature for each sentence in the entire article. • Word Overlap Score (W-overlap): For this fe- ature we compute an overlap score between the headline and the body’s heading as well as bet- ween headline and tail. The method is based on extracting all possible sub-strings from these parts and then finding the longest matching sub-strings. The score is calculated by summing up the square lengths of these matches. • Bias Count (Bias): Based on a bias lexicon as in (Recasens et al., 2013) and (Allen et al., 2014), we compute how many bias lexicon entries appear in the entire article as well as in the headline. 5 EXPERIMENTAL SETTING As noted in section 3.1, the data has four different class labels: Unrelated, Discuss, Agree and Disagree. We trained our classifiers so that they predict one of these four labels. However, the performance of theThe Fake News Challeng resulting models were below the baselines10. After manual inspection of the data we realized that the ar- ticles labeled differently were similar in tone towards the headline and thus difficult for a multi-class labeler to predict the right class. Furthermore, the data is un- balanced and contains mostly Unrelated pairs and re- latively few pairs from the other classes. To overcome these issues we experimented with different training strategies without modifying the training and testing settings defined by FNC-1: • 2-Steps Classifier: We first train the classifier to distinguish only between Unrelated and Related pairs, where Related represents all the categories Discuss, Agree, Disagree. Next, we train a second classifier on the pairs labeled with Discuss, Agree, Disagree. For testing, we first run the first classi- fier on the entire testing data. Any article-headline pair classified as Related is further analyzed with the second classifier to further classify it as Dis- cuss, Agree or Disagree. • 3-Steps Classifier, Setting 1: We further split the classification of the Related classes and cre- ate three classifiers. We keep the first step as it is in the previous 2-step classifier setting (classifica- tion for Unrelated and Related. Then we train a classifier to predict the classes Discuss and Non- Discuss, where Non-Discuss category stands for the original categories Disagree, Agree. For the third step we use a 2-way classification for the re- maining categories Disagree, Agree. For testing we again run first the first classifier to split the data into Unrelated and Related catego- ries. After, the Related data pairs are further clas- sified to obtain Discuss and Non-Discuss article- headline pairs. Finally, for the Non-Discuss pairs we further detail their actual classes using the third classifier and obtain the Disagree Agree clas- ses. • 3-Steps Classifier, Setting 2: We keep the first step as it is, but we used a 2-way classification in the second step for the categories Disagree, Non- Disagree. The Non-Disagree category represents the original categories Discuss, Agree. For the third step we use a 2-way classification for the re- maining categories Discuss, Agree. In all settings for the first two steps we use an L1- regularized logistic regression classifier (Fan et al., 2008). For the third step we use a Random Forest classifier with 100 trees (Breiman, 2001). In each step we used different sets of features. Table 1 shows the features used in each step. 10Classifier predicting all classes led to 77.04% FNC-1 score. Table 1: Classifier steps and the features used in each step. 1st step 2nd step 3rd step W-Overlap H-Q H-Q Lemma Count BoW BoW Ch-Grams Root-Dist Root-Dist N-Grams Neg Neg Cos-Sim SVO SVO Hyp-Sim Sentiments Sentiments PPDB PPDB PPDB W2Vec-Sim W2Vec-Sim Sentence-Len Bias Bias Punct 6 RESULTS AND DISCUSSION Our overall results are shown in Table 2. We report, as in FNC-1 challenge, the results using the FNC-1 score. We also compute accuracy. From the table we see that the best results are obtained with the 3-step classifiers and setting 2. However, we found no diffe- rence in terms of significance to our other settings.11 From the table we also see that the performance of our classifier (3-steps classifier setting 2) is better than the one of the best system participated in the FNC-1 task. Both FNC-1 as well as accuracy figures are better than those of the best performing baseline.12 Tables 3 and 4 show the confusion matrices of the best baseline and our 3-steps classifier with setting 2. According to the matrices, the 3-steps classifier in set- ting 2 predicts more correct Discuss, Disagree, Unre- lated pairs. The baseline, on the other hand, performs better on the Agree class. 6.1 Features Analysis As shown in Table 5 best results are obtained when all features are used. We aimed to understand the con- tribution of each feature to the overall results. Thus we removed a feature at a time, trained the classifiers with the remaining features and tested on the testing data. The difference in results are captured using pai- red t-test and a p-value of p < 0.002813 In the results we see only a significance drop when we remove the BoW feature, in all other settings the results are not significantly different from when there is no feature 11Significance test is performed using student t-test. 12Again in the results we did not find any indication for significance. 13When conducting multiple analyses on the same depen- dent variable, the chance of achieving a significant result by pure chance increases. To correct for this we did a Bon- ferroni correction on the p-value. Results are reported after this correction.KMIS 2018 Table 2: N-Steps classifiers and Baseline. Winning Baseline 2-Step Classifier 3-Steps Classifier setting 1 3-Steps Classifier setting 2 Unrelated 0.98 0.98 0.98 0.98 Discuss 0.76 0.76 0.75 0.76 Agree 0.54 0.52 0.49 0.52 Disagree 0.04 0.05 0.07 0.1 Accuracy 89.1 89.1 88.8 89.18 FNC- Score 82.02 82.0 81.53 82.10 Table 3: Best baseline Confusion Matrix. A, DA, DC and U stands for Agree, Disagree, Discuss and Unrelated respecti- vely. A DA DC U A 1114 17 588 184 DA 275 13 294 115 DC 823 6 3401 234 U 35 0 203 18111 Table 4: 3-Steps Classifier with setting 2 Confusion Matrix. A, DA, DC and U stands for Agree, Disagree, Discuss and Unrelated respectively. A DA DC U A 947 29 799 128 DA 181 39 343 134 DC 589 28 3558 289 U 10 2 219 18118 omission. However, in all removal cases there is a moderate drop in the results indicating that every fea- ture has some contribution to the final results. We also removed combinations of features from the entire set of features used in our final model to show the effect of more than one feature removed at once. The selection of different combinations is cho- sen according to the relatedness of features. We list them in groups: • Group A: Is a group of features used in the first step for distinguishing Related and Unrela- ted classes, namely Ch-grams, N-grams, Lemma Count and W-overlap (see Table 1 for the set of features used in the first step). When removing Group A features, the number of correctly clas- sified instances as Unrelated reduces the most (from 18118 to 18034), hence reducing the cor- rectly classified instances as Discuss. See confu- sion matrices in Tables 6 and 4 for comparison. • Group B: This group holds features related to si- milarity and entailment, namely PPDB, Hyp-Sim, W2Vec-Sim and Cos-Sim. They have lower effect on Related and Unrelated but greater effects on the Agree (reduction from 947 to 913) and Dis- agree (reduction from 39 to 26). See confusion Table 5: Accuracy, and FNC-1 score when using all featu- res compared to results when removing features one by one accordingly. Features Accuracy FNC-1 Score All Features 0.891 82.1 - BoW* 0.870 78.53 - Lemma Count 0.888 82.07 - Ch-Grams 0.888 81.42 - N-Grams 0.890 82.00 - Hyp-Sim 0.891 81.93 - Cos-Sim 0.890 81.86 - W2Vec-Sim 0.891 82.01 - ppdb 0.890 81.70 - w-overlap 0.891 82.02 - H-Len 0.891 81.97 - Root-Dist 0.888 81.57 - SVO 0.889 81.59 - Neg 0.891 81.85 - Sentiments 0.887 81.37 - Bias 0.891 82.00 - Punct 0.889 81.66 - Sentence-Len 0.887 81.41 - Tittle-Q 0.891 81.86 - group A 0.885 81.19 - group B 0.886 80.87 - group C 0.888 81.38 - group D 0.890 81.89 Table 6: Group A: Features without Ch-grams, N-grams, Lemma Count and W-overlap. A DA DC U A 934 27 797 145 DA 180 42 342 133 DC 561 25 3493 385 U 16 1 298 18034 matrices 7 and 4. • Group C: This group contains SVO, Neg, Root- Dist and PPDB features. Confusion matrix 8 shows that by removing these features, the cate- gories Disagree and Discuss are mostly affected. • Group D: This group contains Punct, Bias, Sentence-Len and T-Quest features. Removing this combination has a greater effect on the Agree category. See Table 9.The Fake News Challenge: Table 7: Group B: Features without PPDB, Hyp-Sim, W2Vec-Sim and Cos-Sim. A DA DC U A 913 30 783 177 DA 165 26 328 178 DC 563 30 3485 386 U 9 1 243 18096 Table 8: Group C: Features without SVO, Neg, Root-Dist and PPDB. A DA DC U A 939 26 793 145 DA 212 29 310 146 DC 626 35 3486 317 U 24 1 212 18112 Figures for the accuracy and FNC-1 metrics after re- moving these group features are shown in Table 5. Overall the removal of all group features lead to de- crease in performance. However, similar to the single features cases the decreases are only moderate wit- hout significance relevance. 6.2 Discussion Overall we have seen that our 3-step classifier in set- ting 2 outperforms the state-of-the-art system that par- ticipated in the FNC-1 challenge. Although the diffe- rences in the results are only moderate, nevertheless, they show that it is possible to beat state-of-the-art results with feature engineering as well as traditional machine learning approaches. Furthermore, tackling the problem in hand with such an approach has the advantage that, unlike deep learning approaches, ena- bles feature extraction and later feature analysis. In our case, we carefully picked our features and investi- gated settings including finding article parts and clas- sification steps where they shine best. Feature analysis shows that removing any single feature leads to some drop in performance compared to the results when all features are used. The signifi- cant drop happens when we remove the BoW feature. The BoW feature includes uni-grams and bi-grams ex- tracted from the article heading as well as from the article tail. Thus, it aims to capture what is in those article parts in terms of vocabulary. Those areas of the article introduce and summarize arguments. The chance is very high that they capture the claim intro- duced in the headline. Indeed the results confirm this phenomenon with a significant drop when removing this feature. We also grouped features and removed them al- together from the complete feature set. The overall drop in terms of performance was moderate. Howe- Table 9: Group D: Features without Punct, Bias, Sentence- Len and T-Quest. A DA DC U A 886 26 863 128 DA 173 34 356 134 DC 556 27 3592 289 U 14 2 215 18118 ver, in the confusion matrices we have seen that each feature group has its strength in a specific category or class. Group A features help in the relatedness task (step one of the classification) whereas the ot- her groups find their shining points at later steps and address Agree, Disagree and Discuss classes. Finally we performed error analysis on the final classifier results. We observed the following points: 1. There is ambiguity in Disagree definition. Exam- ple: pair: {headline: ”Justin Bieber Helps De- fend Russian Fisherman...”, body ID: 2373}. This pair’s correct class is ”Disagree”, but it is classi- fied as Unrelated by our classifier. In this exam- ple there is no mention of Justin Bieber. The ar- ticle itself is about a Fisherman being attacked by a bear. However, there is no disagreement about the topic that is introduced in the headline. Thus according to the definition for the category Dis- agree, this pair should be classified as Unrelated. 2. Detecting disagreement is hard in some cases be- cause it depends on the implicit meaning of the ar- ticle. As an example, the pair:{Headline: ”People Actually Believed Argentina’s President Adopted A Jewish Boy...”, body ID: ”2382”,} This pairs correct classification is Disagree, but it is classi- fied as Discuss by our classifier. The article talks about passing a law to stop some act of Argen- tina’s people and it does not refute explicitly what it is in the headline. 3. Detecting unrelated titles to their paired articles is critical when the article uses most of the words mentioned in the title. 4. In most cases there is no clear indications for dif- ferentiating between the classes Agree and Dis- cuss which makes them hard to judge by our clas- sifier. Most of the classifier errors are due to this phenomenon. See Table 4. 7 CONCLUSIONS In this paper we re-investigated the Fake News first challenge of stance detection using traditional ma- chine learning and feature engineering approach.KMIS 2018 Using this method we scored better than the first win- ner’s deep learning model. We performed feature analysis by removing a fea- ture at a time but also groups of features. Any removal led to moderate performance drop. The significance drop happened when the BoW feature was removed. This feature contains uni-grams and bi-grams extrac- ted from the article heading and article tail. As dis- cussed both parts either introduce or summarize argu- ments and are likely to capture what is said in the he- adline. Overall every feature plays a role in the clas- sification. We showed that some features play role in the first step (distinguishing between related and unre- lated pairs) and others play at discriminating between agree, disagree and discuss classes. Our immediate future work will be to use stance to perform judgments about fake news. We will investi- gate how stance can be integrate for the fake news classification.","Masood, R., & Aker, A. (2018). The Fake News Challenge: Stance Detection using Traditional Machine Learning Approaches. In KMIS (pp. 126-133)."
"SSAFC_060","Assessing the News Landscape: A Multi-Module Toolkit for Evaluating the Credibility of News","Today, journalist, information analyst, and everyday news consumers are tasked with discerning and fact-checking the news. This task has became complex due to the ever-growing number of news sources and the mixed tactics of maliciously false sources. To mitigate these problems, we introduce the The News Landscape (NELA) Toolkit: an open source toolkit for the systematic exploration of the news landscape. NELA allows users to explore the credibility of news articles using well-studied content-based markers of reliability and bias, as well as, filter and sort through article predictions based on the user's own needs. In addition, NELA allows users to visualize the media landscape at different time slices using a variety of features computed at the source level. NELA is built with a modular, pipeline design, to allow researchers to add new tools to the toolkit with ease. Our demo is an early transition of automated news credibility research to assist human fact-checking efforts and increase the understanding of the news ecosystem as a whole.","Computer Science","Proceeding","2018","Y","Y","Prototype","Support","Content ranking","4","Understanding and analyzing the news landscape has became a pri- ority for researchers across many disciplines. The production and This paper is published under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. WWW ’18 Companion, April 23–27, 2018, Lyon, France © 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY 4.0 License. ACM ISBN 978-1-4503-5640-4/18/04. consumption of news in today’s media landscape favors clicks and attention, as opposed to in-depth analysis. This drive for attention has lead to the emergence of a large number of media sources with ever increasing visibility. These sources operate under different in- centives: from benign to opportunistic and malicious. Those sources which are partisan or malicious in intent employ a wide-range of tactics to make their message heard. They employ tactics such as re- porting incorrect information, using emotionally charged language, manipulative titles, and mixing true news with fake news. Fake news stories and hyper-partisan news coverage are thought to have influenced various key elections worldwide. This, coupled with the well-known susceptibility of individuals to false and misleading information [ 5 ], has lead to the increasing need for tools that assist researchers, journalists, and every day individuals in the analysis of news. Supporting this notion, in a 2017 agenda for fake news research, Lazer et al. argue that we ”need to translate existing re- search into a form that is digestible by journalist and public-facing organizations [ 4].” However, given the complexity, the problem requires multi-faceted solutions and a better understanding of the wide-range of news sources. In addition, tools should be able to quickly evaluate sources to decide where to dedicate fact-checking efforts (before an article’s spread). To address these problems, we introduce the The News Land- scape (NELA) Toolkit: an open source toolkit for the systematic exploration of the news landscape, through a unique combination of (a) real data from news sources and social media, (b) state-of-the-art tools that predict different factors of credibility, and (c) visualization tools to compare a large number of media sources across differ- ent axes. Specifically, NELA is made up of multiple independent modules, in which users can scrape news articles for article-level predictions or explore source-level characteristics using the built-in NELA data set. In this demonstration, we discuss the first release of the toolkit, and briefly discuss its utility using an initial 7 months of news data from 92 sources across the reliability and bias spectrum. 2 DESCRIPTION OF THE DEMO To use the NELA Toolkit, visit the NELA Toolkit website (nelatoolkit. science). The homepage provides two choices Check a News Arti- cle or Compare News Sources. Under Check a News Article users can provide a url to a news article or manually enter news article text. The tool then performsTrack: DemonstrationWWW 2018, April 23-27, 2018, Lyon, France235 Figure 1: NELA Toolkit architecture several predictions on the article: reliability, political impartiality, title objectivity, text objectivity, and several online community interest predictions. Each of these predictions is displayed as a probability and each article with associated predictions are entered into a table. As more article entries are provided, this table can be sorted and filtered by different predictions using the table filters menu at the top of the page. Further, more details about the article and analysis of the article can be found by clicking on the entry in the table. The ultimate goal of this page is to allow journalist and information analyst to quickly filter articles down to ones that need to be fact-checked or are of interest. Under Compare News Sources users can explore and compare a variety of news sources using content-based features. Specifically, users can select multiple features, sources, and a time range to visualize on a 2-dimensional scatter plot. For example, a user can select reading complexity for the x-axis and negative sentiment for the y-axis using the chart setting menu on the left side of the page. They can then select any number of sources from our data set and a data range over which to explore. The tool will then generate a scatter plot of the selected sources for comparison. If a user wants more details about a source, they can double-click the source bubble in the scatter plot. This detailed page will show source metadata, credibility predictions, and Facebook engagement over time. These details can also be found on the View All Sources page. The overall architecture of the toolkit can be found in Figure 1. Due to lack of space and the many parts of the toolkit, we do not provide screenshots. We encourage readers to visit the NELA Toolkit website (nelatoolkit.science), watch our demo walk-through (nelatoolkit.science/help), or check out our code-base (goo.gl/cSpWmp). 3 DATA Every module in the NELA toolkit is based on real news data. To create a general news data set, we first gather a wide variety of sources using multiple lexicons (opensources.co, Wikipedia) and studies [2]. These news sources include: mainstream sources, satire sources, maliciously false sources, political blogs, and some rela- tively unknown sources. Each news source’s website or RSS feed is scraped twice a day, everyday, between April 2017 and October 2017, totalling in 92 sources and 136K articles. To control for topic, we only collect news from politics pages and feeds. The complete list of sources currently in the data set can be found on the NELA toolkit website. From this general news data set, two subsets are created to build a reliability labeled news data set and a bias labeled news data set. Specifically, we use OpenSources (www.opensources.co/), an expert-curated news source lexicon, to create 4 groups of sources: reliable news, unreliable news, biased news, and unbiased news (Table 2). Opensources has 12 different tags: fake, satire, extreme bias, conspiracy, rumor, state, junk science, hate speech, clickbait, unreliable, political, and reliable. We use the fake and conspiracy tags to create our unreliable group and the bias and political tags to create our biased group. The articles from each labeled source are used in training and testing the two machine learning models, discussed in Sections 4.1 and 4.2. It is important to note this ground truth is a previous behavior- based ground truth rather than a correctness-based ground truth. In other words, if a news source has been found to publish many fake articles in the past, they are an unreliable source, or if a news source has been found to be hyper-partisan many times in the past, they are a biased source. We choose this method for two primary reasons: (1) reliability and bias can be labeled quickly over time, allowing for our tool to be retrained as the news changes. Currently, fact-checking (or biased-checking) articles is a very slow and selec- tive process. Hence, fact-checked data for algorithm training can be very small and time specific, making trained classifiers difficult to maintain over time. (2) We can reasonably classify fake articles using this method. Explicitly, on a small fact-checked, correctness labeled test set (of 100 articles), the reliability labeled classifier per- forms well in detecting fake news as unreliable and real news as reliable (with 90% accuracy). However, our predictions are built to predict the type of source a news article is coming from, not the specific nature of the claims in an article. This notion is further discussed in Section 4. This data will continue to be collected for use in the toolkit and its later release. 4 MODULES IN THE NELA TOOLKIT In this section, we will briefly discuss the basic research behind each module in the NELA Toolkit. 4.1 Reliability prediction The first module predicts the reliability of a user-selected news article. Given a url, the tool scrapes the title and body content from the web page. After the news article is scraped, it is passed through a feature computation pipeline, which computes a large set of content-based features. These features primarily come from [ 2 , 8 ], but are also influence by other studies on persuasion [ 7]. Due toTrack: DemonstrationWWW 2018, April 23-27, 2018, Lyon, France236 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0 True Positive Rate NECO17+CIKM16 (area = 0.89) NECO17 (area = 0.89) CIKM16 (area = 0.76) POS (area = 0.69)0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0 True Positive Rate NECO17+CIKM16 (area = 0.92) NECO17 (area = 0.91) CIKM16 (area = 0.80) POS (area = 0.80)Reliable vs. Unreliable Unbiased vs. Hyper-partisan Table 1: ROC curves for each feature set using a Random Forest machine learning model, where NECO17 is from [2], CIKM16 is from [8], and POS is a standard Part-Of-Speech feature set. space restrictions, descriptions of these features can be found on the NELA Toolkit website. After features are computed, they are passed through a feature selection module, which selects the best features for the reliability prediction based on a previously com- puted variance analysis. Once feature selection is done, the single feature vector, representing the user-selected article, is passed to our machine learning model. The reliability model is a Random Forest classifier trained on news sources labeled by previous be- havior, discussed in section 3. To make the ground truth stronger, we also require news sources in the unreliable category to have published more than 1 completely false article according to online fact checkers (eg. snopes.com, politifact.com, etc.). In the current implementation, we trained the classifier on 4504 articles and tested it on 1130 articles, achieving 0.89 ROC AUC (refer to Table 1). The final output of the classifier is a probability of being reliable rather than a strict binary classification. To do this, we use the mean predicted class probabilities from the trees in the forest. This probability is then colored based on the strength of the prediction (where green is strongly reliable, red is strongly not reliable, and yellow is an edge case). This design choice allows for some notion of certainty or uncertainty in the algorithms predictions. News is inherently not a two-class problem, rather a spectrum between the two-classes; hence, it is important to show the user when a data point is near the edge of the decision boundary. Each result is entered into a sort-able and filterable table to allow for batch article analysis. For example, if an analyst is given a large number of news articles to assess, they can use the NELA Toolkit to quickly filter down to the most interesting articles. 4.2 Bias and subjectivity prediction The next module is made up of two independent classifiers: (1) a Random Forest classifier trained on content-based features to pre- dict hyper-partisan articles, (2) a Naive Bayes classifier trained on objective and subjective labeled sentences. Just as in the reliability module (Section 4.1), a user provides a url, and the title and body content is scraped from the web page. The content is then passed through both feature computation and model-specific feature selec- tion pipelines. The first classifier in this module is very similar to our reliability module, only differing in the data and features selected. The features are based on several studies on news and political bias in text [ 2, 9 ] and the labeled data is discussed in Section 3. The sources are balanced between politically right and politically left hyper-partisan sources. In the current implementation, we trained the classifier on 6158 articles and tested it on 1539 articles, achieving 0.92 ROC AUC (refer to Table 1). The final output from this classifier is a probability of an article being classified as impartial. The second classifier in this module is more generic than the previous, focusing on sentence level objectivity. Specifically, the classifier will provide a probability of being objective for both the title and body of the news article independently. The separation of title and body allows for a finer-grain analysis of title dynamics. This classifier is built using a Naive Bayes model that is trained on 10K sentences from Pang and Lee 2004 [ 6 ], and it achieves a 92% 5-fold cross-validation accuracy. The final outputs of this classifier are the probability of being objective for both the title and body text. The results from both classifiers are also added to the sort-able and filterable table for quick batch analysis. Reliable/Unbiased sources Unreliable sources Hyper-partisan sources Associated Press Infowars Brietbart PBS Liberty News Young Cons NPR Natural News RedState CBS Alt Media Syndi- cate The Blaze USA Today DC Clothesline CNS BBC Newslo Bipartisan Report New York Times Ending the Fed Occupy Democrats The Guardian Daily Buzz Live Daily Kos Intellihub Shareblue Freedom Daily Politicus USA Table 2: Sources used in each category 4.3 Community interest prediction Our next module is built to predict which online groups are in- terested in an article using news communities on reddit.com. To build this module, we first collect recent posts from 4 news com- munities (r/new_right, r/esist, and r/conspiracy). Once these posts are collected, we extract the top 25% of posts by their rank- ing score (roughly upvotes minus downvotes). These posts can be considered the most popular or most widely accepted by the community during the time slice collected. The news article in each post is scraped and content-based features are computed [ 3]. We compare r/news (a general interest community) to the other three subreddits (specific interest communities). Specifically, us- ing these features, we train 3 binary classifiers to predict articles as r/news interest or (r/new_right, r/esist, r/conspiracy) interest. Each classification is shown as a probability, similar to the other modules in the toolkit. In the current implementation, we trained each classifier on 2000 articles and tested each on 500 articles, achieving 0.77 ROC AUC on average. These community interest models are in a very early stage of development. Currently these models are based solely on newsTrack: DemonstrationWWW 2018, April 23-27, 2018, Lyon, France237 content features, but could be significantly improved with topic, source, or community-specific features. In addition, more in-depth feature analysis can provide insights into community differences and similarities. For example, it may be that highly emotional or subjective articles are popular in both r/new_right and r/esist, but the articles differ in slant (due to selection bias, framing bias, etc.). Automatic methods to capture these various types of bias in a general news setting could significantly improve our accuracy. We leave these improvements to future work. 4.4 Feature-based source visualizations Our last module analyzes the news at a source-level granularity, rather than an article-level granularity. Using our data set (refer to Section 3), we computed 260 content-based features [ 1, 2, 8 , 9, 11 ] on each article. Users can pick a set of news sources, a time frame, and 2 to 4 features to visualize on a 2-dimensional plane. This visualization provides a quick and easy comparison of individual sources or clusters of sources. Further, we provide meta data for each source, which can be accessed by clicking on a source bubble in the visualization. The meta data includes: (1) Percentage of articles that were predicted as reliable using our reliability model (2) Percentage of articles that were predicted as impartial using our bias model (3) Top phrases for each month using Autophrase [10] (4) The year the source was founded and the country of origin, if known (5) Facebook shares, reactions, and comments over time As data is collected, this module will be updated to reflect the current predictions and articles from each source, allowing for users to explore changes in sources over time.","Horne, B. D., Dron, W., Khedr, S., & Adali, S. (2018, April). Assessing the news landscape: A multi-module toolkit for evaluating the credibility of news. In Companion Proceedings of the The Web Conference 2018 (pp. 235-238)."
"SSAFC_067","Progress Toward  the Holy Grail ” : The Continued Quest to Automate Fact-Checking","Two years ago at this conference we issued what we said was a call to arms” to advance automated fact-checking [1]. We said the Holy Grail” was a completely automated fact-checking platform that can detect a claim as it appears in real time, and instantly provide the voter with a rating about its accuracy.” We acknowledged that goal may remain far beyond our reach for many, many years to come,” but we called on the journalism and computer science communities to redouble their efforts to make progress. Since then there has been remarkable progress and the Holy Grail” is no longer a distant dream. Although computer scientists and journalists still have significant hurdles to overcome, recent advances with the creation of a global database of structured factchecks and fact-checking tools such as ClaimBuster and iCheck have laid a groundwork for additional advances in the next few years. As we noted in our 2015 paper, fact-checking is a growing form of journalism. According to the Duke Reporters’ Lab, the number of fact-checkers has nearly doubled from 64 in 2015 to 126 today. Fact-checking is uniquely suited for automated journalism because the individual articles have value weeks after they have been published because of the tendency of government officials to repeat political claims. During live events such as debates and speeches, fact-checking organizations have typically relied on editors and reporters to manually match new statements with previously published fact-checks. But in a fully automated system, the statements could be detected and, if they had been previously fact-checked, a link or summary of the conclusion could pop up in real time for the reader. A promising development in this effort is the creation of a schema to identify fact-checking articles. This project, which is led by members of our team from Google and Duke University, has created a global open standard known as ClaimReview so that organizations can identify the people and statements they are checking, as well as their conclusion about the accuracy of claims. Google and Bing are now using the schema for search results. Publishers can generate the schema from their content management systems or use the Share the Facts” widget","Computer Science","Proceeding","2017","Y","N","Prototype","Support","Claim detection","28","Two years ago at this conference we issued what we said was a call to arms” to advance automated fact-checking [1]. We said the Holy Grail” was a completely automated fact-checking platform that can detect a claim as it appears in real time, and instantly provide the voter with a rating about its accuracy.” We acknowledged that goal may remain far beyond our reach for many, many years to come,” but we called on the journalism and computer science communities to redouble their efforts to make progress. Since then there has been remarkable progress and the Holy Grail” is no longer a distant dream. Although computer scientists and journalists still have significant hurdles to overcome, recent advances with the creation of a global database of structured fact- checks and fact-checking tools such as ClaimBuster 1 and iCheck 2 have laid a groundwork for additional advances in the next few years. As we noted in our 2015 paper, fact-checking is a growing form of journalism. According to the Duke Reporters’ Lab, 3 the number of fact-checkers has nearly doubled from 64 in 2015 to 126 today. Fact-checking is uniquely suited for automated journalism because the individual articles have value weeks after they have been published because of the tendency of government officials to repeat political claims. During live events such as debates and speeches, fact-checking organizations have typically relied on editors and reporters to manually match new statements with previously published fact-checks. But in a fully automated system, the statements could be detected and, if they had been previously fact-checked, a link or summary of the conclusion could pop up in real time for the reader. A promising development in this effort is the creation of a schema to identify fact-checking articles. This project, which is led by members of our team from Google and Duke University, has created a global open standard known as ClaimReview so that organizations can identify the people and statements they are checking, as well as their conclusion about the accuracy of claims. Google and Bing are now using the schema for search results. Publishers can generate the schema from their content management systems or use the Share the Facts” widget developed by the Duke Reporters’ Lab. The schema provides a consistent way for search engines to identify and index fact- checking articles. It also solves a problem that has bedeviled anyone who has tried to develop apps: the mishmash of ways that different publishers present their fact-checks. The database of fact-checks identified by the schema creates tremendous potential for automation projects because it could potentially include every fact-check article published around the world. Another promising area for automation is to assist journalists with repetitive and time-consuming tasks such as identifying factual claims. Every day fact-checkers and their college interns have difficulty keeping up with the flood of new factual claims from legislative debates, TV talk shows and other news coverage. ClaimBuster, a tool developed by our team at the University of Texas at Arlington, addresses this need by automating the process of finding factual claims to check. ClaimBuster can do the work of many college interns by quickly analyzing voluminous transcripts and identifying claims that journalists are most interested in checking. In the past two years, we have refined ClaimBuster and have begun to deploy it for daily use by journalists. In Australia, it is used for daily analysis of Hansard, the proceedings of the Australian parliament. 4 In the United States, we are using ClaimBuster to analyze the transcript of a cable news channel and identify the most check-worthy” claims. Once a check-worthy claim is identified, we look for ways to help journalists check it. There are many possibilities for automation, and one focus of our team at Duke University, Google, and the University of Texas at Arlington is checking claims based on data or statistics. These claims are often vague and may be factually correct, but they can still mislead by cherry-picking” partial and biased vantage points of the data. We have developed a tool for perturbation analysis,” which puts the claim into a larger context by automatically exploring a large number of alternative vantage points of the data, in order to evaluate claim qualities such as fairness, robustness, and uniqueness in a principled manner. As a proof of concept, we have developed a website called iCheck and released it to the public in September 2016. The website analyzes the voting records of the U.S. Congress from January 2009 to September 2016, and lets visitors compare how legislators vote with party majorities and the president, and more importantly, explore how the comparison stacks up under Computation + Journalism Symposium, October 2017, Evanston, Illinois USA B. Adair et al. 2 different contexts—over time, among groups of peers, and for key votes” identified by lobbying/political organizations. While the Holy Grail” of fully automated fact-checking still poses significant challenges—some requiring more research and investments over a long term—we believe that some aspects of automated fact-checking are ready for prime time and can deliver substantial benefit to the journalists and the public. In the rest of the paper, we describe our progress, discuss lessons learned, and outline our vision of next steps. 2 ClaimBuster Since December 2014, the team at the University of Texas at Arlington has been building ClaimBuster [2-5], a claim-spotting tool for assisting fact-checkers in discovering factual claims that are worth checking. ClaimBuster monitors the plethora of places where politicians and others make political claims such as interviews, speeches and debates. It gives each sentence a score that indicates how likely it is the sentence contains an important factual claim that should be checked. In this way, ClaimBuster provides a priority ranking on the sentences. The ranking helps fact-checkers avoid having to read massive transcripts and efficiently focus on the top-ranked claims. ClaimBuster’s claim spotter was tested in real-time during the live coverage of all primary election and general election debates for the 2016 U.S. presidential election. Closed captions of the debates on live TV broadcasts, captured by a decoding device, were fed to ClaimBuster, which immediately scored each sentence spoken by the candidates and posted top-scored claims to the project’s website and Twitter account (@ClaimBusterTM). Post- hoc analysis of the claims checked by professional fact-checkers at CNN, PolitiFact.com and FactCheck.org reveals a highly positive correlation between ClaimBuster and journalism organizations in deciding which claims to check. ClaimBuster has also been continuously monitoring Twitter and retweeting the check-worthy factual claims it finds in people’s tweets (see @ClaimBusterTM). Our experience so far suggests a few directions for improving ClaimBuster's accuracy in spotting important factual claims. Currently, the tool scores individual sentences. This is a clear limitation as factual claims may span multiple sentences. Mitigating this limitation entails several natural language processing tasks, including coreference resolution and topic segmentation. Furthermore, structured representation of factual claims is imperative for deep understanding of the claims and thus more accurate spotting of important claims. Such structured representation should capture various aspects of a factual claim, including the domain and topic of the claim, the template of the fact being expressed, the involved entities, and their relationships. It is also crucial to capture the claim’s important elements such as numbers, time points and intervals, comparisons, grouping, and aggregates. ClaimBuster delivers the scores on claims through a variety of channels, including its website, Twitter account, API, and Slackbot. Particularly, the Slackbot allows users to supply their own text, directly as Slack input or through text files in a Dropbox folder, and to receive the claim spotter scores for the sentences in that piece of text. The Slackbot has been published in the public Slack App directory and can be installed from Furthermore, a public ClaimBuster API 5 enables developers to create their own fact- checking applications using ClaimBuster as an underlying service. As part of the team’s next step toward the Holy Grail”, we are extending ClaimBuster into an end-to-end fact-checking assistant for professional fact-checkers. A preliminary version of this extension already produces true-or-false verdicts for certain types of factual claims. Given a factual claim which is scored highly by the aforementioned claim spotting component, ClaimBuster may reach a verdict by a few methods. Particularly, one of the methods is to translate the factual claim into questions and their accompanying answers. It then sends the questions to question- answering systems and compares the returned results with the aforementioned answers. It produces a verdict based on the presence/absence of a discrepancy between these two sets of answers. 3 iCheck We demonstrated iCheck at the 2016 Computation+Journalism Symposium; please see our paper [6] and website for additional details. Here, we focus on summarizing the challenges we identified during this project. As explained earlier, we target number-based claims derived from data or statistics, and we have identified perturbation analysis as a way to formulate the human fact-checking process as a computational problem. Automated perturbation analysis [7] can quickly examine a huge number of different vantage points of data, quantitatively assess various aspects of claim quality, and intelligently suggest counterarguments,” thereby relieving human fact-checkers from the tedious, time-consuming, and error-prone aspects of manual fact-checking. While perturbation analysis has proven to be a remarkable fit for number-based claims, it is by no means a one-size-fits-all solution for all types of claims. For example, checking an assertion that some event occurred or somebody took a particular position on an issue would require different procedures that need to be automated differently. Given the diversity of domains and types of claims, it seems improbable for a single computational approach to be universally effective at automated fact-checking. The process of readying iCheck to the public also taught us valuable lessons. Data extraction, cleaning, and linking took huge amounts of effort. Although we have been blessed with high- quality open-source APIs for the U.S. Congress (we relied heavily on GovTrack.us), a lot of work remained to get other related data for iCheck, such as lists of key votes from various lobbying/political organizations, and properly link them to the congressional voting records. These organizations publish their Progress Toward the Holy Grail”: The Continued Quest to Automate Fact-Checking Computation + Journalism Symposium, October 2017, Evanston, Illinois USA 3 information in different formats and refer to key votes in different ways. References are often incomplete or ambiguous—especially when many roll calls may be associated with the same bill—and linking is further complicated by occasional typos in the data source. While automated data extraction and cleaning techniques have come a long way, they still cannot achieve the accuracy desired for fact-checking. Some of the errors and ambiguities we encountered could only be resolved by input from human experts with intimate knowledge of the Congress. Improving accuracy for specific domains—without a lot of data, let alone expert-labeled data—remains a challenge. Making iCheck user-friendly also required enormous effort. iCheck provides visualization, exploration, and recommendation features, but to make them useful and accurate, a very high degree of customization was necessary. Accuracy is difficult to achieve because the reality always manages to come up with exceptions to assumptions made by analysis and implementation. For example, legislators can switch party affiliations and voting rights of delegates change over time, complicating even simple accounting queries. Recommendation algorithms—for example, for suggesting related claims that are surprising” or best counter” the one being checked—also required lots of expert input and extensive tuning by our developers. Looking back, we ask ourselves whether all the development effort was worthwhile for an application in a specific domain. iCheck was made public in September 2016. While there were some claims during the 2016 elections that perfectly fit iCheck, the bulk of the check-worthy claims in that season turned out to have nothing to do with congressional voting records. As we probably do not have the luxury of developing a system like iCheck from scratch for every single domain, the key question is whether and how we can develop a more general system or a set of tools that work across multiple domains. Since iCheck, our team has been exploring ways to build more general tools that can work with more types of claims and additional domains, while striking some balance between generality and user-friendliness. Recognizing data quality issues, we are also actively doing research on fact-checking in the presence of uncertain data, and developing techniques for prioritizing data cleaning efforts under resource constraints. Overall, it has been a humbling experience for the computer scientists involved in the iCheck project to see the wide gamut of knowledge, skills, and efforts required of human fact-checkers and journalists. Plenty of interesting challenges remain in making automated fact-checking more general and more cost-effective. 4 The ClaimReview Schema and Share the Facts The ClaimReview schema was developed by Jigsaw, a subsidiary of Google, and the Duke Reporters’ Lab in an open process with schema.org. The markup is embedded in articles, providing a consistent way for fact-checkers to identify key elements such as the person or group being checked, the statement and the rating or conclusion. Publishers can use their own content management systems to embed the markup or can use Share the Facts, a free service of the Reporters’ Lab. In addition to the markup, Share the Facts also renders a widget” that can be inserted in an article providing a visual summary. Figure 1: The Share the Facts widget provides a visual summary of a fact-checking article as well as embedding the ClaimReview schema. The widget can be shared on social media and embedded in articles and blog posts like tweets.6 Use of the schema and widget is growing. Approximately 20 fact-checkers around the world are using the ClaimReview schema and an additional 11 are using the Share the Facts widget as of July 2017. Many others have said they plan to adopt one of the two methods in the next six months. Over the past year, Google has announced a series of product features that leverage the ClaimReview schema to surface and highlight fact-checking articles in Google News and search results. In October 2016, the company began identifying articles that contain the markup with a FACT CHECK” tag. 7 Figure 2: In October 2016, Google News began identifying fact-check articles with a unique tag. Computation + Journalism Symposium, October 2017, Evanston, Illinois USA B. Adair et al. 4 In April 2017, Google began highlighting fact-check articles in search results. To distinguish them from other types of content, they were displayed with enriched textual snippets that concisely summarize the findings.8 Figure 3: Google uses the ClaimReview markup to display the fact-check as a rich text snippet in search results. Most recently, in June 2017, Google started presenting a daily collection of fact-checking articles on the desktop homepage of Google News as part of the News Desktop redesign. 9 In July 2017, Bing, the Microsoft search engine, published information for publishers about how to use ClaimReview that said the markup will be used for enhanced captions” in search results.10 An additional benefit of the markup is that structured summaries of the fact-checks can now be collected in a database that provides content for future applications. It is now possible to easily tap into the complete archive of articles by the world’s fact- checkers. 5 Live Pop-Up Fact-Checking We have made some early progress toward the goal of live pop-up fact-checking. When the television networks decided in the 2016 campaign that they were not going to do live fact-checking of presidential debates, the Duke Reporters’ Lab developed FactPopUp, a Chrome browser extension. FactPopUp is a manual tool to present short summaries of fact- checks on top of live video. It uses Twitter and Chrome’s notification feature to display text and images. A fact-checker – for the tests, it was PolitiFact editor Aaron Sharockman – listens to the event and sends a tweet when one of the speakers makes a factual claim that has been previously fact-checked. That triggers a box that pops up on the browser. We conducted beta tests of FactPopUp with mixed results. Our first test, during the final presidential debate of 2016, provided timely fact-checks after the candidates made factual claims. The web video of the debate was delayed about 15 seconds from the live” event on television, which provided sufficient time for the PolitiFact editor to find the relevant fact-check that had been previously published and then tweet it so the pop-up appeared just a few moments after the candidate said the claim on the web video. During the debate, FactPopUp provided about 10-12 high- quality notifications about fact-checks on the candidates. FactPopUp was less successful during the inaugural speech of President Donald Trump. There were only a few factual claims, and the live feed being used for the event happened to be running about 45 seconds behind live television. As a result, the PolitiFact editor triggered the pop-ups when he heard them on live TV, but on the web video, they appeared before Trump actually said the statement. Currently, we are working on the next generation of pop-up fact-checking where the task of matching previously done fact- checks is automated, allowing the public to benefit from the database of fact-checks collected through the ClaimReview schema on a much bigger scale. A user could ask our app to monitor a web page or a video or audio stream for matching claims, or search the database via text or voice. Going beyond keyword searches, our back-end system could make use of any additional contextual signals provided by the app, such as the stream URL being monitored and time into the stream, to improve matching quality. For some streams, the back-end system can obtain additional information useful to matching, such as full-text transcripts and annotations by human experts. Besides leveraging our growing database of fact-checks, our system also syncs with ClaimBuster. Search requests for specific claims indicate users find them check-worthy, and logs of such requests can be used by ClaimBuster as training data to improve its claim identification algorithms. Popularities of claims by request also serve an additional criterion with which human fact- checkers can decide what to check next. Our apps will allow users to subscribe to claims with no existing fact-checks or streams with ongoing fact-checking activities; users are notified as soon as new fact-checks become available. The app can also continue to track published fact- checks, and send any updates and corrections to users. 6 The Challenges and Prospects for Fully Automated Fact-Checking In the past two years we have made significant progress toward the Holy Grail.” ClaimBuster, iCheck and the ClaimReview schema have provided valuable lessons about the next steps toward fully automated fact-checking. The ClaimReview schema provides one key element for the Holy Grail”—a growing database of fact-checks organized as structured data and amenable to automated searching and matching. As we get buy-ins from major technology companies like Google, we are close to offering this part of the Holy Grail” to the public, maximizing the impact of the hard work that human fact-checkers have created. In the near future, we see this direction as one where we can make the most practical gain with the current technology, and our Share the Facts widget and development of a better pop-up” fact-checking app are important first steps. In the longer term, more study is needed on novel mechanisms for introducing the results of fact-checks to Progress Toward the Holy Grail”: The Continued Quest to Automate Fact-Checking Computation + Journalism Symposium, October 2017, Evanston, Illinois USA individuals—especially those with strong prior beliefs who may be less receptive to the results. Regardless of the final delivery mechanism—and there may be many alternatives—the system infrastructure that we are building will serve as a solid foundation. Besides disseminating the results of fact-checks, our work helping journalists produce more fact-checks remains a challenge, but we continue to make good progress and gain new insights. General, end-to-end automated systems are difficult. However, some steps of fact-checking are more amenable to general automation solutions than others. ClaimBuster has identified one sweet spot where AI can help learn what is check-worthy effectively. On the other hand, our experience with iCheck seems to indicate that checking a non-trivial statement automatically starting from just data still requires considerable work. With enough effort, we can probably achieve end-to-end automated fact-checking in specific domains for specific kinds of claims, but generalizing the success to other domains and claim types in a cost-effective manner remains challenging and would require long-term investment in collaborative research between journalists and computer scientists. To continue our research, we have formed the Tech & Check Cooperative, a team that includes our researchers from Duke University, the University of Texas at Arlington and Google, as well as new partners from the Internet Archive and California Polytechnic State University. The Tech & Check Cooperative has received a grant from the Knight Foundation to continue this important research. In addition to developing apps for live fact- checking and expanding the use of ClaimBuster, the team will communicate with other researchers doing work in this area.","Adair, B., Li, C., Yang, J., & Yu, C. (2017). Progress toward “the holy grail”: The continued quest to automate fact-checking. In Computation+ Journalism Symposium,(September)."
